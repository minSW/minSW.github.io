<!DOCTYPE html><html lang="ko"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> &#039;Spark The Definitive Guide&#039; 3장 - 일단 좀 더 잡솨봐 (PART 1 끝) · Look out</title><meta name="description" content="&amp;#039;Spark The Definitive Guide&amp;#039; 3장 - 일단 좀 더 잡솨봐 (PART 1 끝) - Lukka Min"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/cover.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"><link rel="search" type="application/opensearchdescription+xml" href="https://minsw.github.io/atom.xml" title="Look out"><script src="//code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script><meta name="generator" content="Hexo 5.3.0"><link rel="alternate" href="/feed.xml" title="Look out" type="application/atom+xml">
</head><body><div class="wrap"><header><a class="logo-link" href="/"><img src="/cover.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a class="nav-list-link" href="/" target="_self">BLOG</a></li><li class="nav-list-item"><a class="nav-list-link" href="/archives/" target="_self">ARCHIVE</a></li><li class="nav-list-item"><a class="nav-list-link" href="/tags/" target="_self">TAG</a></li><li class="nav-list-item"><a class="nav-list-link" href="https://github.com/minSW" target="_blank">GITHUB</a></li></ul></header><main class="container"></main><div class="post"><article class="post-block"><h1 class="post-title">&#039;Spark The Definitive Guide&#039; 3장 - 일단 좀 더 잡솨봐 (PART 1 끝)</h1><div class="post-info">2021년 1월 24일<span class="categories"><i class="fa fa-bookmark" aria-hidden="true"></i></span><a class="post-category" href="/categories/spark/">#spark</a></div><div class="post-content"><br/>

<h4 id="Part-1-END"><a href="#Part-1-END" class="headerlink" title="[Part 1] END"></a>[Part 1] END</h4><p>파트 1 까지 끝내고 나니 이제 조금씩 스파크 맛은 본 것 같은데, 이번 장은 따라가기 살짝 힘들었다.</p>
<blockquote>
<p>나 : 뭔 말이에요<br>?? : <em>‘XX 장에서 자세히 알아보겠습니다.’</em></p>
<p>나 : 이건 또 뭐여<br>?? : <em>‘이와 관련된 내용은 XX 부에서 자세히 알아보겠습니다.’</em></p>
</blockquote>
<p><img width="150" alt="angry" src="https://user-images.githubusercontent.com/26691216/105637781-85fbe680-5eb2-11eb-8956-b7de17f122de.png
"></p>
<center><i style="color:lightgray">이toRL들이..</i></center>



<p>오늘은 예제 위주로 모르는 부분 찾아가면서 어찌 저찌 이해는 했지만<br>그 다음 파트는 다시 또 <code>구조적 API</code>라 오늘 본 거 대부분은 한참 뒤에야 다시 보게 될텐데..<br>아 이거 무조건인데.. 백퍼 다 까먹는데.. 🤦🏻‍♀️  </p>
<center><h2>_ _ _</h2></center>

<br/>

<hr>
<h1 id="CHAPTER-3-스파크-기능-둘러보기"><a href="#CHAPTER-3-스파크-기능-둘러보기" class="headerlink" title="CHAPTER 3 스파크 기능 둘러보기"></a>CHAPTER 3 스파크 기능 둘러보기</h1><blockquote>
<p>스파크 = 기본 요소 (저수준 API + 구조적 API) + 추가 기능 (일련의 표준 라이브러리)</p>
<ul>
<li>구조적 스트리밍, 고급 분석, 라이브러리 및 에코시스템</li>
<li>구조적 API : Dataset, DataFrame, SQL</li>
<li>저수준 API : RDD, 분산형 변수</li>
</ul>
</blockquote>
<p>CHAPTER 2 는 구조적 API의 핵심개념을 소개했다면<br>CHAPTER 3 은 나머지 API 와 주요 라이브러리, 스파크의 다양한 기능 소개</p>
<h3 id="3-1-운영용-애플리케이션-실행하기"><a href="#3-1-운영용-애플리케이션-실행하기" class="headerlink" title="3.1 운영용 애플리케이션 실행하기"></a>3.1 운영용 애플리케이션 실행하기</h3><details><summary class="point-color-can-hover">[3.1] 예제 펼치기</summary>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /spark-2.4.7-bin-hadoop2.7</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># scalar example</span></span><br><span class="line">$ ./bin/spark-submit --class org.apache.spark.examples.SparkPi --master <span class="built_in">local</span> ./examples/jars/spark-examples_2.11-2.4.7.jar 10</span><br><span class="line">...</span><br><span class="line">21/01/24 13:41:15 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 0.968079 s</span><br><span class="line">Pi is roughly 3.1414071414071416 <span class="comment"># 돌릴때마다 다르게 나온다</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># python example</span></span><br><span class="line">$ ./bin/spark-submit --master <span class="built_in">local</span> ./examples/src/main/python/pi.py 10</span><br><span class="line">Pi is roughly 3.139084 <span class="comment"># 이럴거면 args 는 대체 왜 넣으란걸까</span></span><br></pre></td></tr></table></figure>
</details>

<ul>
<li><code>spark-submit</code> 명령<ul>
<li>애플리케이션 코드를 클러스터에 전송해 실행시키는 역할</li>
<li>대화형 쉘에서 개발한 프로그램 -&gt; 운영용 애플리케이션으로 전환 가능</li>
<li>스파크 애플리케이션은 standalone, Mesos, YARN 클러스터 매니저를 이용해 실행됨 (<code>--master</code> 옵션)</li>
</ul>
</li>
</ul>
<h3 id="3-2-Dataset-타입-안정성을-제공하는-구조적-API"><a href="#3-2-Dataset-타입-안정성을-제공하는-구조적-API" class="headerlink" title="3.2 Dataset : 타입 안정성을 제공하는 구조적 API"></a>3.2 Dataset : 타입 안정성을 제공하는 구조적 API</h3><details><summary class="point-color-can-hover">[3.2] 예제 펼치기</summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Flight</span>(<span class="params"><span class="type">DEST_COUNTRY_NAME</span>: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                  <span class="type">ORIGIN_COUNTRY_NAME</span>: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                  count: <span class="type">BigInt</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">flightsDF</span> </span>= spark.read.parquet(<span class="string">&quot;/data/flight-data/parquet/2010-summary.parquet/&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> flights = flightsDF.as[<span class="type">Flight</span>] <span class="comment">// DataFrame -&gt; Dataset[Flight]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(flights</span><br><span class="line">  .filter(flight_row =&gt; flight_row.<span class="type">ORIGIN_COUNTRY_NAME</span> != <span class="string">&quot;Canada&quot;</span>)</span><br><span class="line">  .map(flight_row =&gt; flight_row)</span><br><span class="line">  .take(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">(flights</span><br><span class="line">  .take(<span class="number">5</span>)</span><br><span class="line">  .filter(flight_row =&gt; flight_row.<span class="type">ORIGIN_COUNTRY_NAME</span> != <span class="string">&quot;Canada&quot;</span>)</span><br><span class="line">  .map(fr =&gt; <span class="type">Flight</span>(fr.<span class="type">DEST_COUNTRY_NAME</span>, fr.<span class="type">ORIGIN_COUNTRY_NAME</span>, fr.count + <span class="number">5</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment">// res45: Array[Flight] = Array(Flight(United States,Romania,1), Flight(United States,Ireland,264), Flight(United States,India,69), Flight(Egypt,United States,24), Flight(Equatorial Guinea,United States,1))</span></span><br><span class="line"><span class="comment">// res46: Array[Flight] = Array(Flight(United States,Romania,6), Flight(United States,Ireland,269), Flight(United States,India,74), Flight(Egypt,United States,29), Flight(Equatorial Guinea,United States,6))</span></span><br></pre></td></tr></table></figure>
</details>

<ul>
<li><strong>Dataset</strong> : Java와 Scala의 정적 데이터 타입에 맞는 코드(statically typed code)를 지원하기 위한 스파크의 구조적 API<ul>
<li>Python, R 사용 X</li>
</ul>
</li>
<li>Dataset API 는 <strong>DataFrame 레코드 =&gt; Java나 Scala로 정의한 클래스에 할당</strong>, Collection 으로 다룰 수 있는 기능 등을 제공<ul>
<li>DataFrame : 다양한 데이터 타입의 테이블형 데이터를 보관할 수 있는 Row 타입 객체로 구성된 분산 컬렉션 (<a href="https://minsw.github.io/2021/01/24/Spark-The-Definitive-Guide-2%EC%9E%A5/#2-6-DataFrame">2장 참고</a>)</li>
<li><strong>타입 안정성을 지원</strong> 하므로 초기화에 사용한 클래스 외 다른 클래스를 사용한 접근은 X</li>
<li>여러 명이 개발하고 잘 정의된 인터페이스로 상호작용하는 대규모 애플리케이션 개발에 유용<br><del>잘 정의된 인터페이스 부터가 실패다 이말이야</del></li>
</ul>
</li>
<li>Dataset 클래스 (Java <code>Dataset&lt;T&gt;</code>, Scala <code>Dataset[T]</code>)<ul>
<li>내부 객체 타입을 매개변수로 사용 (T) =&gt; 해당 클래스 객체만 가질 수 있음</li>
<li>스파크 2.0 에서는 자바의 JavaBean 패턴, 스칼라의 케이스 클래스 유형으로 정의된 클래스 지원</li>
<li>타입 T를 분석해서 Dataset 스키마를 생성해야하므로 타입을 제한할 수 밖에 없음</li>
</ul>
</li>
<li>장점<ul>
<li>필요한 경우 선택적으로 사용 가능하고, map, filter 등 함수 사용 가능</li>
<li>코드 변경 없이 타입 안정성을 보장할 수 있고, 안전하게 데이터 다루기 가능<ul>
<li><code>collect()</code> 나 <code>take()</code> 호출 시 DataFrame의 row 타입 객체가 아닌 Dataset의 지정된 타입(T)의 객체로 반환</li>
</ul>
</li>
</ul>
</li>
<li>Dataset의 자세한 내용은 CHAPTER 11 에서 이어서</li>
</ul>
<h3 id="3-3-구조적-스트리밍"><a href="#3-3-구조적-스트리밍" class="headerlink" title="3.3 구조적 스트리밍"></a>3.3 구조적 스트리밍</h3><details><summary class="point-color-can-hover">[3.3] 예제 펼치기 (정적 DataFrame 버전) </summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 정적 DataFrame 버전</span></span><br><span class="line"><span class="keyword">val</span> staticDataFrame = (spark.read.format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;inferSchema&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">  .load(<span class="string">&quot;/data/retail-data/by-day/*.csv&quot;</span>))</span><br><span class="line"></span><br><span class="line">staticDataFrame.createOrReplaceTempView(<span class="string">&quot;retail_data&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> staticSchema = staticDataFrame.schema</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;window, column, desc, col&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// &#x27;특정 고객(CustomerId)가 대량으로 구매하는 영업 시간&#x27; 구하기</span></span><br><span class="line">(staticDataFrame</span><br><span class="line">  .selectExpr(</span><br><span class="line">    <span class="string">&quot;CustomerId&quot;</span>,</span><br><span class="line">    <span class="string">&quot;(UnitPrice * Quantity) as total_cost&quot;</span>,</span><br><span class="line">    <span class="string">&quot;InvoiceDate&quot;</span>)</span><br><span class="line">  .groupBy(</span><br><span class="line">    col(<span class="string">&quot;CustomerId&quot;</span>), window(col(<span class="string">&quot;InvoiceDate&quot;</span>), <span class="string">&quot;1 day&quot;</span>)) <span class="comment">// 관련 날짜 데이터 그룹화 &amp; 집계</span></span><br><span class="line">  .sum(<span class="string">&quot;total_cost&quot;</span>)</span><br><span class="line">  .show(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// +----------+--------------------+------------------+</span></span><br><span class="line"><span class="comment">// |CustomerId|              window|   sum(total_cost)|</span></span><br><span class="line"><span class="comment">// +----------+--------------------+------------------+</span></span><br><span class="line"><span class="comment">// |   14075.0|[2011-12-05 00:00...|316.78000000000003|</span></span><br><span class="line"><span class="comment">// |   18180.0|[2011-12-05 00:00...|            310.73|</span></span><br><span class="line"><span class="comment">// |   15358.0|[2011-12-05 00:00...| 830.0600000000003|</span></span><br><span class="line"><span class="comment">// |   15392.0|[2011-12-05 00:00...|304.40999999999997|</span></span><br><span class="line"><span class="comment">// |   15290.0|[2011-12-05 00:00...|263.02000000000004|</span></span><br><span class="line"><span class="comment">// +----------+--------------------+------------------+</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<blockquote>
<p>로컬 모드 사용 시 셔플 파티션 수 (default 200) 줄이기를 권장. <code>spark.conf.set(&quot;spark.sql.shuffle.partitions&quot;, &quot;5&quot;)</code></p>
</blockquote>
</details>

<br/>

<details><summary class="point-color-can-hover">[3.3] 예제 펼치기 (Streaming 버전) </summary>


<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Streaming 버전</span></span><br><span class="line"><span class="keyword">val</span> streamingDataFrame = (spark.readStream  <span class="comment">// read =&gt; readStream</span></span><br><span class="line">    .schema(staticSchema)</span><br><span class="line">    .option(<span class="string">&quot;maxFilesPerTrigger&quot;</span>, <span class="number">1</span>)  <span class="comment">// maxFilesPerTrigger (한번에 읽을 파일 수 설정) =&gt; 파일별로 트리거 수행</span></span><br><span class="line">    .format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">    .option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">    .load(<span class="string">&quot;/data/retail-data/by-day/*.csv&quot;</span>))</span><br><span class="line"></span><br><span class="line">streamingDataFrame.isStreaming <span class="comment">// returns true</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> purchaseByCustomerPerHour = (streamingDataFrame</span><br><span class="line">  .selectExpr(</span><br><span class="line">    <span class="string">&quot;CustomerId&quot;</span>,</span><br><span class="line">    <span class="string">&quot;(UnitPrice * Quantity) as total_cost&quot;</span>,</span><br><span class="line">    <span class="string">&quot;InvoiceDate&quot;</span>)</span><br><span class="line">  .groupBy(</span><br><span class="line">    $<span class="string">&quot;CustomerId&quot;</span>, window($<span class="string">&quot;InvoiceDate&quot;</span>, <span class="string">&quot;1 day&quot;</span>))</span><br><span class="line">  .sum(<span class="string">&quot;total_cost&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1) 스트림 시작 &amp; 인메모리 테이블에 저장</span></span><br><span class="line">(purchaseByCustomerPerHour.writeStream</span><br><span class="line">    .format(<span class="string">&quot;memory&quot;</span>) <span class="comment">// memory = 인메모리 테이블에 저장</span></span><br><span class="line">    .queryName(<span class="string">&quot;customer_purchases&quot;</span>) <span class="comment">// 인메모리에 저장될 테이블명</span></span><br><span class="line">    .outputMode(<span class="string">&quot;complete&quot;</span>) <span class="comment">// complete = 모든 카운트 수행 결과를 테이블에 저장</span></span><br><span class="line">    .start())</span><br><span class="line"></span><br><span class="line"><span class="comment">// 인메모리 테이블 확인 (데이터를 많이 읽으면 읽을수록 테이블 구성이 변경)</span></span><br><span class="line">(spark.sql(<span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">  SELECT *</span></span><br><span class="line"><span class="string">  FROM customer_purchases</span></span><br><span class="line"><span class="string">  ORDER BY `sum(total_cost)` DESC</span></span><br><span class="line"><span class="string">  &quot;</span><span class="string">&quot;&quot;</span>)</span><br><span class="line">  .show(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// +----------+--------------------+------------------+</span></span><br><span class="line"><span class="comment">// |CustomerId|              window|   sum(total_cost)|</span></span><br><span class="line"><span class="comment">// +----------+--------------------+------------------+</span></span><br><span class="line"><span class="comment">// |   17450.0|[2011-09-20 00:00...|          71601.44|</span></span><br><span class="line"><span class="comment">// |      null|[2011-11-14 00:00...|          55316.08|</span></span><br><span class="line"><span class="comment">// |      null|[2011-11-07 00:00...|          42939.17|</span></span><br><span class="line"><span class="comment">// |      null|[2011-03-29 00:00...| 33521.39999999998|</span></span><br><span class="line"><span class="comment">// |      null|[2011-12-08 00:00...|31975.590000000007|</span></span><br><span class="line"><span class="comment">// +----------+--------------------+------------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 2) 처리결과 콘솔에 출력</span></span><br><span class="line">(purchaseByCustomerPerHour.writeStream</span><br><span class="line">    .format(<span class="string">&quot;console&quot;</span>) <span class="comment">// console = 콘솔에 결과 출력</span></span><br><span class="line">    .queryName(<span class="string">&quot;customer_purchases_2&quot;</span>)</span><br><span class="line">    .outputMode(<span class="string">&quot;complete&quot;</span>)</span><br><span class="line">    .start())</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</details>

<ul>
<li><strong>구조적 스트리밍</strong> : 스트림 처리용 고수준 API<ul>
<li>구조적 API로 개발된 배치 모드 연산을 <strong>스트리밍 방식으로</strong> 실행 가능하며, 지연 시간을 줄이고 증분 처리 가능</li>
<li>즉 스트리밍 처리로 <u>빠르게 값을 얻을 수 있고</u>, 모든 작업에서 데이터를 <u>증분 처리</u>하면서 수행된다</li>
<li>배치 잡으로 프로토타입 개발 후에 스트리밍 잡으로 변환도 가능</li>
<li>스파크 2.2 버전부터 안정화 (production-ready)</li>
</ul>
</li>
<li>데이터를 그룹화하고 집계하는 방법 (시계열 time-series 데이터  처리)<ul>
<li><code>window()</code> : 집계 시에, 시계열 컬럼 기준으로 각 날짜에 대한 전체 데이터를 가지는 윈도우 구성 =&gt; 날짜, 타임스탬프 처리에 유용</li>
</ul>
</li>
<li>정적 DataFrame 코드 vs 스트리밍 코드<ul>
<li><code>read</code> vs <code>readStream</code></li>
<li> 일반적인 정적 액션 vs <strong>스트리밍 액션</strong></li>
<li>스트리밍 액션은 어딘가에 데이터를 채워넣어야함. <strong>트리거</strong>가 실행된 후 데이터를 갱신<ul>
<li>(인메모리 테이블에 저장 시 - 스파크는 이전 집계값보다 더 큰 값이 발생할 때만 인메모리 테이블 갱신)</li>
</ul>
</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="http://bit.ly/2PvOwCS">예제 retail 데이터 셋</a><ul>
<li>by-day 하루 치 데이터 사용</li>
<li>예제는 인메모리 테이블에 저장 / 파일마다 트리거 실행</li>
<li>예제의 두가지 방식 (메모리/콘솔 출력, 파일별 트리거 수행)은 운영 환경에서는 권장 X</li>
</ul>
</li>
<li>데이터 처리 시점이 아닌 이벤트 시간에 따라 윈도우를 구성하는 방식에 주목<ul>
<li>기존 스파크 스트리밍의 단점 =&gt; <strong>구조적 스트리밍으로 보완</strong> 가능</li>
<li>스트림 처리과정의 스키마 추론방법 및 구조적 스트리밍은 CHAPTER 5 에서 자세히</li>
</ul>
</li>
</ul>
<h3 id="3-4-머신러닝과-고급-분석"><a href="#3-4-머신러닝과-고급-분석" class="headerlink" title="3.4 머신러닝과 고급 분석"></a>3.4 머신러닝과 고급 분석</h3><details><summary class="point-color-can-hover">[3.4] 예제 펼치기 </summary>


<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># MLlib 머신러닝 알고리즘 : 수치형 데이터 필요</span></span><br><span class="line"><span class="comment"># 예제의 (정적) 데이터 =&gt; 수치형으로 변환</span></span><br><span class="line"></span><br><span class="line">staticDataFrame.printSchema()</span><br><span class="line"></span><br><span class="line">root</span><br><span class="line"> |-- InvoiceNo: string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- StockCode: string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- Description: string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- Quantity: <span class="built_in">integer</span> (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- InvoiceDate: timestamp (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- UnitPrice: double (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- CustomerID: double (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- Country: string (nullable = <span class="literal">true</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.date_format</span><br><span class="line"><span class="keyword">val</span> preppedDataFrame = (staticDataFrame</span><br><span class="line">  .na.fill(<span class="number">0</span>) <span class="comment">// 0인 경우 null로 채움</span></span><br><span class="line">  .withColumn(<span class="string">&quot;day_of_week&quot;</span>, date_format($<span class="string">&quot;InvoiceDate&quot;</span>, <span class="string">&quot;EEEE&quot;</span>)) <span class="comment">// Sunday, Monday, ..</span></span><br><span class="line">  .coalesce(<span class="number">5</span>)) <span class="comment">// 파티션 개수 줄임 (default, shuffle = false)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// (1) 데이터 =&gt; 학습 데이터셋, 테스트 데이터셋으로 분리 (2011-07-01 기준)</span></span><br><span class="line"><span class="keyword">val</span> trainDataFrame = (preppedDataFrame</span><br><span class="line">  .where(<span class="string">&quot;InvoiceDate &lt; &#x27;2011-07-01&#x27;&quot;</span>))</span><br><span class="line"><span class="keyword">val</span> testDataFrame = (preppedDataFrame</span><br><span class="line">  .where(<span class="string">&quot;InvoiceDate &gt;= &#x27;2011-07-01&#x27;&quot;</span>))</span><br><span class="line"></span><br><span class="line">trainDataFrame.count()</span><br><span class="line"><span class="comment">// res110: Long = 245903</span></span><br><span class="line">testDataFrame.count()   </span><br><span class="line"><span class="comment">// res111: Long = 296006</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// (2-1) 요일(Sunday, Monday,..)을 수치형(0,1, ..)으로 반환</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.feature.<span class="type">StringIndexer</span></span><br><span class="line"><span class="keyword">val</span> indexer = (<span class="keyword">new</span> <span class="type">StringIndexer</span>()</span><br><span class="line">  .setInputCol(<span class="string">&quot;day_of_week&quot;</span>)</span><br><span class="line">  .setOutputCol(<span class="string">&quot;day_of_week_index&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// (2-2) 숫자로 표현된 범주형 데이터 인코딩 (해당 요일인지 Boolean 타입으로 확인 가능)</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.feature.<span class="type">OneHotEncoder</span></span><br><span class="line"><span class="keyword">val</span> encoder = (<span class="keyword">new</span> <span class="type">OneHotEncoder</span>()</span><br><span class="line">  .setInputCol(<span class="string">&quot;day_of_week_index&quot;</span>)</span><br><span class="line">  .setOutputCol(<span class="string">&quot;day_of_week_encoded&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// (2-3) 수치형 벡터 타입을 입력으로 사용 (가격, 수량, 특정 날짜의 요일)</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.feature.<span class="type">VectorAssembler</span></span><br><span class="line"><span class="keyword">val</span> vectorAssembler = (<span class="keyword">new</span> <span class="type">VectorAssembler</span>()</span><br><span class="line">  .setInputCols(<span class="type">Array</span>(<span class="string">&quot;UnitPrice&quot;</span>, <span class="string">&quot;Quantity&quot;</span>, <span class="string">&quot;day_of_week_encoded&quot;</span>))</span><br><span class="line">  .setOutputCol(<span class="string">&quot;features&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// (3) 파이프라인 설정</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.<span class="type">Pipeline</span></span><br><span class="line"><span class="keyword">val</span> transformationPipeline = (<span class="keyword">new</span> <span class="type">Pipeline</span>()</span><br><span class="line">  .setStages(<span class="type">Array</span>(indexer, encoder, vectorAssembler)))</span><br><span class="line"></span><br><span class="line"><span class="comment">// (4) 변환자(transformer) 를 데이터셋에 적합(fit) =&gt; &#x27;fitted pipeline&#x27;</span></span><br><span class="line"><span class="comment">// 일관되고 반복된 방식으로 데이터 변환 가능. 학습 데이터셋 생성 완료</span></span><br><span class="line"><span class="keyword">val</span> fittedPipeline = transformationPipeline.fit(trainDataFrame)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> transformedTraining = fittedPipeline.transform(trainDataFrame)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 캐싱 사용시 중간 변화된 데이터셋의 복사본을 메모리에 저장. 전체 파이프라인 재실행보다 훨씬 빠르다</span></span><br><span class="line"><span class="comment">// 근데 왜때문에 나는 더 느린 것..? ㅎ.. CHAPTER 4 에서 다시 확인하자</span></span><br><span class="line">transformedTraining.cache()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// (5) 모델 학습 (kmeans 모델 설정 과정은 생략..)</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.clustering.<span class="type">KMeans</span></span><br><span class="line"><span class="keyword">val</span> kmeans = (<span class="keyword">new</span> <span class="type">KMeans</span>()</span><br><span class="line">  .setK(<span class="number">20</span>)</span><br><span class="line">  .setSeed(<span class="number">1</span>L))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> kmModel = kmeans.fit(transformedTraining)</span><br><span class="line"></span><br><span class="line"><span class="comment">// (6) 학습 데이터셋에 대한 비용 (군집 비용) 계산</span></span><br><span class="line">kmModel.computeCost(transformedTraining)</span><br><span class="line"><span class="comment">// res146: Double = 8.455373996537486E7</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 테스트 데이터셋과 비교</span></span><br><span class="line"><span class="comment">// 모델 개선 방법은 CHAPTER 6 에서</span></span><br><span class="line"><span class="keyword">val</span> transformedTest = fittedPipeline.transform(testDataFrame)</span><br><span class="line">kmModel.computeCost(transformedTest)</span><br><span class="line"><span class="comment">// res150: Double = 5.175070947222117E8</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</details>

<ul>
<li><p>내장된 머신러닝 알고리즘 라이브러리 MLlib 사용한 대규모 머신러닝 가능</p>
<ul>
<li>대용량 데이터 대상의 전처리(proprocessing), 멍잉(munging), 모델 학습(model training), 예측(prediction)</li>
<li>구조적 스트리밍에서 예측하고자 할때도 예측 모델 사용 가능</li>
</ul>
</li>
<li><p>스파크는 분류(classification), 회귀(regression), 군집화(clustering), 딥러닝(deep learning) 같은 머신러닝 관련 정교한 API 제공</p>
<ul>
<li>두유 노- <code>k-평균</code> ? : 군집화 표준 알고리즘. 센트로이드(centroid)라는 중심점을 사용해서.. <code>p.99 참고</code></li>
</ul>
</li>
<li><p>k-평균을 사용한 예제</p>
<ul>
<li>원본 데이터를 올바른 포맷으로 만드는 트렌스포메이션 정의. 실제 모델 학습 후 다음 예측 수행</li>
</ul>
</li>
<li><p>스파크 (MLlib DataFrame API) 에서 머신러닝 모델 학습 과정 2단계</p>
<ol>
<li>아직 학습되지 않은 모델 초기화</li>
<li>해당 모델을 학습</li>
</ol>
</li>
</ul>
<blockquote>
<p>알고리즘 명명 규칙 </p>
<ul>
<li>학습 전 알고리즘 명칭 : {Algorithm_name}</li>
<li>학습 후 알고리즘 명칭 : {Algorithm_name} + ‘Model’</li>
</ul>
</blockquote>
<h3 id="3-5-저수준-API"><a href="#3-5-저수준-API" class="headerlink" title="3.5 저수준 API"></a>3.5 저수준 API</h3><ul>
<li>스파크는 <strong>RDD</strong> 를 통해 자바와 파이썬 객체를 다루는데 필요한 다양한 기본 기능 (저수준 API) 제공<ul>
<li>DataFrame을 포함해서 스파크의 거의 모든 기능이 RDD 기반</li>
<li>저수준 명령으로 컴파일 =&gt; 편리하고 매우 효율적인 분산처리</li>
</ul>
</li>
<li>원시 데이터를 다루는 용도로도 쓸 수는 있지만, 대부분 구조적 API 사용이 더 낫다<ul>
<li>대신 파티션과 같은 <strong>물리적 실행 특성을 결정</strong> 할 수 있어, 세밀한 제어가 가능</li>
<li>비정형 데이터, 정제되지 않은 원시 데이터 처리에 사용</li>
</ul>
</li>
<li>언어에 따라 RDD 세부 구현에 차이가 있음<ul>
<li>Scala, Python 모두 사용 가능하지만 RDD가 동일하지 X</li>
<li>(&lt;-&gt; 언어에 관계없이 동일한 실행 특성의 DataFrame API)</li>
</ul>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 메모리에 저장된 원시 데이터를 병렬 처리 (parallize) 하여 RDD[Int] 생성 후 DataFrame으로 변환</span></span><br><span class="line">spark.sparkContext.parallelize(<span class="type">Seq</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)).toDF()</span><br></pre></td></tr></table></figure>
<h3 id="3-6-SparkR"><a href="#3-6-SparkR" class="headerlink" title="3.6 SparkR"></a>3.6 SparkR</h3><ul>
<li>SparkR : 스파크를 R언어로 사용하기 위한 기능<ul>
<li>파이썬 API 와 유사하고, 파이썬에서 사용할 수 있는 기능은 대부분 사용 가능</li>
<li>R 라이브러리 사용하여 스파크 트랜스포메이션 과정을 R과 유사하게 만들 수 있음</li>
<li>CHAPTER 7에서 자세히 알아보자</li>
</ul>
</li>
</ul>
<h3 id="3-7-스파크의-에코시스템과-패키지"><a href="#3-7-스파크의-에코시스템과-패키지" class="headerlink" title="3.7 스파크의 에코시스템과 패키지"></a>3.7 스파크의 에코시스템과 패키지</h3><ul>
<li>스파크의 최고 자랑 = 커뮤니티가 만들어낸 패키지 에코시스템 &amp; 다양한 기능<ul>
<li>스파크 패키지 저장소 : <a target="_blank" rel="noopener" href="https://spark-packages.org/">https://spark-packages.org/</a></li>
<li>그 외 깃헙, 기타 웹사이트 …</li>
</ul>
</li>
</ul>
<h3 id="3-8-정리"><a href="#3-8-정리" class="headerlink" title="3.8 정리"></a>3.8 정리</h3><ul>
<li>스파크를 비즈니스와 기술적 문제 해결에 적용할 수 있는 다양한 방법<ul>
<li>단순하고 강력한 프로그래밍 모델, 손쉬운 적용</li>
<li>다양한 패키지는 여러 비즈니스 문제를 성공적으로 해결할 수 있는 스파크의 능력에 대한 증거</li>
<li>더 성장하도록 더 많은 패키지가 만들어질거다~</li>
</ul>
</li>
</ul>
<h3 id="📒-단어장"><a href="#📒-단어장" class="headerlink" title="📒 단어장"></a>📒 단어장</h3><ul>
<li>정적 타입 코드/언어 (Statically typed) : 자료형이 고정된 언어. 컴파일 때 변수 타입이 결정 (ex. Java, Scala, C, C++ 등)<ul>
<li>&lt;-&gt; 동적 타입 언어 (Dynamically typed) : 런타임에 변수 타입이 결정 (ex. Python, JavaScript 등)</li>
</ul>
</li>
<li>멍잉 (munging) : =data wrangling. 원본 데이터를 다른 형태로 변환하거나 매핑하는 과정</li>
</ul>
</div><p class="post-tags"><i class="fa fa-tags" aria-hidden="true"></i><a href="/tags/study/">#study</a><a href="/tags/book/">#book</a><a href="/tags/spark/">#spark</a><a href="/tags/apache/">#apache</a></p></article></div><footer><div class="paginator"><a class="next" href="/2021/01/24/Spark-The-Definitive-Guide-2%EC%9E%A5/">NEXT</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'minsw-github-io';
var disqus_identifier = '2021/01/24/Spark-The-Definitive-Guide-3장/';
var disqus_title = '&amp;#039;Spark The Definitive Guide&amp;#039; 3장 - 일단 좀 더 잡솨봐 (PART 1 끝)';
var disqus_url = 'https://minsw.github.io/2021/01/24/Spark-The-Definitive-Guide-3장/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//#{theme.disqus}.disqus.com/count.js" async></script><div class="copyright"><p>© 2018 - 2021 <a href="https://minsw.github.io">Lukka Min</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-143001954-1",'auto');ga('send','pageview');</script><link rel="stylesheet" href="//cdn.datatables.net/1.10.7/css/jquery.dataTables.min.css" media="screen" type="text/css"><script src="//cdn.datatables.net/1.10.7/js/jquery.dataTables.min.js"></script><script>$(function(){$('.datatable').dataTable( {"order": [[ 0, "desc" ]],"iDisplayLength": -1,"lengthMenu": [[10, 25, 50, -1], [10, 25, 50, "All"]]} );});</script></body></html>