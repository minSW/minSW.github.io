<!DOCTYPE html><html lang="ko"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> &#039;Spark The Definitive Guide&#039; 5장 - 구조적 API 기본 연산 · Look out</title><meta name="description" content="&amp;#039;Spark The Definitive Guide&amp;#039; 5장 - 구조적 API 기본 연산 - Lukka Min"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/cover.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"><link rel="search" type="application/opensearchdescription+xml" href="https://minsw.github.io/atom.xml" title="Look out"><script src="//code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script><meta name="generator" content="Hexo 5.3.0"><link rel="alternate" href="/feed.xml" title="Look out" type="application/atom+xml">
</head><body><div class="wrap"><header><a class="logo-link" href="/"><img src="/cover.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a class="nav-list-link" href="/" target="_self">BLOG</a></li><li class="nav-list-item"><a class="nav-list-link" href="/archives/" target="_self">ARCHIVE</a></li><li class="nav-list-item"><a class="nav-list-link" href="/tags/" target="_self">TAG</a></li><li class="nav-list-item"><a class="nav-list-link" href="https://github.com/minSW" target="_blank">GITHUB</a></li></ul></header><main class="container"></main><div class="post"><article class="post-block"><h1 class="post-title">&#039;Spark The Definitive Guide&#039; 5장 - 구조적 API 기본 연산</h1><div class="post-info">2021년 1월 26일<span class="categories"><i class="fa fa-bookmark" aria-hidden="true"></i></span><a class="post-category" href="/categories/spark/">#spark</a></div><div class="post-content"><br/>

<p>오늘의 교훈.<br>도커 이미지에 예제 있다고 신나게 돌리고~ 돌리고~ 하다보면<br>터진다는걸 명심하도록 하자 🥺</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.io.IOException: No space left on device</span><br></pre></td></tr></table></figure>
<br/>

<img src="https://user-images.githubusercontent.com/26691216/106086744-1b72d100-6166-11eb-8d99-1deebfa68867.jpg" width="400" alt="jongman">
<center><i style="color:lightgray"> 인생은 실전이야 친구야</i></center>

<img width="300" alt="bomb" src="https://user-images.githubusercontent.com/26691216/106087261-fcc10a00-6166-11eb-80c2-d339e59bbc99.png">



<center><h2>_ _ _</h2></center>

<br/>

<hr>
<h1 id="CHAPTER-5-구조적-API-기본-연산"><a href="#CHAPTER-5-구조적-API-기본-연산" class="headerlink" title="CHAPTER 5 구조적 API 기본 연산"></a>CHAPTER 5 구조적 API 기본 연산</h1><p>CHAPTER 4 는 구조적 API의 핵심 추상화 ‘개념’을 소개<br>CHAPTER 5 는 DataFrame과 그 데이터를 다루는 기본 ‘기능’ 소개</p>
<blockquote>
<p><em>‘DataFrame = Row 타입의 <strong>레코드</strong> + 여러 <strong>컬럼</strong>‘</em><br>(각 컬럼명과 데이터 타입은 <strong>스키마</strong>로 정의)</p>
<p>DataFrame의 <strong>파티셔닝</strong> :  DataFrame (또는 Dataset)이 클러스터에서 물리적으로 배치되는 형태를 정의</p>
<ul>
<li><strong>파티셔닝 스키마</strong> : 파티션을 배치하는 방법 정의</li>
<li>파티셔닝의 분할 기준? =&gt; 특정 컬럼 or 비결정론적(nondeterministically) 값 기반으로 설정</li>
</ul>
</blockquote>
<h3 id="5-1-스키마"><a href="#5-1-스키마" class="headerlink" title="5.1 스키마"></a>5.1 스키마</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.format(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;/data/flight-data/json/2015-summary.json&quot;</span>)</span><br><span class="line">df.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">//  |-- DEST_COUNTRY_NAME: string (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- ORIGIN_COUNTRY_NAME: string (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- count: long (nullable = true)</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>스키마는 <strong>DataFrame의 컬럼명과 데이터 타입을 정의</strong></p>
<ul>
<li>관련된 모든 것을 하나로 묶는 역할</li>
</ul>
</li>
<li><p>데이터 소스에서 스키마를 얻거나 (Schema-on-read), 직접 정의 가능</p>
<ul>
<li>대부분의 비정형 분석 (ad-hoc analysis)에서 schema-on-read 잘 동작</li>
<li>운영 환경 ETL 작업에 스파크 사용시 <strong>직접 정의 필요</strong> (샘플 데이터 타입에 따른 스키마 추론 방지)</li>
</ul>
</li>
<li><p>스키마는 <code>StructType</code> 객체 </p>
<ul>
<li><p>복합 데이터 타입 <code>StructType</code> (=consistOf(<code>StructField</code> 객체))</p>
</li>
<li><p>스파크는 자체 데이터 타입 정보를 사용 =&gt; 언어 별 데이터 타입으로 설정 X</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">spark.read.format(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;/data/flight-data/json/2015-summary.json&quot;</span>).schema</span><br><span class="line"><span class="comment">// res176: org.apache.spark.sql.types.StructType</span></span><br><span class="line"><span class="comment">//  = StructType(</span></span><br><span class="line"><span class="comment">//        StructField(DEST_COUNTRY_NAME,StringType,true),</span></span><br><span class="line"><span class="comment">//        StructField(ORIGIN_COUNTRY_NAME,StringType,true),</span></span><br><span class="line"><span class="comment">//        StructField(count,LongType,true)</span></span><br><span class="line"><span class="comment">//      )</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<details><summary class="point-color-can-hover">[5.1] 예제 펼치기 - DataFrame에 스키마 적용 예제</summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DataFrame에 스키마를 만들고 적용하는 예제</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">StructField</span>, <span class="type">StructType</span>, <span class="type">StringType</span>, <span class="type">LongType</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.<span class="type">Metadata</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> myManualSchema = <span class="type">StructType</span>(<span class="type">Array</span>(</span><br><span class="line">  <span class="type">StructField</span>(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>, <span class="type">StringType</span>, <span class="literal">true</span>),</span><br><span class="line">  <span class="type">StructField</span>(<span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span>, <span class="type">StringType</span>, <span class="literal">true</span>),</span><br><span class="line">  <span class="type">StructField</span>(<span class="string">&quot;count&quot;</span>, <span class="type">LongType</span>, <span class="literal">false</span>,</span><br><span class="line">    <span class="type">Metadata</span>.fromJson(<span class="string">&quot;&#123;\&quot;hello\&quot;:\&quot;world\&quot;&#125;&quot;</span>))</span><br><span class="line">))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df = (spark.read.format(<span class="string">&quot;json&quot;</span>).schema(myManualSchema)</span><br><span class="line">  .load(<span class="string">&quot;/data/flight-data/json/2015-summary.json&quot;</span>))</span><br></pre></td></tr></table></figure>
</details>


<h3 id="5-2-컬럼과-표현식"><a href="#5-2-컬럼과-표현식" class="headerlink" title="5.2 컬럼과 표현식"></a>5.2 컬럼과 표현식</h3><ul>
<li><p>스파크의 ‘컬럼’ (=표현식)</p>
<ul>
<li>스프레드 시트, R의 dataframe, Pandas의 컬럼과 비슷</li>
<li>사용자는 <u><strong>표현식</strong></u>으로 DataFrame의 컬럼을 선택, 조작, 제거 가능</li>
<li>즉 표현식을 사용해 레코드 단위로 계산한 값을 나타내는 논리적 구조. 실제값을 얻으려면 로우 (=&gt; DataFrame) 가 필요</li>
<li>외부 접근시 <strong>반드시 DataFrame 을 통해야 함</strong></li>
</ul>
</li>
<li><p>컬럼 생성 &amp; 참조 : <code>col()</code> <code>column()</code></p>
<ul>
<li>컬럼이 DataFrame에 있는지 없는지는 모름 =&gt; <strong>분석기</strong>가 <strong>카탈로그</strong>에 저장된 정보랑 비교하기 전까지는 미확인 <a href="https://minsw.github.io/2021/01/26/Spark-The-Definitive-Guide-4%EC%9E%A5/#4-4-%EA%B5%AC%EC%A1%B0%EC%A0%81-API%EC%9D%98-%EC%8B%A4%ED%96%89-%EA%B3%BC%EC%A0%95">[4.4] 참고</a></li>
<li>스칼라는 고유 기능 사용 가능 : <code>$&quot;컬럼명&quot;</code> <code>&#39;컬럼명</code> (<code>&#39;</code> : 틱 마크, 심벌)</li>
<li>명시적 참조 : <code>DataFrame.col()</code> (조인시 유용)<br>=&gt; 명시적 컬럼 정의 시, 분석기 실행 단계에서 컬럼 확인 절차 생략</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;col, column&#125;</span><br><span class="line">col(<span class="string">&quot;someCol&quot;</span>)</span><br><span class="line">column(<span class="string">&quot;someCol&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// in Scala</span></span><br><span class="line">$<span class="string">&quot;someCol&quot;</span></span><br><span class="line"><span class="symbol">&#x27;someCol</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 명시적 참조</span></span><br><span class="line">df.col(<span class="string">&quot;someCol&quot;</span>)</span><br></pre></td></tr></table></figure></li>
<li><p><strong>표현식</strong> : <code>expr()</code></p>
<ul>
<li>DataFrame 레코드의 여러 값에 대한 트랜스포메이션 집합</li>
<li>여러 컬럼을 입력받아 식별 -&gt; 다양한 표현식을 각 레코드에 적용 -&gt; <strong>‘단일값’</strong> (복합 데이터 타입) 으로 출력 하는 함수</li>
<li><em>DataFrame의 컬럼은 ‘표현식’이다</em><ul>
<li><code>expr(&quot;someCol&quot;)</code> == <code>col(&quot;someCol&quot;)</code> (동일 동작)</li>
<li>컬럼은 표현식의 일부 기능 제공</li>
</ul>
</li>
</ul>
</li>
<li><p>스파크는 연산 순서를 지정하는 논리적 트리로 컴파일</p>
<ul>
<li>DataFrame 코드나 SQL 표현식 작성 시, 실행 시점에 동일한 논리 트리로 컴파일 되므로 동일한 성능 발휘</li>
<li>예시는 <code>p.129</code> [그림 5-1] 논리적 트리 DAG 참고</li>
<li><code>expr(&quot;someCol - 5&quot;)</code> == <code>col(&quot;someCol&quot;) - 5</code> == <code>expr(&quot;someCol&quot;) - 5</code>  (다 같은 트랜스포메이션 과정)</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 동일한 표현 - col(), expr()</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.expr</span><br><span class="line">expr(<span class="string">&quot;(((someCol + 5) * 200) - 6) &lt; otherCol&quot;</span>)</span><br><span class="line"></span><br><span class="line">(((col(<span class="string">&quot;someCol&quot;</span>) + <span class="number">5</span>) * <span class="number">200</span>) - <span class="number">6</span>) &lt; col(<span class="string">&quot;otherCol&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DataFrame 컬럼 접근 (printScheme() 아닌 프로그래밍 방식)</span></span><br><span class="line">spark.read.format(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;/data/flight-data/json/2015-summary.json&quot;</span>).columns</span><br><span class="line"><span class="comment">// res0: Array[String] = Array(DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count)</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<blockquote>
<p>‘표현식’ 과 ‘컬럼’ 사이 핵심 내용</p>
<ul>
<li>컬럼은 단지 표현식일 뿐</li>
<li>컬럼과 컬럼의 트랜스포메이션은 파싱된 표현식과 동일한 논리적 실행 계획으로 컴파일</li>
</ul>
</blockquote>
<h3 id="5-3-레코드와-로우"><a href="#5-3-레코드와-로우" class="headerlink" title="5.3 레코드와 로우"></a>5.3 레코드와 로우</h3><ul>
<li>스파크의 ‘로우’ (=레코드)<ul>
<li>스파크에서 DataFrame의 각 로우는 하나의 레코드</li>
<li>값을 생성하기 위해 컬럼 표현식으로 Row 객체를 다룸</li>
<li>Row 객체는 내부 바이트 배열을 가지는 인터페이스 =&gt; <strong>오직 컬럼 표현식으로만</strong> 다룰 수 있음 (외부 노출 X)<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Row 확인</span></span><br><span class="line">scala&gt; df.first()</span><br><span class="line"><span class="comment">// res1: org.apache.spark.sql.Row = [United States,Romania,15]</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>로우 생성<ul>
<li>각 컬럼에 해당하는 값으로 직접 Row 객체 생성 가능</li>
<li>그러나 Row 객체는 스키마 정보 X (=&gt; 오직 DataFrame만 가짐)</li>
<li>=&gt; 스키마랑 같은 순서로 값 명시해야함</li>
</ul>
</li>
<li>로우 데이터 접근하려면 =&gt; 원하는 위치 지정<ul>
<li>Python, R 은 올바른 데이터 타입으로 알아서 변환됨</li>
<li>Scala, Java 는 헬퍼 메서드나 데이터타입 명시적 지정 필요 (Dataset API 사용 시 jvm 객체 데이터 셋 얻을 수 있음)<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">val</span> myRow = <span class="type">Row</span>(<span class="string">&quot;Hello&quot;</span>, <span class="literal">null</span>, <span class="number">1</span>, <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">myRow(<span class="number">0</span>) <span class="comment">// type Any</span></span><br><span class="line">myRow(<span class="number">0</span>).asInstanceOf[<span class="type">String</span>] <span class="comment">// String</span></span><br><span class="line">myRow.getString(<span class="number">0</span>) <span class="comment">// String</span></span><br><span class="line">myRow.getInt(<span class="number">2</span>) <span class="comment">// Int</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># python 사용 시</span></span><br><span class="line">myRow[<span class="number">0</span>]</span><br><span class="line">myRow[<span class="number">2</span>]</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<h3 id="5-4-DataFrame의-트랜스포메이션"><a href="#5-4-DataFrame의-트랜스포메이션" class="headerlink" title="5.4 DataFrame의 트랜스포메이션"></a>5.4 DataFrame의 트랜스포메이션</h3><blockquote>
<p>DataFrame을 다루는 방법 (주요 작업 4가지)</p>
<ul>
<li>로우나 컬럼 추가</li>
<li>로우나 컬럼 제거</li>
<li>로우를 컬럼으로 변환하거나, 그 반대로 변환</li>
<li>컬럼값을 기준으로 로우 순서 변경</li>
</ul>
<p>모든 유형의 작업은 트랜스포메이션으로 변환 가능 (ex. 모든 로우의 특정 컬럼값 변경 후 결과 반환)</p>
</blockquote>
<ul>
<li><a href="#1-DataFrame-%EC%83%9D%EC%84%B1">(1) DataFrame 생성</a></li>
<li><a href="#2-select-%EC%99%80-selectExpr">(2) select 와 selectExpr</a></li>
<li><a href="#3-%EC%8A%A4%ED%8C%8C%ED%81%AC-%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%83%80%EC%9E%85%EC%9C%BC%EB%A1%9C-%EB%B3%80%ED%99%98%ED%95%98%EA%B8%B0">(3) 스파크 데이터 타입으로 변환하기</a></li>
<li><a href="#4-%EC%BB%AC%EB%9F%BC-%EC%B6%94%EA%B0%80%ED%95%98%EA%B8%B0">(4) 컬럼 추가하기</a></li>
<li><a href="#5-%EC%BB%AC%EB%9F%BC%EB%AA%85-%EB%B3%80%EA%B2%BD%ED%95%98%EA%B8%B0">(5) 컬럼명 변경하기</a></li>
<li><a href="#6-%EC%98%88%EC%95%BD-%EB%AC%B8%EC%9E%90%EC%99%80-%ED%82%A4%EC%9B%8C%EB%93%9C">(6) 예약 문자와 키워드</a></li>
<li><a href="#7-%EB%8C%80%EC%86%8C%EB%AC%B8%EC%9E%90-%EA%B5%AC%EB%B6%84">(7) 대소문자 구분</a></li>
<li><a href="#8-%EC%BB%AC%EB%9F%BC-%EC%A0%9C%EA%B1%B0%ED%95%98%EA%B8%B0">(8) 컬럼 제거하기</a></li>
<li><a href="#9-%EC%BB%AC%EB%9F%BC%EC%9D%98-%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%83%80%EC%9E%85-%EB%B3%80%EA%B2%BD%ED%95%98%EA%B8%B0">(9) 컬럼의 데이터 타입 변경하기</a></li>
<li><a href="#10-%EB%A1%9C%EC%9A%B0-%ED%95%84%ED%84%B0%EB%A7%81%ED%95%98%EA%B8%B0">(10) 로우 필터링하기</a></li>
<li><a href="#11-%EA%B3%A0%EC%9C%A0%ED%95%9C-%EB%A1%9C%EC%9A%B0-%EC%96%BB%EA%B8%B0">(11) 고유한 로우 얻기</a></li>
<li><a href="#12-%EB%AC%B4%EC%9E%91%EC%9C%84-%EC%83%98%ED%94%8C-%EB%A7%8C%EB%93%A4%EA%B8%B0">(12) 무작위 샘플 만들기</a></li>
<li><a href="#13-%EC%9E%84%EC%9D%98-%EB%B6%84%ED%95%A0%ED%95%98%EA%B8%B0">(13) 임의 분할하기</a></li>
<li><a href="#14-%EB%A1%9C%EC%9A%B0-%ED%95%A9%EC%B9%98%EA%B8%B0%EC%99%80-%EC%B6%94%EA%B0%80%ED%95%98%EA%B8%B0">(14) 로우 합치기와 추가하기</a></li>
<li><a href="#15-%EB%A1%9C%EC%9A%B0-%EC%A0%95%EB%A0%AC%ED%95%98%EA%B8%B0">(15) 로우 정렬하기</a></li>
<li><a href="#16-%EB%A1%9C%EC%9A%B0-%EC%88%98-%EC%A0%9C%ED%95%9C%ED%95%98%EA%B8%B0">(16) 로우 수 제한하기</a></li>
<li><a href="#17-repartition%EA%B3%BC-coalesce">(17) repartition과 coalesce</a></li>
<li><a href="#18-%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B2%84%EB%A1%9C-%EB%A1%9C%EC%9A%B0-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%88%98%EC%A7%91%ED%95%98%EA%B8%B0">(18) 드라이버로 로우 데이터 수집하기</a></li>
</ul>
<br/>

<h4 id="1-DataFrame-생성"><a href="#1-DataFrame-생성" class="headerlink" title="(1) DataFrame 생성"></a>(1) DataFrame 생성</h4><details><summary class="point-color-can-hover">[5.4-1] 예제 펼치기 </summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// A. 원시 데이터 소스 -&gt; DataFrame</span></span><br><span class="line"><span class="keyword">val</span> df = (spark.read.format(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;/data/flight-data/json/2015-summary.json&quot;</span>))</span><br><span class="line"><span class="comment">// (임시 뷰 등록)</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">&quot;dfTable&quot;</span>)</span><br><span class="line">``</span><br><span class="line"></span><br><span class="line"><span class="comment">// B. (Row 객체를 가지는) Seq 타입 -&gt; DataFrame</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">StructField</span>, <span class="type">StructType</span>, <span class="type">StringType</span>, <span class="type">LongType</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> myManualSchema = <span class="keyword">new</span> <span class="type">StructType</span>(<span class="type">Array</span>(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">StructField</span>(<span class="string">&quot;some&quot;</span>, <span class="type">StringType</span>, <span class="literal">true</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="type">StructField</span>(<span class="string">&quot;col&quot;</span>, <span class="type">StringType</span>, <span class="literal">true</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="type">StructField</span>(<span class="string">&quot;names&quot;</span>, <span class="type">LongType</span>, <span class="literal">false</span>)))</span><br><span class="line"><span class="keyword">val</span> myRows = <span class="type">Seq</span>(<span class="type">Row</span>(<span class="string">&quot;Hello&quot;</span>, <span class="literal">null</span>, <span class="number">1</span>L))</span><br><span class="line"><span class="comment">// sc 객체의 parallelize() 로 RDD 생성</span></span><br><span class="line"><span class="keyword">val</span> myRDD = spark.sparkContext.parallelize(myRows)</span><br><span class="line"><span class="comment">// createDataFrame()로 DataFrame 생성</span></span><br><span class="line"><span class="keyword">val</span> myDf = spark.createDataFrame(myRDD, myManualSchema)</span><br><span class="line">myDf.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// +-----+----+-----+</span></span><br><span class="line"><span class="comment">// | some| col|names|</span></span><br><span class="line"><span class="comment">// +-----+----+-----+</span></span><br><span class="line"><span class="comment">// |Hello|null|    1|</span></span><br><span class="line"><span class="comment">// +-----+----+-----+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// Scala 사용 시 toDF() 사용 가능 (import spark.implicits._)</span></span><br><span class="line"><span class="keyword">val</span> myDF = <span class="type">Seq</span>((<span class="string">&quot;Hello&quot;</span>, <span class="number">2</span>, <span class="number">1</span>L)).toDF(<span class="string">&quot;col1&quot;</span>, <span class="string">&quot;col2&quot;</span>, <span class="string">&quot;col3&quot;</span>)</span><br></pre></td></tr></table></figure>
</details>

<ul>
<li>스파크의 implicits (import 필요, <a target="_blank" rel="noopener" href="http://bit.ly/2xrFpML">참고</a>)<ul>
<li>Scala 스파크 콘솔 사용 시 =&gt; Seq 데이터 타입에 <code>toDF()</code> 사용 가능</li>
<li>그러나 null 타입과는 안맞으므로 운영환경 사용은 권장 X</li>
</ul>
</li>
</ul>
<blockquote>
<h4 id="createDataFrame-vs-toDF"><a href="#createDataFrame-vs-toDF" class="headerlink" title="createDataFrame() vs toDF()"></a>createDataFrame() vs toDF()</h4><ul>
<li><code>createDataFrame(rowRDD: RDD[Row], schema: StructType) : DataFrame</code><ul>
<li>모든 schema customization 가능</li>
<li><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/SparkSession.html#createDataFrame(rowRDD:org.apache.spark.rdd.RDD[org.apache.spark.sql.Row],schema:org.apache.spark.sql.types.StructType):org.apache.spark.sql.DataFrame">API docs#SparkSession</a></li>
</ul>
</li>
<li><code>toDF()</code><ul>
<li>스키마 지정 없음. schema 추론 (Dataset API)</li>
<li><code>import spark.implicits._</code> 필요</li>
<li><a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Dataset.html#toDF():org.apache.spark.sql.DataFrame">API docs#Dataset</a></li>
</ul>
</li>
</ul>
</blockquote>
<h4 id="2-select-와-selectExpr"><a href="#2-select-와-selectExpr" class="headerlink" title="(2) select 와 selectExpr"></a>(2) select 와 selectExpr</h4><details><summary class="point-color-can-hover">[5.4-2] 예제 펼치기 </summary>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--- SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> dataFrameTable</span><br><span class="line"><span class="keyword">SELECT</span> columnName <span class="keyword">FROM</span> dataFrameTable</span><br><span class="line"><span class="keyword">SELECT</span> columnName <span class="operator">*</span> <span class="number">10</span>, otherColumn, someOtherCol <span class="keyword">as</span> c <span class="keyword">FROM</span> dataFrameTable</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># select() == SELECT query</span></span><br><span class="line">scala&gt; df.select(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>).show(2)</span><br><span class="line">+-----------------+</span><br><span class="line">|DEST_COUNTRY_NAME|</span><br><span class="line">+-----------------+</span><br><span class="line">|    United States|</span><br><span class="line">|    United States|</span><br><span class="line">+-----------------+</span><br><span class="line"></span><br><span class="line">scala&gt; df.select(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>, <span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span>).show(2)</span><br><span class="line">+-----------------+-------------------+</span><br><span class="line">|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|</span><br><span class="line">+-----------------+-------------------+</span><br><span class="line">|    United States|            Romania|</span><br><span class="line">|    United States|            Croatia|</span><br><span class="line">+-----------------+-------------------+</span><br><span class="line"></span><br><span class="line">--- SQL</span><br><span class="line">SELECT DEST_COUNTRY_NAME FROM dfTable LIMIT 2</span><br><span class="line">SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME FROM dfTable LIMIT 2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 다양한 컬럼 참조 방법</span></span><br><span class="line"><span class="comment"># df.select(col(&quot;DEST_COUNTRY_NAME&quot;), &quot;DEST_COUNTRY_NAME&quot;) =&gt; 에러</span></span><br><span class="line">scala&gt; import org.apache.spark.sql.functions.&#123;expr, col, column&#125;</span><br><span class="line">scala&gt; (df.select(</span><br><span class="line">     |     df.col(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>),</span><br><span class="line">     |     col(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>),</span><br><span class="line">     |     column(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>),</span><br><span class="line">     |     <span class="string">&#x27;DEST_COUNTRY_NAME,</span></span><br><span class="line"><span class="string">     |     $&quot;DEST_COUNTRY_NAME&quot;,</span></span><br><span class="line"><span class="string">     |     expr(&quot;DEST_COUNTRY_NAME&quot;))</span></span><br><span class="line"><span class="string">     |   .show(2))</span></span><br><span class="line"><span class="string">+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+</span></span><br><span class="line"><span class="string">|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|</span></span><br><span class="line"><span class="string">+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+</span></span><br><span class="line"><span class="string">|    United States|    United States|    United States|    United States|    United States|    United States|</span></span><br><span class="line"><span class="string">|    United States|    United States|    United States|    United States|    United States|    United States|</span></span><br><span class="line"><span class="string">+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+</span></span><br><span class="line"><span class="string"></span></span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// expr() 예시 - 컬럼명 DEST_COUNTRY_NAME -&gt; destination -&gt; DEST_COUNTRY_NAME</span></span><br><span class="line">df.select(expr(<span class="string">&quot;DEST_COUNTRY_NAME AS destination&quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line">df.select(expr(<span class="string">&quot;DEST_COUNTRY_NAME as destination&quot;</span>).alias(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// select() + expr() =&gt; selectExpr()</span></span><br><span class="line">df.selectExpr(<span class="string">&quot;DEST_COUNTRY_NAME as newColumnName&quot;</span>, <span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +-------------+-----------------+</span></span><br><span class="line"><span class="comment">// |newColumnName|DEST_COUNTRY_NAME|</span></span><br><span class="line"><span class="comment">// +-------------+-----------------+</span></span><br><span class="line"><span class="comment">// |United States|    United States|</span></span><br><span class="line"><span class="comment">// |United States|    United States|</span></span><br><span class="line"><span class="comment">// +-------------+-----------------+</span></span><br><span class="line"></span><br><span class="line">(df.selectExpr(</span><br><span class="line">    <span class="string">&quot;*&quot;</span>, <span class="comment">// include all original columns</span></span><br><span class="line">    <span class="string">&quot;(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry&quot;</span>)</span><br><span class="line">  .show(<span class="number">2</span>))</span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+-------------+</span></span><br><span class="line"><span class="comment">// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+-------------+</span></span><br><span class="line"><span class="comment">// |    United States|            Romania|   15|        false|</span></span><br><span class="line"><span class="comment">// |    United States|            Croatia|    1|        false|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+-------------+</span></span><br><span class="line"></span><br><span class="line">df.selectExpr(<span class="string">&quot;avg(count)&quot;</span>, <span class="string">&quot;count(distinct(DEST_COUNTRY_NAME))&quot;</span>).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +-----------+---------------------------------+</span></span><br><span class="line"><span class="comment">// | avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|</span></span><br><span class="line"><span class="comment">// +-----------+---------------------------------+</span></span><br><span class="line"><span class="comment">// |1770.765625|                              132|</span></span><br><span class="line"><span class="comment">// +-----------+---------------------------------+</span></span><br></pre></td></tr></table></figure>
</details>


<blockquote>
<p>DataFrame을 다루기 위한 대부분의 트랜스포메이션 작업 해결 가능</p>
<ul>
<li><code>select()</code> : 컬럼이나 표현식을 사용</li>
<li><code>selectExpr()</code> : 문자열 표현식을 사용</li>
<li><code>select()</code>: 메서드로 사용할 수 없는 <code>org.apache.spark.sql.function</code> 에 포함된 다양한 함수</li>
</ul>
</blockquote>
<ul>
<li>DataFrame 컬럼 다룰 시, SQL 사용 가능</li>
<li>컬럼 참조 방법은 다양한 방법을 섞어서 사용할 수 있다. 5.2 재참고<ul>
<li>Column 객체랑 문자열을 함께 섞어쓸수는 X (ex. <code>df.select(col(&quot;DEST_COUNTRY_NAME&quot;), &quot;DEST_COUNTRY_NAME&quot;)</code> =&gt; 컴파일 에러)</li>
<li>가장 유연한 참조 방법 =&gt; <code>expr()</code></li>
</ul>
</li>
<li><code>select()</code> + <code>expr()</code> 패턴을 자주 사용 =&gt; <strong><code>selectExpr()</code></strong> (효율성 ↑)<ul>
<li><i style="color:gray">“?? : 크큭..스파크의 진정한 능력을 보여주지..”</i></li>
<li>새로운 DataFrame 생성하는 복잡한 표현식 간단하게 표현 가능</li>
<li>모든 유효한 비집계형 (non-aggregating) SQL 지정 가능 (단, 컬럼 식별 가능해야)</li>
<li>집계 함수 (avg, count 등) 사용 가능</li>
</ul>
</li>
</ul>
<h4 id="3-스파크-데이터-타입으로-변환하기"><a href="#3-스파크-데이터-타입으로-변환하기" class="headerlink" title="(3) 스파크 데이터 타입으로 변환하기"></a>(3) 스파크 데이터 타입으로 변환하기</h4><details><summary class="point-color-can-hover">[5.4-3] 예제 펼치기 </summary>


<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.lit</span><br><span class="line"></span><br><span class="line">df.select(expr(<span class="string">&quot;*&quot;</span>), lit(<span class="number">1</span>).as(<span class="string">&quot;One&quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+---+</span></span><br><span class="line"><span class="comment">// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+---+</span></span><br><span class="line"><span class="comment">// |    United States|            Romania|   15|  1|</span></span><br><span class="line"><span class="comment">// |    United States|            Croatia|    1|  1|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+---+</span></span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL 에서 리터럴은 상숫값 (동일 표현)</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span>, <span class="number">1</span> <span class="keyword">as</span> <span class="keyword">One</span> <span class="keyword">FROM</span> dfTable LIMIT <span class="number">2</span></span><br></pre></td></tr></table></figure>
</details>

<ul>
<li><strong>리터럴(literal)</strong> : 프로그래밍 언어의 리터럴 값 =&gt; 스파크가 이해할 수 있는 값으로 변환<ul>
<li>때로는 명시적 값 (상수값, 비교에 사용할 무언가 등..) 을 스파크에 전달해야함 =&gt; 리터럴 사용</li>
<li>리터럴은 표현식</li>
<li>어떤 상수나 프로그래밍으로 생성된 변숫값을 특정 컬럼의 값과 비교할 때 사용</li>
</ul>
</li>
</ul>
<h4 id="4-컬럼-추가하기"><a href="#4-컬럼-추가하기" class="headerlink" title="(4) 컬럼 추가하기"></a>(4) 컬럼 추가하기</h4><details><summary class="point-color-can-hover">[5.4-4] 예제 펼치기 </summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">df.withColumn(<span class="string">&quot;numberOne&quot;</span>, lit(<span class="number">1</span>)).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+---------+</span></span><br><span class="line"><span class="comment">// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+---------+</span></span><br><span class="line"><span class="comment">// |    United States|            Romania|   15|        1|</span></span><br><span class="line"><span class="comment">// |    United States|            Croatia|    1|        1|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+---------+</span></span><br><span class="line"></span><br><span class="line">(df.withColumn(<span class="string">&quot;withinCountry&quot;</span>, expr(<span class="string">&quot;ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME&quot;</span>))</span><br><span class="line">  .show(<span class="number">2</span>))</span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+-------------+</span></span><br><span class="line"><span class="comment">// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+-------------+</span></span><br><span class="line"><span class="comment">// |    United States|            Romania|   15|        false|</span></span><br><span class="line"><span class="comment">// |    United States|            Croatia|    1|        false|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+-------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 컬럼명 변경도 가능 (DEST_COUNTRY_NAME -&gt; Destination)</span></span><br><span class="line">df.withColumn(<span class="string">&quot;Destination&quot;</span>, expr(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>)).columns</span><br><span class="line"><span class="comment">// res18: Array[String] = Array(DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count, Destination)</span></span><br></pre></td></tr></table></figure>
</details>

<ul>
<li><code>withColumn(컬럼명, 값을 생성할 표현식)</code> 사용<ul>
<li>공식적 컬럼 추가 방법</li>
<li>컬럼명 변경하여 추가도 가능</li>
</ul>
</li>
</ul>
<h4 id="5-컬럼명-변경하기"><a href="#5-컬럼명-변경하기" class="headerlink" title="(5) 컬럼명 변경하기"></a>(5) 컬럼명 변경하기</h4><details><summary class="point-color-can-hover">[5.4-5] 예제 펼치기 </summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DEST_COUNTRY_NAME -&gt; dest 로 변경</span></span><br><span class="line">df.withColumnRenamed(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>, <span class="string">&quot;dest&quot;</span>).columns</span><br><span class="line"><span class="comment">// res21: Array[String] = Array(dest, ORIGIN_COUNTRY_NAME, count)</span></span><br></pre></td></tr></table></figure>
</details>

<ul>
<li><code>withColumnRenamed(컬럼명, 변경할 문자열)</code> 사용</li>
</ul>
<h4 id="6-예약-문자와-키워드"><a href="#6-예약-문자와-키워드" class="headerlink" title="(6) 예약 문자와 키워드"></a>(6) 예약 문자와 키워드</h4><details><summary class="point-color-can-hover">[5.4-6] 예제 펼치기 </summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 이스케이핑 필요 없는 예시 - 새로운 컬럼명을 나타내는 문자열</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.expr</span><br><span class="line"><span class="keyword">val</span> dfWithLongColName = df.withColumn(</span><br><span class="line">  <span class="string">&quot;This Long Column-Name&quot;</span>,</span><br><span class="line">  expr(<span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span>))</span><br><span class="line"><span class="comment">// dfWithLongColName: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 2 more fields]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 이스케이핑 필요한 예시 - 표현식으로 해당 컬럼을 참조 </span></span><br><span class="line">(dfWithLongColName.selectExpr(</span><br><span class="line">    <span class="string">&quot;`This Long Column-Name`&quot;</span>,</span><br><span class="line">    <span class="string">&quot;`This Long Column-Name` as `new col`&quot;</span>)</span><br><span class="line">  .show(<span class="number">2</span>))</span><br><span class="line"><span class="comment">// +---------------------+-------+</span></span><br><span class="line"><span class="comment">// |This Long Column-Name|new col|</span></span><br><span class="line"><span class="comment">// +---------------------+-------+</span></span><br><span class="line"><span class="comment">// |              Romania|Romania|</span></span><br><span class="line"><span class="comment">// |              Croatia|Croatia|</span></span><br><span class="line"><span class="comment">// +---------------------+-------+</span></span><br><span class="line"></span><br><span class="line">dfWithLongColName.createOrReplaceTempView(<span class="string">&quot;dfTableLong&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 같은 DataFrame 생성</span></span><br><span class="line">dfWithLongColName.select(col(<span class="string">&quot;This Long Column-Name&quot;</span>)).columns</span><br><span class="line">dfWithLongColName.select(expr(<span class="string">&quot;`This Long Column-Name`&quot;</span>)).columns</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL (동일 표현)</span></span><br><span class="line"><span class="keyword">SELECT</span> `This Long <span class="keyword">Column</span><span class="operator">-</span>Name`, `This Long <span class="keyword">Column</span><span class="operator">-</span>Name` <span class="keyword">as</span> `<span class="keyword">new</span> col`</span><br><span class="line"><span class="keyword">FROM</span> dfTableLong LIMIT <span class="number">2</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</details>

<ul>
<li>예약 문자(공백, 하이픈 (-) 등..) 는 컬럼명 사용 불가<ul>
<li>=&gt; 사용하려면 <strong><code>`</code> (백틱문자)</strong> 를 이용한 이스케이핑(escaping) 필요</li>
</ul>
</li>
<li>예약 문자나 키워드를 사용하는 ‘표현식’에는 이스케이프 처리 필요<ul>
<li>‘문자열’로 명시적 컬럼 참조 시에는 리터럴로 해석 =&gt; 예약문자 없이도 참조 가능</li>
</ul>
</li>
</ul>
<h4 id="7-대소문자-구분"><a href="#7-대소문자-구분" class="headerlink" title="(7) 대소문자 구분"></a>(7) 대소문자 구분</h4><details><summary class="point-color-can-hover">[5.4-7] 예제 펼치기 </summary>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> spark.sql.caseSensitive <span class="literal">true</span></span><br></pre></td></tr></table></figure>
</details>

<ul>
<li>기본적으로 스파크는 대소문자를 가리지 않음</li>
<li><code>set spark.sql.caseSenstive true</code> 설정 시 구분 가능</li>
</ul>
<h4 id="8-컬럼-제거하기"><a href="#8-컬럼-제거하기" class="headerlink" title="(8) 컬럼 제거하기"></a>(8) 컬럼 제거하기</h4><details><summary class="point-color-can-hover">[5.4-8] 예제 펼치기 </summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df.drop(<span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span>).columns</span><br><span class="line"><span class="comment">// res30: Array[String] = Array(DEST_COUNTRY_NAME, count)</span></span><br><span class="line"></span><br><span class="line">dfWithLongColName.drop(<span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span>, <span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>)</span><br><span class="line"><span class="comment">// res32: org.apache.spark.sql.DataFrame = [count: bigint, This Long Column-Name: string]</span></span><br></pre></td></tr></table></figure>
</details>

<ul>
<li><code>drop(컬럼명...)</code> 사용<ul>
<li>여러개를 인수로 넣어 다수의 컬럼을 한번에 제거 가능</li>
</ul>
</li>
<li><code>select()</code> 로도 제거 가능</li>
</ul>
<h4 id="9-컬럼의-데이터-타입-변경하기"><a href="#9-컬럼의-데이터-타입-변경하기" class="headerlink" title="(9) 컬럼의 데이터 타입 변경하기"></a>(9) 컬럼의 데이터 타입 변경하기</h4><details><summary class="point-color-can-hover">[5.4-9] 예제 펼치기 </summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// count 컬럼 : Integer -&gt; String 으로 형변환</span></span><br><span class="line">df.withColumn(<span class="string">&quot;count2&quot;</span>, col(<span class="string">&quot;count&quot;</span>).cast(<span class="string">&quot;long&quot;</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL (동일 표현)</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span>, <span class="built_in">cast</span>(count <span class="keyword">as</span> string) <span class="keyword">AS</span> count2 <span class="keyword">FROM</span> dfTable</span><br></pre></td></tr></table></figure>
</details>

<ul>
<li><code>cast()</code> 사용<ul>
<li>특정 데이터 타입 =&gt; 다른 데이터 타입으로 형변환</li>
</ul>
</li>
</ul>
<h4 id="10-로우-필터링하기"><a href="#10-로우-필터링하기" class="headerlink" title="(10) 로우 필터링하기"></a>(10) 로우 필터링하기</h4><details><summary class="point-color-can-hover">[5.4-10] 예제 펼치기 </summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 동일한 표현식</span></span><br><span class="line">df.filter(col(<span class="string">&quot;count&quot;</span>) &lt; <span class="number">2</span>).show(<span class="number">2</span>)</span><br><span class="line">df.where(<span class="string">&quot;count &lt; 2&quot;</span>).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |    United States|            Croatia|    1|</span></span><br><span class="line"><span class="comment">// |    United States|          Singapore|    1|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 여러 필터 적용 시 (순서 무관, 동시 적용)</span></span><br><span class="line">df.where(col(<span class="string">&quot;count&quot;</span>) &lt; <span class="number">2</span>).where(col(<span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span>) =!= <span class="string">&quot;Croatia&quot;</span>).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |    United States|          Singapore|    1|</span></span><br><span class="line"><span class="comment">// |          Moldova|      United States|    1|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL (동일 표현)</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> dfTable <span class="keyword">WHERE</span> count <span class="operator">&lt;</span> <span class="number">2</span> LIMIT <span class="number">2</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> dfTable <span class="keyword">WHERE</span> count <span class="operator">&lt;</span> <span class="number">2</span> <span class="keyword">AND</span> ORIGIN_COUNTRY_NAME <span class="operator">!=</span> &quot;Croatia&quot; LIMIT <span class="number">2</span></span><br></pre></td></tr></table></figure>
</details>

<ul>
<li>필터링을 하려면 참/거짓 판별 표현식 필요<ul>
<li>문자열 표현식, 컬럼을 다루는 기능으로 표현식 만듦</li>
</ul>
</li>
<li><code>where()</code> <code>filter()</code> 사용 가능<ul>
<li>두 메서드 모두 같은 파라미터 타입 및 같은 연산 수행</li>
<li><code>where()</code> 는 SQL과 유사</li>
<li><code>filter()</code> 는 Dataset API를 이용해서 사용하면 Dataset 각 레코드에 적용 할 함수를 사용 가능 (=&gt; 자세한건 11장)</li>
</ul>
</li>
<li>스파크는 필터의 순서와 상관없이 동시에 모든 필터링 작업 수행<ul>
<li>같은 표현식에 여러 필터 적용시</li>
<li>차례대로 AND 필터 연결 후 판단은 스파크에게 맡겨야 함</li>
</ul>
</li>
</ul>
<h4 id="11-고유한-로우-얻기"><a href="#11-고유한-로우-얻기" class="headerlink" title="(11) 고유한 로우 얻기"></a>(11) 고유한 로우 얻기</h4><details><summary class="point-color-can-hover">[5.4-11] 예제 펼치기 </summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df.select(<span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span>, <span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>).distinct().count()</span><br><span class="line"><span class="comment">// res41: Long = 256</span></span><br><span class="line"></span><br><span class="line">df.select(<span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span>).distinct().count()</span><br><span class="line"><span class="comment">// res44: Long = 125</span></span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL (동일 표현)</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span>(ORIGIN_COUNTRY_NAME, DEST_COUNTRY_NAME)) <span class="keyword">FROM</span> dfTable</span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> ORIGIN_COUNTRY_NAME) <span class="keyword">FROM</span> dfTable</span><br></pre></td></tr></table></figure>
</details>

<ul>
<li><code>distinct()</code> 사용<ul>
<li>고윳값이나 중복되지 않은 값을 얻는 연산</li>
</ul>
</li>
</ul>
<h4 id="12-무작위-샘플-만들기"><a href="#12-무작위-샘플-만들기" class="headerlink" title="(12) 무작위 샘플 만들기"></a>(12) 무작위 샘플 만들기</h4><details><summary class="point-color-can-hover">[5.4-12] 예제 펼치기 </summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> seed = <span class="number">5</span></span><br><span class="line"><span class="keyword">val</span> withReplacement = <span class="literal">false</span></span><br><span class="line"><span class="keyword">val</span> fraction = <span class="number">0.5</span></span><br><span class="line">df.sample(withReplacement, fraction, seed).count()</span><br><span class="line"><span class="comment">// res46: Long = 126</span></span><br></pre></td></tr></table></figure>
</details>

<ul>
<li><code>sample(복원추출 여부, 추출 비율, seed)</code> 사용<ul>
<li>표본 데이터 추출 비율 (&lt;=1.0) 지정 가능</li>
<li>복원 추출 (sample with replacement), 비복원 추출 (sample without replacement) 사용 여부 지정 가능</li>
</ul>
</li>
</ul>
<h4 id="13-임의-분할하기"><a href="#13-임의-분할하기" class="headerlink" title="(13) 임의 분할하기"></a>(13) 임의 분할하기</h4><details><summary class="point-color-can-hover">[5.4-13] 예제 펼치기 </summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 총합이 1이 아닐 경우 설정되는 default</span></span><br><span class="line"><span class="keyword">val</span> dataFrames = df.randomSplit(<span class="type">Array</span>(<span class="number">0.25</span>, <span class="number">0.75</span>), seed)</span><br><span class="line">dataFrames(<span class="number">0</span>).count() &gt; dataFrames(<span class="number">1</span>).count()</span><br><span class="line"><span class="comment">// res51: Boolean = false</span></span><br></pre></td></tr></table></figure>
</details>

<ul>
<li>임의 분할 (random split) : 원본 DataFrame 을 임의 크기로 ‘분할’<ul>
<li>머신러닝 알고리즘 사용 시 학습셋, 검증셋, 테스트셋 만들때 주로 사용</li>
</ul>
</li>
<li><code>randomSplit(분할 가중치 Array, seed)</code><ul>
<li>임의성(randomized) 을 가지므로 시드값(seed) 필수</li>
<li>DataFrame 비율은 총합이 1이 되게 지정 (아닐 경우 예제 비율로 지정됨)</li>
</ul>
</li>
</ul>
<h4 id="14-로우-합치기와-추가하기"><a href="#14-로우-합치기와-추가하기" class="headerlink" title="(14) 로우 합치기와 추가하기"></a>(14) 로우 합치기와 추가하기</h4><details><summary class="point-color-can-hover">[5.4-14] 예제 펼치기 </summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">val</span> schema = df.schema</span><br><span class="line"><span class="keyword">val</span> newRows = <span class="type">Seq</span>(</span><br><span class="line">  <span class="type">Row</span>(<span class="string">&quot;New Country&quot;</span>, <span class="string">&quot;Other Country&quot;</span>, <span class="number">5</span>L),</span><br><span class="line">  <span class="type">Row</span>(<span class="string">&quot;New Country 2&quot;</span>, <span class="string">&quot;Other Country 3&quot;</span>, <span class="number">1</span>L)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">val</span> parallelizedRows = spark.sparkContext.parallelize(newRows)</span><br><span class="line"><span class="keyword">val</span> newDF = spark.createDataFrame(parallelizedRows, schema)</span><br><span class="line"></span><br><span class="line"><span class="comment">// df + newDF =&gt; 로우가 추가된 새로운 객체</span></span><br><span class="line">(df.union(newDF)</span><br><span class="line">  .where(<span class="string">&quot;count = 1&quot;</span>)</span><br><span class="line">  .where($<span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span> =!= <span class="string">&quot;United States&quot;</span>)</span><br><span class="line">  .show()) <span class="comment">// get all of them and we&#x27;ll see our new rows at the end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// schema: org.apache.spark.sql.types.StructType = StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,true))</span></span><br><span class="line"><span class="comment">// newRows: Seq[org.apache.spark.sql.Row] = List([New Country,Other Country,5], [New Country 2,Other Country 3,1])</span></span><br><span class="line"><span class="comment">// parallelizedRows: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[74] at parallelize at &lt;console&gt;:29</span></span><br><span class="line"><span class="comment">// newDF: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |    United States|            Croatia|    1|</span></span><br><span class="line"><span class="comment">// |    United States|          Singapore|    1|</span></span><br><span class="line"><span class="comment">// |    United States|          Gibraltar|    1|</span></span><br><span class="line"><span class="comment">// |    United States|             Cyprus|    1|</span></span><br><span class="line"><span class="comment">// |    United States|            Estonia|    1|</span></span><br><span class="line"><span class="comment">// |    United States|          Lithuania|    1|</span></span><br><span class="line"><span class="comment">// |    United States|           Bulgaria|    1|</span></span><br><span class="line"><span class="comment">// |    United States|            Georgia|    1|</span></span><br><span class="line"><span class="comment">// |    United States|            Bahrain|    1|</span></span><br><span class="line"><span class="comment">// |    United States|   Papua New Guinea|    1|</span></span><br><span class="line"><span class="comment">// |    United States|         Montenegro|    1|</span></span><br><span class="line"><span class="comment">// |    United States|            Namibia|    1|</span></span><br><span class="line"><span class="comment">// |    New Country 2|    Other Country 3|    1|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</details>

<ul>
<li>DataFrame은 불변성 (immutability)<ul>
<li>DataFrame을 변경하는 레코드 추가는 불가능</li>
<li>=&gt; 원본 DataFrame을 새로운 DataFrame과 <strong>통합(union)</strong> (결합)</li>
<li>단, 통합하려는 두 DataFrame은 반드시 동일한 스키마와 컬럼 수를 가져야 함</li>
</ul>
</li>
<li><code>union()</code><ul>
<li>현재 스키마가 아닌 컬럼 위치 기반으로 동작 (자동 정렬 X)</li>
<li>로우가 추가된 DataFrame 을 참조하려면 새롭게 만들어진 DataFrame 사용해야하지만, <u>뷰나 테이블로 등록 시에는 동적으로 참조 가능</u></li>
</ul>
</li>
<li>컬럼 표현식과 문자 비교열 비교 시<ul>
<li>(컬럼 표현식이 아닌) 컬럼의 실제값을 비교 대상 문자열과 비교하려면</li>
<li>스칼라 사용 시 반드시 <strong><code>=!=</code> 함수</strong> 사용<ul>
<li><code>=!=</code>, <code>===</code> 는 스파크의 Column 클래스에 정의된 함수</li>
</ul>
</li>
<li>파이썬은 그대로 <code>!=</code>, <code>==</code></li>
</ul>
</li>
</ul>
<h4 id="15-로우-정렬하기"><a href="#15-로우-정렬하기" class="headerlink" title="(15) 로우 정렬하기"></a>(15) 로우 정렬하기</h4><details><summary class="point-color-can-hover">[5.4-15] 예제 펼치기 </summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">df.sort(<span class="string">&quot;count&quot;</span>).show(<span class="number">5</span>)</span><br><span class="line">df.orderBy(<span class="string">&quot;count&quot;</span>, <span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>).show(<span class="number">5</span>)</span><br><span class="line">df.orderBy(col(<span class="string">&quot;count&quot;</span>), col(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>)).show(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 정렬 기준 지정 (desc 오름)</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;desc, asc&#125;</span><br><span class="line"></span><br><span class="line">df.orderBy(expr(<span class="string">&quot;count desc&quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// 이거 왜 내림차순이아니라 오름차순으로 나오나... expr(&quot;count desc&quot;) 설정 안되고 default 정렬 (asc)로 설정되서 나오는 듯한데..?</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |          Moldova|      United States|    1|</span></span><br><span class="line"><span class="comment">// |    United States|            Croatia|    1|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"></span><br><span class="line">df.orderBy(desc(<span class="string">&quot;count&quot;</span>), asc(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +-----------------+-------------------+------+</span></span><br><span class="line"><span class="comment">// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+------+</span></span><br><span class="line"><span class="comment">// |    United States|      United States|370002|</span></span><br><span class="line"><span class="comment">// |    United States|             Canada|  8483|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+------+</span></span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> dfTable <span class="keyword">ORDER</span> <span class="keyword">BY</span> count <span class="keyword">DESC</span>, DEST_COUNTRY_NAME <span class="keyword">ASC</span> LIMIT <span class="number">2</span></span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// sortWithinPartitions() 로 파티션별 정렬</span></span><br><span class="line">(spark.read.format(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;/data/flight-data/json/*-summary.json&quot;</span>)</span><br><span class="line">  .sortWithinPartitions(<span class="string">&quot;count&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// explain() 시</span></span><br><span class="line"><span class="comment">// == Physical Plan ==</span></span><br><span class="line"><span class="comment">// *(1) Sort [count#450L ASC NULLS FIRST], false, 0</span></span><br><span class="line"><span class="comment">// +- *(1) FileScan json [DEST_COUNTRY_NAME#448,ORIGIN_COUNTRY_NAME#449,count#450L] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/data/flight-data/json/2015-summary.json, file:/data/flight-data/json/2012..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:bigint&gt;</span></span><br></pre></td></tr></table></figure>
</details>

<ul>
<li><code>sort()</code> <code>orderBy()</code> 사용<ul>
<li>두 메서드는 완전히 같은 방식으로 동작 (<code>orderBy()</code> 내부에서 <code>sort()</code> 사용)</li>
<li>다수 컬럼 지정, 컬럼 표현식, 문자열 사용 가능</li>
<li>정렬 기준 : <code>asc()</code>, <code>desc()</code> 로 명확한 지정 가능 (기본 동작은 오름차순)</li>
</ul>
</li>
<li>정렬된 DataFrame 의 NULL 값 표시 기준<ul>
<li><code>asc_nulls_first</code>, <code>desc_nulls_first</code>, <code>asc_nulls_last</code>, <code>desc_nulls_last</code> 로 기준 지정 가능</li>
</ul>
</li>
<li>파티션 별 정렬 =&gt; <code>sortWithinPartitions()</code><ul>
<li>트랜스포메이션 처리 전 성능 최적화를 위함</li>
<li>더 자세한 튜닝과 최적화 내용은 3부에서</li>
</ul>
</li>
</ul>
<blockquote>
<p><code>df.orderBy(expr(&quot;count desc&quot;))</code> ?</p>
<ul>
<li>count 컬럼을 desc() (내림차순) 으로 정렬되야 맞나?<ul>
<li>실제로는 그렇게 동작 하지 않음 (=&gt; 오름차순으로 정렬됨)</li>
</ul>
</li>
<li>잘못된 예제인듯한데..<ul>
<li>관련 stackoverflow 질문 <a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/63112281/pyspark-sort-dataframe-by-expression">링크 1</a> / <a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/63373479/sorting-2-columns-in-opposite-direction-does-not-work-using-expr-function">링크 2</a></li>
</ul>
</li>
</ul>
</blockquote>
<h4 id="16-로우-수-제한하기"><a href="#16-로우-수-제한하기" class="headerlink" title="(16) 로우 수 제한하기"></a>(16) 로우 수 제한하기</h4><details><summary class="point-color-can-hover">[5.4-16] 예제 펼치기 </summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">df.limit(<span class="number">5</span>).show()</span><br><span class="line"></span><br><span class="line">df.orderBy(expr(<span class="string">&quot;count desc&quot;</span>)).limit(<span class="number">6</span>).show()</span><br><span class="line"><span class="comment">// +--------------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|</span></span><br><span class="line"><span class="comment">// +--------------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |               Malta|      United States|    1|</span></span><br><span class="line"><span class="comment">// |Saint Vincent and...|      United States|    1|</span></span><br><span class="line"><span class="comment">// |       United States|            Croatia|    1|</span></span><br><span class="line"><span class="comment">// |       United States|          Gibraltar|    1|</span></span><br><span class="line"><span class="comment">// |       United States|          Singapore|    1|</span></span><br><span class="line"><span class="comment">// |             Moldova|      United States|    1|</span></span><br><span class="line"><span class="comment">// +--------------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// 뒷구르기하면서 봐도 결과가 이렇게 나와야할거같은데...</span></span><br><span class="line"><span class="comment">// df.orderBy(desc(&quot;count&quot;)).limit(6).show()</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+------+</span></span><br><span class="line"><span class="comment">// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+------+</span></span><br><span class="line"><span class="comment">// |    United States|      United States|370002|</span></span><br><span class="line"><span class="comment">// |    United States|             Canada|  8483|</span></span><br><span class="line"><span class="comment">// |           Canada|      United States|  8399|</span></span><br><span class="line"><span class="comment">// |    United States|             Mexico|  7187|</span></span><br><span class="line"><span class="comment">// |           Mexico|      United States|  7140|</span></span><br><span class="line"><span class="comment">// |   United Kingdom|      United States|  2025|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+------+</span></span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> dfTable LIMIT <span class="number">6</span></span><br></pre></td></tr></table></figure>
</details>

<ul>
<li><code>limit(로우 수)</code> 사용<ul>
<li>추출할 로우 수 제한하여 추출</li>
</ul>
</li>
</ul>
<h4 id="17-repartition과-coalesce"><a href="#17-repartition과-coalesce" class="headerlink" title="(17) repartition과 coalesce"></a>(17) repartition과 coalesce</h4><details><summary class="point-color-can-hover">[5.4-17] 예제 펼치기 </summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DataFrame 현재 파티션 수 구하기</span></span><br><span class="line">df.rdd.getNumPartitions <span class="comment">// 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 전체 데이터 셔플</span></span><br><span class="line">df.repartition(<span class="number">5</span>)</span><br><span class="line"><span class="comment">// df.repartition(5).rdd.getNumPartitions =&gt; 5</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 특정 컬럼 기준 파티션 재분배</span></span><br><span class="line">df.repartition(col(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>))</span><br><span class="line"><span class="comment">// df.repartition(col(&quot;DEST_COUNTRY_NAME&quot;)).rdd.getNumPartition =&gt; 200</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 특정 컬럼 지정 + 파티션 수 지정</span></span><br><span class="line">df.repartition(<span class="number">5</span>, col(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>))</span><br><span class="line"><span class="comment">// df.repartition(5, col(&quot;DEST_COUNTRY_NAME&quot;)).rdd.getNumPartitions =&gt; 5</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// coalesce() 로 셔플없이 파티션 병합 (1 -&gt; 5 -&gt; 2)</span></span><br><span class="line">df.repartition(<span class="number">5</span>, col(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>)).coalesce(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// df.repartition(5, col(&quot;DEST_COUNTRY_NAME&quot;)).coalesce(2).rdd.getNumPartitions =&gt; 2</span></span><br></pre></td></tr></table></figure>
</details>

<ul>
<li>또 다른 최적화 기법? =&gt; 자주 필터링하는 컬럼 기준으로 데이터 분할<ul>
<li>파티셔닝 스키마와 파티션 수를 포함한 클러스터 전반의 물리적 데이터 구성 제어 가능</li>
</ul>
</li>
<li><code>repartition()</code> : 전체 데이터 셔플<ul>
<li>향후 사용할 파티션 수 &gt; 현재 파티션 수 인 경우 사용 (파티션 수 ↑)</li>
<li>컬럼 기준으로 파티션을 만드는 경우 사용<ul>
<li>자주 필터링되는 컬럼이 있다면 해당 컬럼 기준으로 파티션 재분배 추천</li>
</ul>
</li>
<li>선택적으로 파티션 수 지정 가능</li>
</ul>
</li>
<li><code>coalesce()</code> : 전체 데이터 셔플 없이 파티션 병합<ul>
<li><strong>파티션 수를 줄이려면</strong> coalesce 사용 (<del>repartition</del> X)</li>
</ul>
</li>
<li>DataFrame 파티션 수 확인은 <code>df.rdd.getNumPartitions</code> 로 확인</li>
</ul>
<h4 id="18-드라이버로-로우-데이터-수집하기"><a href="#18-드라이버로-로우-데이터-수집하기" class="headerlink" title="(18) 드라이버로 로우 데이터 수집하기"></a>(18) 드라이버로 로우 데이터 수집하기</h4><details><summary class="point-color-can-hover">[5.4-18] 예제 펼치기 </summary>


</details>

<ul>
<li>스파크는 ‘드라이버’에서 클러스터 상태 정보 유지<ul>
<li>로컬 환경에서 데이터 다룰 때는 ‘드라이버’로 데이터 수집</li>
</ul>
</li>
<li>사용해본 데이터 수집 메서드 일부<ul>
<li><code>collect()</code> : 전체 DataFrame의 모든 데이터 수집</li>
<li><code>take()</code> : 상위 N개 로우 반환</li>
<li><code>show()</code> : 여러 로우를 보기 좋게 출력</li>
</ul>
</li>
<li><code>toLocalIterator()</code> : 전체 데이터셋에 대한 반복(iterate) 처리를 위해 ‘드라이버’로 로우를 모으는 방법<ul>
<li>iterator(반복자) 로 모든 파티션의 데이터를 ‘드라이버’에 전달</li>
<li>데이터셋의 파티션을 차례대로 반복 처리 가능</li>
</ul>
</li>
<li>드라이버로 모든 데이터 컬렉션을 수집하는 건<ul>
<li>=&gt; <strong>매우 큰 비용</strong> (CPU, 메모리, 네트워크)</li>
<li>차례대로 처리하므로 처리 비용 엄청남 (병렬 연산 X)</li>
</ul>
</li>
<li>따라서 대규모 데이터셋에 <code>collect()</code> 나 매우 큰 파티션에 대해 <code>toLocalIterator()</code> 사용 시 =&gt; 드라이버 비정상적 종료</li>
</ul>
<h3 id="5-5-정리"><a href="#5-5-정리" class="headerlink" title="5.5 정리"></a>5.5 정리</h3><ul>
<li>DataFrame 기본 연산</li>
<li>DataFrame 사용에 필요한 개념, 다양한 기능</li>
</ul>
<h3 id="📒-단어장"><a href="#📒-단어장" class="headerlink" title="📒 단어장"></a>📒 단어장</h3><ul>
<li>비결정론적(nondeterministically) : = 매번 변하는</li>
<li>ETL : <code>추출(Extract)</code> - <code>변환(Transform)</code> - <code>적재(Load)</code>  <i style="color:lightgray">(친숙하쥬?)</i></li>
</ul>
</div><p class="post-tags"><i class="fa fa-tags" aria-hidden="true"></i><a href="/tags/study/">#study</a><a href="/tags/book/">#book</a><a href="/tags/spark/">#spark</a><a href="/tags/apache/">#apache</a></p></article></div><footer><div class="paginator"><a class="next" href="/2021/01/26/Spark-The-Definitive-Guide-4%EC%9E%A5/">NEXT</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'minsw-github-io';
var disqus_identifier = '2021/01/26/Spark-The-Definitive-Guide-5장/';
var disqus_title = '&amp;#039;Spark The Definitive Guide&amp;#039; 5장 - 구조적 API 기본 연산';
var disqus_url = 'https://minsw.github.io/2021/01/26/Spark-The-Definitive-Guide-5장/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//#{theme.disqus}.disqus.com/count.js" async></script><div class="copyright"><p>© 2018 - 2021 <a href="https://minsw.github.io">Lukka Min</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-143001954-1",'auto');ga('send','pageview');</script><link rel="stylesheet" href="//cdn.datatables.net/1.10.7/css/jquery.dataTables.min.css" media="screen" type="text/css"><script src="//cdn.datatables.net/1.10.7/js/jquery.dataTables.min.js"></script><script>$(function(){$('.datatable').dataTable( {"order": [[ 0, "desc" ]],"iDisplayLength": -1,"lengthMenu": [[10, 25, 50, -1], [10, 25, 50, "All"]]} );});</script></body></html>