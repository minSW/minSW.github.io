<!DOCTYPE html><html lang="ko"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> &#039;Spark The Definitive Guide&#039; 6장 - 데이터 타입 (비)공식 가이드북 · Look out</title><meta name="description" content="&amp;#039;Spark The Definitive Guide&amp;#039; 6장 - 데이터 타입 (비)공식 가이드북 - Lukka Min"><meta name="og:image" content="https://minsw.github.io/images/og_image.png"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/cover.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"><link rel="search" type="application/opensearchdescription+xml" href="https://minsw.github.io/atom.xml" title="Look out"><script src="//code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script><meta name="generator" content="Hexo 5.3.0"><link rel="alternate" href="/feed.xml" title="Look out" type="application/atom+xml">
</head><body><div class="wrap"><header><a class="logo-link" href="/"><img src="/cover.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a class="nav-list-link" href="/" target="_self">BLOG</a></li><li class="nav-list-item"><a class="nav-list-link" href="/archives/" target="_self">ARCHIVE</a></li><li class="nav-list-item"><a class="nav-list-link" href="/tags/" target="_self">TAG</a></li><li class="nav-list-item"><a class="nav-list-link" href="https://github.com/minSW" target="_blank">GITHUB</a></li></ul></header><main class="container"></main><div class="post"><article class="post-block"><h1 class="post-title">&#039;Spark The Definitive Guide&#039; 6장 - 데이터 타입 (비)공식 가이드북</h1><div class="post-info">2021년 2월 2일<span class="categories"><i class="fa fa-bookmark" aria-hidden="true"></i></span><a class="post-category" href="/categories/spark/">#spark</a></div><div class="post-content"><br/>

<center><p style="color:lightgray">라떼 시절엔,, 가이드북이 하나면 든-든했다,,, 이말이야,,, 총총 @}----</p>
<img width="300" alt="maple" src="https://user-images.githubusercontent.com/26691216/106801127-b149b700-66a4-11eb-9c8f-0802771ebe5f.jpg">
<i>'아파치 스파크' 미인증 비공식 가이드 북<br/>
[전원 증정] 50.00 페이지 포인트 (캐시 아이템 구매 가능)</i></center>


<br/>

<center><h2>_ _ _</h2></center>

<br/>

<hr>
<h1 id="CHAPTER-6-다양한-데이터-타입-다루기"><a href="#CHAPTER-6-다양한-데이터-타입-다루기" class="headerlink" title="CHAPTER 6 다양한 데이터 타입 다루기"></a>CHAPTER 6 다양한 데이터 타입 다루기</h1><p>CHAPTER 5 는 DataFrame의 기본 개념과 핵심 추상화 개념을 소개<br>CHAPTER 6 는 스파크의 구조적 연산에서 가장 중요한 내용인 <strong>표현식 만드는 방법</strong> 소개 + 다양한 데이터 타입 다루는 방법</p>
<blockquote>
<p>다양한 데이터 타입</p>
<ul>
<li>Boolean 타입</li>
<li>수치 타입</li>
<li>문자열 타입</li>
<li>date와 timestamp 타입</li>
<li>null 값 다루기</li>
<li>복합 데이터 아입</li>
<li>사용자 정의 함수</li>
</ul>
</blockquote>
<h3 id="6-1-API는-어디서-찾을까"><a href="#6-1-API는-어디서-찾을까" class="headerlink" title="6.1 API는 어디서 찾을까"></a>6.1 API는 어디서 찾을까</h3><ul>
<li>오늘은 언젠가 내일이 된다<ul>
<li>버전 바뀌면 책의 내용도 다 예전 내용이다~ 이말이야</li>
<li>=&gt; 따라서 <u>데이터 변환용 함수 찾는 방법</u> 을 알아야함</li>
</ul>
</li>
<li>어떻게 찾나?<ul>
<li>DataFrame (Dataset) 메서드<ul>
<li>DatasFrame은 Row타입을 가진 Dataset =&gt; <a target="_blank" rel="noopener" href="http://bit.ly/2rKkALY">Dataset API</a></li>
<li>다양한 메서드를 제공하는 Dataset 하위 모듈 (ex. <a target="_blank" rel="noopener" href="http://bit.ly/2DPYhJC">DataFrameStateFunctions</a> - 통계적 함수 제공, <a target="_blank" rel="noopener" href="http://bit.ly/2DPAqd3">DataFrameNaFunctions</a> - null 데이터 제어)</li>
</ul>
</li>
<li>Column 메서드<ul>
<li><code>alias</code> <code>contains</code> 등의 컬럼 관련 메서드 제공 =&gt; <a target="_blank" rel="noopener" href="http://bit.ly/2FloFbr">Columns API</a></li>
<li><code>org.apache.spark.sql.functions</code>는 데이터 타입 관련 다양한 함수 제공 (ex. <a target="_blank" rel="noopener" href="http://bit.ly/2DPAycx">SQL, DataFrame 함수 등</a>)</li>
</ul>
</li>
</ul>
</li>
<li>모든 함수는 데이터 로우의 특정 포맷이나 구조를 다른 형태로 변환하기 위해 존재<ul>
<li>함수로 더 많은 로우를 만들거나 줄일 수 O</li>
</ul>
</li>
</ul>
<details><summary class="point-color-can-hover">[6.1] 예제 펼치기 - 분석용 DataFrame 생성 예제</summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = (spark.read.format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;inferSchema&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">  .load(<span class="string">&quot;/data/retail-data/by-day/2010-12-01.csv&quot;</span>))</span><br><span class="line">df.printSchema()</span><br><span class="line">df.createOrReplaceTempView(<span class="string">&quot;dfTable&quot;</span>)</span><br><span class="line"><span class="comment">// df: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]</span></span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">//  |-- InvoiceNo: string (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- StockCode: string (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- Description: string (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- Quantity: integer (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- InvoiceDate: timestamp (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- UnitPrice: double (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- CustomerID: double (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- Country: string (nullable = true)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</details>


<h3 id="6-2-스파크-데이터-타입으로-변환하기"><a href="#6-2-스파크-데이터-타입으로-변환하기" class="headerlink" title="6.2 스파크 데이터 타입으로 변환하기"></a>6.2 스파크 데이터 타입으로 변환하기</h3><ul>
<li><code>lit()</code> : 데이터 타입 변환<ul>
<li>다른 프로그래밍 언어 고유 데이터 타입 =&gt; <strong>스파크 데이터 타입</strong> 변환</li>
</ul>
</li>
</ul>
<details><summary class="point-color-can-hover">[6.2] 예제 펼치기</summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.lit</span><br><span class="line"></span><br><span class="line">df.select(lit(<span class="number">5</span>), lit(<span class="string">&quot;five&quot;</span>), lit(<span class="number">5.0</span>))</span><br><span class="line"><span class="comment">// res9: org.apache.spark.sql.DataFrame = [5: int, five: string ... 1 more field]</span></span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL (SQL은 스파크 데이터 타입 변환 필요 X. 직접 값 입력)</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="number">5</span>, &quot;five&quot;, <span class="number">5.0</span></span><br></pre></td></tr></table></figure>
</details>

<h3 id="6-3-불리언-데이터-타입-다루기"><a href="#6-3-불리언-데이터-타입-다루기" class="headerlink" title="6.3 불리언 데이터 타입 다루기"></a>6.3 불리언 데이터 타입 다루기</h3><ul>
<li><p>불리언은 모든 필터링 작업의 기반 (데이터 분석에 필수)</p>
</li>
<li><p>불리언 구문 : <code>and</code>, <code>or</code>, <code>true</code>, <code>false</code></p>
<ul>
<li>불리언 구문으로 논리 문법(true/false) 생성</li>
</ul>
</li>
<li><p><strong>스칼라</strong> 사용 시 동등 여부</p>
<ul>
<li><code>===</code> (일치) / <code>=!=</code> (불일치)</li>
<li><code>not()</code>, <code>equalTO()</code> 사용 가능</li>
</ul>
</li>
<li><p>파이썬, 스칼라 모두 사용할 수 있는</p>
<ul>
<li><p>가장 명확한 방법? =&gt; <u>문자열 표현식에 조건절 명시</u> (ex. <code>where(&quot;InvoiceNo = 536353)</code>)</p>
<details><summary class="point-color-can-hover">[6.3-1] 예제 펼치기 - 문자열 표현식에 조건절 명시 </summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.col</span><br><span class="line"></span><br><span class="line"><span class="comment">// 같은 표현식 (in Scala)</span></span><br><span class="line">(df.where(col(<span class="string">&quot;InvoiceNo&quot;</span>).equalTo(<span class="number">536365</span>))</span><br><span class="line">  .select(<span class="string">&quot;InvoiceNo&quot;</span>, <span class="string">&quot;Description&quot;</span>)</span><br><span class="line">  .show(<span class="number">5</span>, <span class="literal">false</span>))</span><br><span class="line">(df.where(col(<span class="string">&quot;InvoiceNo&quot;</span>) === <span class="number">536365</span>)</span><br><span class="line">  .select(<span class="string">&quot;InvoiceNo&quot;</span>, <span class="string">&quot;Description&quot;</span>)</span><br><span class="line">  .show(<span class="number">5</span>, <span class="literal">false</span>))</span><br><span class="line"><span class="comment">// +---------+-----------------------------------+</span></span><br><span class="line"><span class="comment">// |InvoiceNo|Description                        |</span></span><br><span class="line"><span class="comment">// +---------+-----------------------------------+</span></span><br><span class="line"><span class="comment">// |536365   |WHITE HANGING HEART T-LIGHT HOLDER |</span></span><br><span class="line"><span class="comment">// |536365   |WHITE METAL LANTERN                |</span></span><br><span class="line"><span class="comment">// |536365   |CREAM CUPID HEARTS COAT HANGER     |</span></span><br><span class="line"><span class="comment">// |536365   |KNITTED UNION FLAG HOT WATER BOTTLE|</span></span><br><span class="line"><span class="comment">// |536365   |RED WOOLLY HOTTIE WHITE HEART.     |</span></span><br><span class="line"><span class="comment">// +---------+-----------------------------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 문자열 표현식에 조건절 명시 (가장 명확한 방법) 사용</span></span><br><span class="line">(df.where(<span class="string">&quot;InvoiceNo = 536365&quot;</span>)</span><br><span class="line">  .show(<span class="number">5</span>, <span class="literal">false</span>))</span><br><span class="line"></span><br><span class="line">(df.where(<span class="string">&quot;InvoiceNo &lt;&gt; 536365&quot;</span>)</span><br><span class="line">  .show(<span class="number">5</span>, <span class="literal">false</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// +---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+</span></span><br><span class="line"><span class="comment">// |InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |</span></span><br><span class="line"><span class="comment">// +---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+</span></span><br><span class="line"><span class="comment">// |536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |2010-12-01 08:26:00|2.55     |17850.0   |United Kingdom|</span></span><br><span class="line"><span class="comment">// |536365   |71053    |WHITE METAL LANTERN                |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|</span></span><br><span class="line"><span class="comment">// |536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |2010-12-01 08:26:00|2.75     |17850.0   |United Kingdom|</span></span><br><span class="line"><span class="comment">// |536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|</span></span><br><span class="line"><span class="comment">// |536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|</span></span><br><span class="line"><span class="comment">// +---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// +---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+</span></span><br><span class="line"><span class="comment">// |InvoiceNo|StockCode|Description                  |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |</span></span><br><span class="line"><span class="comment">// +---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+</span></span><br><span class="line"><span class="comment">// |536366   |22633    |HAND WARMER UNION JACK       |6       |2010-12-01 08:28:00|1.85     |17850.0   |United Kingdom|</span></span><br><span class="line"><span class="comment">// |536366   |22632    |HAND WARMER RED POLKA DOT    |6       |2010-12-01 08:28:00|1.85     |17850.0   |United Kingdom|</span></span><br><span class="line"><span class="comment">// |536367   |84879    |ASSORTED COLOUR BIRD ORNAMENT|32      |2010-12-01 08:34:00|1.69     |13047.0   |United Kingdom|</span></span><br><span class="line"><span class="comment">// |536367   |22745    |POPPY&#x27;S PLAYHOUSE BEDROOM    |6       |2010-12-01 08:34:00|2.1      |13047.0   |United Kingdom|</span></span><br><span class="line"><span class="comment">// |536367   |22748    |POPPY&#x27;S PLAYHOUSE KITCHEN    |6       |2010-12-01 08:34:00|2.1      |13047.0   |United Kingdom|</span></span><br><span class="line"><span class="comment">// +---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+</span></span><br></pre></td></tr></table></figure>
</details>
</li>
</ul>
</li>
<li><p>불리언 표현식 사용하는 경우</p>
<ul>
<li><p>항상 모든 표현식을 <code>and</code> 메서드로 묶어 차례대로 필터 적용 해야 함</p>
</li>
<li><p>why?</p>
<ul>
<li>스파크 내부적으로 필터 사이에 <code>and</code> 구문 추가 시</li>
<li>=&gt; 모든 필터를 하나의 문장으로 변환하여 <strong>동시에 모든 필터 처리</strong> 하기 때문</li>
</ul>
</li>
<li><p><code>and</code> 구문 사용 시</p>
<ul>
<li><code>and</code> 구문으로 조건문을 만들 수는 있으나,</li>
<li>차례대로 조건 나열하는게 가독성이 좋음</li>
</ul>
</li>
<li><p><code>or</code> 구문 사용시</p>
<ul>
<li>반드시 동일한 구문에 조건 정의해야 함</li>
</ul>
<details><summary class="point-color-can-hover">[6.3-2] 예제 펼치기 - 불리언 표현식으로 필터링 적용 </summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> priceFilter = col(<span class="string">&quot;UnitPrice&quot;</span>) &gt; <span class="number">600</span></span><br><span class="line"><span class="keyword">val</span> descripFilter = col(<span class="string">&quot;Description&quot;</span>).contains(<span class="string">&quot;POSTAGE&quot;</span>)</span><br><span class="line">(df.where(col(<span class="string">&quot;StockCode&quot;</span>).isin(<span class="string">&quot;DOT&quot;</span>)).where(priceFilter.or(descripFilter))</span><br><span class="line">  .show())</span><br><span class="line"><span class="comment">// priceFilter: org.apache.spark.sql.Column = (UnitPrice &gt; 600)</span></span><br><span class="line"><span class="comment">// descripFilter: org.apache.spark.sql.Column = contains(Description, POSTAGE)</span></span><br><span class="line"><span class="comment">// +---------+---------+--------------+--------+-------------------+---------+----------+--------------+</span></span><br><span class="line"><span class="comment">// |InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|</span></span><br><span class="line"><span class="comment">// +---------+---------+--------------+--------+-------------------+---------+----------+--------------+</span></span><br><span class="line"><span class="comment">// |   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|</span></span><br><span class="line"><span class="comment">// |   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|</span></span><br><span class="line"><span class="comment">// +---------+---------+--------------+--------+-------------------+---------+----------+--------------+</span></span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL (동일 표현)</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> dfTable <span class="keyword">WHERE</span> StockCode <span class="keyword">in</span> (&quot;DOT&quot;) <span class="keyword">AND</span>(UnitPrice <span class="operator">&gt;</span> <span class="number">600</span> <span class="keyword">OR</span></span><br><span class="line">    instr(Description, &quot;POSTAGE&quot;) <span class="operator">&gt;=</span> <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
</details>
</li>
</ul>
</li>
<li><p>불리언 표현식은…</p>
<ul>
<li>필터링 조건에만 사용? =&gt; 🙅🏻‍♀️. 불리언컬럼으로 DataFrame 필터링도 가능</li>
<li>반드시 표현식으로 정의해야? =&gt; 🙅🏻‍♀️. 별도 작업없이 컬럼명만 사용해서 정의도 가능</li>
<li>사실 SQL로 표현하는게 더 익숙할지도.. (성능저하 X)</li>
</ul>
</li>
<li><p>NULL 값 데이터 처리?</p>
<ul>
<li>=&gt; <strong>null-safe</strong> 동치(equivalence) 테스트</li>
<li>ex. <code>df.where(col(&quot;Description&quot;).eqNullSafe(&quot;hello&quot;)).show()</code></li>
</ul>
</li>
<li><p>SQL의 <code>IS [NOT] DISTINCT FROM</code> 구문</p>
<ul>
<li>과 동일한 기능이 뭘 말하나.. =&gt; <code>isNotDistinctFrom()</code> <code>isDistinctFrom()</code>? (지금도 사용하는지?)</li>
<li>since Spark 2.3 (<a target="_blank" rel="noopener" href="https://bit.ly/2x47Obk">이슈 참고</a>)</li>
</ul>
<details><summary class="point-color-can-hover">[6.3-3] 예제 펼치기 - 불리언컬럼으로 DataFrame 필터링</summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DataFrame 필터링 예제</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">DOTCodeFilter</span> = col(<span class="string">&quot;StockCode&quot;</span>) === <span class="string">&quot;DOT&quot;</span></span><br><span class="line"><span class="keyword">val</span> priceFilter = col(<span class="string">&quot;UnitPrice&quot;</span>) &gt; <span class="number">600</span></span><br><span class="line"><span class="keyword">val</span> descripFilter = col(<span class="string">&quot;Description&quot;</span>).contains(<span class="string">&quot;POSTAGE&quot;</span>)</span><br><span class="line">(df.withColumn(<span class="string">&quot;isExpensive&quot;</span>, <span class="type">DOTCodeFilter</span>.and(priceFilter.or(descripFilter)))</span><br><span class="line">  .where(<span class="string">&quot;isExpensive&quot;</span>)</span><br><span class="line">  .select(<span class="string">&quot;unitPrice&quot;</span>, <span class="string">&quot;isExpensive&quot;</span>).show(<span class="number">5</span>))</span><br><span class="line"><span class="comment">// DOTCodeFilter: org.apache.spark.sql.Column = (StockCode = DOT)</span></span><br><span class="line"><span class="comment">// priceFilter: org.apache.spark.sql.Column = (UnitPrice &gt; 600)</span></span><br><span class="line"><span class="comment">// descripFilter: org.apache.spark.sql.Column = contains(Description, POSTAGE)</span></span><br><span class="line"><span class="comment">// +---------+-----------+</span></span><br><span class="line"><span class="comment">// |unitPrice|isExpensive|</span></span><br><span class="line"><span class="comment">// +---------+-----------+</span></span><br><span class="line"><span class="comment">// |   569.77|       true|</span></span><br><span class="line"><span class="comment">// |   607.49|       true|</span></span><br><span class="line"><span class="comment">// +---------+-----------+</span></span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL (동일 표현)</span></span><br><span class="line"><span class="keyword">SELECT</span> UnitPrice, (StockCode <span class="operator">=</span> <span class="string">&#x27;DOT&#x27;</span> <span class="keyword">AND</span></span><br><span class="line">  (UnitPrice <span class="operator">&gt;</span> <span class="number">600</span> <span class="keyword">OR</span> instr(Description, &quot;POSTAGE&quot;) <span class="operator">&gt;=</span> <span class="number">1</span>)) <span class="keyword">as</span> isExpensive</span><br><span class="line"><span class="keyword">FROM</span> dfTable</span><br><span class="line"><span class="keyword">WHERE</span> (StockCode <span class="operator">=</span> <span class="string">&#x27;DOT&#x27;</span> <span class="keyword">AND</span></span><br><span class="line">       (UnitPrice <span class="operator">&gt;</span> <span class="number">600</span> <span class="keyword">OR</span> instr(Description, &quot;POSTAGE&quot;) <span class="operator">&gt;=</span> <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 필터는 SQL로 사용시 더 편리할 수도. (아래 두 문장 동일하게 처리됨)</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;expr, not, col&#125;</span><br><span class="line"></span><br><span class="line">(df.withColumn(<span class="string">&quot;isExpensive&quot;</span>, not(col(<span class="string">&quot;UnitPrice&quot;</span>).leq(<span class="number">250</span>)))</span><br><span class="line">  .filter(<span class="string">&quot;isExpensive&quot;</span>)</span><br><span class="line">  .select(<span class="string">&quot;Description&quot;</span>, <span class="string">&quot;UnitPrice&quot;</span>).show(<span class="number">5</span>))</span><br><span class="line">(df.withColumn(<span class="string">&quot;isExpensive&quot;</span>, expr(<span class="string">&quot;NOT UnitPrice &lt;= 250&quot;</span>))</span><br><span class="line">  .filter(<span class="string">&quot;isExpensive&quot;</span>)</span><br><span class="line">  .select(<span class="string">&quot;Description&quot;</span>, <span class="string">&quot;UnitPrice&quot;</span>).show(<span class="number">5</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</details>


</li>
</ul>
<h3 id="6-4-수치형-데이터-타입-다루기"><a href="#6-4-수치형-데이터-타입-다루기" class="headerlink" title="6.4 수치형 데이터 타입 다루기"></a>6.4 수치형 데이터 타입 다루기</h3><ul>
<li><code>count()</code> <ul>
<li>빅데이터 처리 시, 필터링 다음으로 많이 수행하는 작업</li>
<li>수치형 데이터 타입을 사용한 연산 방식 정의</li>
</ul>
</li>
<li>자주 사용하는 수치형 함수<ul>
<li><code>pow(밑, 지수)</code> (거듭제곱)</li>
<li><code>round()</code> (반올림), <code>bround()</code> (내림)</li>
<li><code>corr()</code> =&gt; 피어슨 상관계수 계산 (= 두 컬럼의 상관관계)</li>
<li><code>describe()</code> =&gt; 관련 컬럼에 대한 집계(count), 평균(mean), 표준편차(stddev), 최솟값(min), 최댓값(max) 등 계산<ul>
<li>하나 이상의 컬럼에대한 요약 통계 계산</li>
<li>그러나 콘솔 확인용으로만 사용해야함 (통계 스키마는 변경 될 수 있음)</li>
<li>정확한 수치 필요 시 =&gt; 해당 함수 임포트해서 적용하는 방식으로 <strong>직접 집계</strong></li>
</ul>
</li>
</ul>
</li>
<li><strong>StatFunction</strong> 패키지 =&gt; 다양한 통계 함수 제공<ul>
<li>다양한 통계값 계산에 사용하는 DataFrame 메서드 =&gt; <code>df.stat</code> 으로 접근</li>
<li><code>approxQuantile()</code> : 데이터 백분위수 계산 (정확하게 or 근사치로?)</li>
<li><code>crosstab()</code> : 교차표(cross-tabulation) 확인</li>
<li><code>freqItems()</code> : 자주 사용하는 항목 쌍 확인<ul>
<li>crosstab, freqItems 등은 결과가 너무 크면 다 출력 X</li>
</ul>
</li>
<li><code>monotonically_increasing_id()</code> : 모든 로우에 고유 ID 값 추가 (0 ~ )</li>
</ul>
</li>
<li>스파크 새로운 버전 나올 때마다 새로운 함수 생김<ul>
<li>=&gt; 스파크 공식 문서 참조<ul>
<li>ex. <code>rand()</code>, <code>randn()</code> (임의 데이터 생성 함수)</li>
</ul>
</li>
<li>최신 버전 StatFunction 패키지는 여러 고급 기법 관련 함수 제공하기도<ul>
<li>bloom 필터링, sketching algorithms ..</li>
<li>자세한 내용은 <a target="_blank" rel="noopener" href="http://bit.ly/2ptAiY2">API docs</a></li>
<li>(사실 현시점 최신버전은 아니고 책기준 최신 2.2 버전인 듯)</li>
</ul>
</li>
</ul>
</li>
</ul>
<details><summary class="point-color-can-hover">[6.4] 예제 펼치기</summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;expr, pow&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 두 컬럼 모두 수치형 =&gt; 곱셈 연산 가능 (+ 덧셈, 뺄셈)</span></span><br><span class="line"><span class="keyword">val</span> fabricatedQuantity = pow(col(<span class="string">&quot;Quantity&quot;</span>) * col(<span class="string">&quot;UnitPrice&quot;</span>), <span class="number">2</span>) + <span class="number">5</span></span><br><span class="line">df.select(expr(<span class="string">&quot;CustomerId&quot;</span>), fabricatedQuantity.alias(<span class="string">&quot;realQuantity&quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +----------+------------------+</span></span><br><span class="line"><span class="comment">// |CustomerId|      realQuantity|</span></span><br><span class="line"><span class="comment">// +----------+------------------+</span></span><br><span class="line"><span class="comment">// |   17850.0|239.08999999999997|</span></span><br><span class="line"><span class="comment">// |   17850.0|          418.7156|</span></span><br><span class="line"><span class="comment">// +----------+------------------+</span></span><br><span class="line"></span><br><span class="line">df.selectExpr(</span><br><span class="line">  <span class="string">&quot;CustomerId&quot;</span>,</span><br><span class="line">  <span class="string">&quot;(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity&quot;</span>).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +----------+------------------+</span></span><br><span class="line"><span class="comment">// |CustomerId|      realQuantity|</span></span><br><span class="line"><span class="comment">// +----------+------------------+</span></span><br><span class="line"><span class="comment">// |   17850.0|239.08999999999997|</span></span><br><span class="line"><span class="comment">// |   17850.0|          418.7156|</span></span><br><span class="line"><span class="comment">// +----------+------------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 반올림(round) 예제</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;round, bround&#125;</span><br><span class="line">df.select(round(col(<span class="string">&quot;UnitPrice&quot;</span>), <span class="number">1</span>).alias(<span class="string">&quot;rounded&quot;</span>), col(<span class="string">&quot;UnitPrice&quot;</span>)).show(<span class="number">5</span>)</span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"><span class="comment">// |rounded|UnitPrice|</span></span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"><span class="comment">// |    2.6|     2.55|</span></span><br><span class="line"><span class="comment">// |    3.4|     3.39|</span></span><br><span class="line"><span class="comment">// |    2.8|     2.75|</span></span><br><span class="line"><span class="comment">// |    3.4|     3.39|</span></span><br><span class="line"><span class="comment">// |    3.4|     3.39|</span></span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 내림(bround) 예제</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.lit</span><br><span class="line">df.select(round(lit(<span class="string">&quot;2.5&quot;</span>)), bround(lit(<span class="string">&quot;2.5&quot;</span>))).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +-------------+--------------+</span></span><br><span class="line"><span class="comment">// |round(2.5, 0)|bround(2.5, 0)|</span></span><br><span class="line"><span class="comment">// +-------------+--------------+</span></span><br><span class="line"><span class="comment">// |          3.0|           2.0|</span></span><br><span class="line"><span class="comment">// |          3.0|           2.0|</span></span><br><span class="line"><span class="comment">// +-------------+--------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 피어슨 상관계수 계산 예제</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;corr&#125;</span><br><span class="line">df.stat.corr(<span class="string">&quot;Quantity&quot;</span>, <span class="string">&quot;UnitPrice&quot;</span>)</span><br><span class="line">df.select(corr(<span class="string">&quot;Quantity&quot;</span>, <span class="string">&quot;UnitPrice&quot;</span>)).show()</span><br><span class="line"><span class="comment">// res52: Double = -0.04112314436835551</span></span><br><span class="line"><span class="comment">// +-------------------------+</span></span><br><span class="line"><span class="comment">// |corr(Quantity, UnitPrice)|</span></span><br><span class="line"><span class="comment">// +-------------------------+</span></span><br><span class="line"><span class="comment">// |     -0.04112314436835551|</span></span><br><span class="line"><span class="comment">// +-------------------------+</span></span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL (동일 표현)</span></span><br><span class="line"><span class="keyword">SELECT</span> customerId, (<span class="built_in">POWER</span>((Quantity <span class="operator">*</span> UnitPrice), <span class="number">2.0</span>) <span class="operator">+</span> <span class="number">5</span>) <span class="keyword">as</span> realQuantity</span><br><span class="line"><span class="keyword">FROM</span> dfTable</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> round(<span class="number">2.5</span>), bround(<span class="number">2.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">corr</span>(Quantity, UnitPrice) <span class="keyword">FROM</span> dfTable</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 콘솔용 요약 통계 (describe)</span></span><br><span class="line">df.describe().show()</span><br><span class="line"><span class="comment">// +-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+</span></span><br><span class="line"><span class="comment">// |summary|        InvoiceNo|         StockCode|         Description|          Quantity|         UnitPrice|        CustomerID|       Country|</span></span><br><span class="line"><span class="comment">// +-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+</span></span><br><span class="line"><span class="comment">// |  count|             3108|              3108|                3098|              3108|              3108|              1968|          3108|</span></span><br><span class="line"><span class="comment">// |   mean| 536516.684944841|27834.304044117645|                null| 8.627413127413128| 4.151946589446603|15661.388719512195|          null|</span></span><br><span class="line"><span class="comment">// | stddev|72.89447869788873|17407.897548583845|                null|26.371821677029203|15.638659854603892|1854.4496996893627|          null|</span></span><br><span class="line"><span class="comment">// |    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|               0.0|           12431.0|     Australia|</span></span><br><span class="line"><span class="comment">// |    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|            607.49|           18229.0|United Kingdom|</span></span><br><span class="line"><span class="comment">// +-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// &#x27;직접 집계&#x27;&#x27; 필요 시 =&gt; 함수 임포트</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;count, mean, stddev_pop, min, max&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// StatFunctions package (다양한 통계 함수) 예제</span></span><br><span class="line"><span class="comment">// approxQuantile() : 데이터 백분위수 계산</span></span><br><span class="line"><span class="keyword">val</span> colName = <span class="string">&quot;UnitPrice&quot;</span></span><br><span class="line"><span class="keyword">val</span> quantileProbs = <span class="type">Array</span>(<span class="number">0.5</span>)</span><br><span class="line"><span class="keyword">val</span> relError = <span class="number">0.05</span></span><br><span class="line">df.stat.approxQuantile(<span class="string">&quot;UnitPrice&quot;</span>, quantileProbs, relError)</span><br><span class="line"><span class="comment">// res61: Array[Double] = Array(2.51)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 1) crosstab() : 교차표 확인</span></span><br><span class="line">df.stat.crosstab(<span class="string">&quot;StockCode&quot;</span>, <span class="string">&quot;Quantity&quot;</span>).show()</span><br><span class="line"><span class="comment">// +------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+</span></span><br><span class="line"><span class="comment">// |StockCode_Quantity| -1|-10|-12| -2|-24| -3| -4| -5| -6| -7|  1| 10|100| 11| 12|120|128| 13| 14|144| 15| 16| 17| 18| 19|192|  2| 20|200| 21|216| 22| 23| 24| 25|252| 27| 28|288|  3| 30| 32| 33| 34| 36|384|  4| 40|432| 47| 48|480|  5| 50| 56|  6| 60|600| 64|  7| 70| 72|  8| 80|  9| 96|</span></span><br><span class="line"><span class="comment">// +------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+</span></span><br><span class="line"><span class="comment">// |             22578|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             21327|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             22064|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             21080|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|</span></span><br><span class="line"><span class="comment">// |             22219|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             21908|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             22818|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |           15056BL|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             72817|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             22545|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             22988|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             22274|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             20750|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |            82616C|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             21703|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             22899|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             22379|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             22422|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             22769|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             22585|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// +------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 2) freqItems() : 자주 사용하는 항목 쌍 확인</span></span><br><span class="line">df.stat.freqItems(<span class="type">Seq</span>(<span class="string">&quot;StockCode&quot;</span>, <span class="string">&quot;Quantity&quot;</span>)).show()</span><br><span class="line"><span class="comment">// +--------------------+--------------------+</span></span><br><span class="line"><span class="comment">// | StockCode_freqItems|  Quantity_freqItems|</span></span><br><span class="line"><span class="comment">// +--------------------+--------------------+</span></span><br><span class="line"><span class="comment">// |[90214E, 20728, 2...|[200, 128, 23, 32...|</span></span><br><span class="line"><span class="comment">// +--------------------+--------------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 3) monotonically_increasing_id() : 로우에 고유 ID 값 추가</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.monotonically_increasing_id</span><br><span class="line">df.select(monotonically_increasing_id()).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +-----------------------------+</span></span><br><span class="line"><span class="comment">// |monotonically_increasing_id()|</span></span><br><span class="line"><span class="comment">// +-----------------------------+</span></span><br><span class="line"><span class="comment">// |                            0|</span></span><br><span class="line"><span class="comment">// |                            1|</span></span><br><span class="line"><span class="comment">// +-----------------------------+</span></span><br></pre></td></tr></table></figure>
</details>

<h3 id="6-5-문자열-데이터-타입-다루기"><a href="#6-5-문자열-데이터-타입-다루기" class="headerlink" title="6.5 문자열 데이터 타입 다루기"></a>6.5 문자열 데이터 타입 다루기</h3><ul>
<li><p>문자열 다루기 = 거의 모든 데이터 처리 과정에서 발생</p>
<ul>
<li>로그 파일에 정규 표현식을 사용한 데이터 추출, 데이터 치환, 문자열 존재 여부, 대/소문자 변환 처리 등</li>
</ul>
</li>
<li><p>대소문자 변환 작업</p>
<ul>
<li><code>initcap()</code> =&gt; 공백으로 구분된 모든 단어의 첫 글자 대문자로 변환</li>
<li><code>lower()</code> (전체 소문자로 변환) / <code>upper()</code> (전체 대문자로 변환)</li>
</ul>
</li>
<li><p>문자열 주변 공백 제거/추가</p>
<ul>
<li><p><code>lpad()</code>, <code>ltrim()</code>, <code>rpad()</code>, <code>rtrim()</code>, <code>trim()</code></p>
<details><summary class="point-color-can-hover">[6.5-1] 예제 펼치기 - 문자열 변환</summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;initcap&#125;</span><br><span class="line">df.select(initcap(col(<span class="string">&quot;Description&quot;</span>))).show(<span class="number">2</span>, <span class="literal">false</span>)</span><br><span class="line"><span class="comment">// +----------------------------------+</span></span><br><span class="line"><span class="comment">// |initcap(Description)              |</span></span><br><span class="line"><span class="comment">// +----------------------------------+</span></span><br><span class="line"><span class="comment">// |White Hanging Heart T-light Holder|</span></span><br><span class="line"><span class="comment">// |White Metal Lantern               |</span></span><br><span class="line"><span class="comment">// +----------------------------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;lower, upper&#125;</span><br><span class="line">df.select(col(<span class="string">&quot;Description&quot;</span>),</span><br><span class="line">  lower(col(<span class="string">&quot;Description&quot;</span>)),</span><br><span class="line">  upper(lower(col(<span class="string">&quot;Description&quot;</span>)))).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +--------------------+--------------------+-------------------------+</span></span><br><span class="line"><span class="comment">// |         Description|  lower(Description)|upper(lower(Description))|</span></span><br><span class="line"><span class="comment">// +--------------------+--------------------+-------------------------+</span></span><br><span class="line"><span class="comment">// |WHITE HANGING HEA...|white hanging hea...|     WHITE HANGING HEA...|</span></span><br><span class="line"><span class="comment">// | WHITE METAL LANTERN| white metal lantern|      WHITE METAL LANTERN|</span></span><br><span class="line"><span class="comment">// +--------------------+--------------------+-------------------------+</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;lit, ltrim, rtrim, rpad, lpad, trim&#125;</span><br><span class="line">df.select(</span><br><span class="line">    ltrim(lit(<span class="string">&quot;    HELLO    &quot;</span>)).as(<span class="string">&quot;ltrim&quot;</span>),</span><br><span class="line">    rtrim(lit(<span class="string">&quot;    HELLO    &quot;</span>)).as(<span class="string">&quot;rtrim&quot;</span>),</span><br><span class="line">    trim(lit(<span class="string">&quot;    HELLO    &quot;</span>)).as(<span class="string">&quot;trim&quot;</span>),</span><br><span class="line">    lpad(lit(<span class="string">&quot;HELLO&quot;</span>), <span class="number">3</span>, <span class="string">&quot; &quot;</span>).as(<span class="string">&quot;lp&quot;</span>),</span><br><span class="line">    rpad(lit(<span class="string">&quot;HELLO&quot;</span>), <span class="number">10</span>, <span class="string">&quot; &quot;</span>).as(<span class="string">&quot;rp&quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +---------+---------+-----+---+----------+</span></span><br><span class="line"><span class="comment">// |    ltrim|    rtrim| trim| lp|        rp|</span></span><br><span class="line"><span class="comment">// +---------+---------+-----+---+----------+</span></span><br><span class="line"><span class="comment">// |HELLO    |    HELLO|HELLO|HEL|HELLO     |</span></span><br><span class="line"><span class="comment">// |HELLO    |    HELLO|HELLO|HEL|HELLO     |</span></span><br><span class="line"><span class="comment">// +---------+---------+-----+---+----------+</span></span><br></pre></td></tr></table></figure>
</details>
</li>
</ul>
</li>
<li><p>정규표현식</p>
<ul>
<li><p>스파크는 <strong>자바 정규 표현식 문법</strong> 사용</p>
</li>
<li><p><code>regexp_extract()</code>, <code>regexp_replace()</code> =&gt; 값 추출 및 치환</p>
</li>
<li><p><code>translate(column, from_string, to_string)</code> 사용한 치환 가능</p>
</li>
<li><p>값 존재 여부 확인 방법?</p>
<ul>
<li>스칼라 사용 시 <code>contains()</code> 사용</li>
<li>파이썬, SQL 사용 시 <code>instr()</code> 사용</li>
</ul>
</li>
<li><p>동적으로 인수의 개수가 변하는 상황에서는</p>
<ul>
<li>스칼라 고유 기능 <code>varargs()</code> 사용</li>
<li>파이썬은 <code>locate()</code> (문자열 위치를 정수로 반환. 위치는 1 ~) + 위치 정보 불리언으로 변환</li>
</ul>
<details><summary class="point-color-can-hover">[6.5-2] 예제 펼치기 - 문자열 변환</summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.regexp_replace</span><br><span class="line"><span class="keyword">val</span> simpleColors = <span class="type">Seq</span>(<span class="string">&quot;black&quot;</span>, <span class="string">&quot;white&quot;</span>, <span class="string">&quot;red&quot;</span>, <span class="string">&quot;green&quot;</span>, <span class="string">&quot;blue&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> regexString = simpleColors.map(_.toUpperCase).mkString(<span class="string">&quot;|&quot;</span>)</span><br><span class="line"><span class="comment">// the | signifies `OR` in regular expression syntax</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// df.select(</span></span><br><span class="line"><span class="comment">//   regexp_replace(col(&quot;Description&quot;), regexString, &quot;COLOR&quot;).alias(&quot;color_clean&quot;),</span></span><br><span class="line"><span class="comment">//   col(&quot;Description&quot;)).show(2)</span></span><br><span class="line"><span class="comment">// regexString: String = BLACK|WHITE|RED|GREEN|BLUE</span></span><br><span class="line"><span class="comment">// +--------------------+--------------------+</span></span><br><span class="line"><span class="comment">// |         color_clean|         Description|</span></span><br><span class="line"><span class="comment">// +--------------------+--------------------+</span></span><br><span class="line"><span class="comment">// |COLOR HANGING HEA...|WHITE HANGING HEA...|</span></span><br><span class="line"><span class="comment">// | COLOR METAL LANTERN| WHITE METAL LANTERN|</span></span><br><span class="line"><span class="comment">// +--------------------+--------------------+</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.translate</span><br><span class="line">(df.select(translate(col(<span class="string">&quot;Description&quot;</span>), <span class="string">&quot;LEET&quot;</span>, <span class="string">&quot;1337&quot;</span>), col(<span class="string">&quot;Description&quot;</span>))</span><br><span class="line">  .show(<span class="number">2</span>))</span><br><span class="line"><span class="comment">// +----------------------------------+--------------------+</span></span><br><span class="line"><span class="comment">// |translate(Description, LEET, 1337)|         Description|</span></span><br><span class="line"><span class="comment">// +----------------------------------+--------------------+</span></span><br><span class="line"><span class="comment">// |              WHI73 HANGING H3A...|WHITE HANGING HEA...|</span></span><br><span class="line"><span class="comment">// |               WHI73 M37A1 1AN73RN| WHITE METAL LANTERN|</span></span><br><span class="line"><span class="comment">// +----------------------------------+--------------------+</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.regexp_extract</span><br><span class="line"><span class="keyword">val</span> regexString = simpleColors.map(_.toUpperCase).mkString(<span class="string">&quot;(&quot;</span>, <span class="string">&quot;|&quot;</span>, <span class="string">&quot;)&quot;</span>)</span><br><span class="line"><span class="comment">// the | signifies OR in regular expression syntax</span></span><br><span class="line">df.select(</span><br><span class="line">     regexp_extract(col(<span class="string">&quot;Description&quot;</span>), regexString, <span class="number">1</span>).alias(<span class="string">&quot;color_clean&quot;</span>),</span><br><span class="line">     col(<span class="string">&quot;Description&quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +-----------+--------------------+</span></span><br><span class="line"><span class="comment">// |color_clean|         Description|</span></span><br><span class="line"><span class="comment">// +-----------+--------------------+</span></span><br><span class="line"><span class="comment">// |      WHITE|WHITE HANGING HEA...|</span></span><br><span class="line"><span class="comment">// |      WHITE| WHITE METAL LANTERN|</span></span><br><span class="line"><span class="comment">// +-----------+--------------------+</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> containsBlack = col(<span class="string">&quot;Description&quot;</span>).contains(<span class="string">&quot;BLACK&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> containsWhite = col(<span class="string">&quot;DESCRIPTION&quot;</span>).contains(<span class="string">&quot;WHITE&quot;</span>)</span><br><span class="line">(df.withColumn(<span class="string">&quot;hasSimpleColor&quot;</span>, containsBlack.or(containsWhite))</span><br><span class="line">  .where(<span class="string">&quot;hasSimpleColor&quot;</span>)</span><br><span class="line">  .select(<span class="string">&quot;Description&quot;</span>).show(<span class="number">3</span>, <span class="literal">false</span>))</span><br><span class="line"><span class="comment">// +----------------------------------+</span></span><br><span class="line"><span class="comment">// |Description                       |</span></span><br><span class="line"><span class="comment">// +----------------------------------+</span></span><br><span class="line"><span class="comment">// |WHITE HANGING HEART T-LIGHT HOLDER|</span></span><br><span class="line"><span class="comment">// |WHITE METAL LANTERN               |</span></span><br><span class="line"><span class="comment">// |RED WOOLLY HOTTIE WHITE HEART.    |</span></span><br><span class="line"><span class="comment">// +----------------------------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> simpleColors = <span class="type">Seq</span>(<span class="string">&quot;black&quot;</span>, <span class="string">&quot;white&quot;</span>, <span class="string">&quot;red&quot;</span>, <span class="string">&quot;green&quot;</span>, <span class="string">&quot;blue&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> selectedColumns = simpleColors.map(color =&gt; &#123;</span><br><span class="line">   col(<span class="string">&quot;Description&quot;</span>).contains(color.toUpperCase).alias(<span class="string">s&quot;is_<span class="subst">$color</span>&quot;</span>)</span><br><span class="line">&#125;):+expr(<span class="string">&quot;*&quot;</span>) <span class="comment">// Column 타입이여야 합니다</span></span><br><span class="line">(df.select(selectedColumns:_*).where(col(<span class="string">&quot;is_white&quot;</span>).or(col(<span class="string">&quot;is_red&quot;</span>)))</span><br><span class="line">  .select(<span class="string">&quot;Description&quot;</span>).show(<span class="number">3</span>, <span class="literal">false</span>))</span><br><span class="line"><span class="comment">// +----------------------------------+</span></span><br><span class="line"><span class="comment">// |Description                       |</span></span><br><span class="line"><span class="comment">// +----------------------------------+</span></span><br><span class="line"><span class="comment">// |WHITE HANGING HEART T-LIGHT HOLDER|</span></span><br><span class="line"><span class="comment">// |WHITE METAL LANTERN               |</span></span><br><span class="line"><span class="comment">// |RED WOOLLY HOTTIE WHITE HEART.    |</span></span><br><span class="line"><span class="comment">// +----------------------------------+</span></span><br></pre></td></tr></table></figure>
</details>


</li>
</ul>
</li>
</ul>
<h3 id="6-6-날짜와-타임스탬프-데이터-타입-다루기"><a href="#6-6-날짜와-타임스탬프-데이터-타입-다루기" class="headerlink" title="6.6 날짜와 타임스탬프 데이터 타입 다루기"></a>6.6 날짜와 타임스탬프 데이터 타입 다루기</h3><ul>
<li><p>날짜/시간 사용 시 시간대 (timezone) 와 포맷의 유효성 확인 필요</p>
<ul>
<li>=&gt; 스파크는 두 가지 정보만 집중적으로 관리</li>
<li><strong>날짜</strong> (date) &amp; <strong>타임스탬프</strong> (timestamp)</li>
<li>inferSchema 옵션 활성화된 경우, 두 정보를 포함해 데이터 타입을 최대한 정확히 식별</li>
<li>스파크는 특정 날짜 포맷 명시 없이도 자체적으로 식별</li>
</ul>
</li>
<li><p>날짜, 시간을 문자열로 저장 &lt;-&gt; 런타임에 날짜 타입으로 변환</p>
<ul>
<li>텍스트, CSV 파일 다룰 시 많이 발생하는 방식</li>
<li>스파크 2.1 이하) 시간대 미지정 시, 시스템 시간대 기준으로 파싱<ul>
<li>시간대 설정? =&gt; <code>spark.conf.sessionLocalTimeZone</code> 속성을 로컬 시간대로 지정 - <a target="_blank" rel="noopener" href="https://bit.ly/2NcW6p2">Java TimeZone 포맷</a> 따름)</li>
</ul>
</li>
<li>스파크 2.3 이상) <code>spark.conf.set(&quot;spark.sql.session.timeZone&quot;, &quot;UTC&quot;)</code> 으로 사용 가능</li>
</ul>
</li>
<li><p>TimestampType 클래스는 초 단위 정밀도까지만 지원</p>
<ul>
<li>밀리세컨드(ms), 마이크로세컨드(μs) 지원 X =&gt; 필요 시 Long 데이터타입 사용해서 우회</li>
<li>특이한 포맷의 날짜/시간 데이터를 다뤄야한다면 =&gt; 각 단계별 데이터타입과 포맷 정확히 파악 후 트랜스포메이션 적용 해야함</li>
</ul>
</li>
<li><p>스파크는 특정 시점에 데이터 포맷이 특이하게 변할 수 있다</p>
<ul>
<li>싫다면 파싱이나 변환 작업 필요</li>
<li>스파크는 <strong>자바의 날짜와 타임스탬프</strong> 사용 (표준 체계)</li>
</ul>
</li>
<li><p>자주 사용하는 함수</p>
<ul>
<li><p>오늘 기준으로 N일 전후 날짜 구하기</p>
<ul>
<li><code>date_sub(컬럼, 뺄 날짜 수)</code> (책에는 sum, 오타?)</li>
<li><code>date_add(컬럼, 더할 날짜 수)</code> </li>
</ul>
</li>
<li><p>두 날짜 사이 차이 구하기</p>
<ul>
<li><code>datediff(컬럼1, 컬럼2)</code> : 두 날짜 사이 일 수 반환</li>
<li><code>months_between(컬럼1, 컬럼2)</code> : 두 날짜 사이 개월 수 반환</li>
</ul>
<details><summary class="point-color-can-hover">[6.6-1] 예제 펼치기 - 날짜 구하기 및 비교</summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// df.printSchema()</span></span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">//  |-- InvoiceNo: string (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- StockCode: string (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- Description: string (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- Quantity: integer (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- InvoiceDate: timestamp (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- UnitPrice: double (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- CustomerID: double (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- Country: string (nullable = true)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 예제1) 오늘 날짜 / 현재 타임스탬프 값 구하기</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;current_date, current_timestamp&#125;</span><br><span class="line"><span class="keyword">val</span> dateDF = (spark.range(<span class="number">10</span>)</span><br><span class="line">  .withColumn(<span class="string">&quot;today&quot;</span>, current_date())</span><br><span class="line">  .withColumn(<span class="string">&quot;now&quot;</span>, current_timestamp()))</span><br><span class="line">dateDF.createOrReplaceTempView(<span class="string">&quot;dateTable&quot;</span>)</span><br><span class="line"></span><br><span class="line">dateDF.printSchema()</span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">//  |-- id: long (nullable = false)</span></span><br><span class="line"><span class="comment">//  |-- today: date (nullable = false)</span></span><br><span class="line"><span class="comment">//  |-- now: timestamp (nullable = false)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 예제2) 오늘 기준으로 5일 전 날짜 구하기</span></span><br><span class="line"><span class="comment">// -- SQL : SELECT date_sub(today, 5), date_add(today, 5) FROM dateTable</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;date_add, date_sub&#125;</span><br><span class="line">dateDF.select(date_sub(col(<span class="string">&quot;today&quot;</span>), <span class="number">5</span>), date_add(col(<span class="string">&quot;today&quot;</span>), <span class="number">5</span>)).show(<span class="number">1</span>)</span><br><span class="line"><span class="comment">// +------------------+------------------+</span></span><br><span class="line"><span class="comment">// |date_sub(today, 5)|date_add(today, 5)|</span></span><br><span class="line"><span class="comment">// +------------------+------------------+</span></span><br><span class="line"><span class="comment">// |        2021-02-01|        2021-02-11|</span></span><br><span class="line"><span class="comment">// +------------------+------------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 예제3) 두 날짜 사이 차이 일수(개월수) 구하기</span></span><br><span class="line"><span class="comment">// -- SQL : SELECT to_date(&#x27;2016-01-01&#x27;), months_between(&#x27;2016-01-01&#x27;, &#x27;2017-01-01&#x27;),</span></span><br><span class="line"><span class="comment">// datediff(&#x27;2016-01-01&#x27;, &#x27;2017-01-01&#x27;)</span></span><br><span class="line"><span class="comment">// FROM dateTable</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;datediff, months_between, to_date&#125;</span><br><span class="line">(dateDF.withColumn(<span class="string">&quot;week_ago&quot;</span>, date_sub(col(<span class="string">&quot;today&quot;</span>), <span class="number">7</span>))</span><br><span class="line">  .select(datediff(col(<span class="string">&quot;week_ago&quot;</span>), col(<span class="string">&quot;today&quot;</span>))).show(<span class="number">1</span>))</span><br><span class="line">(dateDF.select(</span><br><span class="line">    to_date(lit(<span class="string">&quot;2016-01-01&quot;</span>)).alias(<span class="string">&quot;start&quot;</span>),</span><br><span class="line">    to_date(lit(<span class="string">&quot;2017-05-22&quot;</span>)).alias(<span class="string">&quot;end&quot;</span>))</span><br><span class="line">  .select(months_between(col(<span class="string">&quot;start&quot;</span>), col(<span class="string">&quot;end&quot;</span>))).show(<span class="number">1</span>))</span><br><span class="line"><span class="comment">// +-------------------------+</span></span><br><span class="line"><span class="comment">// |datediff(week_ago, today)|</span></span><br><span class="line"><span class="comment">// +-------------------------+</span></span><br><span class="line"><span class="comment">// |                       -7|</span></span><br><span class="line"><span class="comment">// +-------------------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// +--------------------------+</span></span><br><span class="line"><span class="comment">// |months_between(start, end)|</span></span><br><span class="line"><span class="comment">// +--------------------------+</span></span><br><span class="line"><span class="comment">// |              -16.67741935|</span></span><br><span class="line"><span class="comment">// +--------------------------+</span></span><br></pre></td></tr></table></figure>
</details>
</li>
</ul>
</li>
<li><p>날짜 변환 및 파싱</p>
<ul>
<li>올바른 포맷과 타입 사용 시 매우 쉬움</li>
<li>날짜나 타임스탬프 타입 사용 or ‘yyy-MM-dd’ 포맷에 맞는 문자열 지정</li>
<li><code>to_date()</code> : 문자열 =&gt; 날짜로 변환 (option. 날짜 포맷 지정 가능)<ul>
<li>날짜 포맷 : 자바의 <a target="_blank" rel="noopener" href="https://bit.ly/2Mz21Qc">SimpleDateFormat 클래스 지원 포맷 </a>사용</li>
</ul>
</li>
<li><code>to_timestamp()</code> : 날짜 포맷 필수 (미지정시 ‘yyyy-MM-dd HH:mm:ss’ 포맷 default)</li>
</ul>
</li>
<li><p>날짜 파싱 실패 시?</p>
<ul>
<li><p>=&gt; <strong>null 반환</strong> (에러 X)</p>
</li>
<li><p>예상치 못한 포맷의 데이터가 나타날 수 있으므로 디버깅 어려움</p>
</li>
<li><p>문제 회피할 수 있는 방식</p>
<ul>
<li>1. 자바 <a target="_blank" rel="noopener" href="https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html">SimpleDateFormat</a> 표준에 맞춰 날짜 포맷 지정</li>
<li>2. <code>to_date()</code>, <code>to_timestamp()</code> 사용</li>
</ul>
</li>
<li><p>암시적 형변환(implicit type casting)은 위험 =&gt; 명시적으로 데이터 타입 변환해서 사용할 것</p>
<details><summary class="point-color-can-hover">[6.6-2] 예제 펼치기 - 날짜 변환 및 파싱</summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// to_date(문자열) : 문자열 -&gt; 날짜 </span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;to_date, lit&#125;</span><br><span class="line">(spark.range(<span class="number">5</span>).withColumn(<span class="string">&quot;date&quot;</span>, lit(<span class="string">&quot;2017-01-01&quot;</span>))</span><br><span class="line">  .select(to_date(col(<span class="string">&quot;date&quot;</span>))).show(<span class="number">1</span>))</span><br><span class="line"><span class="comment">// +---------------+</span></span><br><span class="line"><span class="comment">// |to_date(`date`)|</span></span><br><span class="line"><span class="comment">// +---------------+</span></span><br><span class="line"><span class="comment">// |     2017-01-01|</span></span><br><span class="line"><span class="comment">// +---------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// SimpleDateFormate 클래스 지원 포맷 사용해야</span></span><br><span class="line">dateDF.select(to_date(lit(<span class="string">&quot;2016-20-12&quot;</span>)),to_date(lit(<span class="string">&quot;2017-12-11&quot;</span>))).show(<span class="number">1</span>)</span><br><span class="line">+---------------------+---------------------+</span><br><span class="line">|to_date(<span class="symbol">&#x27;2016</span><span class="number">-20</span><span class="number">-12</span>&#x27;)|to_date(<span class="symbol">&#x27;2017</span><span class="number">-12</span><span class="number">-11</span>&#x27;)|</span><br><span class="line">+---------------------+---------------------+</span><br><span class="line">|                 <span class="literal">null</span>|           <span class="number">2017</span><span class="number">-12</span><span class="number">-11</span>|</span><br><span class="line">+---------------------+---------------------+</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// to_date(문자열, 날짜 포맷) =&gt; 날짜포맷 Option</span></span><br><span class="line"><span class="comment">// to_timestamp(문자열, 날짜 포맷) =&gt; 날짜포맷 필수</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.to_date</span><br><span class="line"><span class="keyword">val</span> dateFormat = <span class="string">&quot;yyyy-dd-MM&quot;</span></span><br><span class="line"><span class="keyword">val</span> cleanDateDF = spark.range(<span class="number">1</span>).select(</span><br><span class="line">    to_date(lit(<span class="string">&quot;2017-12-11&quot;</span>), dateFormat).alias(<span class="string">&quot;date&quot;</span>),</span><br><span class="line">    to_date(lit(<span class="string">&quot;2017-20-12&quot;</span>), dateFormat).alias(<span class="string">&quot;date2&quot;</span>))</span><br><span class="line">cleanDateDF.createOrReplaceTempView(<span class="string">&quot;dateTable2&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// cleanDateDF.show()</span></span><br><span class="line"><span class="comment">// +----------+----------+</span></span><br><span class="line"><span class="comment">// |      date|     date2|</span></span><br><span class="line"><span class="comment">// +----------+----------+</span></span><br><span class="line"><span class="comment">// |2017-11-12|2017-12-20|</span></span><br><span class="line"><span class="comment">// +----------+----------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.to_timestamp</span><br><span class="line">cleanDateDF.select(to_timestamp(col(<span class="string">&quot;date&quot;</span>), dateFormat)).show()</span><br><span class="line"><span class="comment">// +----------------------------------+</span></span><br><span class="line"><span class="comment">// |to_timestamp(`date`, &#x27;yyyy-dd-MM&#x27;)|</span></span><br><span class="line"><span class="comment">// +----------------------------------+</span></span><br><span class="line"><span class="comment">// |               2017-11-12 00:00:00|</span></span><br><span class="line"><span class="comment">// +----------------------------------+</span></span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL (같은 표현, to_date(), to_timestamp())</span></span><br><span class="line"><span class="keyword">SELECT</span> to_date(<span class="type">date</span>, <span class="string">&#x27;yyyy-dd-MM&#x27;</span>), to_date(date2, <span class="string">&#x27;yyyy-dd-MM&#x27;</span>), to_date(<span class="type">date</span>)</span><br><span class="line"><span class="keyword">FROM</span> dateTable2</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> to_timestamp(<span class="type">date</span>, <span class="string">&#x27;yyyy-dd-MM&#x27;</span>), to_timestamp(date2, <span class="string">&#x27;yyyy-dd-MM&#x27;</span>)</span><br><span class="line"><span class="keyword">FROM</span> dateTable2</span><br></pre></td></tr></table></figure>
</details>
</li>
<li><p>날짜 &lt;-&gt; 타임스탬프 변환</p>
<ul>
<li>SQL (easy)<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">cast</span>(to_date(&quot;2017-01-01&quot;, &quot;yyyy-dd-MM&quot;) <span class="keyword">as</span> <span class="type">timestamp</span>)</span><br></pre></td></tr></table></figure></li>
<li>올바른 포맷과 타입의 날짜와 타임스탬프 사용 시에는 매우 쉽게 비교할 수 있다<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 날짜, 타임스탬프 타입 사용 or &quot;yyyy-MM-dd&quot; 포맷 문자열 사용</span></span><br><span class="line">cleanDateDF.filter(col(<span class="string">&quot;date2&quot;</span>) &gt; lit(<span class="string">&quot;2017-12-12&quot;</span>)).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 스파크가 리터럴로 인식하는 문자열 지정해서 비교도 가능</span></span><br><span class="line">cleanDateDF.filter(col(<span class="string">&quot;date2&quot;</span>) &gt; <span class="string">&quot;&#x27;2017-12-12&#x27;&quot;</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// +----------+----------+</span></span><br><span class="line"><span class="comment">// |      date|     date2|</span></span><br><span class="line"><span class="comment">// +----------+----------+</span></span><br><span class="line"><span class="comment">// |2017-11-12|2017-12-20|</span></span><br><span class="line"><span class="comment">// +----------+----------+</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="6-7-null-값-다루기"><a href="#6-7-null-값-다루기" class="headerlink" title="6.7 null 값 다루기"></a>6.7 null 값 다루기</h3><ul>
<li><p>DataFrame 에서 빈 값은 <strong>NULL</strong> 로 표현하는게 좋다</p>
<ul>
<li>스파크에서는 null 을 사용해야 최적화 수행 (빈 문자열 X, 대체값 X)</li>
</ul>
</li>
<li><p>DataFrame 에서 null 을 다루는 기본 방식 =&gt; <code>.na</code></p>
<ul>
<li>DataFrame의 하위 패키지</li>
<li>연산 수행 중 null 값 제어 방식을 명시적으로 지정하는 함수는 =&gt; 5.4.15 로우정렬하기 / 6.3 불리언 데이터 타입 다루기 참조</li>
</ul>
</li>
<li><p>null 값을 다루는 두가지 방식</p>
<ul>
<li>1. 명시적으로 null 값 제거</li>
<li>2. 전역 or 컬럼 단위로 null 값을 특정 값으로 채우기<blockquote>
<p>null 값은 명시적으로 사용 권장.<br>그러나 null 값을 허용하지 않는 컬럼 선언해도 <strong>강제성 없음</strong></p>
<ul>
<li>즉, notnull 컬럼이여도 null 값이 있을 수 있다</li>
<li>nullable 속성은 스파크 SQL 옵티마이저가 해당 컬럼을 제어하는 동작을 단순하게 도울 뿐</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><p><code>coalesce()</code></p>
<ul>
<li><p>인수의 여러 컬럼 중 null 이 아닌 첫번째 값 반환</p>
</li>
<li><p>모든 컬럼이 null이 아닌 값을 가지면 첫 번째 컬럼 값 반환</p>
<details><summary class="point-color-can-hover">[6.7-1] 예제 펼치기 - coalesce()</summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.coalesce</span><br><span class="line"></span><br><span class="line"><span class="comment">// Description 컬럼 값 null 체크</span></span><br><span class="line"><span class="comment">//  1. null이면 CustomerId 값 반환</span></span><br><span class="line"><span class="comment">//  2. null이 아니면 Description 컬럼 값 반환</span></span><br><span class="line">df.select(coalesce(col(<span class="string">&quot;Description&quot;</span>), col(<span class="string">&quot;CustomerId&quot;</span>))).show()</span><br></pre></td></tr></table></figure>
</details>
</li>
</ul>
</li>
<li><p>SQL 함수</p>
<ul>
<li><p><code>ifnull()</code> : 첫 번째 값이 null이면 두 번째 값 반환, null이 아니면 첫 번째 값 반환</p>
</li>
<li><p><code>nullif()</code> : 두 값이 같으면 null 반환, 다르면 첫 번째 값 반환</p>
</li>
<li><p><code>nvl()</code> : 첫 번째 값이 null이면 두 번째 값 반환, null이 아니면 첫 번째 값 반환</p>
</li>
<li><p><code>nvl2()</code> : 첫 번째 값이 null이 아니면 두 번째 값 반환, null이면 세 번째 값 반환</p>
<figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 이해하는 용도.. like this</span></span><br><span class="line"><span class="function"><span class="keyword">fun</span> <span class="title">ifnull</span><span class="params">(first: <span class="type">Any</span>, default: <span class="type">Any</span>)</span></span> = <span class="keyword">if</span> (first != <span class="literal">null</span>) first <span class="keyword">else</span> default</span><br><span class="line"><span class="function"><span class="keyword">fun</span> <span class="title">nullif</span><span class="params">(first: <span class="type">Any</span>, second: <span class="type">Any</span>)</span></span> = <span class="keyword">if</span> (first != second) first <span class="keyword">else</span> <span class="literal">null</span></span><br><span class="line"><span class="function"><span class="keyword">fun</span> <span class="title">nvl</span><span class="params">(first: <span class="type">Any</span>, default: <span class="type">Any</span>)</span></span> = <span class="keyword">if</span> (first != <span class="literal">null</span>) first <span class="keyword">else</span> default</span><br><span class="line"><span class="function"><span class="keyword">fun</span> <span class="title">nvl2</span><span class="params">(first: <span class="type">Any</span>, notnull_return: <span class="type">Any</span>, null_return: <span class="type">Any</span>)</span></span> = <span class="keyword">if</span> (first != <span class="literal">null</span>) notnull_return <span class="keyword">else</span> null_return</span><br></pre></td></tr></table></figure>
<details><summary class="point-color-can-hover">[6.7-2] 예제 펼치기 - SQL 함수</summary>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  ifnull(<span class="keyword">null</span>, <span class="string">&#x27;return_value&#x27;</span>),</span><br><span class="line">  <span class="built_in">nullif</span>(<span class="string">&#x27;value&#x27;</span>, <span class="string">&#x27;value&#x27;</span>),</span><br><span class="line">  nvl(<span class="keyword">null</span>, <span class="string">&#x27;return_value&#x27;</span>),</span><br><span class="line">  nvl2(<span class="string">&#x27;not_null&#x27;</span>, <span class="string">&#x27;return_value&#x27;</span>, &quot;else_value&quot;)</span><br><span class="line"><span class="keyword">FROM</span> dfTable LIMIT <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- +------------+----+------------+------------+</span></span><br><span class="line"><span class="comment">-- |           a|   b|           c|           d|</span></span><br><span class="line"><span class="comment">-- +------------+----+------------+------------+</span></span><br><span class="line"><span class="comment">-- |return_value|null|return_value|return_value|</span></span><br><span class="line"><span class="comment">-- +------------+----+------------+------------+</span></span><br></pre></td></tr></table></figure>
</details>


</li>
</ul>
</li>
</ul>
<ul>
<li><p><code>drop()</code></p>
<ul>
<li><p>null 값을 가진 로우를 모든 로우 제거</p>
</li>
<li><p>인수</p>
<ul>
<li><code>any</code> (하나라도 null이면 제거) / <code>all</code> (모든 컬럼이 null 또는 NaN이면 제거)</li>
<li>배열 형태 컬럼도 인수로 전달 가능</li>
</ul>
</li>
<li><p>SQL 사용 시 컬럼별로 수행해야함</p>
<details><summary class="point-color-can-hover">[6.7-3] 예제 펼치기 - drop()</summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df.na.drop()</span><br><span class="line">df.na.drop(<span class="string">&quot;any&quot;</span>) <span class="comment">// 하나라도 컬럼이 null (또는 NaN) 이면 로우 제거</span></span><br><span class="line">df.na.drop(<span class="string">&quot;all&quot;</span>) <span class="comment">// 모든 컬럼이 null (또는 NaN) 이면 로우 제거</span></span><br><span class="line"></span><br><span class="line">df.na.drop(<span class="string">&quot;all&quot;</span>, <span class="type">Seq</span>(<span class="string">&quot;StockCode&quot;</span>, <span class="string">&quot;InvoiceNo&quot;</span>)) <span class="comment">// 컬럼(배열형태) 전달 가능</span></span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL 사용 시 컬럼 별 수행해야</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> dfTable <span class="keyword">WHERE</span> Description <span class="keyword">IS</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span></span><br></pre></td></tr></table></figure>
</details>
</li>
</ul>
</li>
<li><p><code>fill()</code></p>
<ul>
<li><p>하나 이상의 컬럼을 특정 값으로 채움</p>
</li>
<li><p>인수</p>
<ul>
<li>채워넣을 값, 컬럽 집합으로 구성된 맵</li>
<li>컬럼명 배열로 인수 사용 및 다수 컬럼에도 적용 가능 (ex. <code>df.na.fill(5:Integer)</code>, <code>df.na.fill(5:Double)</code>)</li>
</ul>
</li>
<li><p>스칼라 Map 타입 사용도 인수로 가능 (key:value = 컬럼명:null값을 채울 값)</p>
<details><summary class="point-color-can-hover">[6.7-4] 예제 펼치기 - fill()</summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">df.na.fill(<span class="string">&quot;All Null values become this string&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 다수 컬럼에 적용</span></span><br><span class="line">df.na.fill(<span class="number">5</span>, <span class="type">Seq</span>(<span class="string">&quot;StockCode&quot;</span>, <span class="string">&quot;InvoiceNo&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// Map 타입으로 다수 컬럼에 적용</span></span><br><span class="line"><span class="keyword">val</span> fillColValues = <span class="type">Map</span>(<span class="string">&quot;StockCode&quot;</span> -&gt; <span class="number">5</span>, <span class="string">&quot;Description&quot;</span> -&gt; <span class="string">&quot;No Value&quot;</span>)</span><br><span class="line">df.na.fill(fillColValues)</span><br></pre></td></tr></table></figure>
</details>
</li>
</ul>
</li>
<li><p><code>relace()</code></p>
<ul>
<li><p>조건에 따라 다른 값으로 대체</p>
</li>
<li><p>단, 변경하고자하는 값 == 원래 값 데이터 타입 같아야</p>
<details><summary class="point-color-can-hover">[6.7-5] 예제 펼치기 - replace()</summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.na.replace(<span class="string">&quot;Description&quot;</span>, <span class="type">Map</span>(<span class="string">&quot;&quot;</span> -&gt; <span class="string">&quot;UNKNOWN&quot;</span>))</span><br></pre></td></tr></table></figure>
</details>

</li>
</ul>
</li>
</ul>
<h3 id="6-8-정렬하기"><a href="#6-8-정렬하기" class="headerlink" title="6.8 정렬하기"></a>6.8 정렬하기</h3><ul>
<li><code>asc_nulls_first()</code>, <code>desc_nulls_first()</code>, <code>asc_nulls_last()</code>, <code>desc_nulls_last()</code><ul>
<li>DataFrame 정렬 시 null 값 표시 기준 지정 가능</li>
</ul>
</li>
<li>(=&gt; <a href="https://minsw.github.io/2021/01/26/Spark-The-Definitive-Guide-5%EC%9E%A5/#15-%EB%A1%9C%EC%9A%B0-%EC%A0%95%EB%A0%AC%ED%95%98%EA%B8%B0">5.4.15 - 로우정렬하기</a> 다시 참고~)</li>
</ul>
<h3 id="6-9-복합-데이터-타입-다루기"><a href="#6-9-복합-데이터-타입-다루기" class="headerlink" title="6.9 복합 데이터 타입 다루기"></a>6.9 복합 데이터 타입 다루기</h3><ul>
<li><p>복합 데이터 타입 : 구조체 (struct), 배열 (array), 맵(map)</p>
</li>
<li><p>구조체 = DataFrame 내부의 DataFrame</p>
<ul>
<li>쿼리문에서 다수의 컬럼을 괄호로 묶어서 =&gt; 구조체 만듦</li>
<li>복합 데이터 타입을 가진 DataFrame 사용<ul>
<li>=&gt; 다른 DataFrame 조회하는것과 동일하게 사용 가능</li>
<li>차이점은 문법에 점 (.) 사용 or <code>getField()</code> 사용</li>
<li><code>*</code> 문자로 모든 값 조회 가능 (모든 컬럼을 최상위 수준으로 끌어올리기 가능?)</li>
</ul>
</li>
</ul>
<details><summary class="point-color-can-hover">[6.8,1] 예제 펼치기 - 구조체</summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">df.selectExpr(<span class="string">&quot;(Description, InvoiceNo) as complex&quot;</span>, <span class="string">&quot;*&quot;</span>)</span><br><span class="line">df.selectExpr(<span class="string">&quot;struct(Description, InvoiceNo) as complex&quot;</span>, <span class="string">&quot;*&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.struct</span><br><span class="line"><span class="keyword">val</span> complexDF = df.select(struct(<span class="string">&quot;Description&quot;</span>, <span class="string">&quot;InvoiceNo&quot;</span>).alias(<span class="string">&quot;complex&quot;</span>))</span><br><span class="line">complexDF.createOrReplaceTempView(<span class="string">&quot;complexDF&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 조회 시 점(.) 또는 getField() 사용</span></span><br><span class="line">complexDF.select(<span class="string">&quot;complex.Description&quot;</span>)</span><br><span class="line">complexDF.select(col(<span class="string">&quot;complex&quot;</span>).getField(<span class="string">&quot;Description&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// * 로 모든 값 조회 가능</span></span><br><span class="line">complexDF.select(<span class="string">&quot;complex.*&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> complex.<span class="operator">*</span> <span class="keyword">FROM</span> complexDF</span><br></pre></td></tr></table></figure>
</details>
</li>
<li><p>배열</p>
<ul>
<li>example) 해당하는 컬럼의 모든 단어를 하나의 로우로 변환</li>
<li><code>split(target, delimiter)</code> : 구분자 기준으로 나누어 배열로 변환<ul>
<li>복합 데이터 타입을 또 다른 컬럼처럼 다룰 수 있는 기능</li>
</ul>
</li>
<li>배열의 길이 : 배열 size 조회해서 길이 알 수 있음</li>
<li><code>array_contains()</code> : 배열에 특정 값 존재하는지 확인 가능<ul>
<li>하지만 시나리오 완성은 불가능</li>
</ul>
</li>
<li><code>explode(배열타입 칼럼)</code> : 인수의 컬럼 배열 갑셍 포함된 모든 값을 로우로 변환 (나머지 컬럼 값은 중복되어 표시)</li>
</ul>
<details><summary class="point-color-can-hover">[6.8.2] 예제 펼치기 - 배열</summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// split() : 배열로 변환</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.split</span><br><span class="line">df.select(split(col(<span class="string">&quot;Description&quot;</span>), <span class="string">&quot; &quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +---------------------+</span></span><br><span class="line"><span class="comment">// |split(Description,  )|</span></span><br><span class="line"><span class="comment">// +---------------------+</span></span><br><span class="line"><span class="comment">// | [WHITE, HANGING, ...|</span></span><br><span class="line"><span class="comment">// | [WHITE, METAL, LA...|</span></span><br><span class="line"><span class="comment">// +---------------------+</span></span><br><span class="line"></span><br><span class="line">(df.select(split(col(<span class="string">&quot;Description&quot;</span>), <span class="string">&quot; &quot;</span>).alias(<span class="string">&quot;array_col&quot;</span>))</span><br><span class="line">  .selectExpr(<span class="string">&quot;array_col[0]&quot;</span>).show(<span class="number">2</span>))</span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |array_col[0]|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |       WHITE|</span></span><br><span class="line"><span class="comment">// |       WHITE|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// size() : 배열의 길이</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.size</span><br><span class="line">df.select(size(split(col(<span class="string">&quot;Description&quot;</span>), <span class="string">&quot; &quot;</span>))).show(<span class="number">2</span>) <span class="comment">// shows 5 and 3</span></span><br><span class="line"><span class="comment">// +---------------------------+</span></span><br><span class="line"><span class="comment">// |size(split(Description,  ))|</span></span><br><span class="line"><span class="comment">// +---------------------------+</span></span><br><span class="line"><span class="comment">// |                          5|</span></span><br><span class="line"><span class="comment">// |                          3|</span></span><br><span class="line"><span class="comment">// +---------------------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// array_contains() : 배열에 특정값 존재하는지 확인</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.array_contains</span><br><span class="line">df.select(array_contains(split(col(<span class="string">&quot;Description&quot;</span>), <span class="string">&quot; &quot;</span>), <span class="string">&quot;WHITE&quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +--------------------------------------------+</span></span><br><span class="line"><span class="comment">// |array_contains(split(Description,  ), WHITE)|</span></span><br><span class="line"><span class="comment">// +--------------------------------------------+</span></span><br><span class="line"><span class="comment">// |                                        true|</span></span><br><span class="line"><span class="comment">// |                                        true|</span></span><br><span class="line"><span class="comment">// +--------------------------------------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// explode() : 입력된 컬럼의 배열값(split(Description) 결과물) 에 포함된 모든 값을 로우로 변환 (나머지값(InvoiceNo)은 중복)</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;split, explode&#125;</span><br><span class="line">(df.withColumn(<span class="string">&quot;splitted&quot;</span>, split(col(<span class="string">&quot;Description&quot;</span>), <span class="string">&quot; &quot;</span>))</span><br><span class="line">  .withColumn(<span class="string">&quot;exploded&quot;</span>, explode(col(<span class="string">&quot;splitted&quot;</span>)))</span><br><span class="line">  .select(<span class="string">&quot;Description&quot;</span>, <span class="string">&quot;InvoiceNo&quot;</span>, <span class="string">&quot;exploded&quot;</span>).show(<span class="number">2</span>))</span><br><span class="line"><span class="comment">// +--------------------+---------+--------+</span></span><br><span class="line"><span class="comment">// |         Description|InvoiceNo|exploded|</span></span><br><span class="line"><span class="comment">// +--------------------+---------+--------+</span></span><br><span class="line"><span class="comment">// |WHITE HANGING HEA...|   536365|   WHITE|</span></span><br><span class="line"><span class="comment">// |WHITE HANGING HEA...|   536365| HANGING|</span></span><br><span class="line"><span class="comment">// +--------------------+---------+--------+</span></span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- split()</span></span><br><span class="line"><span class="keyword">SELECT</span> split(Description, <span class="string">&#x27; &#x27;</span>) <span class="keyword">FROM</span> dfTable</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> split(Description, <span class="string">&#x27; &#x27;</span>)[<span class="number">0</span>] <span class="keyword">FROM</span> dfTable</span><br><span class="line"></span><br><span class="line"><span class="comment">-- array_contains()</span></span><br><span class="line"><span class="keyword">SELECT</span> array_contains(split(Description, <span class="string">&#x27; &#x27;</span>), <span class="string">&#x27;WHITE&#x27;</span>) <span class="keyword">FROM</span> dfTable</span><br><span class="line"></span><br><span class="line"><span class="comment">-- explode()</span></span><br><span class="line"><span class="keyword">SELECT</span> Description, InvoiceNo, exploded</span><br><span class="line"><span class="keyword">FROM</span> (<span class="keyword">SELECT</span> <span class="operator">*</span>, split(Description, &quot; &quot;) <span class="keyword">as</span> splitted <span class="keyword">FROM</span> dfTable)</span><br><span class="line"><span class="keyword">LATERAL</span> <span class="keyword">VIEW</span> explode(splitted) <span class="keyword">as</span> exploded</span><br></pre></td></tr></table></figure>
</details>
</li>
<li><p>맵</p>
<ul>
<li><code>map()</code> + 키-값 쌍</li>
<li>적합한 키로 데이터 조회 가능, 없을 시 null 반환</li>
<li>map 타입 분해 -&gt; 컬럼 변환 가능</li>
</ul>
<details><summary class="point-color-can-hover">[6.8.3] 예제 펼치기 - 맵</summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.map</span><br><span class="line">df.select(map(col(<span class="string">&quot;Description&quot;</span>), col(<span class="string">&quot;InvoiceNo&quot;</span>)).alias(<span class="string">&quot;complex_map&quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +--------------------+</span></span><br><span class="line"><span class="comment">// |         complex_map|</span></span><br><span class="line"><span class="comment">// +--------------------+</span></span><br><span class="line"><span class="comment">// |[WHITE HANGING HE...|</span></span><br><span class="line"><span class="comment">// |[WHITE METAL LANT...|</span></span><br><span class="line"><span class="comment">// +--------------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 키로 데이터 조회 (없을 시 null 반환)</span></span><br><span class="line">(df.select(map(col(<span class="string">&quot;Description&quot;</span>), col(<span class="string">&quot;InvoiceNo&quot;</span>)).alias(<span class="string">&quot;complex_map&quot;</span>))</span><br><span class="line">  .selectExpr(<span class="string">&quot;complex_map[&#x27;WHITE METAL LANTERN&#x27;]&quot;</span>).show(<span class="number">2</span>))</span><br><span class="line"><span class="comment">// +--------------------------------+</span></span><br><span class="line"><span class="comment">// |complex_map[WHITE METAL LANTERN]|</span></span><br><span class="line"><span class="comment">// +--------------------------------+</span></span><br><span class="line"><span class="comment">// |                            null|</span></span><br><span class="line"><span class="comment">// |                          536365|</span></span><br><span class="line"><span class="comment">// +--------------------------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// map 타입 분해 -&gt; 컬럼으로 변환 가능</span></span><br><span class="line">(df.select(map(col(<span class="string">&quot;Description&quot;</span>), col(<span class="string">&quot;InvoiceNo&quot;</span>)).alias(<span class="string">&quot;complex_map&quot;</span>))</span><br><span class="line">  .selectExpr(<span class="string">&quot;explode(complex_map)&quot;</span>).show(<span class="number">2</span>))</span><br><span class="line"><span class="comment">// +--------------------+------+</span></span><br><span class="line"><span class="comment">// |                 key| value|</span></span><br><span class="line"><span class="comment">// +--------------------+------+</span></span><br><span class="line"><span class="comment">// |WHITE HANGING HEA...|536365|</span></span><br><span class="line"><span class="comment">// | WHITE METAL LANTERN|536365|</span></span><br><span class="line"><span class="comment">// +--------------------+------+</span></span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> map(Description, InvoiceNo) <span class="keyword">as</span> complex_map <span class="keyword">FROM</span> dfTable</span><br><span class="line"><span class="keyword">WHERE</span> Description <span class="keyword">IS</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span></span><br></pre></td></tr></table></figure>
</details>

</li>
</ul>
<h3 id="6-10-JSON-다루기"><a href="#6-10-JSON-다루기" class="headerlink" title="6.10 JSON 다루기"></a>6.10 JSON 다루기</h3><ul>
<li>스파크에서 JSON 데이터 다루기 위한 고유 기능 제공<ul>
<li>문자열 형태 JSON 조작, JSON 파싱, JSON 객체로 변환 등</li>
</ul>
</li>
<li><code>get_json_object()</code><ul>
<li>JSON 객체 (딕셔너리, 배열) 인라인 쿼리로 조회 가능</li>
<li>중첩 없는 단일 JSON일 시, <code>json_tuble</code> 사용 가능</li>
</ul>
</li>
<li><code>to_json()</code> : StructType -&gt; JSON 문자열. 데이터 소스와 동일한 형태의 딕셔너리(맵) 인자로 사용 가능</li>
<li><code>from_json()</code> : JSON 문자열 -&gt; 객체. 단 스키마 지정 필수 (option. 맵 데이터 타입 옵션)</li>
</ul>
<details><summary class="point-color-can-hover">[6.10] 예제 펼치기</summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// JSON 컬럼 생성</span></span><br><span class="line"><span class="keyword">val</span> jsonDF = spark.range(<span class="number">1</span>).selectExpr(<span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">  &#x27;&#123;&quot;</span>myJSONK<span class="string">ey&quot; : &#123;&quot;</span>myJSONV<span class="string">alue&quot; : [1, 2, 3]&#125;&#125;&#x27; as jsonString&quot;</span><span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// get_json_object() 로 JSON 객체 조회</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;get_json_object, json_tuple&#125;</span><br><span class="line">jsonDF.select(</span><br><span class="line">    get_json_object(col(<span class="string">&quot;jsonString&quot;</span>), <span class="string">&quot;$.myJSONKey.myJSONValue[1]&quot;</span>) as <span class="string">&quot;column&quot;</span>,</span><br><span class="line">    json_tuple(col(<span class="string">&quot;jsonString&quot;</span>), <span class="string">&quot;myJSONKey&quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// =&gt; SQL 사용한 처리 (동일 표현)</span></span><br><span class="line">jsonDF.selectExpr(</span><br><span class="line">  <span class="string">&quot;get_json_object(jsonString, &#x27;$.myJSONKey.myJSONValue[1]&#x27;) as column&quot;</span>, </span><br><span class="line">  <span class="string">&quot;json_tuple(jsonString, &#x27;myJSONKey&#x27;)&quot;</span>).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +------+--------------------+</span></span><br><span class="line"><span class="comment">// |column|                  c0|</span></span><br><span class="line"><span class="comment">// +------+--------------------+</span></span><br><span class="line"><span class="comment">// |     2|&#123;&quot;myJSONValue&quot;:[1...|</span></span><br><span class="line"><span class="comment">// +------+--------------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// to_json() : StructType -&gt; JSON 문자열</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.to_json</span><br><span class="line">(df.selectExpr(<span class="string">&quot;(InvoiceNo, Description) as myStruct&quot;</span>)</span><br><span class="line">  .select(to_json(col(<span class="string">&quot;myStruct&quot;</span>))))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// from_json() : JSON 문자열 -&gt; 객체</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.from_json</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"><span class="keyword">val</span> parseSchema = <span class="keyword">new</span> <span class="type">StructType</span>(<span class="type">Array</span>(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">StructField</span>(<span class="string">&quot;InvoiceNo&quot;</span>,<span class="type">StringType</span>,<span class="literal">true</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="type">StructField</span>(<span class="string">&quot;Description&quot;</span>,<span class="type">StringType</span>,<span class="literal">true</span>)))</span><br><span class="line">(df.selectExpr(<span class="string">&quot;(InvoiceNo, Description) as myStruct&quot;</span>)</span><br><span class="line">  .select(to_json(col(<span class="string">&quot;myStruct&quot;</span>)).alias(<span class="string">&quot;newJSON&quot;</span>))</span><br><span class="line">  .select(from_json(col(<span class="string">&quot;newJSON&quot;</span>), parseSchema), col(<span class="string">&quot;newJSON&quot;</span>)).show(<span class="number">2</span>))</span><br><span class="line"><span class="comment">// +----------------------+--------------------+</span></span><br><span class="line"><span class="comment">// |jsontostructs(newJSON)|             newJSON|</span></span><br><span class="line"><span class="comment">// +----------------------+--------------------+</span></span><br><span class="line"><span class="comment">// |  [536365, WHITE HA...|&#123;&quot;InvoiceNo&quot;:&quot;536...|</span></span><br><span class="line"><span class="comment">// |  [536365, WHITE ME...|&#123;&quot;InvoiceNo&quot;:&quot;536...|</span></span><br><span class="line"><span class="comment">// +----------------------+--------------------+</span></span><br></pre></td></tr></table></figure>
</details>

<h3 id="6-11-사용자-정의-함수"><a href="#6-11-사용자-정의-함수" class="headerlink" title="6.11 사용자 정의 함수"></a>6.11 사용자 정의 함수</h3><ul>
<li><p><strong>사용자 정의 함수</strong> (user defined function, <strong>UDF</strong>)</p>
<ul>
<li>스파크의 가장 강력한 기능 중 하나</li>
<li>파이썬, 스칼라, 외부 라이브러리등 사용 =&gt; 사용자가 원하는 형태로 트랜스포메이션 생성</li>
</ul>
</li>
<li><p>특징</p>
<ul>
<li>하나 이상의 컬럼을 입력/반환 가능</li>
<li>스파크 UDF는 다양한 언어로 개발 가능</li>
<li>레코드 별로 데이터를 처리하는 함수이므로, 독특하거나 도메인 특화 (DSL) 언어 사용 X</li>
<li>=&gt; 기본적으로 특정 SparkSession이나 Context에서 사용할 수 있게 <u>임시 함수 형태로 등록</u></li>
</ul>
</li>
<li><p>다양한 언어로 UDF 개발 가능</p>
<ul>
<li>그러나 언어별로 성능에 영향 존재<ul>
<li>예제 참고</li>
<li>함수를 만들고 모든 워커 노드에서 해당 함수를 사용할 수 있도록 스파크에 등록<ul>
<li>스파크는 드라이버에서 함수 직렬화 -&gt; 네트워크 통해서 모든 익스큐터 프로세스로 전달</li>
<li>(언어에 관계없이 발생하는 과정)</li>
</ul>
</li>
<li>함수를 개발한 언어에 따라 기본적으로 동작하는 방식이 달라짐<ul>
<li>애초에 스칼라, 자바 사용 시 JVM 환경에서만 사용 가능<ul>
<li>스파크 내장함수 장점 사용 X =&gt; 성능 ↓</li>
<li>많은 객체 생성시에도 성능 문제</li>
</ul>
</li>
<li>파이썬 사용 시 모든 데이터를 직렬화하고, 파이썬 프로세스에 있는 데이터의 로우마다 함수 실행 및 JVM과 스파크에 처리 결과를 반환<ul>
<li>일단 직렬화 과정에서 큰 부하 발생</li>
<li>데이터가 파이썬으로 전달되면 스파크에서 워커 메모리 관리의 어려움</li>
</ul>
</li>
</ul>
</li>
<li>=&gt; 따라서 사용자 정의 함수는 <strong>자바나 스칼라로 작성</strong> 을 권장</li>
</ul>
</li>
</ul>
</li>
<li><p>기본은 사용자 정의 함수(UTF)는 DataFrame에서만 사용 가능 (문자열 표현식 X)</p>
<ul>
<li><strong>스파크 SQL 함수 등록하면?</strong></li>
<li>=&gt; 모든 프로그래밍 언어와 SQL 에서 사용자 정의 함수 사용 가능<ul>
<li>파이썬에서도 우회적으로 사용 가능하지만 DataFrame 함수 대신 SQL 표현식으로 사용해야함</li>
<li>스파크는 자체 데이터 타입(파이썬X)을 사용하므로 <strong>변환 타입 지정 권!장!</strong></li>
<li>반환될 타입과 다른 데이터 타입 지정시 =&gt; null 반환</li>
</ul>
</li>
</ul>
</li>
<li><p>사용자 정의 함수에서 선택적 값 반환</p>
<ul>
<li>파이썬 = <code>None</code> / 스칼라 = <code>Option</code> 반환</li>
</ul>
<details><summary class="point-color-can-hover">[6.11] 예제 펼치기</summary>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> udfExampleDF = spark.range(<span class="number">5</span>).toDF(<span class="string">&quot;num&quot;</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">power3</span></span>(number:<span class="type">Double</span>):<span class="type">Double</span> = number * number * number</span><br><span class="line">power3(<span class="number">2.0</span>) <span class="comment">// 8.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// UDF 실행 + 함수 등록 및 사용 (=&gt; DataFrame에서 사용 가능)</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.udf</span><br><span class="line"><span class="keyword">val</span> power3udf = udf(power3(_:<span class="type">Double</span>):<span class="type">Double</span>)</span><br><span class="line"></span><br><span class="line">udfExampleDF.select(power3udf(col(<span class="string">&quot;num&quot;</span>))).show()</span><br><span class="line"><span class="comment">// +--------+</span></span><br><span class="line"><span class="comment">// |UDF(num)|</span></span><br><span class="line"><span class="comment">// +--------+</span></span><br><span class="line"><span class="comment">// |     0.0|</span></span><br><span class="line"><span class="comment">// |     1.0|</span></span><br><span class="line"><span class="comment">// |     8.0|</span></span><br><span class="line"><span class="comment">// |    27.0|</span></span><br><span class="line"><span class="comment">// |    64.0|</span></span><br><span class="line"><span class="comment">// +--------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// UDF를 스파크 SQL로 등록하면 =&gt; 모든 프로그래밍 언어, SQL 에서 사용 가능 (+문자열 표현식)</span></span><br><span class="line">spark.udf.register(<span class="string">&quot;power3&quot;</span>, power3(_:<span class="type">Double</span>):<span class="type">Double</span>)</span><br><span class="line">udfExampleDF.selectExpr(<span class="string">&quot;power3(num)&quot;</span>).show(<span class="number">2</span>)</span><br><span class="line">+-------------------------------+</span><br><span class="line">|<span class="type">UDF</span>:power3(cast(num as double))|</span><br><span class="line">+-------------------------------+</span><br><span class="line">|                            <span class="number">0.0</span>|</span><br><span class="line">|                            <span class="number">1.0</span>|</span><br><span class="line">+-------------------------------+</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%spark.pyspark</span><br><span class="line">udfExampleDF.selectExpr(<span class="string">&quot;power3(num)&quot;</span>).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment"># =&gt; Scala로 등록된 UDF 사용</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 반환 데이터타입이 Integer인데 DoubeType() 으로 변환 시 =&gt; null 반환</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> IntegerType, DoubleType</span><br><span class="line">spark.udf.register(<span class="string">&quot;power3py&quot;</span>, power3, DoubleType())</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL 에서도 등록된 UDF 사용 가능</span></span><br><span class="line"><span class="keyword">SELECT</span> power3(<span class="number">12</span>), power3py(<span class="number">12</span>) <span class="comment">-- 반환 데이터 타입 문제로 동작하지 않음</span></span><br></pre></td></tr></table></figure>
</details>


</li>
</ul>
<h3 id="6-12-Hive-UDF"><a href="#6-12-Hive-UDF" class="headerlink" title="6.12 Hive UDF"></a>6.12 Hive UDF</h3><ul>
<li><p>하이브 문법을 사용해서 만든 함수 =&gt; <strong>UDF</strong>, <strong>UDAF</strong> 사용 가능</p>
<ul>
<li>UDF (User Defined Function)</li>
<li>UDAF (User Defined Aggregate Function)</li>
</ul>
</li>
<li><p>단, 하이브 지원 기능 활성화 필요</p>
<ul>
<li>=&gt; <code>SparkSession.builder().enableHiveSupport()</code> 명시</li>
<li>하이브 지원 활성화 되면 SQL로 UDF 등록 가능</li>
<li>사전에 컴파일된 스칼라, 자바 패키지만 지원 (라이브러리 의존성 명시 필요)</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- TEMPORARY 키워드 제거 시 =&gt; 하이브 메타스토어에 영구(permanent) 함수로 등록</span></span><br><span class="line"><span class="keyword">CREATE</span> TEMPORARY <span class="keyword">FUNCTION</span> myFunc <span class="keyword">AS</span> <span class="string">&#x27;com.organization.hive.udf.FunctionName&#x27;</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="6-13-정리"><a href="#6-13-정리" class="headerlink" title="6.13 정리"></a>6.13 정리</h3><ul>
<li>스파크 SQL을 사용목적에 맞게 확장하는 방식<ul>
<li>간단한 함수만으로도 확장 가능 (DSL X)</li>
</ul>
</li>
<li>스파크 SQL은 복잡한 비즈니스 로직 구현에 사용할 수 있는 강력한 기능</li>
</ul>
<h3 id="📒-단어장"><a href="#📒-단어장" class="headerlink" title="📒 단어장"></a>📒 단어장</h3></div><p class="post-tags"><i class="fa fa-tags" aria-hidden="true"></i><a href="/tags/book/">#book</a><a href="/tags/study/">#study</a><a href="/tags/spark/">#spark</a><a href="/tags/apache/">#apache</a></p></article></div><footer><div class="paginator"><a class="prev" href="/2021/02/03/Spark-The-Definitive-Guide-7%EC%9E%A5/">PREV</a><a class="next" href="/2021/01/26/Spark-The-Definitive-Guide-5%EC%9E%A5/">NEXT</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'minsw-github-io';
var disqus_identifier = '2021/02/02/Spark-The-Definitive-Guide-6장/';
var disqus_title = '&amp;#039;Spark The Definitive Guide&amp;#039; 6장 - 데이터 타입 (비)공식 가이드북';
var disqus_url = 'https://minsw.github.io/2021/02/02/Spark-The-Definitive-Guide-6장/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//#{theme.disqus}.disqus.com/count.js" async></script><div class="copyright"><p>© 2018 - 2021 <a href="https://minsw.github.io">Lukka Min</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-143001954-1",'auto');ga('send','pageview');</script><link rel="stylesheet" href="//cdn.datatables.net/1.10.7/css/jquery.dataTables.min.css" media="screen" type="text/css"><script src="//cdn.datatables.net/1.10.7/js/jquery.dataTables.min.js"></script><script>$(function(){$('.datatable').dataTable( {"order": [[ 0, "desc" ]],"iDisplayLength": -1,"lengthMenu": [[10, 25, 50, -1], [10, 25, 50, "All"]]} );});</script></body></html>