<!DOCTYPE html><html lang="ko"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> &#039;Spark The Definitive Guide&#039; 12장 - RDD · Look out</title><meta name="description" content="&amp;#039;Spark The Definitive Guide&amp;#039; 12장 - RDD - Lukka Min"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/cover.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"><link rel="search" type="application/opensearchdescription+xml" href="https://minsw.github.io/atom.xml" title="Look out"><script src="//code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script><meta name="generator" content="Hexo 5.3.0"><link rel="alternate" href="/feed.xml" title="Look out" type="application/atom+xml">
</head><body><div class="wrap"><header><a class="logo-link" href="/"><img src="/cover.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a class="nav-list-link" href="/" target="_self">BLOG</a></li><li class="nav-list-item"><a class="nav-list-link" href="/archives/" target="_self">ARCHIVE</a></li><li class="nav-list-item"><a class="nav-list-link" href="/tags/" target="_self">TAG</a></li><li class="nav-list-item"><a class="nav-list-link" href="https://github.com/minSW" target="_blank">GITHUB</a></li></ul></header><main class="container"></main><div class="post"><article class="post-block"><h1 class="post-title">&#039;Spark The Definitive Guide&#039; 12장 - RDD</h1><div class="post-info">2021년 3월 7일<span class="categories"><i class="fa fa-bookmark" aria-hidden="true"></i></span><a class="post-category" href="/categories/spark/">#spark</a></div><div class="post-content"><br/>

<h4 id="📌-Part-3-저수준-API"><a href="#📌-Part-3-저수준-API" class="headerlink" title="📌 [ Part 3 저수준 API ]"></a>📌 [ Part 3 저수준 API ]</h4><blockquote>
<p>대부분의 상황에서는 구조적 API 사용이 좋음 (Part 2)</p>
<p>그러나 이러한 고수준(hive-level) API로 처리할 수 없는 비즈니스나 기술적 문제는? =&gt; <U><strong>저수준 API</strong></U> 사용</p>
<ul>
<li>RDD</li>
<li>SparkContext</li>
<li>분산형 공유 변수 (distributed shared variables)<ul>
<li>어큐뮬레이터 (accumulator)</li>
<li>브로드캐스트 변수 (broadcast variable)</li>
</ul>
</li>
</ul>
</blockquote>
<br/>

<img src="https://user-images.githubusercontent.com/26691216/110234092-04e14600-7f6c-11eb-966f-153324981b5c.jpg" width=300/>

<center><h2>_ _ _</h2></center>

<br/>

<hr>
<h1 id="CHAPTER-12-RDD"><a href="#CHAPTER-12-RDD" class="headerlink" title="CHAPTER 12 RDD"></a>CHAPTER 12 RDD</h1><h3 id="12-1-저수준-API란"><a href="#12-1-저수준-API란" class="headerlink" title="12.1 저수준 API란"></a>12.1 저수준 API란</h3><ul>
<li>스파크의 저수준 API 종류 2가지<ul>
<li>분산 데이터 처리를 위한 <strong>RDD</strong></li>
<li><strong>분산형 공유 변수</strong>를 배포하고 다루기위한 API </li>
</ul>
</li>
<li>When?<ul>
<li>고수준 API에서 제공하지 않는 기능이 필요한 경우<ul>
<li>ex. 클러스터의 물리적 데이터 배치를 세밀하게 제어</li>
</ul>
</li>
<li>RDD를 사용해 개발된 기존 코드 유지</li>
<li>사용자가 정의한 공유 변수를 다뤄야하는 경우 (=&gt; 14장)</li>
</ul>
</li>
<li>장점<ul>
<li>스파크의 모든 워크로드는 저수준 기능을 사용하는 기초 형태로 컴파일됨 =&gt; 이해하여 워크로드 디버깅 작업에 도움</li>
<li>세밀한 제어 방법 제공 (치명적인 실수 방지)</li>
</ul>
</li>
<li>How?<ul>
<li>저수준 API 기능 사용의 진입 지점 =&gt; <code>SparkContext</code></li>
<li>자세한 내용은 15장에서<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 스파크 연산 수행 시 필요한 도구 <span class="symbol">&#x27;SparkSessio</span>n&#x27; 으로 접근 가능</span><br><span class="line">spark.sparkContext</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<h3 id="12-2-RDD-개요"><a href="#12-2-RDD-개요" class="headerlink" title="12.2 RDD 개요"></a>12.2 RDD 개요</h3><ul>
<li>RDD는 스파크 1.x 의 핵심 API <ul>
<li>2.x 부터는 잘 사용안하지만, 모든 DataFrame/Dataset 코드는 RDD로 컴파일</li>
<li>스파크 UI에서도 RDD단위로 잡 수행</li>
</ul>
</li>
<li><strong>RDD</strong> 란<ul>
<li>불변성을 가지고 병렬로 처리할 수 있는 파티셔닝된 레코드의 모음<ul>
<li>DataFrame의 레코드 : 스키마를 알고 있는 필드로 구성된 구조화된 로우</li>
<li>RDD의 레코드 : 그저 프로그래머가 선택하는 자바/스칼라/파이썬의 객체</li>
</ul>
</li>
<li>RDD의 모든 레코드는 객체이므로 완벽하게 제어 가능<ul>
<li>개발자가 강력한 제어권을 가짐</li>
<li>모든 값을 다루거나 값 사이 상호작용에 대해 반드시 수동 정의 필요</li>
<li>=&gt; <U><em>“Reinvent the wheel”</em></U></li>
</ul>
</li>
<li>내부 구조를 파악할 수 없음 (구조적 API는 가능)<ul>
<li>최적화에 많은 수작업 요구</li>
</ul>
</li>
<li>=&gt; <strong>결론은 스파크 구조적 API 사용을 강력하게 권고</strong></li>
</ul>
</li>
<li>RDD API 는 Dataset과 유사<ul>
<li>단, RDD는 구조화된 데이터 엔진을 사용해서 데이터를 저장하거나 다루지 X</li>
<li>RDD &lt;-&gt; Dataset 전환 쉬움</li>
<li>두 API의 각각의 장점 동시에 활용 가능</li>
</ul>
</li>
</ul>
<h4 id="RDD-유형"><a href="#RDD-유형" class="headerlink" title="RDD 유형"></a>RDD 유형</h4><ul>
<li>수많은 하위 클래스 존재<ul>
<li>대부분 DataFrame API의 최적화된 물리적 실행 계획 만드는데 사용</li>
</ul>
</li>
<li>RDD 타입 2 가지 (객체의 컬렉션 표현)<ul>
<li>제네릭 RDD 타입</li>
<li>키-값 RDD (특수연산 + 키를 이용한 사용자 지정 파티셔닝 개념 포함)</li>
</ul>
</li>
<li>RDD 의 주요 속성 5가지<ul>
<li>파티션의 목록</li>
<li>각 조각을 연산하는 함수</li>
<li>다른 RDD와의 의존성 목록</li>
<li>부가적으로 키-값 RDD를 위한 Partitioner (ex. RDD는 hash partitioned)</li>
<li>부가적으로 각 조각을 연산하기 위한 기본 위치 목록 (Ex. 하둡 분산 파일 시스템의 블록 위치)</li>
</ul>
</li>
<li>각 속성은 사용자 프로그램을 스케쥴링, 실행 하는 스파크의 모든 처리방식을 결정<ul>
<li>속성 구현체로 새로운 데이터 소스 정의도 가능</li>
<li><strong>Partitioner</strong> 는 RDD 사용하는 주된 이유 중 하나<ul>
<li>올바른 사용자 정의 Partitioner =&gt; 성능, 안정성 ↑</li>
<li>자세한 내용은 13장</li>
</ul>
</li>
</ul>
</li>
<li>RDD는 스파크 프로그래밍 패러다임을 그대로 따른다<ul>
<li>지연 처리 방식의 <U>트랜스포메이션</U>, 즉시 실행 방식의 <U>액션</U> 제공</li>
<li>DataFrame, Dataset과 동일한 방식으로 동작하지만 <strong>‘로우’ 개념이 없음</strong><ul>
<li>개별 레코드는 객체일뿐 (구조적 API가 제공하는 함수들의 동작을 수동으로 처리해야함)</li>
</ul>
</li>
</ul>
</li>
<li>RDD API는 스칼라, 자바, 파이썬 모두 사용 가능<ul>
<li>스칼라, 자바 =&gt; 대부분 비슷한 성능 (단, 원형 객체 사용시 성능 ↓)</li>
<li>파이썬 =&gt; 상당한 성능 저하 (높은 오버헤드)</li>
</ul>
</li>
</ul>
<h4 id="RDD는-언제-사용할까"><a href="#RDD는-언제-사용할까" class="headerlink" title="RDD는 언제 사용할까"></a>RDD는 언제 사용할까</h4><ul>
<li>A. 물리적으로 분산된 데이터 (자체 구성한 데이터 파티셔닝) 에 세부적인 제어가 필요할 때 =&gt; RDD 사용</li>
<li>하지만 정말 필요한 경우 빼고는 수동 RDD 생성 비권장<ul>
<li>구조적 API의 최적화 기법 사용 X</li>
<li>DataFrame이 훨씬 효율적, 안정적, 좋은 표현력</li>
</ul>
</li>
</ul>
<h4 id="Dataset과-RDD의-케이스-클래스"><a href="#Dataset과-RDD의-케이스-클래스" class="headerlink" title="Dataset과 RDD의 케이스 클래스"></a>Dataset과 RDD의 케이스 클래스</h4><ul>
<li>Dataset vs 케이스 클래스로 만들어진 RDD ?</li>
<li>Dataset은 구조적 API가 제공하는 풍부한 기능과 최적화 기법 제공<ul>
<li>JVM 데이터 타입 / 스파크 데이터 타입 중 고민 필요 X (성능 동일)</li>
</ul>
</li>
</ul>
<h3 id="12-3-RDD-생성하기"><a href="#12-3-RDD-생성하기" class="headerlink" title="12.3 RDD 생성하기"></a>12.3 RDD 생성하기</h3><ul>
<li><p>DataFrame, Dataset으로 RDD 생성하기</p>
<ul>
<li>가장 쉬운 방법</li>
<li>DataFrame이나 Dataset의 <code>rdd()</code> 호출<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// (Scala) Dataset[Long] =&gt; RDD[Long] 변환</span></span><br><span class="line">spark.range(<span class="number">500</span>).rdd</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># (python) Row타입의 RDD 반환 (Python은 Dataset X)</span></span><br><span class="line">sprak.<span class="built_in">range</span>(<span class="number">10</span>).rdd</span><br></pre></td></tr></table></figure></li>
<li>Row 타입 : 스파크가 구조적 API에서 데이터를 표현하는 데 사용하는 내부 카탈리스트 포맷<ul>
<li>상황에 따라 구조적 API &lt;-&gt; 저수준 API</li>
</ul>
</li>
<li>Dataset API와 RDD API의 유사성</li>
</ul>
</li>
<li><p>로컬 컬렉션으로 RDD 생성하기</p>
<ul>
<li><code>sparkContext.parallelize()</code> : 컬렉션 객체 =&gt; RDD<ul>
<li>단일 노드에 있는 컬렉션을 병렬 컬렉션으로 전환</li>
<li>파티션 수 명시적 지정 가능</li>
</ul>
</li>
<li>RDD에 이름 지정 시 스파크 UI에 해당 이름으로 노출</li>
</ul>
</li>
<li><p>데이터소스로 RDD 생성하기</p>
<ul>
<li>DataSource API 사용<ul>
<li>데이터소스나 텍스트 파일을 이용한 직접 생성보다 바람직</li>
<li>데이터를 읽는 가장 좋은 방법</li>
</ul>
</li>
<li>RDD에는 DataFrame이 제공하는 ‘Datasource API’ 개념 X<ul>
<li><code>sparkContext</code> 를 사용하여 RDD로 읽기 가능</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="12-4-RDD-다루기"><a href="#12-4-RDD-다루기" class="headerlink" title="12.4 RDD 다루기"></a>12.4 RDD 다루기</h3><ul>
<li>DataFrame을 다루는 방식과 매우 유사</li>
<li>RDD vs DataFrame, Dataset<ul>
<li>스파크 데이터 타입이 아닌 자바, 스칼라 객체를 다룬다는 점이 가장 큰 차이</li>
<li>‘헬퍼’ 메서드(연산 단순화) 나 함수 부족</li>
<li>=&gt; 필터, 맵 함수, 집계 등의 다양한 함수를 직접 정의해야함</li>
</ul>
</li>
</ul>
<h3 id="12-5-트랜스포메이션"><a href="#12-5-트랜스포메이션" class="headerlink" title="12.5 트랜스포메이션"></a>12.5 트랜스포메이션</h3><ul>
<li>대부분의 RDD 트랜스 포메이션 =&gt; 구조적 API에서 사용 가능<ul>
<li>RDD에도 트랜스포메이션을 지정해서 새로운 RDD 생성 가능 + 의존성 정의 (like DataFrame, Dataset)</li>
</ul>
</li>
<li><code>distinct()</code> : RDD의 중복된 데이터 제거</li>
<li><code>filter()</code> : 조건 함수를 만족하는 레코드만 반환<ul>
<li>모든 로우는 어떤 경우라도 입력값을 가져야 함</li>
<li>함수는 각 레코드를 개별적으로 처리 후, 결과를 사용한 언어의 데이터 타입으로 반환한다 (like Dataset API) =&gt; 강제로 Row 타입으로 변환할 필요가 없어서?</li>
</ul>
</li>
<li><code>map()</code> : 주어진 입력을 원하는 값으로 반환하는 함수를 레코드 별로 적용<ul>
<li><code>flatMap()</code> : 단일 로우 → 여러 로우로 변환</li>
</ul>
</li>
<li><code>sortBy()</code> : 함수의 추출 값 기준으로 RDD 정렬</li>
<li><code>randomSplit()</code> : RDD를 임의로 분할해 배열로 만듦<ul>
<li>가중치, 난수시드(random seed) 배열을 파라미터로 사용</li>
</ul>
</li>
</ul>
<h3 id="12-6-액션"><a href="#12-6-액션" class="headerlink" title="12.6 액션"></a>12.6 액션</h3><ul>
<li>지정된 트랜스포메이션 연산을 시작하기 위해 ‘액션’ 사용<ul>
<li>데이터를 드라이버로 모으거나, 외부 데이터 소스로 내보내기 가능</li>
</ul>
</li>
<li><code>reduce()</code> : RDD의 모든 값을 하나로 만듦<ul>
<li>파티션에 대한 리듀스 연산은 비결정적 (not deterministic) =&gt; 결과가 매번 달라질 수 있음</li>
</ul>
</li>
<li><code>count()</code> : RDD의 전체 로우 수 반환<ul>
<li><code>countApprox()</code> : 제한시간 내에 count 근사치 계산 (confidence = 신뢰도 = 오차율)</li>
<li><code>countApproxDistinct()</code> 는 2가지 구현체 존재<ul>
<li>상대 정확도(relative accuracy)를 파라미터로 사용 : 더 많은 메모리 공간 사용. (설정값 &gt;= 0.000017)</li>
<li>일반 (regular) 데이터를 위한 파라미터 &amp; 희소표현을 위한 파라미터 사용</li>
</ul>
</li>
<li><code>countByValue()</code> : RDD값의 개수 반환<ul>
<li>연산결과가 메모리에 모두 적재되므로 결과가 작을때만 사용 </li>
</ul>
</li>
<li><code>countByValueApprox()</code> : 제한시간 내에 countByValue의 근사치 계산 (+ confidence)</li>
</ul>
</li>
<li><code>first()</code> : 첫 번째 값 반환</li>
<li><code>max()</code>, <code>min()</code> : 각각 최댓값, 최솟값 반환</li>
<li><code>take()</code> : 지정한 개수 만큼 값 반환<ul>
<li>먼저 하나의 파티션 스캔 후, 결과 수를 이용해 필요한 추가 파티션 수를 예측</li>
<li>다양한 유사함수 존재 (<code>takeOrdered()</code>, <code>takeSample()</code>, <code>top()</code> ..)</li>
</ul>
</li>
</ul>
<h3 id="12-7-파일-저장하기"><a href="#12-7-파일-저장하기" class="headerlink" title="12.7 파일 저장하기"></a>12.7 파일 저장하기</h3><ul>
<li>데이터 처리 결과를 일반 텍스트 파일로 쓰는 것<ul>
<li>RDD는 일반적 의미의 데이터소스에 ‘저장’ 할 수 없음<ul>
<li>각 파티션의 내용을 저장하려면 전체 파티션을 순회하면서 외부 데이터베이스에 저장</li>
<li>(== 고수준 API의 내부 처리 과정)</li>
</ul>
</li>
<li>스파크는 각 파티션의 데이터를 파일로 저장</li>
</ul>
</li>
<li><code>saveAsTextFile()</code> : 지정한 경로로 텍스트 파일 저장<ul>
<li>필요 시 압축 코덱 사용 가능 (코덱은 <a target="_blank" rel="noopener" href="https://bit.ly/2p3dnT3">org.apache.hadoop.io.compress</a> 참고)</li>
</ul>
</li>
<li>시퀀스 파일<ul>
<li>바이너리 키-값 쌍으로 구성된 플랫 파일</li>
<li>맵리듀스의 입출력 포맷으로 알려짐</li>
<li>=&gt; <code>saveAsObjectFile()</code>이나 명시적 키-값 쌍 데이터 저장 방식으로 시퀀스 파일 작성 가능</li>
</ul>
</li>
<li>하둡 파일<ul>
<li>데이터를 저장하는데 사용할 수 있는 여러 하둡 파일 포맷 존재</li>
<li>하둡 파일 포맷 사용 시 클래스, 출력 포맷, 하둡 설정, 압축 방식 지정 가능 (‘하둡 완벽 가이드’ 도서 참고)</li>
</ul>
</li>
</ul>
<h3 id="12-8-캐싱"><a href="#12-8-캐싱" class="headerlink" title="12.8 캐싱"></a>12.8 캐싱</h3><ul>
<li>DataFrame, Dataset 캐싱과 동일 원칙 적용<ul>
<li>RDD 는 캐시, 저장(persist) 가능<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">words.cache()</span><br></pre></td></tr></table></figure></li>
<li>기본적으로 캐시와 저장은 메모리에 있는 데이터가 대상</li>
<li><code>setName()</code> : 캐시된 RDD에 이름 지정 가능</li>
</ul>
</li>
<li>저장소 수준<ul>
<li>싱글턴 객체 <code>org.apache.spark.storage.StorageLevel</code> 의 속성(메모리, 디스크, 메모리+디스크 조합, off-heap) 중 하나로 지정 가능</li>
<li>저장소 수준은 <code>getStorageLevel()</code> 로 조회 가능</li>
</ul>
</li>
</ul>
<h3 id="12-9-체크포인팅"><a href="#12-9-체크포인팅" class="headerlink" title="12.9 체크포인팅"></a>12.9 체크포인팅</h3><ul>
<li><strong>체크포인팅 (checkpointing)</strong> : RDD =&gt; 디스크에 저장<ul>
<li>DataFrame API에서 사용할 수 없는 기능 중 하나</li>
<li>저장된 RDD 참조 시, 원본 데이터소스를 다시 계산해 디스크에 저장된 중간 결과 파티션을 참조 (RDD 생성 X)</li>
<li>캐싱과 유사 (단, 메모리가 아닌 디스크에 저장)</li>
<li>반복적인 연산 수행시 유용 (최적화 good)</li>
</ul>
</li>
<li><code>checkpoint()</code> 설정 =&gt; RDD 참조 시 데이터소스의 데이터 대신 체크포인트에 저장된 RDD 사용</li>
</ul>
<h3 id="12-10-RDD를-시스템-명령으로-전송하기"><a href="#12-10-RDD를-시스템-명령으로-전송하기" class="headerlink" title="12.10 RDD를 시스템 명령으로 전송하기"></a>12.10 RDD를 시스템 명령으로 전송하기</h3><ul>
<li><code>pipe()</code><ul>
<li>파이핑 요소로 생성된 RDD =&gt; 외부 프로세스로 전달</li>
<li>외부 프로세스는 파티션마다 한번씩 처리 =&gt; 결과 RDD 생성<ul>
<li>입력 파티션 : 각 입력 파티션의 모든 요소는 개행 문자 단위로 분할, 여러 줄의 입력 데이터로 변경 =&gt; 프로세스의 표준 입력 (stdin) 으로 전달</li>
<li>결과 파티션 =&gt; 프로세스의 표준 출력 (stdout) 에 전달. 표준 출력의 각 줄은 출력 파티션의 하나의 요소가 됨</li>
</ul>
</li>
<li>비어있는 파티션 처리 시에도 프로세스는 실행됨</li>
<li>사용자가 정의한 두 함수를 인수로 전달 시, 출력 방식을 원하는대로 변경 가능 (<a target="_blank" rel="noopener" href="https://bit.ly/2OhgRwd">API docs</a> 참고)</li>
</ul>
</li>
<li><code>mapPartitions()</code><ul>
<li>스파크는 실제 코드 실행 시 파티션 단위로 동작<ul>
<li><code>map()</code> 이 반환하는 RDD =&gt; <code>MapPartitionsRDD</code></li>
<li><U>map은 mapPartitions의 로우 단위 처리를 위한 별칭일 뿐이다</U></li>
</ul>
</li>
<li>mapPartitions는 개별 파티션(이터레이터로 표현) 에 대해 <code>map</code> 연산 수행 가능<ul>
<li>클러스터에서 물리적 단위로 개별 파티션을 처리하기 때문 (로우 단위 처리 X)</li>
</ul>
</li>
<li>기본적으로 파티션 단위로 작업 수행<ul>
<li>=&gt; <strong>전체</strong> 파티션에 대한 연산 수행 가능</li>
<li>RDD의 전체 하위 데이터셋에 원하는 연산 수행 가능</li>
</ul>
</li>
<li><code>mapPartitionsWithIndex()</code> : 인덱스 (파티션 범위의 인덱스, RDD의 파티션 번호) 와 파티션의 모든 아이템을 순회하는 이터레이터를 가진 함수를 인수로 사용</li>
</ul>
</li>
<li><code>foreachPartition()</code><ul>
<li>개별 파티션에서 특정 작업을 수행하는데 적합</li>
<li><code>mapPartitions()</code> vs <code>foreachPartition()</code><ul>
<li>mapPartitions 는 처리 결과 반환, foreachPartition 는 결과 반환 X (순회만)</li>
</ul>
</li>
</ul>
</li>
<li><code>glom()</code><ul>
<li>데이터셋의 모든 파티션 =&gt; 배열로 변환</li>
<li>데이터를 드라이버로 모으고, 데이터가 존재하는 파티션의 배열이 필요한 경우 유용</li>
<li>(단, 파티션이 크거나 많으면 드라이버가 비정상적으로 종료)</li>
</ul>
</li>
</ul>
<h3 id="12-11-정리"><a href="#12-11-정리" class="headerlink" title="12.11 정리"></a>12.11 정리</h3><ul>
<li>단일 RDD 처리 방법</li>
<li>RDD API의 기초</li>
</ul>
<h3 id="📒-단어장"><a href="#📒-단어장" class="headerlink" title="📒 단어장"></a>📒 단어장</h3></div><p class="post-tags"><i class="fa fa-tags" aria-hidden="true"></i><a href="/tags/study/">#study</a><a href="/tags/book/">#book</a><a href="/tags/spark/">#spark</a><a href="/tags/apache/">#apache</a></p></article></div><footer><div class="paginator"><a class="next" href="/2021/03/01/Spark-The-Definitive-Guide-11%EC%9E%A5/">NEXT</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'minsw-github-io';
var disqus_identifier = '2021/03/07/Spark-The-Definitive-Guide-12장/';
var disqus_title = '&amp;#039;Spark The Definitive Guide&amp;#039; 12장 - RDD';
var disqus_url = 'https://minsw.github.io/2021/03/07/Spark-The-Definitive-Guide-12장/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//#{theme.disqus}.disqus.com/count.js" async></script><div class="copyright"><p>© 2018 - 2021 <a href="https://minsw.github.io">Lukka Min</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-143001954-1",'auto');ga('send','pageview');</script><link rel="stylesheet" href="//cdn.datatables.net/1.10.7/css/jquery.dataTables.min.css" media="screen" type="text/css"><script src="//cdn.datatables.net/1.10.7/js/jquery.dataTables.min.js"></script><script>$(function(){$('.datatable').dataTable( {"order": [[ 0, "desc" ]],"iDisplayLength": -1,"lengthMenu": [[10, 25, 50, -1], [10, 25, 50, "All"]]} );});</script></body></html>