<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Look out</title>
  
  
  <link href="https://minsw.github.io/feed.xml" rel="self"/>
  
  <link href="https://minsw.github.io/"/>
  <updated>2021-06-15T04:56:43.501Z</updated>
  <id>https://minsw.github.io/</id>
  
  <author>
    <name>Lukka Min</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>&amp;#039;데이터 중심 어플리케이션 설계&amp;#039; 10장 - 일괄 처리</title>
    <link href="https://minsw.github.io/2021/06/15/Designing-Data-Intensive-Applications-10/"/>
    <id>https://minsw.github.io/2021/06/15/Designing-Data-Intensive-Applications-10/</id>
    <published>2021-06-15T04:45:17.000Z</published>
    <updated>2021-06-15T04:56:43.501Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>데이터 중심 어플리케이션 설계 [PART 2]</strong><br>Designing Data Intensive Applications - O’REILLY</p><p>10. Batch Processing</p></blockquote><br/><h4 id="Part-3-파생"><a href="#Part-3-파생" class="headerlink" title="[Part 3] 파생"></a>[Part 3] 파생</h4><blockquote><p>특정 데이터셋에서 다른 데이터셋을 파생하는 시스템 (주로 in 이종 시스템)</p><ul><li>10장 : 맵리듀스와 같은 일괄 처리 방식 데이터플로 시스템, 관련된 도구, 대규모 데이터 시스템을 구축하기 위한 원리</li><li>11장 : 데이터 스트림에 적용한 동일 아이디어</li><li>12장 : 책의 결론 (for Reliable, Scalable, and Maintainable Applications)</li></ul></blockquote><p>1, 2부에서는 분산 데이터베이스로 가기 위해 고려해야할 모든 주요 사항을 다룸 (in 단일 데이터베이스)<br>3부에서는 데이터 모델도, 최적화된 접근 방식도 다른 여러 데이터 시스템을 <strong>일관성 있는 하나의 애플리케이션 아키텍처로 통합</strong> 문제 검토</p><details><summary class="point-color-can-hover"> 💡 레코드 시스템과 파생 데이터 시스템? </summary><ul><li>레코드 시스템<ul><li>진실의 근원 (source of truth)</li><li>일반적으로 정규화를 거쳐 정확하게 한번 표현</li></ul></li><li>파생 데이터 시스템<ul><li>원천 데이터를 가져와 특정 방식으로 변환하고 처리한 결과 (ex. 캐시)</li><li>비정규화 값, 색인, 구체화 뷰, 추천 시스템의 예측 요약 데이터</li><li>파생 데이터 특징<ul><li>중복(redundant) =&gt; 기존 데이터를 복제</li><li>대개 비정규화 과정으로 생성</li><li>단일 원천 데이터로 여러 데이터셋을 추출해 각 데이터셋마다 서로 다른 “관점”으로 데이터를 봄</li></ul></li></ul></li><li>레코드 시스템 vs 파생 데이터 시스템<ul><li>구분하면 시스템 전체 데이터플로가 명확해짐 (입출력 형태, 의존 관계 등)</li><li>애플리케이션에서 어떻게 사용할지에 따라 결정됨 (데이터베이스에 따라 결정 x. it’s 단지 도구)</li></ul></li><li>시스템 아키텍처를 망치지 않고 명료성을 갖추려면 <em>“데이터가 어떤 데이터로부터 파생되었는지 명확히 해야함”</em> =&gt; 3부 주제</details></li></ul><br/><hr><br/><h3 id="📖-Overview"><a href="#📖-Overview" class="headerlink" title="📖 Overview"></a>📖 Overview</h3><p><a href="#10-1-%EC%9C%A0%EB%8B%89%EC%8A%A4-%EB%8F%84%EA%B5%AC%EB%A1%9C-%EC%9D%BC%EA%B4%84-%EC%B2%98%EB%A6%AC%ED%95%98%EA%B8%B0">10-1. 유닉스 도구로 일괄 처리하기</a></p><ul><li><a href="#%EB%8B%A8%EC%88%9C-%EB%A1%9C%EA%B7%B8-%EB%B6%84%EC%84%9D">단순 로그 분석</a></li><li><a href="#%EC%9C%A0%EB%8B%89%EC%8A%A4-%EC%B2%A0%ED%95%99">유닉스 철학</a></li></ul><p><a href="#10-2-%EB%A7%B5%EB%A6%AC%EB%93%80%EC%8A%A4%EC%99%80-%EB%B6%84%EC%82%B0-%ED%8C%8C%EC%9D%BC-%EC%8B%9C%EC%8A%A4%ED%85%9C">10-2. 맵리듀스와 분산 파일 시스템</a></p><ul><li><a href="#%EB%A7%B5%EB%A6%AC%EB%93%80%EC%8A%A4-%EC%9E%91%EC%97%85-%EC%8B%A4%ED%96%89%ED%95%98%EA%B8%B0">맵리듀스 작업 실행하기</a></li><li><a href="#%EB%A6%AC%EB%93%80%EC%8A%A4-%EC%82%AC%EC%9D%B4%EB%93%9C-%EC%A1%B0%EC%9D%B8%EA%B3%BC-%EA%B7%B8%EB%A3%B9%ED%99%94">리듀스 사이드 조인과 그룹화</a></li><li><a href="#%EB%A7%B5-%EC%82%AC%EC%9D%B4%EB%93%9C-%EC%A1%B0%EC%9D%B8">맵 사이드 조인</a></li><li><a href="#%EC%9D%BC%EA%B4%84-%EC%B2%98%EB%A6%AC-%EC%9B%8C%ED%81%AC%ED%94%8C%EB%A1%9C%EC%9D%98-%EC%B6%9C%EB%A0%A5">일괄 처리 워크플로의 출력</a></li><li><a href="#%ED%95%98%EB%91%A1%EA%B3%BC-%EB%B6%84%EC%82%B0-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4%EC%9D%98-%EB%B9%84%EA%B5%90">하둡과 분산 데이터베이스의 비교</a></li></ul><p><a href="#10-3-%EB%A7%B5%EB%A6%AC%EB%93%80%EC%8A%A4%EB%A5%BC-%EB%84%98%EC%96%B4">10-3. 맵리듀스를 넘어</a></p><ul><li><a href="#%EC%A4%91%EA%B0%84-%EC%83%81%ED%83%9C-%EA%B5%AC%EC%B2%B4%ED%99%94">중간 상태 구체화</a></li><li><a href="#%EA%B7%B8%EB%9E%98%ED%94%84%EC%99%80-%EB%B0%98%EB%B3%B5-%EC%B2%98%EB%A6%AC">그래프와 반복 처리</a></li><li><a href="#%EA%B3%A0%EC%88%98%EC%A4%80-API%EC%99%80-%EC%96%B8%EC%96%B4">고수준 API와 언어</a></li></ul><p><a href="#10-%EC%A0%95%EB%A6%AC">10 정리</a></p><p><a href="#Reference">Reference</a></p><br/><h1 id="10장-일괄-처리"><a href="#10장-일괄-처리" class="headerlink" title="10장 일괄 처리"></a>10장 일괄 처리</h1><ul><li>시스템의 유형 3가지 구분<ul><li>서비스 (온라인 시스템)<ul><li>클라이언트로부터 요청/지시가 올 때까지 대기</li><li>성능 측정 지표 = 응답 시간 (+ 가용성)</li></ul></li><li><strong>일괄 처리 시스템 (오프라인 시스템)</strong><ul><li>큰 입력 데이터를 받아 처리하는 작업을 수행하고 결과 데이터 생산 (Scheduling)</li><li>성능 측정 지표 = 처리량</li></ul></li><li>스트림 처리 시스템 (준실시간 시스템)<ul><li>준실시간 처리(near-real-time processing, nearline processing)</li><li>입력 이벤트 발생 직후, 입력 데이터 소비 및 출력 데이터 생산 =&gt; 지연시간 ↓</li></ul></li></ul></li><li>맵 리듀스 (MapReduce)<ul><li>“구글을 대규모로 확장 가능하게 만든 알고리즘”</li><li>ex. 하둡, 카우치DB, 몽고DB</li><li>병렬 처리 시스템보다 저수준이지만 데이터 규모면에서 상당히 진보</li></ul></li><li>표준 유닉스 도구<ul><li>유닉스의 철학이 대규모 이기종 분산 시스템으로 그대로 이어짐</li></ul></li></ul><br/><h2 id="10-1-유닉스-도구로-일괄-처리하기"><a href="#10-1-유닉스-도구로-일괄-처리하기" class="headerlink" title="10-1 유닉스 도구로 일괄 처리하기"></a>10-1 유닉스 도구로 일괄 처리하기</h2><h3 id="단순-로그-분석"><a href="#단순-로그-분석" class="headerlink" title="단순 로그 분석"></a>단순 로그 분석</h3><ul><li><code>awk</code>, <code>sed</code>, <code>grep</code>, <code>sort</code>, <code>uniq</code>, <code>xargs</code> 조합으로 데이터 분석이 놀라울 정도로 잘 수행됨</li><li>예제<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /var/log/nginx/access.log</span></span><br><span class="line"><span class="comment"># format : $remote_addr - $remote_user [$time_local] &quot;$request&quot; $status $body_bytes_sent &quot;$http_referer&quot; &quot;$http_user_agent&quot; </span></span><br><span class="line"><span class="comment"># (sample) 216.xx.xx.xx - - [27/Feb/2015:17:55:11 +0000] &quot;GET /css/typrography.css HTTP/1.1&quot; 200 3377 &quot;http://...com&quot; &quot;Mozilla/5.0 (.....)</span></span><br><span class="line"></span><br><span class="line">cat /var/<span class="built_in">log</span>/nginx/access.log | \</span><br><span class="line">  awk <span class="string">&#x27;&#123;print $7&#125;&#x27;</span> | \ <span class="comment"># 7번째 필드 출력 ($0은 전체)</span></span><br><span class="line">  sort  | \ <span class="comment"># 정렬</span></span><br><span class="line">  uniq -c | \ <span class="comment"># 중복 제거 (&#x27;-c&#x27;: 중복횟수 첫번째로 함께 출력) </span></span><br><span class="line">  sort -r -n | \ <span class="comment"># 정렬 (&#x27;-r&#x27;: 내림차순, &#x27;-n&#x27; : 숫자로 정렬)</span></span><br><span class="line">  head -n 5 <span class="comment"># 맨 앞 5줄만 출력</span></span><br></pre></td></tr></table></figure></li><li>유닉스 연쇄 명령 vs 맞춤형 프로그램<ul><li>취향의 문제 (간결성, 가독성 등)</li><li>단, 실행 흐름은 크게 다름</li></ul></li><li>정렬 vs 인메모리 집계<ul><li>유닉스 파이프라인 : 해시 테이블 x 정렬된 목록에서 같은 url 반복 노출</li><li>맞춤형 프로그램 (ex. 스크립트) : 해시 테이블을 메모리에 유지</li><li>뭐가 더 적합한가? =&gt; 작업 세트 (임의 접근이 필요한 메모리양) 에 따라<ul><li>작업세트가 작으면 인메모리 해시 테이블도 ok</li><li>작업세트가 허용 메모리보다 크면 정렬 접근법 권장 (디스크를 효율적으로 사용하는 병합 정렬)</li></ul></li><li>리눅스에 포함된 sort 유틸리티는 메모리보다 큰 데이터셋을 자동으로 디스크로 보내고 여러 CPU 코어에서 병렬로 정렬 =&gt; 손 쉽게 큰 데이터 셋으로 확장 가능</li></ul></li></ul><h3 id="유닉스-철학"><a href="#유닉스-철학" class="headerlink" title="유닉스 철학"></a>유닉스 철학</h3><ul><li>유닉스 파이프<ul><li><em>“다른방법으로 데이터 처리가 필요할 때 정원 호스와 같이 여러 다른 프로그램을 연결하는 방법이 필요하다. 이것은 I/O 방식이기도 하다. (Dog McIlory, 1964)”</em></li></ul></li><li>유닉스 철학 (1978)<ul><li>(1) 각 프로그램이 한가지 일만 하도록 작성</li><li>(2) 모든 프로그램의 출력은 아직 알려지지 않은 다른 프로그램의 입력으로 쓸 수 있다고 생각할 것 (출력에는 필요한 정보만. 입력 형식을 엄격하게 맞추거나 이진 형태 x, 대화형 입력 고집 x)</li><li>(3) S/W와 OS는 빠르게 써볼 수 있게 설계/구축</li><li>(4) 프로그래밍 작업을 줄이려면 미숙한 도움보다는 도구를 사용</li></ul></li><li>=&gt; Agile, DevOps 와 매우 흡사</li><li>유닉스 도구들은 유연하게 조합할 수 있고 조합하여 사용했을 때 강력<ul><li>유닉스의 결합성 &lt;= <strong>동일 인터페이스</strong></li></ul></li></ul><h4 id="유닉스-철학-동일-인터페이스"><a href="#유닉스-철학-동일-인터페이스" class="headerlink" title="[유닉스 철학] 동일 인터페이스"></a>[유닉스 철학] 동일 인터페이스</h4><ul><li>특정 출력을 다른 <strong>어떤</strong> 프로그램의 입력으로 쓰려면 (2) =&gt; <strong>모두</strong> 호환 가능한 인터페이스를 사용해야 함<ul><li>동일 인터페이스 예 : 파일, URL, HTTP</li></ul></li><li>유닉스의 인터페이스 =&gt; 파일 (파일 디스크립터)<ul><li>파일은 단지 순서대로 정렬된 바이트 연속. 여러가지 것 함께 표현 가능</li><li>ex. 실제 파일, 프로세스 간 통신채널(유닉스 소켓, stdin, stdout), 장치 드라이버, TCP 연결 소켓 등..</li></ul></li><li>아스키(Ascii) 텍스트<ul><li>관례상 많은 유닉스 프로그램들이 연속된 바이트를 아스키 텍스트 취급</li><li>같은 레코드 분리자(\n) 사용하는 유닉스 유틸리티 =&gt; 상호 운용 가능<ul><li>0x0A, LF, Line feed(Ctrl + J)</li><li>0x1E, RS, Record Separator(Ctrl + ^)</li></ul></li><li>큰 문제는 없지만 그다지 깔끔하지는 x</li></ul></li><li>데이터베이스<ul><li>동일한 데이터 모델인 데이터베이스 간에도 데이터 이동이 쉽지않음 (통합 부족)</li><li>데이터 발칸화(Balkanization) : 인터넷이 고립된 여러 섬처럼 나뉜 현상이나 프로그램 언어나 데이터 파일 포맷등이 분화 발전하는 현상</li></ul></li></ul><h4 id="유닉스-철학-로직과-연결의-분리"><a href="#유닉스-철학-로직과-연결의-분리" class="headerlink" title="[유닉스 철학] 로직과 연결의 분리"></a>[유닉스 철학] 로직과 연결의 분리</h4><ul><li>표준 입력(stdin), 표준 출력(stdout) 사용<ul><li>유닉스 도구의 또 다른 특징</li><li>한 프로세스의 stdout 을 다른 프로세스의 stdin 과 연결 =&gt; 중간 데이터 디스크에 쓰지않고 <strong>작은 인메모리 버퍼</strong> 사용하여 전달</li></ul></li><li>입출력이 어떻게 이루어지는지 신경 쓸 필요 x<ul><li>느슨한 결합 (loose coupling)</li><li>지연 바인딩 (late binding)</li><li>제어 반전 (inverse of control)</li></ul></li><li>직접 작성한 프로그램을 끼워넣어 os 지원 도구처럼 사용 가능</li><li>제약 사항<ul><li>여러 개의 입력을 받거나 여러 개의 출력이 필요한 경우 사용이 까다로움<ul><li>출력을 파이프를 이용해 네트워크와 연결 불가 (without <code>netcat</code>, <code>curl</code>)</li></ul></li><li>프로그램의 I/O가 프로그램 자체와 서로 묶이는 경우<ul><li>프로그램이 파일을 직접 열어 읽고 쓰거나, 서브 프로세스로 다른 프로그램 구동하거나, 네트워크 연결하거나</li><li>입출력 연결하는 유연성 감소</li></ul></li></ul></li></ul><h4 id="유닉스-철학-투명성과-실험"><a href="#유닉스-철학-투명성과-실험" class="headerlink" title="[유닉스 철학] 투명성과 실험"></a>[유닉스 철학] 투명성과 실험</h4><ul><li>유닉스 도구는 불친절하고 단순하지만, 진행 사항 파악이 쉬움<ul><li>일반적으로 입력파일은 불변 처리</li><li>어느 시점이든 파이프라인 중단하고 출력 확인 가능 (<code>less</code>)</li><li>특정 파이프라인 단계의 출력을 파일에 쓰고, 다음 단계에 입력으로 사용 가능 (재시작 good)</li></ul></li><li>가장 큰 제약은 <strong>단일 장비</strong> 에만 실행된다는 점<ul><li>=&gt; 하둡같은 도구가 필요한 이유</li></ul></li></ul><br/><h2 id="10-2-맵리듀스와-분산-파일-시스템"><a href="#10-2-맵리듀스와-분산-파일-시스템" class="headerlink" title="10-2 맵리듀스와 분산 파일 시스템"></a>10-2 맵리듀스와 분산 파일 시스템</h2><ul><li>유닉스 도구와 비슷하지만 <strong>수천 대의 장비로 분산 실행</strong> 가능<ul><li>입력 수정 x =&gt; 부수 효과 x</li><li>출력 파일은 순차적으로 한번씩만 작성 (수정 x)</li></ul></li><li>입출력<ul><li>유닉스 도구 : stdin / stdout</li><li>맵리듀스 : 분산 파일 시스템 상의 파일 (하둡 =&gt; HDFS)</li></ul></li><li>분산 파일 시스템<ul><li><strong>HDFS</strong> (Haddop Distributed File System)</li><li>GlusterFS, OFS(Quantcast File)</li><li>AWS S3, Azure Blob Storage, Openstack Swift (객체저장소)</li></ul></li><li>HDFS는 비공유 원칙 기반<ul><li>비공유 아키텍처 (shared-nothing, scale out)<ul><li>일반적 데이터센터 네트워크에 연결된 컴퓨터면 충분</li></ul></li><li>vs 공유 디스크 방식<ul><li>NAS(network Attached Storage), SAN(Storage Area Network)</li><li>중앙 집중 저장 장치를 위한 맞춤 하드웨어나 특별한 네트워크 인프라 필요</li></ul></li></ul></li><li>HDFS는 개념적으로 큰 하나의 파일 시스템<ul><li>각 장비에서 실행되는 데몬 프로세스로 구성 =&gt; 다른 노드가 해당 장비의 파일에 접근 가능하게끔 네트워크 서비스 제공</li><li>데몬이 실행중인 모든 장비의 디스크 사용 가능</li><li>중앙서버 (네임노드, NameNode)가 파일 블록이 저장된 장비 위치 추적</li></ul></li><li>HDFS는 뛰어난 확장성<ul><li>대규모 확장 가능 (범용 하드웨어 + 오픈소스 소프트웨어로 훨씬 저렴한 비용)</li></ul></li><li>파일 블록 복제<ul><li>여러 장비에 동일 데이터 복사 [5장]</li><li><strong>삭제 코딩(erasure coding)</strong> 방식으로 적은 부담으로 손실된 데이터 복구 (like RAID)</li></ul></li></ul><blockquote><h4 id="삭제-코딩-erasure-coding"><a href="#삭제-코딩-erasure-coding" class="headerlink" title="삭제 코딩 (erasure coding)"></a>삭제 코딩 (erasure coding)</h4><p>n개의 데이터 셀에서 k개의 패러티 셀 데이터로 인코딩하고,<br>데이터 손실시 디코딩 과정을 거쳐 원본 데이터를 복구하는 <strong>데이터 복구 기법</strong> 중 하나 (백업 목적 X)<br>(XOR 방식, Reed-Solomon(RS) code)</p><p>장점</p><ul><li>스토리지 효율성 👍🏻: n/(n+k) </li><li>(단, EC는 데이터 크기에 따라 Namenode에 더욱 많은 블록 정보를 관리를 요구하게 됨으로 오버헤드 존재)</li></ul><p>제약사항</p><ul><li>지역성(data locality) x </li><li>overwrite가 많은 환경엔 추천 x</li><li>4개 이상의 노드일때만 사용 가능</li></ul><p>reference</p><ul><li><a href="https://joonyon.tistory.com/148">https://joonyon.tistory.com/148</a></li><li><a href="https://www.youtube.com/watch?v=f9ntIbw43xI">https://www.youtube.com/watch?v=f9ntIbw43xI</a></li></ul></blockquote><h3 id="맵리듀스-작업-실행하기"><a href="#맵리듀스-작업-실행하기" class="headerlink" title="맵리듀스 작업 실행하기"></a>맵리듀스 작업 실행하기</h3><ul><li>맵 리듀스 = 분산 파일 시스템 위에서 대용량 데이터셋을 처리하는 코드 작성 프로그래밍 프레임워크</li><li>유닉스 도구로의 단순 분석과 유사한 데이터 처리 패턴<ul><li>(1) 입력파일을 레코드로 쪼갠다 (separator = <code>\n</code>)</li><li>(2) 각 레코드마다 매퍼 함수를 호출해 키와 값 추출</li><li>(3) 키 기준으로 키-값 쌍 모두 정렬</li><li>(4) 정렬된 키-값 쌍 전체 댇상으로리듀스 함수 호출 (같은 키값은 서로 인접 -&gt; 쉽게 결합)</li></ul></li><li>맵리듀스 작업 4단계<ul><li>1단계) 입력 형식 파서 : 파일 → 레코드</li><li>2단계) 맵 (Map) : 레코드 → 키, 값 추출</li><li>3단계) 정렬 단계 : 매퍼 출력이 내부적으로 이미 정렬</li><li>4단계) 리듀스 (Reduce) : 키, 값 → 출력 레코드 </li></ul></li><li>2 가지 콜백 함수 구현 필요<ul><li><strong>매퍼 (Mapper)</strong> : 정렬에 적합한 형태로 데이터 준비<ul><li>모든 입력 레코드마다 독립적으로 한 번씩만 호출</li></ul></li><li><strong>리듀서 (Reduccer)</strong> : 정렬된 데이터 가공<ul><li>같은 키를 모으고 해당 값의 집합을 반복해 리듀서 함수 호출</li></ul></li></ul></li><li>맵리듀스 분산 실행<ul><li>병렬 수행 코드를 직접 작성안하고도 동시 처리 가능 (신경x)</li><li>매퍼, 리듀서 : 하둡 (Java Class), MongoDB, CouchDB (Javascript Function)</li><li>파티셔닝 기반<ul><li>매퍼 - 입력 파일 블록수 기반</li><li>리듀서 - 사용자 지정 (키 해시값 사용)</li></ul></li><li>데이터 가까이에서 연산하기 (입력 파일있는 장비에서 맵리듀스 작업 수행) =&gt; 지역성 ↑ 네트워크 부하 ↓</li><li>셔플 (shuffle)<ul><li>리듀서를 기준으로 파티셔닝하고 정렬한 뒤 매퍼로부터 데이터 파티션을 복사하는 과정</li><li>정렬된 순서를 유지하며 병합 (임의 x)</li></ul></li></ul></li><li>맵리듀스 워크플로 (workflow)<ul><li>하나의 맵리듀스 출력을 다른 맵리듀스의 입력으로 연결 (파일 경로를 통한 암묵적 방식)</li><li>일괄 처리 작업의 출력은 성공했을 때만 유효 (실패시 남은 출력 제거)</li><li>하둡 도구 예<ul><li>맵리듀스 작업간 수행 의존성을 위한 스케줄러 예 : Oozie, Azkaban, Luigi, <strong>Airflow</strong>, Pinball</li><li>다중 맵리듀스 연결을 위한 하둡용 고수준 도구 예 : Pig, Hive, Cascading, Crunch, FlumeJava</li></ul></li></ul></li></ul><h3 id="리듀스-사이드-조인과-그룹화"><a href="#리듀스-사이드-조인과-그룹화" class="headerlink" title="리듀스 사이드 조인과 그룹화"></a>리듀스 사이드 조인과 그룹화</h3><ul><li>사용자 활동 이벤트 분석 예제<ul><li>활동 이벤트 (activity event) or 클릭스트림 데이터 (clickstream data)</li><li>원격 데이터베이스에 질의한다는 건 일괄 처리가 비결정적이라는 뜻</li><li>데이터베이스의 사본 (ex. ETL) 를 추출해 분산 파일 시스템에 넣는 방법</li></ul></li><li><strong><U>리듀스 사이드 조인 (Reduce-Side Join)</U></strong><ul><li>실제 조인 로직을 리듀서에서 수행</li><li>매퍼는 입력데이터 준비 역할 (입력 데이터에 가정 x)</li></ul></li><li><strong>정렬 병합 조인</strong> (SMB, Sort-Merge Join) <ul><li>매퍼 출력이 키로 정렬된 후 (sort) 리듀서가 조인의 양측에 정렬된 레코드 목록 병합 (merge) </li><li>특정 id의 모든 레코드를 한번에 처리. 한번에 한 id의 레코드만 메모리에 유지. 네트워크 x</li><li>보조 정렬 (secondary sort) : 리듀서가 작업 레코드 재배열</li></ul></li><li>같은 곳으로 연관된 데이터 가져오기<ul><li>같은 키 (주소) 를 가진 키-값 쌍은 모두 같은 리듀서 호출</li><li>맵리듀스는 데이터 모으는 연산 (물리적 네트워크 통신) 과 처리하는 로직 (애플리케이션 로직) 분리</li><li>실패가 발생해도 애플리케이션 코드에서는 고민 no (재시도)</li></ul></li><li>그룹화<ul><li>SQL <code>GROUP BY</code></li><li>맵리듀스로 그룹화 구현 =&gt; 키-값 생성 시 <strong>그룹화할 대상을 키</strong>로 설정</li><li>그룹화 사용 예시) 세션화 (sessionization) </li></ul></li><li>쏠림(skew) 다루기<ul><li>불균형한 활성 데이터베이스 레코드 = 린치핀 객체 (linchpin object) = 핫 키 (hot key)</li><li>한 리듀서에 많은 레코드가 쏠리는 현상 = 핫스팟</li><li>맵 리듀서는 모든 매퍼, 리듀서가 끝나야하므로 지연시간 ↑</li><li>핫스팟 완화 알고리즘<ul><li>Pig의 쏠린 조인(skewed join)</li><li>Crunch의 공유 조인(shared join)</li><li>Hive의 맵 사이드 조인 (map-side-join)</li></ul></li><li>핫 키로 그룹화/집계하는 2 단계<ul><li>(1) 레코드를 임의의 리듀서로 보내 처리해서 핫 키 레코드의 일부를 그룹화 하고 간소화 값 출력</li><li>(2) 첫 단계의 출력으로 나온 값을 키별로 모두 결합해 하나의 값으로</li></ul></li></ul></li></ul><h3 id="맵-사이드-조인"><a href="#맵-사이드-조인" class="headerlink" title="맵 사이드 조인"></a>맵 사이드 조인</h3><ul><li><strong><U>맵 사이드 조인 (Map-Side Join)</U></strong><ul><li>입력 데이터에 대한 특정 가정</li><li>축소된 맵리듀스 (리듀서 x 정렬 x)</li><li>매퍼는 단지 입력 파일 블럭 하나를 읽어 다시 분산 파일 시스템에 출력</li></ul></li><li><strong>브로드캐스트 해시 조인</strong> (Broadcast Hash Join)<ul><li>메모리에 올릴 정도로 작은 데이터 셋과 큰 데이터 셋을 조인</li><li>큰 입력 파티션 하나를 처리하는 각 매퍼는 작은 입력 전체를 읽고 (broadcast), 작은 데이터셋은 각 파티션의 해시 테이블에 적재 (hash)</li><li>인메모리 해시 테이블 적재 대신 로컬 디스크 읽기 전용 색인으로 저장도 가능</li><li>Pig의 복제 조인, Hive의 맵 조인, 캐스케이딩, 크런치, Impala (질의 엔진)</li></ul></li><li><strong>파티션 해시 조인</strong> (Partitioned Hash Join)<ul><li>두 입력 모두를 같은 키, 같은 해시 함수, 같은 수로 파티셔닝하여 조인</li><li>각 맵퍼 해시 테이블에 적재해야 할 데이터의 양 ↓</li><li>Hive의 버킷 맵 조인(bucketed map join)</li></ul></li><li>맵 사이드 병합 조인 (map-side merge join)<ul><li>입력 데이터셋이 같은 파티셔닝, 같은 키 기준 <strong>정렬</strong> 된 경우 사용 가능 (sort-merge)</li></ul></li><li>맵 사이드 조인을 사용하는 맵리듀스 워크플로<ul><li>맵 사이드 조인 vs 리듀스 사이드 조인 =&gt; 출력구조 다름<ul><li>리듀스 사이드 조인 : 조인 키로 파티셔닝, 정렬</li><li>맵 사이드 조인 : 큰 입력과 동일한 방법으로 파티셔닝, 정렬</li></ul></li><li>맵 사이드 조인은 크기, 정렬, 입력 데이터의 파티셔닝 같은 제약 사항 =&gt; 물리적 레이아웃 파악 필수<ul><li>HCatalog, Hive metastore</li></ul></li></ul></li></ul><h3 id="일괄-처리-워크플로의-출력"><a href="#일괄-처리-워크플로의-출력" class="headerlink" title="일괄 처리 워크플로의 출력"></a>일괄 처리 워크플로의 출력</h3><blockquote><p>🤔 애초에 그래서 일괄처리를 왜 쓰는데?</p></blockquote><ul><li>데이터 베이스 질의 구분 =&gt; OLTP를 분석 목적과 구별<ul><li>OLTP 질의 : 색인 사용하여 사용자에게 보여줄 소량의 레코드만 특정키로 조회</li><li>OLAP 분석 질의 : 대량의 레코드를 스캔해 그룹화/집계 연산하여 보고서 형태로 출력</li></ul></li><li><strong>그렇다면 일괄 처리는?</strong><ul><li>트랜잭션 처리도 분석도 X</li><li>분석에 가깝지만 SQL 질의도 아니고 출력은 보고서가 아닌 다른 형태 구조</li></ul></li><li>검색 색인 구축<ul><li>일괄 처리로 색인 구축 효율적 (병렬화, 읽기 전용, 불변)</li><li>색인 갱신 방법 : 전체 색인 워크플로 재수행하여 색인 대치, 증분 색인</li></ul></li><li>일괄 처리의 출력으로 키-값을 저장<ul><li>일괄 처리 워크플로 출력 예 : 검색 색인, 머신러닝 시스템 (분류기), 추천 시스템 ..</li><li><strong>일괄 처리의 출력</strong> =&gt; 일종의 <U>데이터 베이스</U>가 됨</li><li>질의 방법 =&gt; 일괄 처리 작업 내부에 새로운 데이터베이스를 구축해 분산 파일 시스템의 작업 출력 디렉터리에 데이터베이스 파일 저장 (직접 하나씩 데이터베이스에 요청보내는 것은 Bad)</li><li>데이터 파일은 읽기전용, 불변, 서버에 bulk</li></ul></li><li>일괄 처리 출력에 관한 철학<ul><li>인적 내결함성 (human fault tolerance) : 버그 코드로 부터 복원 가능 여부</li><li>비가역성 최소화 (minimizing irreversibility)</li><li>입력 불변, 실패 출력 폐기 =&gt; 실패 시 재실행 반복 가능</li><li>동일 입력 파일 집합 사용</li><li>연결작업과 로직의 분리</li></ul></li><li>유닉스와의 차이점<ul><li>구조화된 파일형식 (avro, parquet) 사용으로 저수준 구문 변환 작업 최소화 가능 + 스키마 발전 가능</li></ul></li></ul><h3 id="하둡과-분산-데이터베이스의-비교"><a href="#하둡과-분산-데이터베이스의-비교" class="headerlink" title="하둡과 분산 데이터베이스의 비교"></a>하둡과 분산 데이터베이스의 비교</h3><ul><li><strong>대규모 병렬 처리 (MPP, Massively Parallel Processing)</strong> 데이터베이스<ul><li>MPP 데이터베이스 : 하나의 쿼리를 여러개의 프로세스로 병렬처리하는 데이터베이스<ul><li><a href="https://www.comworld.co.kr/news/articleView.html?idxno=49459">‘MPP 데이터베이스란’</a> 참고</li></ul></li><li>맵리듀스의 개념은 이미 수십년전에 MPP DB에서 구현된 것</li><li>MPP vs 맵리듀스<ul><li>💣MPP 데이터베이스 : 장비 클러스에서 분석 SQL 질의를 병렬 수행하는 것에 초점</li><li>🗂맵리듀스 + 분산 파일 시스템 : 아무 프로그램이나 실행할 수 있는 OS 같은 속성 제공</li></ul></li></ul></li><li>저장소의 다양성<ul><li>💣데이터베이스는 특정 모델 (관계형 or 문서형) 에 따라 데이터 구조화 필요</li><li>🗂하둡은 어떤 형태라도 상관없이 HDFS 덤프가능</li><li>현실적으로 하둡처럼 데이터를 <strong>빨리</strong> 사용가능하게, <strong>한 곳에 모으는</strong> 작업만으로도 가치 (like 데이터 웨어하우스)<ul><li>data lake, enterprise data hub</li><li>데이터 해석은 소비자에게 (schema-on-read)</li><li>“원시 데이터가 오히려 좋아” =&gt; 초밥 원리 (sushi principle)</li></ul></li><li>ETL 구현에 사용하기도 (데이터 모델링을 하더라도 수집과는 분리된 단계)</li></ul></li><li>처리 모델의 다양성<ul><li>💣MPP 데이터베이스는 monolithic 구조. 설계된 질의 유형에 좋은 성능 (but 한정)</li><li>🗂맵리듀스 이용 시 자신이 작성한 코드를 대용량 데이터 셋에서 쉽게 실행 가능</li><li>HDFS + 맵리듀스 + SQL 질의 엔진 (Hive) =&gt; 어려운 다양한 일괄 처리 가능</li><li>하둡의 개방성<ul><li>SQL, 맵리듀스보다 더 다양한 모델 등장</li><li>데이터 이동 필요 없이 유연한 지원</li><li>임의 접근 가능한 OLTP 데이터베이스 (Hbase), MPP 스타일의 분석 데이터베이스 (Impala) =&gt; HDFS</li></ul></li></ul></li><li>빈번하게 발생하는 결함을 줄이는 설계<ul><li>MPP 데이터베이스 vs 맵리듀스 2가지 차이<ul><li>결함을 다루는 방식</li><li>메모리 및 디스크 사용 방식</li></ul></li><li>💣MPP 데이터베이스는 한 장비만 죽어도 전체 질의 중단. 가능하면 메모리에 많은 데이터 유지</li><li>🗂맵 리듀스는 개별 태스크 실패에 큰 영향 x. 되도록 디스크에 데이터 기록 (내결함성, 데이터량)</li><li>맵 리듀스는 대용량 작업과 예상치 못한 태스크 종료가 빈번한 경우 적합</li></ul></li></ul><br/><h2 id="10-3-맵리듀스를-넘어"><a href="#10-3-맵리듀스를-넘어" class="headerlink" title="10-3 맵리듀스를 넘어"></a>10-3 맵리듀스를 넘어</h2><ul><li>맵리듀스는 분산 시스템에서 가능한 여러 프로그래밍 모델 중 단지 하나<ul><li>데이터 양, 자료 구조, 데이터 처리 방식에 따라 다른 도구가 더 적합할 수도</li><li>맵리듀스를 편하게 쓰기위해 추상화된 다양한 고수준 프로그래밍 모델 (단, 모델 자체 문제 주의)</li></ul></li></ul><h3 id="중간-상태-구체화"><a href="#중간-상태-구체화" class="headerlink" title="중간 상태 구체화"></a>중간 상태 구체화</h3><ul><li>맵리듀스는 다른작업과 모두 독립적 (로직과 연결의 분리)</li><li><strong>중간 상태 (Intermediate state)</strong> 를 파일로 기록하는 과정 =&gt; <strong>구체화 (materialization)</strong><ul><li>장점<ul><li>내구성 (내결함성 확보)</li></ul></li><li>단점<ul><li>모든 선행 작업 태스크가 종료될때까지 대기 (수행시간 slow)</li><li>매퍼 중복</li><li>임시 데이터 (중간 상태) 도 복제되는 과잉 조치</li></ul></li><li>vs 스트리밍 (ex. 유닉스 파이프의 인메모리 버퍼를 사용한 입출력 전달)</li></ul></li><li><strong>데이터플로 엔진 (dataflow engine)</strong><ul><li>분산 일괄 처리 연산 엔진. 전체 워크플로를 독립된 하위작업이 아닌, <U>작업 하나로서 다루는 엔진</U></li><li><strong>Spark</strong>, Tez, Flink</li><li>vs 맵리듀스<ul><li>더 유연한 방법으로 <U>함수</U> 조합 가능 =&gt; <strong>연산자 (operator)</strong></li><li>연산자의 출력과 다른 연산자의 입력을 연결하는 여러가지 선택지 (키로 재파티셔닝 및 정렬, 정렬 스킵, 브로드캐스트 ..)</li><li>수행속도 훨씬 빠름</li></ul></li><li>장점<ul><li>값비싼 작업(ex. 정렬)은 실제 필요할때만 수행</li><li>필요없는 맵 태스크는 없다</li><li>모든 조인과 데이터 의존 관계를 명시적 선언 =&gt; 지역성 최적화</li><li>연산자 간 중간 상태는 로컬 디스크나 메모리에 기록</li><li>입력 준비되는 즉시 실행 가능 (선행 단계 전체 완료 대기 x)</li><li>새로운 연산자 실행 시 이미 존재하는 JVM 사용</li></ul></li></ul></li><li>데이터플로 엔진의 내결함성 (Fault tolerance)<ul><li>중간 상태를 사용하지않는 데이터플로 엔진의 내결함성 확보 접근법 =&gt; 재계산</li><li>Spark (RDD 추상화 - 데이터 조상 추적), Flink (연산자 상태 체크포인트)</li><li>데이터 재연산의 포인트는 “해당 연산이 <strong>결정적인지</strong> 파악” 하는 것 (= 비결정적 원인 제거)</li></ul></li><li>데이터플로 엔진의 구체화<ul><li>데이터플로 엔진은 일부(agg)를 제외하고 파이프라인 방식 실행 가능</li><li>작업 완료시 출력을 지속성 있는 어떤 곳 (=분산 파일 시스템) 에 다시 기록</li><li>모든 중간 상태를 기록하는 수고 x</li></ul></li></ul><h3 id="그래프와-반복-처리"><a href="#그래프와-반복-처리" class="headerlink" title="그래프와 반복 처리"></a>그래프와 반복 처리</h3><ul><li>그래프 처리의 필요성<ul><li>그래프 처리 != 비순환 방향 그래프 (directed acyclic graph, DAG)<ul><li>DAG는 데이터플로 엔진의 작업 연산자. 데이터 흐름이 그래프로 구성</li><li>그래프 처리는 데이터 자체가 그래프 형식</li></ul></li><li>이행적 폐쇄 (transitive closure) : 특정 조건에 도달할때까지 인접 정점 조인<ul><li>맵리듀스로 반복적 스타일로 구현시 비효율적</li></ul></li></ul></li><li>프리글 (Pregel) 처리 모델<ul><li>= 벌크 동기식 병렬 (BSP, bulk synchronous parallel) : 일괄 처리 그래프 최적화 방법</li><li>한 정점이 다른 정점으로 ‘메세지를 보낼’ 수 있다.<ul><li>맵리듀스가 매퍼가 특정 리듀서를 호출해 ‘메세지를 전달’ 과 비슷</li><li>차이점은 반복에서 사용한 메모리 상태 기억</li></ul></li><li>정점 상태 제외하고, 정점 사이 메세지는 내결함성/지속성. 메세지 처리는 고정 횟수 내 처리<ul><li>액터 모델 (분산 액터 프레임워크) 랑 비슷</li><li>차이점은 타이밍 보장 (각 반복에서 이전 반복에서 보내진 모든 메세지 전달)</li></ul></li><li>내결함성<ul><li>반복이 끝나는 시점에 모든 정점 상태를 주기적 저장 (체크포인트)</li><li>프로그래밍 모델 단순화를 위해 프레임워크 차원에서 완벽히 결함 복구</li></ul></li><li>병렬 실행<ul><li>어떤 물리장비에서 정점이 실행되는지 알필요 x. “정점과 같이 생각하기”</li><li>그래프 알고리즘은 장비간 통신 오버헤드 ↑ (최적화 파티셔닝 x)</li><li>그래프가 단일 장비에 넣기 너무 크다면 프리글 같은 분산 접근법 필수</li></ul></li></ul></li></ul><h3 id="고수준-API와-언어"><a href="#고수준-API와-언어" class="headerlink" title="고수준 API와 언어"></a>고수준 API와 언어</h3><ul><li>고수준 API와 언어<ul><li>직접 맵리듀스 작업 작성은 매우 어려움 =&gt; 고수준 API 인기</li><li>이런 데이터플로 API는 일반적으로 관계형 스타일의 빌딩 블록을 사용해 연산 표현</li><li>장점<ul><li>적은 코드 작성</li><li>대화식 사용 지원 (여러 접근법 실험 가능)</li><li>시스템의 생산성 높은 사용 및 장비의 효율적 사용</li></ul></li></ul></li><li>선언형 질의 언어로 전환<ul><li>선언적인 방법으로 조인 지정시 =&gt; (맵리듀스, 데이터플로 계승자들의 내장된) 질의 최적화기가 최적 방법 결정</li><li>장점 <ul><li><strong>코드 임의 실행 및 임의 형식의 데이터 읽기 가능</strong></li><li>칼럼 기반 저장 레이아웃으로 최적화 가능</li><li>Hive, Spark DataFrame, Imapala Vectorized 수행  =&gt; 캐시 히트율 ↑ or 함수 호출 회피</li></ul></li></ul></li><li>다양한 분야를 지원하기 위한 전문화<ul><li>표준화된 처리 패턴 =&gt; 재사용 가능한 공통 빌딩 블록 구현</li><li>재사용 구현의 예 : Mahout, MADlib, 공간 알고리즘 (ex. K-nearest neighbor) </li><li>일괄 처리 엔진은 점차 광범위한 영역에서 필요 알고리즘을 분산 수행하는데 사용</li></ul></li><li>일괄 처리 시스템 vs MPP 데이터베이스<ul><li>점차 비슷해지고 있다 (결국엔 둘다 데이터 저장하고 처리하는 시스템)</li><li>일괄 처리 엔진은 내장 기능 + 고수준 선언적 연산자</li><li>MPP 는 프로그래밍이 가능한 유연성</li></ul></li></ul><br/><h2 id="10-정리"><a href="#10-정리" class="headerlink" title="10 정리"></a>10 정리</h2><ul><li>일괄 처리<ul><li>유닉스 도구 및 이의 설계 철학이 어떻게 맵리듀스와 데이터플로 엔진에 녹아있는지</li><li>설계 원리 : 입력은 불변 &amp; 출력은 다른 프로그램의 입력. 복잡한 문제는 ‘한가지 일을 잘하는’ 작은 도구를 엮어서 해결</li></ul></li><li>인터페이스<ul><li>유닉스 환경에서의 프로그램 간 연결하는 단일 인터페이스 =&gt; 파일, 파이프</li><li>맵리듀스의 인터페이스 =&gt; 분산 파일 시스템</li><li>데이터 플로 엔진 =&gt; 자체 데이터 전송 메커니즘 (입출력 HDFS 사용)</li></ul></li><li>분산 일괄 처리 프레임 워크의 해결해야할 2가지 문제<ul><li>파티셔닝<ul><li>매퍼 : 입력 파일 블록에 따라 파티셔닝</li><li>리듀서 : 매퍼의 출력 재파티셔닝 &amp; 정렬 =&gt; 사용자 지정 파티션 개수로 병합</li><li>(데이터 플로 엔진은 필요한 경우가 아니면 정렬 x)</li></ul></li><li>내결함성<ul><li>맵리듀스는 번번히 디스크에 기록</li><li>데이터플로 엔진은 메모리에 상태 유지 (중간 상태를 최대한 구체화 x)</li><li>(결정적 연산자로 재계산 필요 데이터량 절약 가능)</li></ul></li></ul></li><li>맵리듀스의 조인 알고리즘 (=&gt; MPP DB, 데이터플로 엔진 내부에서 사용)<ul><li>정렬 병합 조인<ul><li>각 입력이 조인 키를 추출하는 매퍼 통과 =&gt; 파티셔닝, 정렬, 병합 =&gt; 같은 키를 가지는 모든 레코드는 하나의 리듀서에서 호출 (=&gt; 병합된 레코드 출력가능)</li></ul></li><li>브로드캐스트 해시 조인<ul><li>조인할 입력 둘 중 하나가 상대적으로 작은 경우, 파티셔닝하지 않고 해시 테이블에 모두 적재</li></ul></li><li>파티션 해시 조인<ul><li>조인 입력 두개를 같은 방식으로 파티셔닝 (같은 키, 해시함수, 파티션 수) =&gt; 각 파티션 별 독립적 해시 테이블 방식 사용</li></ul></li></ul></li><li>분산 일괄 처리 엔진은 의도적으로 제한된 프로그래밍 모델 제공<ul><li>매퍼/리듀서같은 콜백함수는 상태정보 x, 지정된 출력외 부수 효과 x …</li><li>why? 분산 시스템 내 발생하는 문제들을 추상화 아래로 숨길 수 있음</li><li>문제발생해도 태스크는 안전한 재시도, 실패 태스크의 출력은 폐기 =&gt; 최종 출력이 결함이 생기지 않을 때와 동일 보장</li><li>=&gt; 내결함성 매커니즘 구현 필요 x (신뢰성의 시맨틱)</li></ul></li><li>일괄 처리 작업의 특징<ul><li>입력을 수정하지 않고 읽어 출력 생산</li><li>입력 데이터는 고정된 크기로 한정 (끝 판단 및 작업 종료 가능)</li><li>vs 스트림 처리 [11장] =&gt; 입력이 한정되지 않고 끝이 없음</li></ul></li></ul><br/><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li>MPP 데이터베이스란<ul><li><a href="https://www.comworld.co.kr/news/articleView.html?idxno=49459">https://www.comworld.co.kr/news/articleView.html?idxno=49459</a></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;데이터 중심 어플리케이션 설계 [PART 2]&lt;/strong&gt;&lt;br&gt;Designing Data Intensive Applications - O’REILLY&lt;/p&gt;
&lt;p&gt;10. Batch Processing&lt;/</summary>
      
    
    
    
    <category term="ddia" scheme="https://minsw.github.io/categories/ddia/"/>
    
    
    <category term="data" scheme="https://minsw.github.io/tags/data/"/>
    
    <category term="data_intensive_application" scheme="https://minsw.github.io/tags/data-intensive-application/"/>
    
    <category term="ddia" scheme="https://minsw.github.io/tags/ddia/"/>
    
    <category term="book" scheme="https://minsw.github.io/tags/book/"/>
    
    <category term="study" scheme="https://minsw.github.io/tags/study/"/>
    
  </entry>
  
  <entry>
    <title>&amp;#039;데이터 중심 어플리케이션 설계&amp;#039; 6장 - 파티셔닝</title>
    <link href="https://minsw.github.io/2021/05/25/Designing-Data-Intensive-Applications-06/"/>
    <id>https://minsw.github.io/2021/05/25/Designing-Data-Intensive-Applications-06/</id>
    <published>2021-05-24T16:06:13.000Z</published>
    <updated>2021-06-15T04:46:54.578Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>데이터 중심 어플리케이션 설계 [PART 2]</strong><br>Designing Data Intensive Applications - O’REILLY</p><p>06. Partitioning</p></blockquote><h3 id="📖-Overview"><a href="#📖-Overview" class="headerlink" title="📖 Overview"></a>📖 Overview</h3><p><a href="#06-1-%ED%82%A4-%EA%B0%92-%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%8C%8C%ED%8B%B0%EC%85%94%EB%8B%9D">06-1. 키-값 데이터 파티셔닝</a></p><ul><li><a href="#%ED%82%A4-%EB%B2%94%EC%9C%84-%EA%B8%B0%EC%A4%80-%ED%8C%8C%ED%8B%B0%EC%85%94%EB%8B%9D">키 범위 기준 파티셔닝</a></li><li><a href="#%ED%82%A4%EC%9D%98-%ED%95%B4%EC%8B%9C%EA%B0%92-%EA%B8%B0%EC%A4%80-%ED%8C%8C%ED%8B%B0%EC%85%94%EB%8B%9D">키의 해시값 기준 파티셔닝</a></li><li><a href="#%EC%8F%A0%EB%A6%B0-%EC%9E%91%EC%97%85%EB%B6%80%ED%95%98%EC%99%80-%ED%95%AB%EC%8A%A4%ED%8C%9F-%EC%99%84%ED%99%94">쏠린 작업부하와 핫스팟 완화</a></li></ul><p><a href="#06-2-%ED%8C%8C%ED%8B%B0%EC%85%94%EB%8B%9D%EA%B3%BC-%EB%B3%B4%EC%A1%B0-%EC%83%89%EC%9D%B8">06-2. 파티셔닝과 보조 색인</a></p><ul><li><a href="#%EB%AC%B8%EC%84%9C-%EA%B8%B0%EC%A4%80-%EB%B3%B4%EC%A1%B0-%EC%83%89%EC%9D%B8-%ED%8C%8C%ED%8B%B0%EC%85%94%EB%8B%9D">문서 기준 보조 색인 파티셔닝</a></li><li><a href="#%EC%9A%A9%EC%96%B4-%EA%B8%B0%EC%A4%80-%EB%B3%B4%EC%A1%B0-%EC%83%89%EC%9D%B8-%ED%8C%8C%ED%8B%B0%EC%85%94%EB%8B%9D">용어 기준 보조 색인 파티셔닝</a></li></ul><p><a href="#06-3-%ED%8C%8C%ED%8B%B0%EC%85%98-%EC%9E%AC%EA%B7%A0%ED%98%95%ED%99%94">06-3. 파티션 재균형화</a></p><ul><li><a href="#%EC%9E%AC%EA%B7%A0%ED%98%95%ED%99%94-%EC%A0%84%EB%9E%B5">재균형화 전략</a></li><li><a href="#%EC%9A%B4%EC%98%81-%EC%9E%90%EB%8F%99-%EC%9E%AC%EA%B7%A0%ED%98%95%ED%99%94%EC%99%80-%EC%88%98%EB%8F%99-%EC%9E%AC%EA%B7%A0%ED%98%95%ED%99%94">운영: 자동 재균형화와 수동 재균형화</a></li></ul><p><a href="#06-4-%EC%9A%94%EC%B2%AD-%EB%9D%BC%EC%9A%B0%ED%8C%85">06-4. 요청 라우팅</a></p><ul><li><a href="#%EB%B3%91%EB%A0%AC-%EC%A7%88%EC%9D%98-%EC%8B%A4%ED%96%89">병렬 질의 실행</a></li></ul><p><a href="#06-%EC%A0%95%EB%A6%AC">06 정리</a></p><br/><h1 id="6장-파티셔닝"><a href="#6장-파티셔닝" class="headerlink" title="6장 파티셔닝"></a>6장 파티셔닝</h1><h4 id="배경"><a href="#배경" class="headerlink" title="배경"></a>배경</h4><ul><li><p>파티셔닝이란?</p><ul><li>데이터셋이 매우 크거나, 질의 처리량이 매우 높은 경우 데이터를 의도적으로 작은 단위(‘파티션’)로 쪼개는 작업 (=샤딩)</li><li><strong><em>파티션</em></strong><ul><li>= 샤드(MongoDB, ES, Solr ..)</li><li>= 리전 (Hbase)</li><li>= 태블릿 (Bigtable)</li><li>= 브이노드 (Cassandra, Riak)</li><li>= 브이버켓 (Couchbase)</li></ul></li><li>파티션은 보통 각 데이터 단위(레코드, 로우, 문서) 가 하나의 파티션에 속함<ul><li>즉, 파티션 그 자체로 작은 데이터베이스가 됨</li></ul></li></ul></li><li><p>파티셔닝의 목적</p><ul><li>확장성, 부하 분산 (디스크, 질의 부하)</li><li>파티셔닝 지원 데이터베이스의 용도 2가지 : 트랜잭션 작업부하용, 분석용</li><li>용도에 따른 차이는 시스템 튜닝의 차이일뿐, <strong>파티셔닝의 기본 원칙은 동일하게 적용</strong></li></ul></li></ul><h4 id="그래서-6장에서는-💡"><a href="#그래서-6장에서는-💡" class="headerlink" title="그래서 6장에서는 💡"></a>그래서 6장에서는 💡</h4><ul><li>파티셔닝 방법</li><li>데이터 색인과 파티셔닝 간의 상호 작용</li><li>노드 추가/제거 시의 필요 작업 (=&gt; 재균형화)</li><li>데이터베이스가 올바른 파티션을 찾아 요청을 전달하고 질의를 실행하는 방법</li></ul><br/><h2 id="06-1-파티셔닝과-복제"><a href="#06-1-파티셔닝과-복제" class="headerlink" title="06-1 파티셔닝과 복제"></a>06-1 파티셔닝과 복제</h2><ul><li>사용 방법<ul><li>보통 파티셔닝 + 복제 함께 적용 (단, 방식은 독립적 선택)</li><li>각 리더는 하나의 노드에 할당</li></ul></li><li>파티셔닝 목적<ul><li>(데이터와 질의) 부하의 고른 분산</li><li>부하 분산이 안된 Bad Case =&gt; <strong><em>“쏠렸다 (skew)”</em></strong> (부하가 쏠린 파티션 = ‘핫스팟’)</li></ul></li><li>파티셔닝 방법<ul><li>Skew 및 핫스팟을 회피해야함</li><li>가장 심플한 솔루션은 ‘랜덤’ =&gt; 나조차도 어디 저장됐는지도 알 수 없음 (찾으려면 모든 노드에 질의해야)</li></ul></li></ul><h3 id="키-범위-기준-파티셔닝"><a href="#키-범위-기준-파티셔닝" class="headerlink" title="키 범위 기준 파티셔닝"></a>키 범위 기준 파티셔닝</h3><img src="https://user-images.githubusercontent.com/26691216/119421514-d2104800-bd39-11eb-803e-429f19a4a7e5.jpeg" width=400><ul><li>각 파티션의 연속된 범위 (MIN~MAX) 의 키 할당<ul><li>Like 백과사전</li></ul></li><li>파티션 경계 (키 범위 크기)<ul><li>동일하지 않아도 됨</li><li>관리자가 수동 선택 or 데이터베이스가 자동으로 선택하도록</li></ul></li><li>장단점<ul><li>장점 : 범위 스캔에 유용 (파티션 내 키 정렬 저장, 다중 칼럼 색인)</li><li>단점 : 특정 접근 패턴으로인한 핫스팟 발생 가능성 (ex. 실시간으로 기록되는 key = timestamp 센서)</li></ul></li><li>사용 예<ul><li>Bigtable, HBase, RethinkDB, MongoDB(&lt;2.4)</li></ul></li></ul><h3 id="키의-해시값-기준-파티셔닝"><a href="#키의-해시값-기준-파티셔닝" class="headerlink" title="키의 해시값 기준 파티셔닝"></a>키의 해시값 기준 파티셔닝</h3><ul><li>Skew와 핫스팟 회피를 위해 해시함수 많이 사용<ul><li>좋은 해시함수는 유사한 데이터가 인입되어도 균일되게 분산</li><li>프로그래밍 언어의 내장 해시 함수는 파티셔닝에 적합하지 않을 수도 (ex. Java의 <code>Object.hashCode()</code>)</li></ul></li><li>파티션 경계<ul><li>동일하지 않아도 됨 (무작위에 가깝게 선택도 가능)</li><li>= ‘일관성 해싱’? <ul><li>부하를 균등하게 분산시키는 방법 (잘 사용 x)</li></ul></li></ul></li><li>장단점<ul><li>장점 : 키를 파티션 사이에 균일하게 분산</li><li>단점 : 범위 질의 효율성 ↓ 정렬 순서 유지 x</li></ul></li><li>사용 예<ul><li>Riak, Couchbase, Voldmort, MongoDB (<a href="https://docs.mongodb.com/manual/core/hashed-sharding/">해시기반 샤딩 모드</a>)</li><li>Cassandra : 해시값 + 범위 기준 파티셔닝 전략 타협안 사용 (=&gt; 복합 기본키)</li></ul></li></ul><h3 id="쏠린-작업부하와-핫스팟-완화"><a href="#쏠린-작업부하와-핫스팟-완화" class="headerlink" title="쏠린 작업부하와 핫스팟 완화"></a>쏠린 작업부하와 핫스팟 완화</h3><img src="https://user-images.githubusercontent.com/26691216/119422588-64b1e680-bd3c-11eb-9197-9f680970d428.gif" width=150><center style="color:lightgray;"><i>(암호화폐는 유망하지만 투자는 조심혔어야재-)</i></center><ul><li>핫스팟을 완전히 제거할 수는 없다<ul><li>극단적 상황의 작업부하 (ex. 유명인의 SNS 활동의 영향 =&gt; 동일 id이므로 해싱 소용 x)</li></ul></li><li>현대 데이터 시스템은 skew에 대한 자동 보정이 어려움<ul><li>대부분 어플리케이션 단에서 완화</li><li>이로 인한 불필요한 오버헤드는 방지할 것 (세분화)</li><li>=&gt; <em>트레이드 오프를 따져볼 것</em></li></ul></li></ul><br/><h2 id="06-2-파티셔닝과-보조-색인"><a href="#06-2-파티셔닝과-보조-색인" class="headerlink" title="06-2 파티셔닝과 보조 색인"></a>06-2 파티셔닝과 보조 색인</h2><ul><li>키-값 데이터모델에 의존한 파티셔닝 방식은 명확함<ul><li>기본키를 통한 레코트 식별</li></ul></li><li>여기에 <strong>보조 색인</strong> 이 연관되면? =&gt; 파티션에 깔끔하게 대응되지 x<ul><li>보조 색인 : 특정한 값이 발생한 항목을 검색하는 수단</li><li>데이터 모델링에 유용 (관계형, 문서 데이터베이스에서도 흔히 사용)</li></ul></li><li>보조 색인이 있는 데이터베이스 색인 방식 2가지<ul><li>문서 기반 파티셔닝 (local index)</li><li>용어 기반 파티셔닝 (global index)</li></ul></li></ul><h3 id="문서-기준-보조-색인-파티셔닝"><a href="#문서-기준-보조-색인-파티셔닝" class="headerlink" title="문서 기준 보조 색인 파티셔닝"></a>문서 기준 보조 색인 파티셔닝</h3><img src="https://user-images.githubusercontent.com/26691216/119423722-e4d94b80-bd3e-11eb-9c1f-891bb225682d.png" width=400><ul><li>문서 파티셔닝 색인 (지역 색인, local index)<ul><li>각 파티션은 자신의 보조 색인을 유지</li><li>보조 색인도 그 파티션이 속하는 문서만 담당</li><li>즉, 문서 CUD 시 쓰려고하는 파티션만 다루면 된다</li></ul></li><li>장단점<ul><li>장점 : write =&gt; 파티션의 보조색인이 100% 독립적</li><li>단점 : read 시 <strong>모든</strong> 파티션으로 요청 + 취합 과정 필요 (scatter/gather)<ul><li>꼬리 지연 시간 증폭 발생 가능성</li><li>질의가 단일 파티션에서만 실행되도록 권장되지만 어렵다 (특히 한번에 여러 보조 색인 사용시)</li></ul></li></ul></li><li>사용 예<ul><li>MongoDB, Riak, Cassandra, Elasticsearch, SolrCloud, VoltDB</li></ul></li></ul><h3 id="용어-기준-보조-색인-파티셔닝"><a href="#용어-기준-보조-색인-파티셔닝" class="headerlink" title="용어 기준 보조 색인 파티셔닝"></a>용어 기준 보조 색인 파티셔닝</h3><img src="https://user-images.githubusercontent.com/26691216/119423728-ea369600-bd3e-11eb-825f-667157e56a21.png" width=400><ul><li>용어 기준 파티셔닝 색인 (전역 색인, global index)<ul><li>“용어 기준으로 파티셔닝되다 (term-partitioned)”</li><li>용어 = 문서에 등장하는 모든 단어. from 전문 색인 (특별한 종류의 보조 색인)</li></ul></li><li>전역 색인도 파티셔닝은 필요<ul><li>한 노드에 저장시 병목 발생</li><li>방법 : 용어 자체 사용 (범위 스캔 good) or 용어 해시값 사용 (분산 good)</li></ul></li><li>장단점<ul><li>장점 : read =&gt; 모든 파티션에 질의 할 필요 x</li><li>단점 : write가 복잡하고 느림 (여러 파티션에 영향)<ul><li>색인을 항상 최신상태로 유지하려면 =&gt; 쓰기에 영향받는 모든 파티션에 대한 ‘분산 트랜잭션’ 필요</li><li>실제로는 대부분 비동기로 갱신</li></ul></li></ul></li><li>사용 예<ul><li>Riak 검색, Oracle Warehouse (지역/전역 선택 가능)</li></ul></li></ul><br/><h2 id="06-3-파티션-재균형화"><a href="#06-3-파티션-재균형화" class="headerlink" title="06-3 파티션 재균형화"></a>06-3 파티션 재균형화</h2><ul><li>재균형화 (리밸런싱, Rebalancing)<ul><li>장비의 추가/제거로 인해 데이터와 요청을 다른노드로 옮기는 과정</li></ul></li><li>재균형화의 최소 요구사항<ol><li> 리밸런싱 후 부하가 노드간 균등하게 분배될 것</li><li> 리밸런싱 중에도 읽기/쓰기 요청을 받아들일 것</li><li> 리밸런싱 소요 시간과 네트워크, 디스크 IO 부하를 최소화 할 것 (이동 데이터 최소화)</li></ol></li></ul><h3 id="재균형화-전략"><a href="#재균형화-전략" class="headerlink" title="재균형화 전략"></a>재균형화 전략</h3><ul><li>파티션을 노드에 (새로) 할당하는 방법 3가지</li><li>(나쁜 예 : mod N 연산 =&gt; N 변경시 대부분의 데이터가 이동되어야함)</li></ul><blockquote><h4 id="파티션-개수-고정"><a href="#파티션-개수-고정" class="headerlink" title="파티션 개수 고정"></a>파티션 개수 고정</h4><p>파티션 크기 ∝ 전체 데이터셋 크기</p></blockquote><ul><li>파티션을 노드 대수보다 많이 만들어서 각 노드당 여러 파티션을 할당</li><li>노드 추가/제거 시) 기존 노드에서 파티션 몇개를 뺏어옴<ul><li>파티션은 노드 사이에서 통째로 이동 (파티션 개수 및 내용 유지)</li><li>네트워크를 통해 데이터 이동 (시간 소요) =&gt; 리밸런싱 중 요청은 기존 할당된 파티션 사용</li></ul></li><li>장단점<ul><li>장점 : 파티션 통째로 이동. 바뀌는점은 “어느 노드에 어느 파티션이 할당되었나” 뿐</li><li>단점 : 파티션 수는 고정인데 데이터셋 크기 변동이 심할 경우 =&gt; 적절한 파티션 수 지정 어려움<ul><li>대부분 처음 설정된 파티션 수 = 사용 가능한 노드 대수의 최대치</li><li>파티션이 너무 작으면 오버헤드↑, 너무 크면 재균형화와 노드 장애 시 복구 비용↑</li></ul></li></ul></li><li>사용 예<ul><li>Riak, ES, CouchBase, Voldmort</li></ul></li></ul><blockquote><h4 id="동적-파티셔닝"><a href="#동적-파티셔닝" class="headerlink" title="동적 파티셔닝"></a>동적 파티셔닝</h4><p>파티션 크기 ∝ 데이터 셋</p></blockquote><ul><li>파티션 개수가 고정되면 문제가 생기는 경우 (ex. 키 범위 파티셔닝)</li><li>동적 파티셔닝<ul><li>파티션 크기가 설정된 값을 넘어서면 두개로 쪼갬</li><li>파티션 크기가 임곗값 아래로 떨어지면 인접한 파티션과 합침 (like B트리)</li></ul></li><li>사전 분할(pre-splitting)<ul><li>빈 데이터베이스의 초기 파티션 집합 설정</li></ul></li><li>장단점<ul><li>장점 : 전체 데이터양에 맞게 조정되는 적절한 파티션 수 (오버헤드 ↓)</li><li>단점 : 시작 시에는 파티션이 하나 (사전정보 x)</li></ul></li><li>사용 예<ul><li>HBase, RethinkDB (키 범위), MongoDB (&gt;2.4, 키 범위 &amp; 해시)</li></ul></li></ul><blockquote><h4 id="노드-비례-파티셔닝"><a href="#노드-비례-파티셔닝" class="headerlink" title="노드 비례 파티셔닝"></a>노드 비례 파티셔닝</h4><p>파티션 개수 ∝ 노드 대수</p></blockquote><ul><li>노드당 할당되는 파티션 개수를 고정<ul><li>노드 대수가 동일하면 (= 파티션 개수 고정)</li><li>노드 대수가 증가하면 =&gt; 파티션 수 ↓ (절반은 두고 절반은 새 노드에 할당)</li></ul></li><li>장단점<ul><li>장점 : 개별 파티션 크기가 안정적</li><li>단점 : 신규 노드 추가로 인한 파티션 분할 시 무작위 분할 =&gt; 균등하지 않은 분할 가능성<ul><li>무작위 분할 =&gt; 해시 기반 파티셔닝 사용 (=일관성 해싱)</li></ul></li></ul></li></ul><h3 id="운영-자동-재균형화와-수동-재균형화"><a href="#운영-자동-재균형화와-수동-재균형화" class="headerlink" title="운영: 자동 재균형화와 수동 재균형화"></a>운영: 자동 재균형화와 수동 재균형화</h3><ul><li>재균형화는 자동으로해야할까 수동으로해야할까?<ul><li>완전 자동 재균형화 vs 완전 수동 재균형화</li><li>그 중간 어디쯤.. =&gt; 시스템이 자동으로 파티션 할당을 제안하고 + 관리자가 확정해야 반영되는</li></ul></li><li>자동 재균형화<ul><li>편리하지만, 예측이 어렵고 성능 저하 이슈나 연쇄 장애 가능성</li></ul></li><li>수동 재균형화<ul><li>느리지만 예상치 못한 이슈 방지 가능</li></ul></li></ul><br/><h2 id="06-4-요청-라우팅"><a href="#06-4-요청-라우팅" class="headerlink" title="06-4 요청 라우팅"></a>06-4 요청 라우팅</h2><p>“Service Discovery”</p><ul><li>Service Discovery 접근법 (라우팅 방법) 3가지<ul><li>노드 : 아무 노드나 읽어서 어느 노드에 있는지 찾아서 요청</li><li>라우팅 계층 : 모든 요청을 라우팅 계층으로 보내서 전달 (Partition-aware LB)</li><li>클라이언트 : 클라Client가 파티셔닝 방법 및 할당 위치를 알고 직접 해당 노드에 요청</li></ul></li><li>핵심 문제는? =&gt; <strong><em>“파티션의 변경 사항을 어떻게 알것인가”</em></strong><ul><li>외부의 별도 코디네이션 (ex. Zookeeper) 사용 : 변경사항에 대해 모든걸 관리하고 알려줌</li><li>가십 프로토콜 (gossip protocol): 클러스터 상태 변화를 노드 사이에 퍼뜨림. 노드 복잡도 ↑ 외부의존도 ↓</li></ul></li></ul><h3 id="병렬-질의-실행"><a href="#병렬-질의-실행" class="headerlink" title="병렬 질의 실행"></a>병렬 질의 실행</h3><ul><li>더 복잡한 종류의 질의 (분석용)<ul><li>=&gt; 대규모 병렬 처리 (MPP, Massively Parallel Processing) 관계형 DB</li></ul></li><li>MPP 질의 최적화기가 복잡한 질의를 분해하여 서로 다른 노드에서 병렬적 실행 가능하게함<ul><li>웨어하우스 질의 (join, filter, grouping, aggregation)</li></ul></li><li>데이터 웨어하우스 질의 고속 병렬 실행</li></ul><br/><h2 id="06-정리"><a href="#06-정리" class="headerlink" title="06 정리"></a>06 정리</h2><ul><li>파티셔닝이란<ul><li>대용량 데이터셋을 더욱 작은 데이터셋으로 파티셔닝</li></ul></li><li>파티셔닝의 목적<ul><li>핫스팟이 생기지 않게 + 데이터와 질의부하를 균일하게 분배</li></ul></li><li>주요 파티셔닝 기법 2가지<ul><li>키 범위 파티셔닝 (보통 동적)</li><li>해시 파티셔닝 (보통 고정 파티션수)</li></ul></li><li>파티셔닝 + 보조 색인<ul><li>문서 파티셔닝 색인 (지역)</li><li>용어 파티셔닝 색인 (전역)</li></ul></li><li>질의 라우팅 기법</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;데이터 중심 어플리케이션 설계 [PART 2]&lt;/strong&gt;&lt;br&gt;Designing Data Intensive Applications - O’REILLY&lt;/p&gt;
&lt;p&gt;06. Partitioning&lt;/p&gt;
&lt;</summary>
      
    
    
    
    <category term="ddia" scheme="https://minsw.github.io/categories/ddia/"/>
    
    
    <category term="data" scheme="https://minsw.github.io/tags/data/"/>
    
    <category term="data_intensive_application" scheme="https://minsw.github.io/tags/data-intensive-application/"/>
    
    <category term="ddia" scheme="https://minsw.github.io/tags/ddia/"/>
    
    <category term="book" scheme="https://minsw.github.io/tags/book/"/>
    
    <category term="study" scheme="https://minsw.github.io/tags/study/"/>
    
  </entry>
  
  <entry>
    <title>&amp;#039;데이터 중심 어플리케이션 설계&amp;#039; 2장 - 데이터 모델과 질의 언어</title>
    <link href="https://minsw.github.io/2021/05/11/Designing-Data-Intensive-Applications-02/"/>
    <id>https://minsw.github.io/2021/05/11/Designing-Data-Intensive-Applications-02/</id>
    <published>2021-05-10T16:02:19.000Z</published>
    <updated>2021-06-15T04:46:53.234Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>데이터 중심 어플리케이션 설계 [PART 1]</strong><br>Designing Data Intensive Applications - O’REILLY</p><p>02. Data Models and Query Languages</p></blockquote><br/><h3 id="📖-Overview"><a href="#📖-Overview" class="headerlink" title="📖 Overview"></a>📖 Overview</h3><p><a href="#02-1-%EA%B4%80%EA%B3%84%ED%98%95-%EB%AA%A8%EB%8D%B8%EA%B3%BC-%EB%AC%B8%EC%84%9C-%EB%AA%A8%EB%8D%B8">02-1. 관계형 모델과 문서 모델</a></p><ul><li><a href="#NoSQL%EC%9D%98-%ED%83%84%EC%83%9D">NoSQL의 탄생</a></li><li><a href="#%EA%B0%9D%EC%B2%B4-%EA%B4%80%EA%B3%84%ED%98%95-%EB%B6%88%EC%9D%BC%EC%B9%98">객체 관계형 불일치</a></li><li><a href="#%EB%8B%A4%EB%8C%80%EC%9D%BC%EA%B3%BC-%EB%8B%A4%EB%8C%80%EB%8B%A4-%EA%B4%80%EA%B3%84">다대일과 다대다 관계</a></li><li><a href="#%EB%AC%B8%EC%84%9C-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4%EB%8A%94-%EC%97%AD%EC%82%AC%EB%A5%BC-%EB%B0%98%EB%B3%B5%ED%95%98%EA%B3%A0-%EC%9E%88%EB%82%98">문서 데이터베이스는 역사를 반복하고 있나?</a></li><li><a href="#%EA%B4%80%EA%B3%84%ED%98%95-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4%EC%99%80-%EC%98%A4%EB%8A%98%EB%82%A0%EC%9D%98-%EB%AC%B8%EC%84%9C-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4">관계형 데이터베이스와 오늘날의 문서 데이터베이스</a></li></ul><p><a href="#02-2-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A5%BC-%EC%9C%84%ED%95%9C-%EC%A7%88%EC%9D%98-%EC%96%B8%EC%96%B4">02-2. 데이터를 위한 질의 언어</a></p><ul><li><a href="#%EC%9B%B9%EC%97%90%EC%84%9C%EC%9D%98-%EC%84%A0%EC%96%B8%ED%98%95-%EC%A7%88%EC%9D%98">웹에서의 선언형 질의</a></li><li><a href="#%EB%A7%B5%EB%A6%AC%EB%93%80%EC%8A%A4-%EC%A7%88%EC%9D%98">맵리듀스 질의</a></li></ul><p><a href="#02-3-%EA%B7%B8%EB%9E%98%ED%94%84%ED%98%95-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%AA%A8%EB%8D%B8">02-3. 그래프형 데이터 모델</a></p><ul><li><a href="#%EC%86%8D%EC%84%B1-%EA%B7%B8%EB%9E%98%ED%94%84">속성 그래프</a></li><li><a href="#%EC%82%AC%EC%9D%B4%ED%8D%BC-%EC%A7%88%EC%9D%98-%EC%96%B8%EC%96%B4">사이퍼 질의 언어</a></li><li><a href="#SQL%EC%9D%98-%EA%B7%B8%EB%9E%98%ED%94%84-%EC%A7%88%EC%9D%98">SQL의 그래프 질의</a></li><li><a href="#%ED%8A%B8%EB%A6%AC%ED%94%8C-%EC%A0%80%EC%9E%A5%EC%86%8C%EC%99%80-%EC%8A%A4%ED%8C%8C%ED%81%B4">트리플 저장소와 스파클</a></li><li><a href="#%EC%B4%88%EC%84%9D-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A1%9C%EA%B7%B8">초석: 데이터로그</a></li></ul><p><a href="#02-%EC%A0%95%EB%A6%AC">02 정리</a><br><a href="#Reference">Reference</a></p><br/><h1 id="02-데이터-모델과-질의-언어"><a href="#02-데이터-모델과-질의-언어" class="headerlink" title="02. 데이터 모델과 질의 언어"></a>02. 데이터 모델과 질의 언어</h1><h4 id="배경"><a href="#배경" class="headerlink" title="배경"></a>배경</h4><blockquote><p><em>“내 언어의 한계는 내 세계의 한계를 의미한다”</em> (..?)</p></blockquote><ul><li><p>데이터 모델 = 소프트웨어 개발에서 제일 중요한 부분</p><ul><li>(why?) 소프트웨어가 어떻게 작성됐는지 + 해결하려는 <strong>문제를 어떻게 생각해야 하는지</strong> 에도 영향</li></ul></li><li><p>하나의 데이터 모델을 다른 데이터 모델 위에 계층을 둬서 만드는 것이 일반적 애플리케이션 개발</p></li><li><p>각 계층의 핵심적 문제는 다음 하위 계층 관점에서 데이터 모델을 <strong>표현</strong>하는 방법</p><details><blockquote><p>(1) 애플리케이션 개발자는 현실을 보고 객체나 데이터구조, API를 모델링 (for 애플리케이션)<br><U>(2) 데이터 구조 저장 시에는 <strong>범용 데이터 모델</strong>로 표현 (JSON/XML 문서, 관계형 DB 테이블, 그래프 모델)</U> =&gt; <strong>2장 내용</strong><br>(3) DB SW 개발하는 엔지니어는 위 데이터를 메모리나 디스크, 네트워크 상의 바이트 단위로 표현하는 방법 결정 (표현은 다양한방법으로 질의,탐색,조작,처리 등을 가능하게함) =&gt; 3장<br>(4) HW 개발 엔지니어는 전류, 빛의파동, 자기장등의 관점에서 바이트를 표현하는 방법 발견</p></blockquote></details></li><li><p>각 계층은 명확한 데이터 모델을 제공해 하위 계층의 복잡성을 숨긴다 (추상화)</p></li><li><p>다양한 유형의 데이터 모델 =&gt; 각 데이터 모델은 사용 방법에 대한 가정을 나타냄</p></li><li><p>애플리케이션에 적합한 데이터 모델을 선택하는 작업은 매우 중요</p><ul><li>(why?) 데이터모델은 그 위에서 SW가 할수 있는/없는 일에 지대한 영향을 주므로</li></ul></li></ul><h4 id="그래서-2장에서는-💡"><a href="#그래서-2장에서는-💡" class="headerlink" title="그래서 2장에서는 💡"></a>그래서 2장에서는 💡</h4><ul><li>데이터 저장과 질의를 위한 다양한 범용 데이터 모델 알아보기<ul><li>관계형 모델 (relational model)</li><li>문서 모델 (document model)</li><li>그래프 기반 데이터 모델 (graph-based data model)</li></ul></li><li>다양한 질의 언어 &amp; 사용 사례 비교</li></ul><br/><h2 id="02-1-관계형-모델과-문서-모델"><a href="#02-1-관계형-모델과-문서-모델" class="headerlink" title="02-1 관계형 모델과 문서 모델"></a>02-1 관계형 모델과 문서 모델</h2><p>Most Famous 데이터모델은? =&gt; 관계형 모델 기반의 <strong>SQL</strong></p><h4 id="관계형-모델"><a href="#관계형-모델" class="headerlink" title="관계형 모델"></a>관계형 모델</h4><ul><li>데이터 구성<ul><li>관계 (relation) : 순서 없는 튜플의 모음 (= SQL의 테이블)</li><li>튜플 (tuple) : (= SQL의 로우(row))</li></ul></li><li>관계형 데이터베이스 관리 시스템 (<strong>RDBMS</strong>, relational database management system)<ul><li>RDBMS + SQL</li><li>정규화된 구조로 데이터를 저장/질의 가능</li></ul></li><li>관계형 데이터베이스 (RDB) 의 근원 =&gt; <strong>비즈니스 데이터 처리</strong><ul><li>사용 사례 : 트랜젝션 처리, 일괄처리</li></ul></li><li>관계형 모델의 목표<ul><li>“정리된 인터페이스 뒤로 구현 세부사항을 숨기는 것”</li></ul></li><li>접근 방식의 경쟁<ul><li>네트워크 모델, 계층 모델 제시 (1970-80s)</li><li>객체 데이터 베이스 반짝 (1980-90s)</li><li>XML 데이터베이스 인기no (2000s)</li><li>결국은 <U>관계형 모델이 오랜시간 우위 지속</U></li><li>NoSQL은? (2010s) =&gt; <a href="#NoSQL%EC%9D%98-%ED%83%84%EC%83%9D">NoSQL의 탄생</a></li></ul></li><li>컴퓨터의 발전 &amp; 네트워크화로 인한 변화<ul><li>컴퓨터의 목적이 다양해짐</li><li>관계형 데이터베이스 =&gt; 비즈니스 데이터 처리 + a</li></ul></li></ul><h3 id="NoSQL의-탄생"><a href="#NoSQL의-탄생" class="headerlink" title="NoSQL의 탄생"></a>NoSQL의 탄생</h3><ul><li>NoSQL 이란?<ul><li>처음엔 인기 트위터 해시태그 <em>#NoSQL</em></li><li><strong>“Not Only SQL”</strong> 로 재해석됨</li></ul></li><li>NoSQL의 원동력 (장점)<ul><li>뛰어난 확장성</li><li>무료 오픈소스</li><li>특수질의 동작</li><li>제한↓ 동적이고 풍부한 표현력</li></ul></li><li>다중 저장소 지속성(polyglot persistence)<ul><li>애플리케이션의 요구사항에 따라 적절한 다양한 데이터 저장소를 동시에 사용</li><li>(관계형 데이터베이스 + 비관계형 데이터스토어) 사용도 가능</li></ul></li></ul><h3 id="객체-관계형-불일치"><a href="#객체-관계형-불일치" class="headerlink" title="객체 관계형 불일치"></a>객체 관계형 불일치</h3><ul><li>객체 지향 프로그래밍(OOP) 언어로 개발하는 어플리케이션 + SQL<ul><li>애플리케이션 코드 &lt;==(전환 계층 필요)==&gt; 데이터 베이스 모델 객체</li><li>RDB의 SQL 과 프로그래밍 언어 사이의 데이터 구조, 기능등의 차이로 인한 충돌</li><li>= <strong>임피던스 불일치(impedance mismatch)</strong></li></ul></li><li>객체 관계형 매핑(ORM) 프레임워크<ul><li>ORM (Object-Relational Mapping) : 객체와 관계형 데이터베이스의 데이터를 자동으로 매핑(연결)해주는 것</li><li>=&gt; 전환 계층의 Boilerplate Code 감소 (but 차이는 여전)</li></ul></li><li>예제<ul><li><code>p.30 ~</code> Linkedin 예제</li><li>이력서 같이 모든 내용을 갖추고 있는 <strong>문서</strong> 형태의 데이터 구조 (=&gt; JSON  👍🏻)</li></ul></li><li><strong>문서 지향 데이터베이스</strong> (document-oriented database)<ul><li>JSON 포맷 지원</li><li>ex. MongoDB, RethinkDB, CouchDB, Espresso</li></ul></li><li>JSON 표현<ul><li>임피던스 불일치 감소, 스키마 유연성, 더 나은 <strong>지역성(locality)</strong></li><li><U>일대다(1:N)</U> 관계 (데이터 트리 구조)</li><li>But, 데이터 부호화 형식으로서 가지는 문제 有 =&gt; [4장]</li></ul></li></ul><h3 id="다대일과-다대다-관계"><a href="#다대일과-다대다-관계" class="headerlink" title="다대일과 다대다 관계"></a>다대일과 다대다 관계</h3><ul><li>평문(텍스트 문자열) 저장 vs ID 사용<ul><li>ID : 의미있는 정보는 한 곳에만 저장. 참조는 모두 ID 사용</li><li>텍스트 : 그것을 사용하는 모든 레코드에서 중복 저장</li><li>즉, ID 사용의 장점은 =&gt; <U>중복 제거, 변경 용이</U></li></ul></li><li>중복된 데이터의 정규화<ul><li><strong>다대일(N:1)</strong> 관계 필요</li><li><U>관계형</U> &gt; 문서형</li></ul></li><li>문서 데이터베이스의 JOIN ?<ul><li>보통 지원 X (트리구조, 조인 필요 x)</li><li>DB가 지원하지않으면 <U>다중 질의를 만들어 조인을 흉내내야함</U></li></ul></li><li>Q. 앗 나는 join-free 문서여도 문제 없다구요?<ul><li>기능이 추가될수록 데이터는 상호 연결되는 경향 =&gt; <strong>다대다(N:M)</strong> 관계</li><li>따라서 참조 표현 / 질의를 위한 조인이 필요하게 될 수도 있음</li></ul></li></ul><h3 id="문서-데이터베이스는-역사를-반복하고-있나"><a href="#문서-데이터베이스는-역사를-반복하고-있나" class="headerlink" title="문서 데이터베이스는 역사를 반복하고 있나?"></a>문서 데이터베이스는 역사를 반복하고 있나?</h3><ul><li><em>“‘다대다 관계’ 를 어떻게 표현할건가? “</em> 의 논쟁<ul><li>관계형 데이터베이스 : “ssap-possible”</li><li>문서 데이터 베이스 &amp; NoSQL : “…”</li><li>오래된 ‘대논쟁’. 가장 초기의 전산화 데이터베이스 시스템으로 돌아가면 …</li></ul></li><li>IBM의 정보 관리 시스템 (IMS, Information Management System)<ul><li><strong>계층 모델</strong> 사용 (= JSON 모델과 비슷)</li><li>일대다 (o) / 다대다 (x) 조인(x)</li></ul></li><li>계층 모델의 한계를 극복하기 위한 2가지 솔루션<ul><li><strong>관계형 모델</strong> (SQL)</li><li><strong>네트워크 모델</strong> (희미..)</li></ul></li></ul><h4 id="네트워크-모델"><a href="#네트워크-모델" class="headerlink" title="네트워크 모델"></a>네트워크 모델</h4><ul><li>= <strong>코다실 모델</strong><ul><li>코다실 (CODASYL, Conference on Data Systems Languages) 에서 표준화</li></ul></li><li>특징<ul><li>계층 모델을 일반화 (+ 다중 부모 가능)</li><li>레코드 간 연결은 foreign key 보다는 ‘프로그래밍 언어의 포인터’ 와 비슷</li><li>사실상 삽입시 조인 수행</li></ul></li><li><strong>접근 경로</strong><ul><li>레코드 접근의 유일한 방법</li><li>최상위 레코드(root) 에서부터 연속된 연결 경로를 따르는 방법</li></ul></li><li>접근 경로의 문제점<ul><li>다중 부모 가질 시, 다양한 관계를 모두 추적해야 함</li><li>수동 접근 경로는 성능은 효율, 코드의 복잡성 및 유연X</li></ul></li><li>즉, 네트워크 모델 (계층 모델) 은 원하는 데이터가 경로에 없으면 GG<ul><li>새로운 접근 경로 다룰 시, 질의 코드 재작성 필요 (☠️)</li></ul></li></ul><h4 id="관계형-모델-1"><a href="#관계형-모델-1" class="headerlink" title="관계형 모델"></a>관계형 모델</h4><ul><li>관계형 모델의 하는 일<ul><li>모든 데이터 배치</li><li>관계(테이블) = 튜플(로우)의 컬렉션</li></ul></li><li>특징 (vs 네트워크 모델)<ul><li>복잡한 접근 경로 X</li><li>임의 조건에 일치하는 로우 읽기 가능</li><li>다른 테이블과의 foreign key 관계와 무관하게 로우 삽입 가능</li><li>질의 시 조인</li></ul></li><li>“접근경로가 없다?”<ul><li>없다기보단 <U>Query Optimizer가 자동으로 대신</U> 만드는 것</li></ul></li><li>관계형 데이터베이스의 ‘질의 최적화기 (Query Optimizer)’<ul><li>새로운 기능 추가가 쉬움<ul><li>새로운 색인을 위해 질의를 바꿀 필요 X (자동으로 가장 적합한 색인 사용)</li></ul></li><li>범용 최적화기 사용시 모든 애플리케이션이 혜택</li></ul></li></ul><h4 id="문서-데이터베이스와의-비교-공통점"><a href="#문서-데이터베이스와의-비교-공통점" class="headerlink" title="문서 데이터베이스와의 비교 (공통점)"></a>문서 데이터베이스와의 비교 (공통점)</h4><ul><li>문서 데이터베이스 = 계층 모델<ul><li>상위 레코드 내에 중첩된 레코드 저장 (별도 테이블 x)</li></ul></li><li>문서 데이터베이스 = 관계형 데이터베이스<ul><li>다대일, 다대다 관계 표현의 근본적 동작 =&gt; 관련 항목은 <U>고유한 식별자로 참조</U><ul><li>관계형 모델 : <strong>외래 키 (foreign key)</strong></li><li>문서 모델 : <strong>문서 참조 (document reference)</strong></li><li>=&gt; 조인이나 후속질의를 사용해 읽기 시점에 확인</li></ul></li></ul></li></ul><h3 id="관계형-데이터베이스와-오늘날의-문서-데이터베이스"><a href="#관계형-데이터베이스와-오늘날의-문서-데이터베이스" class="headerlink" title="관계형 데이터베이스와 오늘날의 문서 데이터베이스"></a>관계형 데이터베이스와 오늘날의 문서 데이터베이스</h3><ul><li>관계형 데이터베이스 vs 문서 데이터베이스<ul><li>내결함성(5장), 동시성 처리(7장), …</li><li>여기서는 <strong>‘데이터 모델’의 차이점</strong> 만 집중</li></ul></li><li>데이터 모델 비교<ul><li>관계형 DB : 조인, 다대일/다대다 관계 지원 good</li><li>문서 DB : 스키마 유연성, 지역성 (성능 ↑), 애플리케이션의 데이터구조와 유사<ul><li>한계) 문서 내 중첩항목 바로 참조 X, 미흡한 조인 지원</li></ul></li></ul></li><li><em>“아 ㅋㅋ 그래서 어떤게 더 간단한데?”</em><ul><li>=&gt; 데이터 항목 간의 관계 유형에 따라 다름</li></ul></li></ul><blockquote><p>a) 데이터가 문서랑 비슷한 구조일 경우 =&gt; <strong>문서 모델</strong></p><ul><li>여러 테이블로 찢는(shredding) 관계형 기법은 불필요한 복잡도</li></ul><p>b) 다대다 관계 사용 =&gt; <strong>관계형</strong> 사용</p><ul><li>비정규화 데이터의 일관성을 유지하기 위한 코드 복잡도 및 다중 요청 (성능 ↓)</li></ul><p>c) 상호 연결이 많은 데이터 =&gt; 관계형은 무난, <strong>그래프 모델</strong> 사용</p></blockquote><h4 id="문서-모델에서의-스키마-유연성"><a href="#문서-모델에서의-스키마-유연성" class="headerlink" title="문서 모델에서의 스키마 유연성"></a>문서 모델에서의 스키마 유연성</h4><ul><li>스키마 강요<ul><li>JSON (문서, 관계형) : 스키마 강요 X</li><li>XML (관계형) : 선택적 스키마 유효성 검사 포함</li></ul></li><li>문서 데이터베이스는 <strong>스키마리스(Schemaless)</strong> ?<ul><li>“스키마가 없다”<ul><li>= 임의의 키와 값을 문서에 추가 가능</li><li>= 읽을 때 필드 존재 여부를 보장하지 않음</li></ul></li><li>암묵적 스키마는 있지만 DB 는 강요 X</li></ul></li><li>스키마 접근 방식<ul><li>쓰기 스키마 (schema-on-write) : 스키마는 명시적이고 DB는 모든 데이터가 스키마를 따름을 보장 (RDB 접근 방식)<ul><li>(= Statically typed)</li><li>모든 레코드가 동일한 구조일 경우 good</li></ul></li><li>읽기 스키마 (schema-on -read) : 데이터 구조는 암묵적이고 데이터를 읽을 때만 해석<ul><li>(= Dynamically typed)</li><li>컬렉션 내 항목이 모두 동일한 구조가 아닐 경우 good</li></ul></li></ul></li><li>데이터 타입 변경 예시<ul><li>쓰기 스키마 : <strong>마이그레이션</strong> 수행 필요 (중단시간 ↑)</li><li>읽기 스키마 : 애플리케이션에서 이전 문서에 대한 처리 코드 추가</li></ul></li><li>더 자세한 스키마 내용은 [4장] 에서</li></ul><h4 id="질의를-위한-데이터-지역성"><a href="#질의를-위한-데이터-지역성" class="headerlink" title="질의를 위한 데이터 지역성"></a>질의를 위한 데이터 지역성</h4><ul><li><strong>저장소 지역성</strong> (storage locality)<ul><li>한번에 해당 문서의 많은 부분을 필요로 하는 경우 good</li></ul></li><li>문서 모델의 저장소 지역성<ul><li>큰 문서에서는 낭비일 수도 (갱신시에도 전체 문서 적재)<ul><li>부호화된 크기를 바꾸지 않는 수정은 쉽게 수행 가능 (<a href="https://www.mongodb.com/blog/post/schema-design-for-time-series-data-in-mongodb">ref #19</a>)</li><li>= update() vs upsert()</li></ul></li><li>따라서, 문서는 작게 유지하면서 크기가 커지는 쓰기를 지양</li></ul></li><li>문서 모델이 아닌 경우의 지역성<ul><li>구글의 Spanner DB 의 로우 교차 배치 스키마 (<a href="https://static.googleusercontent.com/media/research.google.com/ko//archive/spanner-osdi2012.pdf">ref $28</a>)</li><li>오라클의 다중 테이블 색인 클러스터 테이블(multi-table index cluster table)</li><li>빅테이블(Bigtable) 데이터 모델의 칼럼 패밀리(column-family) 개념</li></ul></li><li>더 자세한 지역성 내용은 [3장] 에서</li></ul><h4 id="문서-데이터베이스와-관계형-데이터베이스의-통합"><a href="#문서-데이터베이스와-관계형-데이터베이스의-통합" class="headerlink" title="문서 데이터베이스와 관계형 데이터베이스의 통합"></a>문서 데이터베이스와 관계형 데이터베이스의 통합</h4><ul><li>관계형 =&gt; 문서<ul><li>대부분의 RDBMS (MySQL 빼고) : XML 지원</li><li>PostgresQL(&gt;=9.3), MySQL(&gt;=5.7), IBM DB2(&gt;=10.5) : JSON 지원</li></ul></li><li>문서 =&gt; 관계형<ul><li>RethinkDB : 관계형 조인 지원</li><li>MongoDB driver : 자동으로 데이터베이스 참조 확인 (클라이언트 측 조인)</li></ul></li><li>관계형 DB 와 문서 DB는 점점 더 비슷해지는 중 (상호보완)</li></ul><img src="https://user-images.githubusercontent.com/26691216/117712091-d94e3680-b20e-11eb-94db-14b315bdfabd.png" width=200/><center><i>skrrr~🦷</i></center><br/><h2 id="02-2-데이터를-위한-질의-언어"><a href="#02-2-데이터를-위한-질의-언어" class="headerlink" title="02-2 데이터를 위한 질의 언어"></a>02-2 데이터를 위한 질의 언어</h2><p>관계형 모델의 등장 =&gt; 데이터를 <U>질의하는 새로운 방법</U> 도 등장</p><ul><li><strong>선언형</strong> 질의언어 (SQL, 관계대수)<ul><li>결과가 <strong>충족해야하는 조건</strong> + 데이터를 <strong>어떻게 변환</strong>할지 지정 (정렬, 그룹화, 집계, ..)</li><li>어떻게 실행할지는 Optimizer의 몫</li></ul></li><li><strong>명령형</strong> 코드 (IBM, 코다실)<ul><li>특정 순서로 특정 연산을 수행하게끔 컴퓨터에게 지시</li><li>목표를 달성하기 위한 <strong>방법</strong></li></ul></li><li>선언형의 장점 (vs 명령형)<ul><li>선언형은 더 간결하고 쉬운 작업</li><li>질의 변경 없이도 성능 향상 가능 (기능적으로 더 제한적 = 자동 최적화 여지 더 많음)</li><li>종종 병렬 실행에 적합 (순서 의존 x)</li></ul></li></ul><h3 id="웹에서의-선언형-질의"><a href="#웹에서의-선언형-질의" class="headerlink" title="웹에서의 선언형 질의"></a>웹에서의 선언형 질의</h3><ul><li>예제<ul><li><code>p.44 ~</code> 웹 사이트 예제</li><li>CSS, XSL (선언형) vs JS의 코어 DOM API (명령형)</li><li>명령형 접근 방식의 문제점<ul><li>클래스 삭제 감지 x (페이지 리로딩 필요)</li><li>API 변경 시 코드 재작성 필요</li></ul></li></ul></li><li>결론은 선언형 &gt; 명령형 ?<ul><li>웹 브라우저 : 선언형 CSS 스타일 사용이 낫다</li><li>데이터베이스 : 선언형 질의언어 SQL등이 명령형 질의 API보다 낫다</li></ul></li></ul><h3 id="맵리듀스-질의"><a href="#맵리듀스-질의" class="headerlink" title="맵리듀스 질의"></a>맵리듀스 질의</h3><ul><li><strong>맵리듀스 (MapReduce)</strong><ul><li>많은 컴퓨터에서 대량의 데이터를 처리하기 위한 프로그래밍 모델</li><li>(구글때문에 유명해졌다? (<a href="https://static.googleusercontent.com/media/research.google.com/ko//archive/mapreduce-osdi04.pdf">ref #33</a>))</li></ul></li><li>일부 NoSQL 데이터 저장소 =&gt; 제한된 형태의 맵리듀스 제공<ul><li>MongoDB, CouchDB</li><li>많은 문서를 대상으로 read-only 질의 시 사용</li></ul></li><li>예제 - MongoDB의 맵리듀스<ul><li>선언형 질의와 명령형 질의 API 그 사이 어디쯤</li><li>FP의 <code>map (collect)</code> &amp; <code>reduce (fold/inject)</code> 함수 기반<ul><li>단, <strong>순수 함수 (pure function)</strong> 여야함 (side-effect X)</li><li>임의 순서로 실행 가능, 재실행 가능</li></ul></li><li><strong>집계 파이프라인(aggregation pipeline)</strong> 지원 (&gt;= 2.2)<ul><li>JSON 기반 구문</li><li>맵리듀스 함수 작성의 어려움 해소를 위함</li><li>선언형 질의 언어 (=&gt; Optimizer 열일 가능)</li></ul></li></ul></li><li>맵리듀스 (low-level) vs SQL (high-level)<ul><li>반대 개념이 아님. 분산 SQL 구현도 가능하고, MR 사용한/사용하지 않은 구현 모두 O</li><li>질의 중간에 자바스크립트 사용/확장 가능 (MR, 일부 SQL)</li></ul></li><li>더 자세한 맵리듀스는 [10장] 에서</li></ul><br/><h2 id="02-3-그래프형-데이터-모델"><a href="#02-3-그래프형-데이터-모델" class="headerlink" title="02-3 그래프형 데이터 모델"></a>02-3 그래프형 데이터 모델</h2><blockquote><ul><li>1:N (트리구조), 레코드간 관계 無 =&gt; 문서모델</li><li>N:1, N:M 관계 =&gt; 관계형 모델</li><li>복잡한 N:M 관계 =&gt; <strong>그래프형 모델</strong></li></ul></blockquote><ul><li>구성<ul><li>정점 (vertex) : (=노드, 엔티티)</li><li>간선 (edge) : (=관계, 호(arc))</li></ul></li><li>특징<ul><li>많은 유형의 데이터 모델링 가능</li><li><strong>동종</strong> 데이터 뿐만아니라 다른 유형의 객체도 일관성있게 저장 가능</li></ul></li><li>그래프의 데이터 구조화<ul><li><strong>속성 그래프</strong> 모델 : Neo4j, Titan, InfiniteGraph</li><li><strong>트리플 저장소</strong> 모델 : Datomic, Allegrograph</li></ul></li><li>그래프의 질의 방식<ul><li><U>선언형 질의 언어 : Cypher, SPARQL, Datalog</U></li><li>명령형 질의 언어 : Gremlin</li><li>그래프 처리 프레임워크  : Pregel =&gt; [10장]</li></ul></li></ul><h3 id="속성-그래프"><a href="#속성-그래프" class="headerlink" title="속성 그래프"></a>속성 그래프</h3><ul><li>정점(vertex) 구성 요소<ul><li>고유한 식별자</li><li>유출(outgoing) 간선 집합</li><li>유입(incoming) 간선 집합</li><li>속성 컬렉션 (key-value)</li></ul></li><li>간선(edge) 구성 요소<ul><li>고유한 식별자</li><li>간선 시작 정점 (<strong>꼬리 정점</strong>)</li><li>간선 끝 정점 (<strong>머리 정점</strong>)</li><li>두 정점 간 관계 유형을 설명하는 <U>레이블</U></li><li>속성 컬렉션 (key-value)</li></ul></li><li>특징<ul><li>1. 정점은 다른 정점과 간선으로 연결됨 (특정 유형 제한 스키마 x)</li><li>2. 일련의 정점을 따라 앞뒤 방향으로 순회</li><li>3. 다른 유형의 관계에 서로 다른 레이블 사용 (=&gt; 깔끔한 모델 유지)</li></ul></li><li>즉, <strong>많은 유연성</strong> 제공<ul><li>기능 추가 시 쉬운 확장 ok</li><li>구조가 다르거나 데이터 입도가 가지각색인 경우도 ok</li><li>데이터 <strong>입도 (granularity)</strong><ul><li>“데이터가 얼마나 자세히 분할되었는가”</li><li>= 얼마나 세밀하게 나눌지 (Fine-grained) + 거칠게 묶을지(Coarse-grained)</li></ul></li></ul></li></ul><h3 id="사이퍼-질의-언어"><a href="#사이퍼-질의-언어" class="headerlink" title="사이퍼 질의 언어"></a>사이퍼 질의 언어</h3><ul><li><strong>사이퍼 (Cypher)</strong><ul><li>속성 그래프를 위한 <strong>선언형</strong> 질의 언어</li><li>Neo4j 그래프 데이터베이스용으로 제작</li></ul></li><li>사이퍼 질의<ul><li>정점 : 상징적 이름 (id) 포함</li><li>간선 생성 : <code>(꼬리노드 id) -[:간선 label]=&gt; (머리노드 id)</code></li><li><strong><code>MATCH</code></strong> 질의 사용 예제<ul><li><code>:WITHIN*0</code> = Regex <code>*</code> (0회 이상 반복)</li></ul></li></ul></li><li>다양한 질의 실행의 방법 =&gt; 고민 x (Optimizer 가 알아서)</li></ul><h3 id="SQL의-그래프-질의"><a href="#SQL의-그래프-질의" class="headerlink" title="SQL의 그래프 질의"></a>SQL의 그래프 질의</h3><ul><li>관계형 DB &lt;=&gt; 그래프 데이터<ul><li>관계형 ➡ 그래프 표현 : 가능 ([예제 2-2] 참고)</li><li>그래프 ➡ 관계형 구조로 + SQL 쿼리 : 가…가능.. (△)</li></ul></li><li>그래프 데이터 vs 관계형 데이터베이스 (변환이 어려운 이유)<ul><li>관계형 : 질의에 필요한 조인 미리 파악 가능</li><li>그래프 : 가변적인 간선 순회 (조인 개수 고정 X)</li></ul></li><li><strong>재귀 공통 테이블 식 (recursive common table expression)</strong><ul><li><code>WITH RECURSIVE</code> 문</li><li>관계형 DB에서도 가변 순회 경로에 대한 질의 표현 가능</li><li>But 예제에서는 훨씬 길고 복잡한 질의문.. ([예제 2-4 vs 2-5])</li></ul></li><li>=&gt; 사용 사례에 맞는 데이터 모델 선택이 중요</li></ul><h3 id="트리플-저장소와-스파클"><a href="#트리플-저장소와-스파클" class="headerlink" title="트리플 저장소와 스파클"></a>트리플 저장소와 스파클</h3><ul><li>트리플 저장소 모델 == 속성 그래프 모델 ?<ul><li>거의 동등. 같은 생각을 다른 용어로 표현</li></ul></li><li>정보 저장 형식 =&gt; <U><strong>세 부분 구문</strong> (three-part statements)</U><ul><li>(<code>주어(subject)</code>, <code>서술어(predicate)</code>, <code>목적어(object)</code>)</li><li>주어 = 그래프의 정점 (V)</li><li>목적어 = (1) 원시 데이터타입 값 (value) or (2) 다른 그래프의 정점(V)</li><li>서술어 = (1) 속성 (key) or (2) 간선(E)</li></ul></li><li>예시<ul><li>(1) 정점+속성의 키+값 : ex. <code>(루시, 나이(key), 33(value))</code></li><li>(2) 꼬리정점+간선+머리정점 : ex. <code>(루시, 결혼하다, 알랭)</code></li></ul></li><li><strong>터틀 (Turtle)</strong> 형식의 트리플<ul><li>터틀은 <strong><a href="https://en.wikipedia.org/wiki/Notation3">Notation3(N3)</a></strong> 의 부분 집합</li><li>정점 : <code>_:someName</code><ul><li>서술어 <code>a</code> 는 뭘까 🤔 =&gt; 정점 선언? (ex. <code>_:usa  a  :Location</code>)</li></ul></li><li>세미콜론(;) 사용 시, 동일 주어에 대한 반복 작업 수행 가능</li></ul></li></ul><h4 id="시맨틱-웹"><a href="#시맨틱-웹" class="headerlink" title="시맨틱 웹"></a>시맨틱 웹</h4><ul><li>트리플 저장소 데이터 모델 != 시맨틱 웹<ul><li>어떠한 관계도 주장하지는 않으나 (ex. Datomic), 많은 사람들이 밀접한 관계로 생각 </li></ul></li><li>시맨틱 웹 : 컴퓨터(기계)가 읽을 수 있는 (Ontology) 데이터로 정보를 게시하고 처리하는 방식의 웹<ul><li>과대평가 후 현재는 부진..</li></ul></li></ul><h4 id="RDF-데이터-모델"><a href="#RDF-데이터-모델" class="headerlink" title="RDF 데이터 모델"></a>RDF 데이터 모델</h4><ul><li><strong>자원 기술 프레임워크</strong> (<strong>RDF</strong>, Resource Description Framework)<ul><li>서로 다른 웹 사이트가 일관된 형식으로 데이터를 게시하기 위한 방법 제안</li><li>=&gt; <strong>데이터 웹 (web of data)</strong> 에 자동으로 결합 가능 (=database of everything)</li></ul></li><li>RDF는 인터넷 전체의 데이터 교환을 위해 설계<ul><li><code>(주어, 서술어, 목적어)</code> 가 주로 URI</li><li>접속 가능 여부와는 무관 (RDF 관점에선 just 네임스페이스)</li></ul></li><li>형식<ul><li>터틀 언어 (보기 쉬움)<ul><li>RDF 데이터를 human-readable format 으로 표현</li></ul></li><li>XML 형식 (가능은 하지만 장황)</li><li>서로 다른 RDF 형식으로 변환하는 도구 =&gt; <U>아파치 제나(Jena)</U></li></ul></li></ul><h4 id="스파클-질의-언어"><a href="#스파클-질의-언어" class="headerlink" title="스파클 질의 언어"></a>스파클 질의 언어</h4><ul><li><strong>스파클</strong> (<strong>SPARQL</strong>, SPARQL Protocol And RDF Query Language) <ul><li>RDF 데이터 모델을 사용한 트리플 저장소 (선언형) 질의 언어</li><li>사이퍼와 유사 (<code>p61</code> 참고)</li><li>시맨틱 웹이 아니더라도 애플리케이션 내부적 사용하는 강력한 도구가 될 수 있음</li></ul></li><li>스파클 질의<ul><li>변수는 ?로 시작</li></ul></li><li>RDF는 서술어만 사용 (속성/간선 구별 x)</li></ul><blockquote><h4 id="그래프-데이터베이스-vs-네트워크-모델-차이"><a href="#그래프-데이터베이스-vs-네트워크-모델-차이" class="headerlink" title="그래프 데이터베이스 vs 네트워크 모델 (차이)"></a>그래프 데이터베이스 vs 네트워크 모델 (차이)</h4><ul><li>스키마<ul><li>코다실 : 다른 레코드 타입, 중첩가능 레코드 타입 지정하는 스키마 존재</li><li>그래프 : 제한 x (모든 정점은 다른 정점으로 가는 간선 ok)</li></ul></li><li>접근 방식<ul><li>코다실 : only 접근 경로 중 하나 탐색</li><li>그래프 : 고유 ID로 정점 직접 참조 or 색인으로 빠르게 찾기</li></ul></li><li>정렬<ul><li>코다실 : 레코드 하위 항목은 정렬된 집합 (신규 삽입시 위치 고려)</li><li>그래프 : 정렬 x. 질의 시에만 결과 정렬</li></ul></li><li>질의<ul><li>코다실 : 모든 질의는 명령형. 스키마 변경 시 질의 쉽게 손상</li><li>그래프 : 대부분 고수준 선언형 질의언어 사용. (명령형도 가능은 o)</li></ul></li></ul></blockquote><h3 id="초석-데이터로그"><a href="#초석-데이터로그" class="headerlink" title="초석: 데이터로그"></a>초석: 데이터로그</h3><ul><li><strong>데이터로그 (Datalog)</strong><ul><li>스파클, 사이퍼보다 훨씬 오래된 언어.. 말그대로 질의언어의 기반 초석 제공</li><li>사용 예) Datomic, Cascalog (s-expression 문법)</li></ul></li><li>정보 저장 형식<ul><li><code>서술어(predicate)</code> (<code>주어(subject)</code>, <code>목적어(object)</code>)</li><li>트리플 보다 좀 더 일반화</li></ul></li><li>데이터로그 질의<ul><li>복잡한 질의를 작은부분으로 나눠 단계별로 차례대로 나아감 =&gt; <strong>규칙(rule)</strong> 정의</li><li>규칙은 다른 규칙 참조 가능</li></ul></li><li>예제<ul><li><code>p63 ~</code> (예제는 Prolog 문법)</li><li>서술어 = 데이터나 다른 규칙으로부터 파생 (트리플 x)</li><li>변수 = 대문자로 시작하는 단어</li></ul></li><li>특징<ul><li>이전의 질의 언어와는 다른 사고</li><li>다른 질의의 규칙을 결합/재사용 가능하다는 점이 강력 (복잡한 데이터에 효과적)</li></ul></li></ul><br/><h2 id="02-정리"><a href="#02-정리" class="headerlink" title="02 정리"></a>02 정리</h2><ul><li>데이터 모델은 애플리케이션 요구사항에 가장 적합한 모델을 찾는 것이 중요 (만능 솔루션 x)</li><li>데이터 표현을 위한 발전<ul><li>데이터를 큰 트리(<strong>계층 모델</strong>) 로 표현 하려니 __ 다대다 관계 표현에 부적합</li><li>=&gt; <U><strong>관계형 모델</strong></U> 고안 __ 애플리케이션 요구사항에 부적합한 케이스 존재</li><li>=&gt; <strong>비관계형 데이터저장소 (NoSQL)</strong> 등장</li></ul></li><li>NoSQL의 2가지 갈래<ul><li><U><strong>문서 데이터베이스</strong></U> (문서 간 관계 ↓)</li><li><U><strong>그래프 데이터베이스</strong></U> (모든것의 관계 ↑)</li></ul></li><li>스키마 유연성의 차이<ul><li>스키마가 명시적인지(쓰기에 강요)</li><li>스키마가 암시적인지(읽기에 다룸)</li></ul></li><li>각 데이터 모델은 고유한 질의 언어 or 프레임워크 제공</li><li>그 외 다양한 데이터 모델들<ul><li>GenBank, ROOT, 전문(full-text) 검색, …</li></ul></li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li>ORM이란 : <a href="https://gmlwjd9405.github.io/2019/02/01/orm.html">https://gmlwjd9405.github.io/2019/02/01/orm.html</a></li><li>02장 - 참고문헌</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;데이터 중심 어플리케이션 설계 [PART 1]&lt;/strong&gt;&lt;br&gt;Designing Data Intensive Applications - O’REILLY&lt;/p&gt;
&lt;p&gt;02. Data Models and Qu</summary>
      
    
    
    
    <category term="ddia" scheme="https://minsw.github.io/categories/ddia/"/>
    
    
    <category term="data" scheme="https://minsw.github.io/tags/data/"/>
    
    <category term="data_intensive_application" scheme="https://minsw.github.io/tags/data-intensive-application/"/>
    
    <category term="ddia" scheme="https://minsw.github.io/tags/ddia/"/>
    
    <category term="book" scheme="https://minsw.github.io/tags/book/"/>
    
    <category term="study" scheme="https://minsw.github.io/tags/study/"/>
    
  </entry>
  
  <entry>
    <title>&amp;#039;데이터 중심 어플리케이션 설계&amp;#039; 1장 - 신뢰! 확장! 유지보수!</title>
    <link href="https://minsw.github.io/2021/05/11/Designing-Data-Intensive-Applications-01/"/>
    <id>https://minsw.github.io/2021/05/11/Designing-Data-Intensive-Applications-01/</id>
    <published>2021-05-10T16:02:13.000Z</published>
    <updated>2021-06-15T04:47:00.950Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><strong>데이터 중심 어플리케이션 설계 [PART 1]</strong><br>Designing Data Intensive Applications - O’REILLY</p><p>01. Reliable, Scalable, and Maintainable Applications</p></blockquote><img src="https://user-images.githubusercontent.com/26691216/117692731-45717000-b1f8-11eb-971b-f8ffae0054ae.gif" width=400/><center><i>신뢰! 확장! 유지보수!<br/>마 열쪙있따!</i></center><br/><h4 id="Part-1-데이터-시스템의-기초"><a href="#Part-1-데이터-시스템의-기초" class="headerlink" title="[Part 1] 데이터 시스템의 기초"></a>[Part 1] 데이터 시스템의 기초</h4><h1 id="01-신뢰할-수-있고-확장-가능하며-유지보수하기-쉬운-애플리케이션"><a href="#01-신뢰할-수-있고-확장-가능하며-유지보수하기-쉬운-애플리케이션" class="headerlink" title="01. 신뢰할 수 있고 확장 가능하며 유지보수하기 쉬운 애플리케이션"></a>01. 신뢰할 수 있고 확장 가능하며 유지보수하기 쉬운 애플리케이션</h1><blockquote><p>to be continued ..</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;데이터 중심 어플리케이션 설계 [PART 1]&lt;/strong&gt;&lt;br&gt;Designing Data Intensive Applications - O’REILLY&lt;/p&gt;
&lt;p&gt;01. Reliable, Scalable</summary>
      
    
    
    
    <category term="ddia" scheme="https://minsw.github.io/categories/ddia/"/>
    
    
    <category term="data" scheme="https://minsw.github.io/tags/data/"/>
    
    <category term="data_intensive_application" scheme="https://minsw.github.io/tags/data-intensive-application/"/>
    
    <category term="ddia" scheme="https://minsw.github.io/tags/ddia/"/>
    
    <category term="book" scheme="https://minsw.github.io/tags/book/"/>
    
    <category term="study" scheme="https://minsw.github.io/tags/study/"/>
    
  </entry>
  
  <entry>
    <title>&amp;#039;Spark The Definitive Guide&amp;#039; 14장 - 분산형 공유 변수 (PART 3 끝)</title>
    <link href="https://minsw.github.io/2021/03/20/Spark-The-Definitive-Guide-14%EC%9E%A5/"/>
    <id>https://minsw.github.io/2021/03/20/Spark-The-Definitive-Guide-14%EC%9E%A5/</id>
    <published>2021-03-19T16:02:00.000Z</published>
    <updated>2021-03-19T18:35:01.780Z</updated>
    
    <content type="html"><![CDATA[<br/><h4 id="Part-3-END"><a href="#Part-3-END" class="headerlink" title="[Part 3] END"></a>[Part 3] END</h4><blockquote><p><em><a href="https://minsw.github.io/2021/01/20/Spark-The-Definitive-Guide-1%EC%9E%A5/">“… 완벽 가이드라니까 일단 파트 3까지는 아묻따 따라가보자.”</a></em></p></blockquote><p>마침내 🎈약속의 파트 쓰리! 🎈   🙌🏻<br>빠르지는 않았지만 꾸준함에 의미를 두고 싶다.</p><p>스파크 너란 녀석.. 이젠 뭔지 조금은 알지도…?<br><i style="color:lightgray">( 『5252.. 이제부턴 ‘실전’이다ㅡ 』 )</i></p><img alt="dooly" src="https://user-images.githubusercontent.com/26691216/111810920-dd9a5980-8919-11eb-8d2d-2b26434a01ae.jpg" width=300/><center style="color:lightgray">선 넘네..</center><br/><center><h2>_ _ _</h2></center><br/><hr><h1 id="CHAPTER-14-분산형-공유-변수"><a href="#CHAPTER-14-분산형-공유-변수" class="headerlink" title="CHAPTER 14 분산형 공유 변수"></a>CHAPTER 14 분산형 공유 변수</h1><p>스파크의 저수준 API의 두 번째 유형, <strong>‘분산형 공유 변수’</strong><br>분산형 공유 변수 타입이 만들어지게 된 계기, 사용 방법 소개</p><ul><li>클러스터 실행 시 특별한 속성을 가진 사용자 정의함수 (ex. RDD, DataFrame을 다루는 map 함수)에서 사용 가능</li><li>분산형 공유 변수의 2가지 타입 : <strong>브로드캐스트 변수</strong>, <strong>어큐뮬레이터</strong></li></ul><h3 id="14-1-브로드캐스트-변수"><a href="#14-1-브로드캐스트-변수" class="headerlink" title="14.1 브로드캐스트 변수"></a>14.1 브로드캐스트 변수</h3><blockquote><p>브로드캐스트 변수는 <strong>모든 워커노드에 큰 값 저장</strong> =&gt; 재 전송 없이 많은 스파크 액션에서 재사용 가능</p></blockquote><img width="400" alt="bv" src="https://user-images.githubusercontent.com/26691216/111816566-543a5580-8920-11eb-88eb-ba873e1384fc.png"><ul><li><p><strong>브로드캐스트 변수</strong></p><ul><li>변하지 않는 값(불변성 값)을 클로저(closure) 함수의 변수로 캡슐화하지 않고, 클러스터에서 효율적으로 공유하는 방법 제공</li></ul></li><li><p>(1) 태스크에서 드라이버 노드의 변수 사용 시 <U>클로저 함수 내부에서 단순 참조</U></p><ul><li>=&gt; <strong>비효율적</strong>. 특히 큰 변수 사용시 (ex. 룩업 데테이블, 머신러닝 모델)<ul><li>Why?</li><li>클로저 함수에서 변수 사용 시 =&gt; 워커 노드에서 여러 번 (태스크당 한번) 역직렬화 발생</li><li>여러 스파크 액션과 잡에서 동일 변수 사용시 =&gt; 잡 실행 때마다 워커로 큰 변수 재전송</li></ul></li><li>그러면 어떻게 하나?<ul><li>=&gt; <U><strong>브로드캐스트 변수 사용</strong>해라!</U></li></ul></li></ul></li><li><p>(2) 브로드 캐스트 변수 사용 시</p><ul><li>모든 태스크마다 직렬화 X =&gt; <strong>클러스터의 모든 머신에 캐시하는 불변성 공유 변수</strong></li><li>익스큐터 메모리에 맞는 조회용 테이블을 전달하고 함수에서 사용</li></ul></li><li><p>How to use ?</p><ul><li><code>spark.sparkContext.broadcast()</code> 로 참조 (= 불변성 값)<ul><li>액션을 실행할때 클러스터 모든 노드에 지연 처리 방식으로 복제됨</li></ul></li><li><code>value</code> 메서드로 브로드캐스트된 값 참조 가능<ul><li>직렬화된 함수에서 브로드캐스트된 데이터를 직렬화 하지않아도 접근 가능</li><li>데이터를 보다 효율적으로 전송. <strong>직렬화/역직렬화 부하 ↓</strong></li></ul></li></ul></li><li><p>클로저에 담아 전달 vs 브로드캐스트 변수 사용</p><ul><li>말해뭐해. <strong>브로드캐스트 변수가 훨씬 더 효율적</strong><ul><li>데이터 총량, 익스큐터에 따라 다를 수는 있지만.. (작은 데이터를 작은 클러스터에서 돌릴 땐 별 차이 X)</li></ul></li><li>브로드캐스트 변수에 작은 크기의 딕셔너리(dictionary) 타입 사용 시 부하 크게 발생 X (?)<ul><li>훨씬 큰 크기 데이터 사용 시 전체 데이터 직렬화 시 발생 부하 커질 수도</li></ul></li></ul></li><li><p>RDD 영역에서 브로드캐스트 변수 사용 (UDF, Dataset도 사용 가능. 동일 효과)</p></li></ul><details><summary class="point-color-can-hover">[14.1] '브로드캐스트 변수' 예제 펼치기 </summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// &#x27;단어의 값과 목록 + 수 KB~GB 크기를 가지는 다른 정보&#x27; 브로드캐스트 후 RDD로 변환하는 예제</span></span><br><span class="line"><span class="keyword">val</span> myCollection = (<span class="string">&quot;Spark The Definitive Guide : Big Data Processing Made Simple&quot;</span></span><br><span class="line">  .split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"><span class="keyword">val</span> words = spark.sparkContext.parallelize(myCollection, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// (== SQL Right JOIN)</span></span><br><span class="line"><span class="keyword">val</span> supplementalData = <span class="type">Map</span>(<span class="string">&quot;Spark&quot;</span> -&gt; <span class="number">1000</span>, <span class="string">&quot;Definitive&quot;</span> -&gt; <span class="number">200</span>,</span><br><span class="line">                           <span class="string">&quot;Big&quot;</span> -&gt; <span class="number">-300</span>, <span class="string">&quot;Simple&quot;</span> -&gt; <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 해당 구조체 브로드캐스트 =&gt; suppBroadcast 변수로 참조 가능 (불변)</span></span><br><span class="line"><span class="keyword">val</span> suppBroadcast = spark.sparkContext.broadcast(supplementalData)</span><br><span class="line"><span class="comment">// suppBroadcast: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Map[String,Int]] = Broadcast(0)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// value 메서드로 직렬화 하지 않아도 접근 가능</span></span><br><span class="line">suppBroadcast.value</span><br><span class="line"><span class="comment">// res11: scala.collection.immutable.Map[String,Int] = Map(Spark -&gt; 1000, Definitive -&gt; 200, Big -&gt; -300, Simple -&gt; 100)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 브로드캐스트된 데이터로 키-값 RDD 변환 가능 (+ 값 없을 시 0 으로 처리)</span></span><br><span class="line">(words.map(word =&gt; (word, suppBroadcast.value.getOrElse(word, <span class="number">0</span>)))</span><br><span class="line">  .sortBy(wordPair =&gt; wordPair._2)</span><br><span class="line">  .collect())</span><br><span class="line"><span class="comment">// res14: Array[(String, Int)] = Array((Big,-300), (The,0), (Guide,0), (:,0), (Data,0), (Processing,0), (Made,0), (Simple,100), (Definitive,200), (Spark,1000))</span></span><br></pre></td></tr></table></figure></details><h3 id="14-2-어큐뮬레이터"><a href="#14-2-어큐뮬레이터" class="headerlink" title="14.2 어큐뮬레이터"></a>14.2 어큐뮬레이터</h3><blockquote><p>어큐뮬레이터는 <strong>모든 태스크 데이터를 공유 결과에 추가 가능</strong> (ex. 잡의 입력 레코드 파싱하면서 오류 발생 확인 카운터 구현 가능)</p></blockquote><img width="400" alt="av" src="https://user-images.githubusercontent.com/26691216/111816573-569caf80-8920-11eb-8906-13a25b3707d1.png"><ul><li><p><strong>어큐뮬레이터</strong></p><ul><li>트랜스포메이션 내부의 다양한 값 갱신하는데 사용</li><li>내고장성 보장 + 효율적인 방식으로 드라이버에 값 전달 가능</li></ul></li><li><p>어큐뮬레이터는 스파크 클러스터에서 <U>로우 단위로 안전하게 값을 갱신할 수 있는 변경 가능한 변수</U> 제공</p><ul><li><strong>디버깅용, 저수준 집계 생성용</strong>으로 사용 가능 (ex. 파티션별로 특정 변수 값 추적 용도)</li><li>결합성, 가환성을 가진 연산을 통해서만 더할 수 있는 변수 =&gt; 병렬 처리 과정에서 효율적 사용 가능</li><li>카운터(==맵리듀스의 카운터)나 합계 구하는 용도로 사용 가능</li></ul></li><li><p>스파크는 기본적으로 수치형 어큐뮬레이터 지원. 사용자 정의 어큐뮬레이터 만들어서 사용도 OK</p></li><li><p>어큐뮬레이터의 값은 <strong>액션</strong> 처리 과정에서만 갱신됨</p><ul><li>스파크는 <U><strong>각 태스크에서 어큐뮬레이터를 한 번만 갱신</strong></U>하도록 제어 (재시작한 태스크는 갱신X)</li><li>트랜스포메이션에서 태스크 or 잡 스테이지 재처리 시? =&gt; 각 태스크 갱신 작업이 두 번 이상 적용될 수도</li></ul></li><li><p>어큐뮬레이터는 스파크의 지연 연산 모델에 영향 X</p><ul><li>어큐뮬레이터가 RDD 처리 중 갱신되면? =&gt; RDD 연산이 실제로 수행된 지점에 딱 한번만 값 갱신  (해당 or 부모 RDD에 액션을 실행하는 시점)</li><li>지연 처리 형태의 트랜스포메이션 (ex. <code>map()</code>) 에서 어큐뮬레이터 갱신 작업 수행시? =&gt; 실제 실행 전까지는 갱신 X</li></ul></li><li><p>어큐뮬레이터 이름은 선택적 지정 가능</p><ul><li>생성 시 파리미터로 이름을 붙이거나 <code>spark.sparkContext.register(어큐뮬레이터, 이름)</code> 사용</li><li>이름이 지정된 (named) 어큐뮬레이터의 실행결과는 스파크 UI에 표시</li><li>이름이 지정되지 않은 어큐뮬레이터는 표시 X</li></ul></li><li><p><strong>사용자 정의 어큐뮬레이터</strong></p><ul><li>어큐뮬레이터 직접 정의 시 =&gt; <code>AccumulatorV2</code> 클래스 상속하여 구현</li><li>파이썬 사용 시 <a href="https://bit.ly/2x7RnL7">AccumulatorParam</a> 상속</li></ul></li></ul><details><summary class="point-color-can-hover">[14.2.1] '어큐뮬레이터' 예제 펼치기 </summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Dataset API 사용 (RDD API X)</span></span><br><span class="line"><span class="comment">// 출발지나 도착지가 중국인 항공편의 수를 구하는 어큐뮬레이터 생성하기 예제</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Flight</span>(<span class="params"><span class="type">DEST_COUNTRY_NAME</span>: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                  <span class="type">ORIGIN_COUNTRY_NAME</span>: <span class="type">String</span>, count: <span class="type">BigInt</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">flights</span> </span>= (spark.read</span><br><span class="line">  .parquet(<span class="string">&quot;/data/flight-data/parquet/2010-summary.parquet&quot;</span>)</span><br><span class="line">  .as[<span class="type">Flight</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.util.<span class="type">LongAccumulator</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 이름이 지정되지 않은 어큐뮬레이터 생성</span></span><br><span class="line"><span class="keyword">val</span> accUnnamed = <span class="keyword">new</span> <span class="type">LongAccumulator</span></span><br><span class="line"><span class="keyword">val</span> acc = spark.sparkContext.register(accUnnamed)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 이름이 지정된 어큐뮬레이터 생성</span></span><br><span class="line"><span class="keyword">val</span> accChina = <span class="keyword">new</span> <span class="type">LongAccumulator</span></span><br><span class="line"><span class="keyword">val</span> accChina2 = spark.sparkContext.longAccumulator(<span class="string">&quot;China&quot;</span>)</span><br><span class="line">spark.sparkContext.register(accChina, <span class="string">&quot;China&quot;</span>)<span class="comment">// register() 로 이름 지정 가능</span></span><br><span class="line"><span class="comment">// accChina: org.apache.spark.util.LongAccumulator = Un-registered Accumulator: LongAccumulator</span></span><br><span class="line"><span class="comment">// accChina2: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 101, name: Some(China), value: 0)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 어큐뮬레이터에 값 더하는 방법 정의</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accChinaFunc</span></span>(flight_row: <span class="type">Flight</span>) = &#123;</span><br><span class="line">  <span class="keyword">val</span> destination = flight_row.<span class="type">DEST_COUNTRY_NAME</span></span><br><span class="line">  <span class="keyword">val</span> origin = flight_row.<span class="type">ORIGIN_COUNTRY_NAME</span></span><br><span class="line">  <span class="keyword">if</span> (destination == <span class="string">&quot;China&quot;</span>) &#123;</span><br><span class="line">    accChina.add(flight_row.count.toLong)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (origin == <span class="string">&quot;China&quot;</span>) &#123;</span><br><span class="line">    accChina.add(flight_row.count.toLong)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// foreach() 는 액션 =&gt; DataFrame의 매 로우마다 함수 한번씩 적용 (어큐뮬레이터 갱신)</span></span><br><span class="line">flights.foreach(flight_row =&gt; accChinaFunc(flight_row))</span><br><span class="line"></span><br><span class="line">accChina.value <span class="comment">// 953</span></span><br></pre></td></tr></table></figure><blockquote><p>Spark UI (이름이 지정된 어큐뮬레이터)</p></blockquote><img width="400" alt="china-result" src="https://user-images.githubusercontent.com/26691216/111823196-00336f00-8928-11eb-93b4-14ae1c731ab2.png"></details><details><summary class="point-color-can-hover">[14.2.2] 사용자 정의 어큐뮬레이터 구현 예제 펼치기 </summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 사용자 정의 어큐뮬레이터 (AccumulatorV2 상속 클래스 구현)</span></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable.<span class="type">ArrayBuffer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.util.<span class="type">AccumulatorV2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> arr = <span class="type">ArrayBuffer</span>[<span class="type">BigInt</span>]()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EvenAccumulator</span> <span class="keyword">extends</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">util</span>.<span class="title">AccumulatorV2</span>[<span class="type">BigInt</span>, <span class="type">BigInt</span>] </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> num:<span class="type">BigInt</span> = <span class="number">0</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.num = <span class="number">0</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(intValue: <span class="type">BigInt</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (intValue % <span class="number">2</span> == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">this</span>.num += intValue</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(other: <span class="type">AccumulatorV2</span>[<span class="type">BigInt</span>,<span class="type">BigInt</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.num += other.value</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">value</span></span>():<span class="type">BigInt</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.num</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>(): <span class="type">AccumulatorV2</span>[<span class="type">BigInt</span>,<span class="type">BigInt</span>] = &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">EvenAccumulator</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">isZero</span></span>():<span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.num == <span class="number">0</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">val</span> acc = <span class="keyword">new</span> <span class="type">EvenAccumulator</span></span><br><span class="line"><span class="keyword">val</span> newAcc = sc.register(acc, <span class="string">&quot;evenAcc&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">acc.value <span class="comment">// 0</span></span><br><span class="line">flights.foreach(flight_row =&gt; acc.add(flight_row.count))</span><br><span class="line">acc.value <span class="comment">// 31390</span></span><br></pre></td></tr></table></figure></details><h3 id="14-3-정리"><a href="#14-3-정리" class="headerlink" title="14.3 정리"></a>14.3 정리</h3><ul><li>분산형 공유 변수 (브로드캐스트 변수, 어큐뮬레이터)</li><li>분산형 공유 변수는 디버깅이나 최적화 작업에 유용한 도구</li></ul><h3 id="📒-단어장"><a href="#📒-단어장" class="headerlink" title="📒 단어장"></a>📒 단어장</h3><ul><li>내고장성 (Fault tolerance) : 시스템 일부에 문제가 발생하여도 정상 동작이 가능한 </li><li>결합성 (Associative) : 둘 이상의 이항연산 중첩시, 연산 결과가 순서에 관계없이 동일<blockquote><p><em>(a+b)+c = a+(b+c) = a+b+c</em></p></blockquote></li><li>가환성 (Commutative) : 연산 결과가 순서에 관계없이 동일 (=&gt; <a href="https://minsw.github.io/2021/01/24/Spark-The-Definitive-Guide-2%EC%9E%A5/#%F0%9F%93%92-%EB%8B%A8%EC%96%B4%EC%9E%A5">2장 단어장</a>)<blockquote><p><em>a+b = b+a</em></p></blockquote></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;br/&gt;

&lt;h4 id=&quot;Part-3-END&quot;&gt;&lt;a href=&quot;#Part-3-END&quot; class=&quot;headerlink&quot; title=&quot;[Part 3] END&quot;&gt;&lt;/a&gt;[Part 3] END&lt;/h4&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&quot;h</summary>
      
    
    
    
    <category term="spark" scheme="https://minsw.github.io/categories/spark/"/>
    
    
    <category term="book" scheme="https://minsw.github.io/tags/book/"/>
    
    <category term="study" scheme="https://minsw.github.io/tags/study/"/>
    
    <category term="spark" scheme="https://minsw.github.io/tags/spark/"/>
    
    <category term="apache" scheme="https://minsw.github.io/tags/apache/"/>
    
  </entry>
  
  <entry>
    <title>&amp;#039;Spark The Definitive Guide&amp;#039; 13장 - RDD 심화반</title>
    <link href="https://minsw.github.io/2021/03/15/Spark-The-Definitive-Guide-13%EC%9E%A5/"/>
    <id>https://minsw.github.io/2021/03/15/Spark-The-Definitive-Guide-13%EC%9E%A5/</id>
    <published>2021-03-15T13:05:03.000Z</published>
    <updated>2021-03-15T17:29:17.190Z</updated>
    
    <content type="html"><![CDATA[<br/><p>RDD 쓰지 말래매…<br>자꾸 왜 이렇게 뭘 자세하게 알려주려고 하는데… 😔</p><img alt="class" src="https://user-images.githubusercontent.com/26691216/111161260-4b731800-85de-11eb-8870-31d56a87f978.gif" width=400/><center><h2>_ _ _</h2></center><br/><hr><h1 id="CHAPTER-13-RDD-고급-개념"><a href="#CHAPTER-13-RDD-고급-개념" class="headerlink" title="CHAPTER 13 RDD 고급 개념"></a>CHAPTER 13 RDD 고급 개념</h1><p>CHAPTER 12 는 RDD를 다루기 위한 기초적 내용과 생성하는 방법, 적합한 적용 케이스 등을 소개<br>CHAPTER 13 은 키-값 RDD 중심의 <strong>RDD 고급 연산</strong> 과 사용자 정의 파티션 등의 고급 주제 소개</p><ul><li>집계와 키-값 형태의 RDD</li><li>사용자 정의 파티셔닝</li><li>RDD 조인</li></ul><h3 id="13-1-키-값-형태의-기초-키-값-형태의-RDD"><a href="#13-1-키-값-형태의-기초-키-값-형태의-RDD" class="headerlink" title="13.1 키-값 형태의 기초(키-값 형태의 RDD)"></a>13.1 키-값 형태의 기초(키-값 형태의 RDD)</h3><ul><li><p>RDD에는 데이터를 key-value 형태로 다루는 다양한 메서드 존재</p><ul><li>=&gt; <code>&lt;연산명&gt;ByKey()</code> (PairRDD 타입만 사용 가능)</li></ul></li><li><p><strong>PairRDD 타입</strong></p><ul><li>만드는 가장 쉬운 방법?</li><li>=&gt; RDD에 맵 연산해서 키-값 구조로 만들기<ul><li>== RDD 레코드에는 두 개의 값이 존재한다<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">words.map(word =&gt; (word.toLowerCase, <span class="number">1</span>))</span><br><span class="line"><span class="comment">// org.apache.spark.rdd.RDD[(String, Int)]</span></span><br></pre></td></tr></table></figure></li></ul></li></ul></li><li><p><code>KeyBy()</code> : 현재 값으로부터 키 생성</p><ul><li>원본 단어는 생성된 RDD <strong>값</strong> 으로 유지</li></ul></li><li><p>값 매핑하기</p><ul><li>튜플 형태 데이터 사용 시 =&gt; 첫 번째를 키, 두 번째를 값 으로 추정 (<code>(key, value)</code>)</li><li><code>mapValues()</code> : 튜플에서 값만 추출 가능 (키 제외)</li><li><code>flatMap()</code> : 값에 대해 flatMap 적용하여 배열 반환 가능 (ex. 반환 결과의 각 로우가 문자를 나타내게)<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">keyword.mapValues(word =&gt; word.toUpperCase).collect()</span><br><span class="line"><span class="comment">// res12: Array[(String, String)] = Array((s,SPARK), (t,THE), (d,DEFINITIVE), (g,GUIDE), (:,:), (b,BIG), (d,DATA), (p,PROCESSING), (m,MADE), (s,SIMPLE))</span></span><br><span class="line"></span><br><span class="line">keyword.flatMapValues(word =&gt; word.toUpperCase).collect()</span><br><span class="line"><span class="comment">// res15: Array[(String, Char)] = Array((s,S), (s,P), (s,A), (s,R), (s,K), (t,T), (t,H), (t,E), (d,D), (d,E), (d,F), (d,I), (d,N), (d,I), (d,T), (d,I), (d,V), (d,E), (g,G), (g,U), (g,I), (g,D), (g,E), (:,:), (b,B), (b,I), (b,G), (d,D), (d,A), (d,T), (d,A), (p,P), (p,R), (p,O), (p,C), (p,E), (p,S), (p,S), (p,I), (p,N), (p,G), (m,M), (m,A), (m,D), (m,E), (s,S), (s,I), (s,M), (s,P), (s,L), (s,E))</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p>키와 값 추출하기</p><ul><li><code>.keys</code>, <code>.values</code> 메서드로 키나 값 전체 추출 가능</li></ul></li><li><p><code>lookup()</code> : 특정 키에 관한 결과 찾기</p><ul><li>단, 오직 하나의 키만 찾을 수는 X (찾기 결과 전체 반환)</li></ul></li><li><p><code>sampleByKey()</code> : 근사치/정확도로 RDD 샘플 생성</p><ul><li>특정 키를 부분 샘플링 가능</li><li><code>sampleByKeyExtract()</code> : 99% 신뢰도를 가진 모든 키 값에 대해 RDD 추가 처리</li><li><code>sampleByKey</code> vs <code>sampleByKeyExtract</code><ul><li><code>sum(math.ceil(numItems * samplingRate))</code> 과 ‘거의’ 동일한 크기냐, ‘완전히’ 동일한 크기의 샘플을 만드느냐의 차이 </li><li>(.. 이게 무슨말이지..? =&gt; <a href="https://spark.apache.org/docs/2.2.0/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions">API docs</a> 참고)</li></ul></li><li>선택적으로 비복원 추출 사용 가능<ul><li>비복원 추출 사용 시 =&gt; 샘플 크기 보장을 위해 RDD 한번 더 통과</li><li>복원 추출 =&gt; RDD 두번 더 통과</li></ul></li></ul></li></ul><h3 id="13-2-집계"><a href="#13-2-집계" class="headerlink" title="13.2 집계"></a>13.2 집계</h3><ul><li><p>사용하는 메서드에 따라 <strong>일반 RDD</strong> or <strong>PairRDD</strong> 사용</p></li><li><p><code>countByKey()</code> : 각 키의 아이템 수 구하고 + 로컬 맵으로 결과 수집</p><ul><li>Scala, Java 사용 시 =&gt; 제한 시간(timeout), 신뢰도 인수로 지정하여 근사치 구하기 가능 (=<code>countByApprox</code>)</li></ul></li><li><p>집계 연산 구현 방식 이해하기</p><ul><li>PairRDD (키-값 형태) 생성을 위한 구현 방식은 중요 (for 잡의 안정성)</li><li><code>groupBy</code> vs <code>reduce</code> 구현 방식 비교 (동일한 기본 원칙)<ul><li>예제 - 각 키의 총 레코드 수 구하기</li><li><code>groupByKey().map()</code> : 모든 익스큐터에서 함수 적용 전에 해당 키와 관련된 <strong>모든 값을 메모리로</strong> 읽음 (=&gt; 파티션 쏠림 현상 시 OOM 발생 가능성)</li><li><code>reduceByKey()</code> : 각 파티션에서 리듀스 작업 수행하고, 최종 리듀스 외 모든 작업은 개별 워커에서 처리 (연산 중 셔플 X) . 모든 값 메모리 유지 필요 없음</li></ul></li><li><code>groupByKey()</code><ul><li>각 키에 대한 값의 크기가 일정하고, 익스큐터에 할당된 메모리에서 처리 가능한 정도의 크기일 경우에만 사용</li><li>결과 순서 보장 O</li></ul></li><li><code>reduceByKey()</code><ul><li>키별 그룹 RDD 반환 (개별요소 정렬X) =&gt; 결과의 순서가 중요한 경우에는 비적합</li><li>안정성, 연산수행속도 빠름. 작업 부하 감소에 good</li></ul></li></ul></li><li><p>기타 집계 메서드</p><ul><li>구조적 API는 훨씬 간단하게 집계 가능 (권장)<ul><li>저수준 사용시, 사용자 워크로드에 따라 세부 구현 방법에 차이 존재</li><li>그러나 아주 구체적이고 매우 세밀한 제어 가능</li></ul></li><li><code>aggregate(시작값)(func1, func2)</code><ul><li>null or 집계 시작값 필요 (두 함수 모두 시작값 사용)</li><li>func1 은 파티션 내에서 수행, func2는 모든 파티션에 걸쳐 수행</li><li><code>aggregate</code> 는 드라이버에서 최종 집계<ul><li>=&gt; 성능에 영향 有 (익스큐터 결과가 크면 OOM 발생가능성)</li></ul></li><li><code>treeAggregate()</code> 는 드라이버 최종 집계 전 익스큐터끼리 형성한 트리로 집계 처리 일부 하위 과정을 ‘push down’ 방식으로 먼저 수행<ul><li>집계 처리를 여러 단계로 구성 (메모리 전체 소비 방지) =&gt; 안정적</li></ul></li></ul></li><li><code>aggregateByKey(시작값)(func1, func2)</code><ul><li>aggregate와 동일. 단 파티션 대신 <strong>키 기준</strong>으로 연산 수행</li></ul></li><li><code>combineByKey(컴바이너, func1, func2, (파티션 수))</code><ul><li>집계 함수대신 ‘컴바이너’ 사용</li><li><strong>컴바이너(combiner)</strong> : 키 기준으로 연산 수행 + 파라미터의 함수로 값 병합 + 여러 컴바이너 값을 병합한 결과 반환</li><li>사용자 정의 파티셔너로 출력 파티션 수 지정 가능</li></ul></li><li><code>foldByKey(제로값, func)</code><ul><li>결합 함수와 항등원인 ‘제로값’ 으로 각 키의 값 병합</li><li><strong>제로값</strong>은 여러번 사용될 수 있으나 결과 값을 변경할 수 없는 수 (ex. 덧셈의 0, 곱셈의 1)</li></ul></li></ul></li></ul><h3 id="13-3-cogroup"><a href="#13-3-cogroup" class="headerlink" title="13.3 cogroup"></a>13.3 cogroup</h3><ul><li><code>cogroup()</code><ul><li>RDD에 대한 그룹 기반의 조인 수행<ul><li>스칼라는 최대 3개, 파이썬은 최대 2개의 키-값 형태 RDD 그룹화 가능</li><li>각 키 기준으로 값 결합</li></ul></li><li>사용자 정의 파티션 함수 파라미터로 사용 가능<ul><li>=&gt; 출력 파티션 수, 클러스터에 데이터 분산 방식 정확하게 제어 가능</li></ul></li></ul></li><li>결과 =&gt; 키-값 형태의 배열 반환<ul><li>key : 그룹화된 키</li><li>value : 키와 관련된 모든 값</li></ul></li></ul><h3 id="13-4-조인"><a href="#13-4-조인" class="headerlink" title="13.4 조인"></a>13.4 조인</h3><ul><li><p>구조적 API 와 거의 동일한 조인 방식</p><ul><li>더 많은 부분에 사용자의 관여 필요</li><li>조인 대상인 두 RDD (+ 출력 파티션 수, 사용자 정의 파티션 함수)</li></ul></li><li><p>내부 조인</p><ul><li><code>RDD.join(RDD, 파티션 수)</code>로 출력 파티션 수 지정 가능</li><li>다른 조인 함수도 동일한 기본 형식 따름 (=&gt; <a href="https://minsw.github.io/2021/02/15/Spark-The-Definitive-Guide-8%EC%9E%A5/">8장</a> 참고)<ul><li><code>fullOuterJoin</code> : 외부 조인</li><li><code>leftOuterJoin</code> : 왼쪽 외부 조인</li><li><code>rightOuterJoin</code> : 오른쪽 외부 조인</li><li><code>cartesian</code> : 교차 조인 (danger⚠️)</li></ul></li></ul></li><li><p><code>zip()</code></p><ul><li>진짜 조인은 아니지만, <strong>두 RDD 결합</strong><ul><li>zipper 잠그듯..🤐  =&gt; 연결된 PairRDD 반환</li><li>한 RDD의 키 배열에 + 다른 RDD의 값이 지퍼처럼 연결</li><li>(예제로는 넘버링에 유용할 듯)</li></ul></li><li>두 RDD는 동일한 수의 요소 &amp; 파티션을 가져야함</li></ul></li></ul><h3 id="13-5-파티션-제어하기"><a href="#13-5-파티션-제어하기" class="headerlink" title="13.5 파티션 제어하기"></a>13.5 파티션 제어하기</h3><ul><li>RDD를 사용한다는건..<ul><li>=&gt; 클러스터 전체에 물리적으로 정확히 분산되는 방식 정의 가능</li><li>일부 메서드는 구조적 API 메서드와 기본적으로 동일</li><li>차이점은 <em>“<strong>파티션 함수(사용자 지정 Partitioner)를 파라미터로</strong> 사용할 수 있다는 것”</em></li></ul></li><li><code>coalesce(파티션 수)</code> :  동일한 워커에 존재하는 파티션 합침 (파티션 수 ↓)<ul><li>파티션 재분배 시 발생하는 데이터 셔플 X</li></ul></li><li><code>repartition(파티션 수)</code> : 파티션 수 증감 가능<ul><li>처리 시 노드 간 셔플 발생 가능</li><li>파티션 수 ↑ =&gt; 맵, 필터 타입 연산 시 병렬 처리 수준 ↑</li></ul></li><li><code>repartitionAndSortWithinPartitions(파티셔너)</code> : 파티션 재분배 &amp; 결과 파티션 정렬 방식 지정 가능<ul><li>파티셔닝, 키 비교 모두 사용자 지정 가능</li><li><a href="https://bit.ly/2NCHCOH">API docs</a> 참고</li></ul></li></ul><h4 id="사용자-정의-파티셔닝"><a href="#사용자-정의-파티셔닝" class="headerlink" title="사용자 정의 파티셔닝"></a>사용자 정의 파티셔닝</h4><ul><li><strong>사용자 정의 파티셔닝 (custom partitioning)</strong><ul><li>RDD를 사용하는 가장 큰 이유 중 하나<ul><li>저수준 API의 세부적인 구현 방식</li><li>구조적 API에서는 사용 불가 (논리적 대응책 X)</li></ul></li><li>잡이 성공적으로 동작되는지 여부에 상당한 영향<ul><li>ex. 페이지랭크(PageRank) : 사용자 정의 파티셔닝 → 클러스터의 데이터 배치 구조 제어 및 셔플 회피</li></ul></li><li>목표 : <strong>데이터 치우침(skew)</strong> 문제 회피를 위한 데이터 균등 분배</li></ul></li><li>구조적 API + RDD 장점 같이 활용하는 법<ul><li>구조적 API로 RDD 얻기</li><li>=&gt; 사용자 정의 파티셔너 적용</li><li>=&gt; 다시 DataFrame/Dataset 으로 변환</li></ul></li><li>How to use?<ul><li>Partitioner 확장 클래스 구현 <i style="color:lightgray">(아 자신있으면 쓰라고 ㅋㅋ)</i></li><li>단일 값, 다수 값(다수 컬럼) 파티셔닝 시 =&gt; <U>DataFrame API</U> 사용 권장</li></ul></li><li>RDD, 구조적 API 모두 사용 가능한 <strong>내장형 파티셔너</strong> (기초적 기능 제공)<ul><li><code>HashPartitioner</code> : 이산형 값</li><li><code>RangePartitioner</code> : 연속형 값</li></ul></li><li>매우 큰 데이터나 심각하게 치우친 키를 다룰 경우 고급 파티셔닝 기능 필요 (=&gt; 키 치우침)<ul><li><strong>키 치우침</strong> : 어떤 키가 다른 키에 비해 아주 많은 데이터를 가지는 현상<ul><li>=&gt; 최대한 키 분할 필요 (병렬성 개선 + OOM 방지)</li></ul></li><li>키 분할이 필요한 예 : 키가 특정한 형태를 띠는 경우 (예제 참고)<ul><li><em>“우스꽝스러운 예제일 수 있지만, 실무에서 비슷한 상황 만날지도 모릅니다”</em></li></ul></li></ul></li><li>이러한 <strong>사용자 정의 키 분배로직</strong>은 RDD 수준에서만 사용 가능<ul><li>It’s 임의의 로직을 사용해 물리적인 방식으로 클러스터에 데이터를 분배하는 강력한 방법</li></ul></li></ul><details><summary class="point-color-can-hover">[13.5] 사용자 정의 키 분배로직 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">Partitioner</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 분석 복잡도를 높이는 두 고객(17850, 12583) 과 다른 고객 정보 분리</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DomainPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span> </span>= <span class="number">3</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">   <span class="keyword">val</span> customerId = key.asInstanceOf[<span class="type">Double</span>].toInt</span><br><span class="line">   <span class="keyword">if</span> (customerId == <span class="number">17850.0</span> || customerId == <span class="number">12583.0</span>) &#123;</span><br><span class="line">     <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">   &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     <span class="keyword">return</span> <span class="keyword">new</span> java.util.<span class="type">Random</span>().nextInt(<span class="number">2</span>) + <span class="number">1</span></span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 각 파티션 수 확인</span></span><br><span class="line"><span class="comment">// (두 고객은 partition 0에, 나머지 고객들은 남은 partition 1, 2에 분산)</span></span><br><span class="line">(keyedRDD</span><br><span class="line">  .partitionBy(<span class="keyword">new</span> <span class="type">DomainPartitioner</span>).map(_._1).glom().map(_.toSet.toSeq.length)</span><br><span class="line">  .take(<span class="number">5</span>))</span><br><span class="line"><span class="comment">// res83: Array[Int] = Array(2, 4290, 4304)</span></span><br></pre></td></tr></table></figure><blockquote><ul><li><code>glom()</code> : 각 파티션의 모든 요소를 배열 하나로 모으고, 이 배열들을 요소로 포함하는 새로운 RDD를 반환</li></ul><p>(ref. <a href="https://thebook.io/006908/part01/ch04/02/04/02/">https://thebook.io/006908/part01/ch04/02/04/02/</a>)</p></blockquote></details><h3 id="13-6-사용자-정의-직렬화"><a href="#13-6-사용자-정의-직렬화" class="headerlink" title="13.6 사용자 정의 직렬화"></a>13.6 사용자 정의 직렬화</h3><ul><li><strong>Kryo 직렬화</strong><ul><li>병렬화 대상인 모든 객체나 함수는 직렬화 가능해야함</li></ul></li><li>“빠르다!”<ul><li>기본 직렬화 기능은 매우 느릴 수 있음</li><li>스파크는 Kryo 라이브러리 (ver. 2) 사용 =&gt; <strong>더 빠른 직렬화</strong> (자바보다 성능 10배 이상 ↑, 간결)</li></ul></li><li>단점<ul><li>모든 직렬화 유형 지원 X</li><li>최상의 성능을 위해서는 사용할 클래스 사전에 등록 필요</li></ul></li><li>How to use ?<ul><li>SparkConf 로 잡 초기화 시점에서 지정<ul><li><code>spark.serializer = org.apache.spark.serializer.KryoSerializer</code></li><li>Kryo 자세한 내용은 4부 에서</li></ul></li><li><code>spark.serializer</code> : 워커 노드 간 데이터 셔플링과 RDD를 직렬화해 디스크에 저장하는 용도로 사용할 시리얼라이즈 지정</li></ul></li><li>Kryo가 기본 값이 아닌 이유?<ul><li>사용자가 직접 클래스를 등록해야 하므로</li><li>네트워크에 민감한 애플리케이션에서 사용 권장</li><li>+ (since 스파크 2.0.0)<ul><li>단순 데이터 타입, 단순 데이터 타입의 배열, 문자열 데이터 타입의 RDD 셔플링 시 =&gt; 내부적으로 KryoSerializer 사용</li></ul></li></ul></li><li>클래스 등록<ul><li>스파크는 <a href="https://github.com/twitter/chill">Twitter chill 라이브러리</a> 의 <code>AllScalaRegistrar</code> 핵심 스칼라 클래스를 자동으로 KryoSerializer 에 등록</li><li>사용자 정의 클래스 등록하려면? =&gt;<code>registerKyroClasses()</code> 사용</li></ul></li></ul><h3 id="13-7-정리"><a href="#13-7-정리" class="headerlink" title="13.7 정리"></a>13.7 정리</h3><ul><li>RDD의 여러 고급 주제들</li><li>특히 <a href="#%EC%82%AC%EC%9A%A9%EC%9E%90-%EC%A0%95%EC%9D%98-%ED%8C%8C%ED%8B%B0%EC%85%94%EB%8B%9D">‘사용자 정의 파티셔닝’ [13.5.4]</a> 주목</li></ul><h3 id="📒-단어장"><a href="#📒-단어장" class="headerlink" title="📒 단어장"></a>📒 단어장</h3><ul><li>항등원 (neutral) : 어떤 원소와 연산을 취해도 자기 자신이 되게하는 원소. (= neutral value = identity element)</li><li>이산형 값 (discrete variable) : 값을 하나 하나 셀 수 있는 (=이산) 변수 (Like 정수)</li><li>연속형 값 (continuous variable) : 구간 내 모든 값을 가질 수 있는 변수 (Like 실수) </li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;br/&gt;

&lt;p&gt;RDD 쓰지 말래매…&lt;br&gt;자꾸 왜 이렇게 뭘 자세하게 알려주려고 하는데… 😔&lt;/p&gt;
&lt;img alt=&quot;class&quot; src=&quot;https://user-images.githubusercontent.com/26691216/11116126</summary>
      
    
    
    
    <category term="spark" scheme="https://minsw.github.io/categories/spark/"/>
    
    
    <category term="book" scheme="https://minsw.github.io/tags/book/"/>
    
    <category term="study" scheme="https://minsw.github.io/tags/study/"/>
    
    <category term="spark" scheme="https://minsw.github.io/tags/spark/"/>
    
    <category term="apache" scheme="https://minsw.github.io/tags/apache/"/>
    
  </entry>
  
  <entry>
    <title>&amp;#039;Spark The Definitive Guide&amp;#039; 12장 - RDD</title>
    <link href="https://minsw.github.io/2021/03/07/Spark-The-Definitive-Guide-12%EC%9E%A5/"/>
    <id>https://minsw.github.io/2021/03/07/Spark-The-Definitive-Guide-12%EC%9E%A5/</id>
    <published>2021-03-07T06:44:49.000Z</published>
    <updated>2021-03-07T08:41:39.790Z</updated>
    
    <content type="html"><![CDATA[<br/><h4 id="📌-Part-3-저수준-API"><a href="#📌-Part-3-저수준-API" class="headerlink" title="📌 [ Part 3 저수준 API ]"></a>📌 [ Part 3 저수준 API ]</h4><blockquote><p>대부분의 상황에서는 구조적 API 사용이 좋음 (Part 2)</p><p>그러나 이러한 고수준(hive-level) API로 처리할 수 없는 비즈니스나 기술적 문제는? =&gt; <U><strong>저수준 API</strong></U> 사용</p><ul><li>RDD</li><li>SparkContext</li><li>분산형 공유 변수 (distributed shared variables)<ul><li>어큐뮬레이터 (accumulator)</li><li>브로드캐스트 변수 (broadcast variable)</li></ul></li></ul></blockquote><br/><img src="https://user-images.githubusercontent.com/26691216/110234092-04e14600-7f6c-11eb-966f-153324981b5c.jpg" width=300/><center><h2>_ _ _</h2></center><br/><hr><h1 id="CHAPTER-12-RDD"><a href="#CHAPTER-12-RDD" class="headerlink" title="CHAPTER 12 RDD"></a>CHAPTER 12 RDD</h1><h3 id="12-1-저수준-API란"><a href="#12-1-저수준-API란" class="headerlink" title="12.1 저수준 API란"></a>12.1 저수준 API란</h3><ul><li>스파크의 저수준 API 종류 2가지<ul><li>분산 데이터 처리를 위한 <strong>RDD</strong></li><li><strong>분산형 공유 변수</strong>를 배포하고 다루기위한 API </li></ul></li><li>When?<ul><li>고수준 API에서 제공하지 않는 기능이 필요한 경우<ul><li>ex. 클러스터의 물리적 데이터 배치를 세밀하게 제어</li></ul></li><li>RDD를 사용해 개발된 기존 코드 유지</li><li>사용자가 정의한 공유 변수를 다뤄야하는 경우 (=&gt; 14장)</li></ul></li><li>장점<ul><li>스파크의 모든 워크로드는 저수준 기능을 사용하는 기초 형태로 컴파일됨 =&gt; 이해하여 워크로드 디버깅 작업에 도움</li><li>세밀한 제어 방법 제공 (치명적인 실수 방지)</li></ul></li><li>How?<ul><li>저수준 API 기능 사용의 진입 지점 =&gt; <code>SparkContext</code></li><li>자세한 내용은 15장에서<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 스파크 연산 수행 시 필요한 도구 <span class="symbol">&#x27;SparkSessio</span>n&#x27; 으로 접근 가능</span><br><span class="line">spark.sparkContext</span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="12-2-RDD-개요"><a href="#12-2-RDD-개요" class="headerlink" title="12.2 RDD 개요"></a>12.2 RDD 개요</h3><ul><li>RDD는 스파크 1.x 의 핵심 API <ul><li>2.x 부터는 잘 사용안하지만, 모든 DataFrame/Dataset 코드는 RDD로 컴파일</li><li>스파크 UI에서도 RDD단위로 잡 수행</li></ul></li><li><strong>RDD</strong> 란<ul><li>불변성을 가지고 병렬로 처리할 수 있는 파티셔닝된 레코드의 모음<ul><li>DataFrame의 레코드 : 스키마를 알고 있는 필드로 구성된 구조화된 로우</li><li>RDD의 레코드 : 그저 프로그래머가 선택하는 자바/스칼라/파이썬의 객체</li></ul></li><li>RDD의 모든 레코드는 객체이므로 완벽하게 제어 가능<ul><li>개발자가 강력한 제어권을 가짐</li><li>모든 값을 다루거나 값 사이 상호작용에 대해 반드시 수동 정의 필요</li><li>=&gt; <U><em>“Reinvent the wheel”</em></U></li></ul></li><li>내부 구조를 파악할 수 없음 (구조적 API는 가능)<ul><li>최적화에 많은 수작업 요구</li></ul></li><li>=&gt; <strong>결론은 스파크 구조적 API 사용을 강력하게 권고</strong></li></ul></li><li>RDD API 는 Dataset과 유사<ul><li>단, RDD는 구조화된 데이터 엔진을 사용해서 데이터를 저장하거나 다루지 X</li><li>RDD &lt;-&gt; Dataset 전환 쉬움</li><li>두 API의 각각의 장점 동시에 활용 가능</li></ul></li></ul><h4 id="RDD-유형"><a href="#RDD-유형" class="headerlink" title="RDD 유형"></a>RDD 유형</h4><ul><li>수많은 하위 클래스 존재<ul><li>대부분 DataFrame API의 최적화된 물리적 실행 계획 만드는데 사용</li></ul></li><li>RDD 타입 2 가지 (객체의 컬렉션 표현)<ul><li>제네릭 RDD 타입</li><li>키-값 RDD (특수연산 + 키를 이용한 사용자 지정 파티셔닝 개념 포함)</li></ul></li><li>RDD 의 주요 속성 5가지<ul><li>파티션의 목록</li><li>각 조각을 연산하는 함수</li><li>다른 RDD와의 의존성 목록</li><li>부가적으로 키-값 RDD를 위한 Partitioner (ex. RDD는 hash partitioned)</li><li>부가적으로 각 조각을 연산하기 위한 기본 위치 목록 (Ex. 하둡 분산 파일 시스템의 블록 위치)</li></ul></li><li>각 속성은 사용자 프로그램을 스케쥴링, 실행 하는 스파크의 모든 처리방식을 결정<ul><li>속성 구현체로 새로운 데이터 소스 정의도 가능</li><li><strong>Partitioner</strong> 는 RDD 사용하는 주된 이유 중 하나<ul><li>올바른 사용자 정의 Partitioner =&gt; 성능, 안정성 ↑</li><li>자세한 내용은 13장</li></ul></li></ul></li><li>RDD는 스파크 프로그래밍 패러다임을 그대로 따른다<ul><li>지연 처리 방식의 <U>트랜스포메이션</U>, 즉시 실행 방식의 <U>액션</U> 제공</li><li>DataFrame, Dataset과 동일한 방식으로 동작하지만 <strong>‘로우’ 개념이 없음</strong><ul><li>개별 레코드는 객체일뿐 (구조적 API가 제공하는 함수들의 동작을 수동으로 처리해야함)</li></ul></li></ul></li><li>RDD API는 스칼라, 자바, 파이썬 모두 사용 가능<ul><li>스칼라, 자바 =&gt; 대부분 비슷한 성능 (단, 원형 객체 사용시 성능 ↓)</li><li>파이썬 =&gt; 상당한 성능 저하 (높은 오버헤드)</li></ul></li></ul><h4 id="RDD는-언제-사용할까"><a href="#RDD는-언제-사용할까" class="headerlink" title="RDD는 언제 사용할까"></a>RDD는 언제 사용할까</h4><ul><li>A. 물리적으로 분산된 데이터 (자체 구성한 데이터 파티셔닝) 에 세부적인 제어가 필요할 때 =&gt; RDD 사용</li><li>하지만 정말 필요한 경우 빼고는 수동 RDD 생성 비권장<ul><li>구조적 API의 최적화 기법 사용 X</li><li>DataFrame이 훨씬 효율적, 안정적, 좋은 표현력</li></ul></li></ul><h4 id="Dataset과-RDD의-케이스-클래스"><a href="#Dataset과-RDD의-케이스-클래스" class="headerlink" title="Dataset과 RDD의 케이스 클래스"></a>Dataset과 RDD의 케이스 클래스</h4><ul><li>Dataset vs 케이스 클래스로 만들어진 RDD ?</li><li>Dataset은 구조적 API가 제공하는 풍부한 기능과 최적화 기법 제공<ul><li>JVM 데이터 타입 / 스파크 데이터 타입 중 고민 필요 X (성능 동일)</li></ul></li></ul><h3 id="12-3-RDD-생성하기"><a href="#12-3-RDD-생성하기" class="headerlink" title="12.3 RDD 생성하기"></a>12.3 RDD 생성하기</h3><ul><li><p>DataFrame, Dataset으로 RDD 생성하기</p><ul><li>가장 쉬운 방법</li><li>DataFrame이나 Dataset의 <code>rdd()</code> 호출<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// (Scala) Dataset[Long] =&gt; RDD[Long] 변환</span></span><br><span class="line">spark.range(<span class="number">500</span>).rdd</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># (python) Row타입의 RDD 반환 (Python은 Dataset X)</span></span><br><span class="line">sprak.<span class="built_in">range</span>(<span class="number">10</span>).rdd</span><br></pre></td></tr></table></figure></li><li>Row 타입 : 스파크가 구조적 API에서 데이터를 표현하는 데 사용하는 내부 카탈리스트 포맷<ul><li>상황에 따라 구조적 API &lt;-&gt; 저수준 API</li></ul></li><li>Dataset API와 RDD API의 유사성</li></ul></li><li><p>로컬 컬렉션으로 RDD 생성하기</p><ul><li><code>sparkContext.parallelize()</code> : 컬렉션 객체 =&gt; RDD<ul><li>단일 노드에 있는 컬렉션을 병렬 컬렉션으로 전환</li><li>파티션 수 명시적 지정 가능</li></ul></li><li>RDD에 이름 지정 시 스파크 UI에 해당 이름으로 노출</li></ul></li><li><p>데이터소스로 RDD 생성하기</p><ul><li>DataSource API 사용<ul><li>데이터소스나 텍스트 파일을 이용한 직접 생성보다 바람직</li><li>데이터를 읽는 가장 좋은 방법</li></ul></li><li>RDD에는 DataFrame이 제공하는 ‘Datasource API’ 개념 X<ul><li><code>sparkContext</code> 를 사용하여 RDD로 읽기 가능</li></ul></li></ul></li></ul><h3 id="12-4-RDD-다루기"><a href="#12-4-RDD-다루기" class="headerlink" title="12.4 RDD 다루기"></a>12.4 RDD 다루기</h3><ul><li>DataFrame을 다루는 방식과 매우 유사</li><li>RDD vs DataFrame, Dataset<ul><li>스파크 데이터 타입이 아닌 자바, 스칼라 객체를 다룬다는 점이 가장 큰 차이</li><li>‘헬퍼’ 메서드(연산 단순화) 나 함수 부족</li><li>=&gt; 필터, 맵 함수, 집계 등의 다양한 함수를 직접 정의해야함</li></ul></li></ul><h3 id="12-5-트랜스포메이션"><a href="#12-5-트랜스포메이션" class="headerlink" title="12.5 트랜스포메이션"></a>12.5 트랜스포메이션</h3><ul><li>대부분의 RDD 트랜스 포메이션 =&gt; 구조적 API에서 사용 가능<ul><li>RDD에도 트랜스포메이션을 지정해서 새로운 RDD 생성 가능 + 의존성 정의 (like DataFrame, Dataset)</li></ul></li><li><code>distinct()</code> : RDD의 중복된 데이터 제거</li><li><code>filter()</code> : 조건 함수를 만족하는 레코드만 반환<ul><li>모든 로우는 어떤 경우라도 입력값을 가져야 함</li><li>함수는 각 레코드를 개별적으로 처리 후, 결과를 사용한 언어의 데이터 타입으로 반환한다 (like Dataset API) =&gt; 강제로 Row 타입으로 변환할 필요가 없어서?</li></ul></li><li><code>map()</code> : 주어진 입력을 원하는 값으로 반환하는 함수를 레코드 별로 적용<ul><li><code>flatMap()</code> : 단일 로우 → 여러 로우로 변환</li></ul></li><li><code>sortBy()</code> : 함수의 추출 값 기준으로 RDD 정렬</li><li><code>randomSplit()</code> : RDD를 임의로 분할해 배열로 만듦<ul><li>가중치, 난수시드(random seed) 배열을 파라미터로 사용</li></ul></li></ul><h3 id="12-6-액션"><a href="#12-6-액션" class="headerlink" title="12.6 액션"></a>12.6 액션</h3><ul><li>지정된 트랜스포메이션 연산을 시작하기 위해 ‘액션’ 사용<ul><li>데이터를 드라이버로 모으거나, 외부 데이터 소스로 내보내기 가능</li></ul></li><li><code>reduce()</code> : RDD의 모든 값을 하나로 만듦<ul><li>파티션에 대한 리듀스 연산은 비결정적 (not deterministic) =&gt; 결과가 매번 달라질 수 있음</li></ul></li><li><code>count()</code> : RDD의 전체 로우 수 반환<ul><li><code>countApprox()</code> : 제한시간 내에 count 근사치 계산 (confidence = 신뢰도 = 오차율)</li><li><code>countApproxDistinct()</code> 는 2가지 구현체 존재<ul><li>상대 정확도(relative accuracy)를 파라미터로 사용 : 더 많은 메모리 공간 사용. (설정값 &gt;= 0.000017)</li><li>일반 (regular) 데이터를 위한 파라미터 &amp; 희소표현을 위한 파라미터 사용</li></ul></li><li><code>countByValue()</code> : RDD값의 개수 반환<ul><li>연산결과가 메모리에 모두 적재되므로 결과가 작을때만 사용 </li></ul></li><li><code>countByValueApprox()</code> : 제한시간 내에 countByValue의 근사치 계산 (+ confidence)</li></ul></li><li><code>first()</code> : 첫 번째 값 반환</li><li><code>max()</code>, <code>min()</code> : 각각 최댓값, 최솟값 반환</li><li><code>take()</code> : 지정한 개수 만큼 값 반환<ul><li>먼저 하나의 파티션 스캔 후, 결과 수를 이용해 필요한 추가 파티션 수를 예측</li><li>다양한 유사함수 존재 (<code>takeOrdered()</code>, <code>takeSample()</code>, <code>top()</code> ..)</li></ul></li></ul><h3 id="12-7-파일-저장하기"><a href="#12-7-파일-저장하기" class="headerlink" title="12.7 파일 저장하기"></a>12.7 파일 저장하기</h3><ul><li>데이터 처리 결과를 일반 텍스트 파일로 쓰는 것<ul><li>RDD는 일반적 의미의 데이터소스에 ‘저장’ 할 수 없음<ul><li>각 파티션의 내용을 저장하려면 전체 파티션을 순회하면서 외부 데이터베이스에 저장</li><li>(== 고수준 API의 내부 처리 과정)</li></ul></li><li>스파크는 각 파티션의 데이터를 파일로 저장</li></ul></li><li><code>saveAsTextFile()</code> : 지정한 경로로 텍스트 파일 저장<ul><li>필요 시 압축 코덱 사용 가능 (코덱은 <a href="https://bit.ly/2p3dnT3">org.apache.hadoop.io.compress</a> 참고)</li></ul></li><li>시퀀스 파일<ul><li>바이너리 키-값 쌍으로 구성된 플랫 파일</li><li>맵리듀스의 입출력 포맷으로 알려짐</li><li>=&gt; <code>saveAsObjectFile()</code>이나 명시적 키-값 쌍 데이터 저장 방식으로 시퀀스 파일 작성 가능</li></ul></li><li>하둡 파일<ul><li>데이터를 저장하는데 사용할 수 있는 여러 하둡 파일 포맷 존재</li><li>하둡 파일 포맷 사용 시 클래스, 출력 포맷, 하둡 설정, 압축 방식 지정 가능 (‘하둡 완벽 가이드’ 도서 참고)</li></ul></li></ul><h3 id="12-8-캐싱"><a href="#12-8-캐싱" class="headerlink" title="12.8 캐싱"></a>12.8 캐싱</h3><ul><li>DataFrame, Dataset 캐싱과 동일 원칙 적용<ul><li>RDD 는 캐시, 저장(persist) 가능<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">words.cache()</span><br></pre></td></tr></table></figure></li><li>기본적으로 캐시와 저장은 메모리에 있는 데이터가 대상</li><li><code>setName()</code> : 캐시된 RDD에 이름 지정 가능</li></ul></li><li>저장소 수준<ul><li>싱글턴 객체 <code>org.apache.spark.storage.StorageLevel</code> 의 속성(메모리, 디스크, 메모리+디스크 조합, off-heap) 중 하나로 지정 가능</li><li>저장소 수준은 <code>getStorageLevel()</code> 로 조회 가능</li></ul></li></ul><h3 id="12-9-체크포인팅"><a href="#12-9-체크포인팅" class="headerlink" title="12.9 체크포인팅"></a>12.9 체크포인팅</h3><ul><li><strong>체크포인팅 (checkpointing)</strong> : RDD =&gt; 디스크에 저장<ul><li>DataFrame API에서 사용할 수 없는 기능 중 하나</li><li>저장된 RDD 참조 시, 원본 데이터소스를 다시 계산해 디스크에 저장된 중간 결과 파티션을 참조 (RDD 생성 X)</li><li>캐싱과 유사 (단, 메모리가 아닌 디스크에 저장)</li><li>반복적인 연산 수행시 유용 (최적화 good)</li></ul></li><li><code>checkpoint()</code> 설정 =&gt; RDD 참조 시 데이터소스의 데이터 대신 체크포인트에 저장된 RDD 사용</li></ul><h3 id="12-10-RDD를-시스템-명령으로-전송하기"><a href="#12-10-RDD를-시스템-명령으로-전송하기" class="headerlink" title="12.10 RDD를 시스템 명령으로 전송하기"></a>12.10 RDD를 시스템 명령으로 전송하기</h3><ul><li><code>pipe()</code><ul><li>파이핑 요소로 생성된 RDD =&gt; 외부 프로세스로 전달</li><li>외부 프로세스는 파티션마다 한번씩 처리 =&gt; 결과 RDD 생성<ul><li>입력 파티션 : 각 입력 파티션의 모든 요소는 개행 문자 단위로 분할, 여러 줄의 입력 데이터로 변경 =&gt; 프로세스의 표준 입력 (stdin) 으로 전달</li><li>결과 파티션 =&gt; 프로세스의 표준 출력 (stdout) 에 전달. 표준 출력의 각 줄은 출력 파티션의 하나의 요소가 됨</li></ul></li><li>비어있는 파티션 처리 시에도 프로세스는 실행됨</li><li>사용자가 정의한 두 함수를 인수로 전달 시, 출력 방식을 원하는대로 변경 가능 (<a href="https://bit.ly/2OhgRwd">API docs</a> 참고)</li></ul></li><li><code>mapPartitions()</code><ul><li>스파크는 실제 코드 실행 시 파티션 단위로 동작<ul><li><code>map()</code> 이 반환하는 RDD =&gt; <code>MapPartitionsRDD</code></li><li><U>map은 mapPartitions의 로우 단위 처리를 위한 별칭일 뿐이다</U></li></ul></li><li>mapPartitions는 개별 파티션(이터레이터로 표현) 에 대해 <code>map</code> 연산 수행 가능<ul><li>클러스터에서 물리적 단위로 개별 파티션을 처리하기 때문 (로우 단위 처리 X)</li></ul></li><li>기본적으로 파티션 단위로 작업 수행<ul><li>=&gt; <strong>전체</strong> 파티션에 대한 연산 수행 가능</li><li>RDD의 전체 하위 데이터셋에 원하는 연산 수행 가능</li></ul></li><li><code>mapPartitionsWithIndex()</code> : 인덱스 (파티션 범위의 인덱스, RDD의 파티션 번호) 와 파티션의 모든 아이템을 순회하는 이터레이터를 가진 함수를 인수로 사용</li></ul></li><li><code>foreachPartition()</code><ul><li>개별 파티션에서 특정 작업을 수행하는데 적합</li><li><code>mapPartitions()</code> vs <code>foreachPartition()</code><ul><li>mapPartitions 는 처리 결과 반환, foreachPartition 는 결과 반환 X (순회만)</li></ul></li></ul></li><li><code>glom()</code><ul><li>데이터셋의 모든 파티션 =&gt; 배열로 변환</li><li>데이터를 드라이버로 모으고, 데이터가 존재하는 파티션의 배열이 필요한 경우 유용</li><li>(단, 파티션이 크거나 많으면 드라이버가 비정상적으로 종료)</li></ul></li></ul><h3 id="12-11-정리"><a href="#12-11-정리" class="headerlink" title="12.11 정리"></a>12.11 정리</h3><ul><li>단일 RDD 처리 방법</li><li>RDD API의 기초</li></ul><h3 id="📒-단어장"><a href="#📒-단어장" class="headerlink" title="📒 단어장"></a>📒 단어장</h3>]]></content>
    
    
      
      
    <summary type="html">&lt;br/&gt;

&lt;h4 id=&quot;📌-Part-3-저수준-API&quot;&gt;&lt;a href=&quot;#📌-Part-3-저수준-API&quot; class=&quot;headerlink&quot; title=&quot;📌 [ Part 3 저수준 API ]&quot;&gt;&lt;/a&gt;📌 [ Part 3 저수준 API ]&lt;/h</summary>
      
    
    
    
    <category term="spark" scheme="https://minsw.github.io/categories/spark/"/>
    
    
    <category term="book" scheme="https://minsw.github.io/tags/book/"/>
    
    <category term="study" scheme="https://minsw.github.io/tags/study/"/>
    
    <category term="spark" scheme="https://minsw.github.io/tags/spark/"/>
    
    <category term="apache" scheme="https://minsw.github.io/tags/apache/"/>
    
  </entry>
  
  <entry>
    <title>&amp;#039;Spark The Definitive Guide&amp;#039; 11장 - 셋뚜셋뚜 데이터셋뚜 (PART 2 끝)</title>
    <link href="https://minsw.github.io/2021/03/01/Spark-The-Definitive-Guide-11%EC%9E%A5/"/>
    <id>https://minsw.github.io/2021/03/01/Spark-The-Definitive-Guide-11%EC%9E%A5/</id>
    <published>2021-02-28T15:56:15.000Z</published>
    <updated>2021-03-03T08:02:20.997Z</updated>
    
    <content type="html"><![CDATA[<br/><h4 id="Part-2-END"><a href="#Part-2-END" class="headerlink" title="[Part 2] END"></a>[Part 2] END</h4><p>드디어 길고 길었던 <a href="https://minsw.github.io/2021/01/26/Spark-The-Definitive-Guide-4%EC%9E%A5/">구조적 API</a> 파트 끝 🎉<br>예제가 많아서 계속 차근차근 따라가면서 읽다보니 한 챕터 당 소요시간이 훨씬 길었다.. 후…</p><p>남은 파트들은 템포가 좀 짧았으면 좋겠다 (´；ω；`)</p><p style="color:lightgray"><i>(해치웠나..?)</i></p><br/><img src="https://user-images.githubusercontent.com/26691216/109769822-1362f100-7c3e-11eb-9f22-9a1d3be261dd.png" width=400/><center>쎘뚜쎘뚜~ 데이터프레임이랑 셋뚜~</center><center><h2>_ _ _</h2></center><br/><hr><h1 id="CHAPTER-11-Dataset"><a href="#CHAPTER-11-Dataset" class="headerlink" title="CHAPTER 11 Dataset"></a>CHAPTER 11 Dataset</h1><p>Dataset은 구조적 API의 기본 데이터 타입 (DataFrame = Row 타입의 Dataset <code>Dataset[Row]</code>)<br>스팤잘알들은 ‘타입형 API’ 라고 부르기도 한다</p><ul><li>Dataset은 JVM 사용하는 Scala, Java에서만 사용 가능 (DataFrame은 다양한 언어 사용 가능)<ul><li>Scala는 스키마가 정의된 케이스 클래스 객체로 정의</li><li>Java는 자바 빈 객체로 정의</li></ul></li></ul><h4 id="인코더-encoder"><a href="#인코더-encoder" class="headerlink" title="인코더 (encoder) ?"></a>인코더 (encoder) ?</h4><ul><li>인코더 : 도메인별 특정 객체 T → 스파크의 내부 데이터 타입으로 매핑하는 시스템 <ul><li>인코더가 스파크에게 지시 (런타임 환경) : 객체 (클래스)  → 바이너리 구조로 직렬화 코드 생성 </li><li>DataFrame 이나 ‘표준’ 구조적 API 사용 시 :  Row 타입 → 바이너리 구조로 변환</li></ul></li><li>도메인에 특화된 객체를 만들어 사용하려면 <U><strong>사용자 정의 데이터 타입</strong></U> 정의 필요<ul><li>Scala의 <code>case class</code>, Java의 <code>JavaBean</code> 형태로</li><li>스파크는 Row 타입 대신 사용자 정의 데이터 타입을 분산 방식으로 다루기 가능</li></ul></li><li>Dataset API 사용 시<ul><li>스파크가 데이터셋에 접근할 때마다 사용자 정의 데이터 타입으로 변환 (Row 포맷 X)</li><li>=&gt; <strong>느리다 (성능 ↓)</strong> / 대신 더 많은 유연성</li><li>사용자 정의 함수 (Python) 이랑 비슷?<ul><li>사용자 정의 데이터 타입 &gt;&gt;&gt;&gt;&gt;&gt; 사용자 정의 함수 (언어 전환이 훨씬 느림)</li></ul></li></ul></li></ul><br/><h3 id="11-1-Dataset을-사용할-시기"><a href="#11-1-Dataset을-사용할-시기" class="headerlink" title="11.1 Dataset을 사용할 시기"></a>11.1 Dataset을 사용할 시기</h3><blockquote><p><del><em>“그럼 Dataset 성능 구린데 왜 쓰나?”</em></del></p><p><strong>Dataset을 사용해야하는 2가지 이유</strong></p><ol><li>DataFrmae 기능만으로는 수행할 연산을 표현할 수 없는 경우</li><li>성능 저하를 감수하더라도, 타입 안정성(type-safe)를 가진 데이터 타입을 사용하고 싶은 경우</li></ol></blockquote><ul><li>(1) 구조적 API로 표현할 수 없는 작업들<ul><li>ex. 복잡한 비즈니스로직을 단일 함수(SQL X, DataFrame X) 로 인코딩해야하는 경우</li></ul></li><li>(2) 타입 안정성 (정확도↑, 방어적 코드)<ul><li>데이터 타입이 유효하지 않은 작업 =&gt; 컴파일 타임에 오류 발생 (런타임 X)</li><li>잘못된 데이터로부터 보호는 X</li><li>그러나 보다 우아하게 데이터를 제어, 구조화 가능</li></ul></li><li>+ <strong>로컬과 분산 환경의 워크로드에서 재사용 가능</strong><ul><li>When?<ul><li>단일 노드의 워크로드와 스파크 워크로드에서 전체 로우에 대한 다양한 트랜스포메이션을 재사용하고자할 때</li></ul></li><li>How? <ul><li>스파크 API = Scala의 Sequence 타입 API 가 일부 반영 + 분산 방식 동작 (<a href="http://bit.ly/2NFqKGZ">Scala 만든 형 said</a>)</li><li>케이스 클래스로 구현된 데이터 타입으로 모든 데이터와 트랜스포메이션을 정의하면 재사용 가능</li></ul></li><li>또한 올바른 클래스, 데이터 타입이 지정된 DataFrame을 로컬 디스크에 저장하면 다음 처리 과정에서 사용 가능 (more easy)</li></ul></li><li>DataFrame + Dataset 동시 사용<ul><li>성능 &lt;-&gt; 타입 안정성 (Trade-Off)</li><li>대량의 DataFrame 기반의 ETL 트랜스포메이션의 <U>마지막 단계</U>에서 사용 가능<ul><li>ex. 드라이버로 데이터를 수집 후, 단일 노드의 라이브러리로 수집된 데이터 처리하는 경우</li></ul></li><li>트랜스포메이션 <U>첫 단계</U>에서 사용도 가능<ul><li>ex. 스파크 SQL에서 필터링 전에 로우 단위로 데이터 파싱하는 경우</li></ul></li></ul></li></ul><h3 id="11-2-Dataset-생성"><a href="#11-2-Dataset-생성" class="headerlink" title="11.2 Dataset 생성"></a>11.2 Dataset 생성</h3><ul><li>Dataset 생성은 수동 작업<ul><li>정의할 스키마를 미리 알고 있어야 함</li></ul></li><li>자바 (Java) : Encoders<ul><li>데이터 타입 클래스 정의 후,  DataFrame (<code>= Dataset&lt;Row&gt;</code>) 에 지정해서 인코딩<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.Encoders;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Flight</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  String DEST_COUNTRY_NAME;</span><br><span class="line">  String ORIGIN_COUNTRY_NAME;</span><br><span class="line">  Long DEST_COUNTRY_NAME;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Dataset&lt;Flight&gt; flights = spark.read</span><br><span class="line">  .parquet(<span class="string">&quot;/data/flight-data/parquet/2010-summary.parquet/&quot;</span>)</span><br><span class="line">  .as(Encoders.bean(Flight.class)); <span class="comment">// Encoders 지정</span></span><br></pre></td></tr></table></figure></li></ul></li><li>스칼라 (Scala) : 케이스 클래스<ul><li>Scala의 케이스 클래스 (<code>case class</code>) 는 정규 클래스(regular class)</li><li>특징<ul><li><strong>불변성</strong></li><li>패턴 매칭으로 분해가능</li><li>참조값 대신 클래스 구조를 기반으로 비교</li><li>사용하기 쉽고 다루기 편함</li></ul></li><li>장점<ul><li>불변성 =&gt; 객체의 변경 추적 필요 X</li><li>값 대신 구조로 비교 가능 =&gt; 클래스 인스턴스가 값으로 비교되는지 참조로 비교되는지 걱정 X (값 비교시 인스턴스를 primitive 데이터 타입 값 처럼 비교함)</li><li>패턴 매칭 =&gt; 로직 분기 단순화 (버그 ↓ 가독성 ↑)</li><li>(더 자세한 건 <a href="https://bit.ly/2xdwSfd">스칼라 문서</a> 참고)</li></ul></li><li>case class 로 데이터 타입 정의. DataFrame의 <code>as()</code> 로 변환<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Flight</span>(<span class="params"><span class="type">DEST_COUNTRY_NAME</span>: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                  <span class="type">ORIGIN_COUNTRY_NAME</span>: <span class="type">String</span>, count: <span class="type">BigInt</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">flightsDF</span> </span>= spark.read</span><br><span class="line">  .parquet(<span class="string">&quot;/data/flight-data/parquet/2010-summary.parquet/&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> flights = flightsDF.as[<span class="type">Flight</span>] <span class="comment">// Dataset</span></span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="11-3-액션"><a href="#11-3-액션" class="headerlink" title="11.3 액션"></a>11.3 액션</h3><ul><li>Dataset, DataFrame의 힘보다 “액션을 적용할 수 있다”는 사실이 더 중요하다<ul><li><code>collect()</code>, <code>take()</code>, <code>count()</code> …</li></ul></li><li>케이스 클래스에 실제로 접근 시 “어떠한 데이터 타입도 필요하지 않다”는 사실도 알아라<ul><li>‘속성 명’ 으로 해당 값 &amp; 데이터 타입 모두 반환됨</li></ul></li></ul><p style="color:lightgray">말투 무엇..</p><h3 id="11-4-트랜스포메이션"><a href="#11-4-트랜스포메이션" class="headerlink" title="11.4 트랜스포메이션"></a>11.4 트랜스포메이션</h3><ul><li>Dataset의 트랜스포메이션 == DataFrame의 트랜스포메이션<ul><li>Dataset은 원형의 JVM 데이터 타입을 다루므로, 더 복잡하고 강력한 데이터 타입으로 사용 가능</li><li>원형 객체 다루는 법? =&gt; <strong>필터링 &amp; 매핑</strong></li></ul></li><li>필터링<ul><li>불리언 값을 반환하는 함수 (=&gt; <strong>일반 함수</strong>) 정의<ul><li>스파크 SQL은 <strong>사용자 정의 함수</strong> 정의</li><li>스파크는 정의된 함수로 모든 로우를 평가 (자원 사용량 ↑)</li><li>단순 필터의 경우 SQL 표현식 사용 권장</li><li>데이터 필터링 비용 ↓ 다음 처리과정에서 Dataset으로 데이터 다루기 가능</li></ul></li><li>단순 트랜스포메이션</li></ul></li><li>매핑<ul><li>특정 값을 다른 값으로 매핑 작업 (값 추출/비교 등의 정교한 처리)</li><li>DataFrame의 매핑 = Dataset의 <code>select()</code></li><li>컴파일 타임에 데이터 타입 유효성 검사 가능 (스파크가 결과로 반환되는 JVM 데이터 타입을 알고 있기 때문)</li></ul></li><li>사실 DataFrame 사용을 권장함<ul><li>매핑 작업보다 더 많은 장점.. (ex. 코드 생성 기능) 대다수의 매핑작업 수행가능..</li><li>하지만 훨씬 정교하게 로우 단위 처리가 필요하다면 Dataset</li></ul></li></ul><h3 id="11-5-조인"><a href="#11-5-조인" class="headerlink" title="11.5 조인"></a>11.5 조인</h3><ul><li>조인도 DataFrame과 동일하게 제공<ul><li>Dataset은 정교한 메서드 제공 =&gt; <code>joinWith()</code></li></ul></li><li><code>joinWith()</code><ul><li>co-group (RDD) 과 거의 유사</li><li>Dataset 안쪽에 다른 두 개의 중첩된 Dataset으로 구성<ul><li>각 컬럼은 단일 Dataset =&gt; Dataset 객체를 컬럼처럼 다루기 가능</li></ul></li><li>=&gt; 조인 시 더 많은 정보 유지 가능. 고급 맵이나 필터처럼 정교한 데이터 다루기 가능</li></ul></li><li>일반 조인 (<code>join()</code>)<ul><li>=&gt; 결과가 DataFrame으로 반환. JVM 데이터 타입 정보를 잃음</li><li>DataFrame + Dataset 조인도 문제 X (동일 결과 반환) </li></ul></li></ul><h3 id="11-6-그룹화와-집계"><a href="#11-6-그룹화와-집계" class="headerlink" title="11.6 그룹화와 집계"></a>11.6 그룹화와 집계</h3><ul><li>동일한 기본 표준을 따름<ul><li><code>groupBy()</code>, <code>rollup()</code>, <code>cube()</code> 그대로 사용 가능</li><li>단, DataFrame을 반환 (데이터 타입 정보 잃음)</li></ul></li><li>데이터 타입 정보를 유지하려면?<ul><li>ex. <code>groupByKey()</code><ul><li>Dataset 특정 키 기준으로 그룹화하고 형식화된 Dataset 반환</li><li>파라미터는 함수 사용 (컬럼명 X) =&gt; 유연성 good / 최적화 X<ul><li>스파크는 함수랑 JVM 데이터 타입 최적화 X</li><li>(성 능 차 이)</li></ul></li></ul></li><li>groupByKey vs groupBy<ul><li>데이터 스캔 직후에 집계를 수행하는 groupBy 보다 처리비용이 더 비쌈</li><li>따라서  사용자가 정의한 인코딩으로 세밀한 처리가 필요하는 등의 필요한 경우에만 사용할 것</li></ul></li></ul></li><li>Dataset은 빅데이터 처리 파이프라인의 처음과 끝에서 주로 사용 할 것</li></ul><h3 id="11-7-정리"><a href="#11-7-정리" class="headerlink" title="11.7 정리"></a>11.7 정리</h3><ul><li>Dataset의 기초와 Dataset 사용이 적합한 경우</li><li>Dataset을 사용하기 위한 기본 지식 및 사용 방법</li><li>Dataset = 고수준의 구조적 API + 저수준 RDD API</li></ul><h3 id="📒-단어장"><a href="#📒-단어장" class="headerlink" title="📒 단어장"></a>📒 단어장</h3>]]></content>
    
    
      
      
    <summary type="html">&lt;br/&gt;

&lt;h4 id=&quot;Part-2-END&quot;&gt;&lt;a href=&quot;#Part-2-END&quot; class=&quot;headerlink&quot; title=&quot;[Part 2] END&quot;&gt;&lt;/a&gt;[Part 2] END&lt;/h4&gt;&lt;p&gt;드디어 길고 길었던 &lt;a href=&quot;https:/</summary>
      
    
    
    
    <category term="spark" scheme="https://minsw.github.io/categories/spark/"/>
    
    
    <category term="book" scheme="https://minsw.github.io/tags/book/"/>
    
    <category term="study" scheme="https://minsw.github.io/tags/study/"/>
    
    <category term="spark" scheme="https://minsw.github.io/tags/spark/"/>
    
    <category term="apache" scheme="https://minsw.github.io/tags/apache/"/>
    
  </entry>
  
  <entry>
    <title>&amp;#039;Spark The Definitive Guide&amp;#039; 10장 - SQL 너마저</title>
    <link href="https://minsw.github.io/2021/02/23/Spark-The-Definitive-Guide-10%EC%9E%A5/"/>
    <id>https://minsw.github.io/2021/02/23/Spark-The-Definitive-Guide-10%EC%9E%A5/</id>
    <published>2021-02-23T06:42:28.000Z</published>
    <updated>2021-03-03T08:01:47.137Z</updated>
    
    <content type="html"><![CDATA[<img src="https://user-images.githubusercontent.com/26691216/109770707-3e9a1000-7c3f-11eb-9e63-1a6235580fa8.jpg" width=300/><center><h2>_ _ _</h2></center><br/><hr><h1 id="CHAPTER-10-스파크-SQL"><a href="#CHAPTER-10-스파크-SQL" class="headerlink" title="CHAPTER 10 스파크 SQL"></a>CHAPTER 10 스파크 SQL</h1><p><strong>스파크 SQL</strong>은 스파크에서 가장 중요하고 강력한 기능 중 하나.<br>스파크 SQL은 DataFrame, Dataset API에 통합되어 있다<br>(=&gt; 즉 SQL, DataFrame 기능 모두 사용 가능 &amp; 동일한 실행 코드로 컴파일)</p><ul><li>DB에 생성된 뷰(view)나 테이블에 SQL쿼리 실행 가능</li><li>시스템 함수 사용, 사용자 정의 함수 정의 가능</li><li>쿼리 실행 계획 분석 가능 (=&gt; 워크로드 최적화)</li></ul><h3 id="10-1-SQL이란"><a href="#10-1-SQL이란" class="headerlink" title="10.1 SQL이란"></a>10.1 SQL이란</h3><ul><li>SQL (Structured Query Launguage, 구조적 질의 언어)<ul><li>데이터에 대한 관계형 연산을 표한하기 위한 <strong>도메인 특화 언어</strong></li><li><U>모든 관계형 데이터베이스</U>에서 사용</li><li>NoSQL 데이터베이스에서도 쉽게 사용 가능한 변형된 자체 SQL 제공</li><li><p style="color:lightgray"> 없어질거다~ 진다~ 해도 없어질 수가 없는 데이터 처리 언어 계의 짱짱맨 </p></li></ul></li><li>스파크 SQL<ul><li><a href="https://bit.ly/2NcpcVl">ANSI SQL:2003</a> 의 일부를 구현</li><li>=&gt; 대부분의 SQL DB에서 채택하고 있는 표준 (so 유명 벤치마크도 통과)</li></ul></li></ul><h3 id="10-2-빅데이터와-SQL-아파치-하이브"><a href="#10-2-빅데이터와-SQL-아파치-하이브" class="headerlink" title="10.2 빅데이터와 SQL: 아파치 하이브"></a>10.2 빅데이터와 SQL: 아파치 하이브</h3><ul><li>스파크 등장 전<ul><li>사실상 빅데이터 SQL 접근 계층의 표준 =&gt; <strong>Hive</strong> (made by Facebook)</li><li>하둡을 다양한 사업군으로 진출하는데 도움</li></ul></li><li>스파크는 RDD를 이용하는 범용  처리 엔진으로 시작했지만, 현재는 많은 이용자가<br>스파크 SQL 사용 중</li></ul><h3 id="10-3-빅데이터와-SQL-스파크-SQL"><a href="#10-3-빅데이터와-SQL-스파크-SQL" class="headerlink" title="10.3 빅데이터와 SQL: 스파크 SQL"></a>10.3 빅데이터와 SQL: 스파크 SQL</h3><ul><li>Hive 지원하는 상위 호환 기능 지원<ul><li>스파크 2.0 버전. 자체 개발된 SQL 파서 포함</li><li>=&gt; ANSI-SQL, HiveQL 모두 지원</li></ul></li><li>스파크 SQL은 DataFrame과의 뛰어난 호환성<ul><li>다양한 기업에서 강력한 기능으로 자리매김할 수 있는 이유</li><li>ex. Facebook 의 <a href="https://bit.ly/2bLrvJG">스파크 워크로드 (2016)</a> (블로그 내용 <code>p.280-281</code>)</li></ul></li><li>스파크 SQL의 강력함을 만드는 핵심 요인<ul><li>SQL 분석가) Thrift Server 나 SQL 인터페이스에 접속해, 스파크 연산 능력 활용 가능</li><li>데이터 엔지니어 &amp; 과학자) 전체 데이터 처리 파이프라인에 스파크 SQL 사용 가능</li></ul></li><li>스파크 SQL (통합형 API) 으로 인해 가능해진 전체 과정<ul><li>SQL 로 데이터 조회</li><li>=&gt; DataFrame으로 변환</li><li>=&gt; 스파크 MLlib (대규모 머신러닝 알고리즘) 수행 결과를 다른 데이터 소스에 저장</li></ul></li><li>단, 스파크 SQL은 <strong>OLAP</strong> 데이터베이스로 동작 (OLTP X)<ul><li><strong>OLAP</strong> (OnLine Analytic Processing, 온라인 분석용)</li><li>OLTP (OnLine Transaction Processing, 온라인 트랜잭션 처리)</li><li>=&gt; 따라서 매우 낮은 지연 시간이 필요한 쿼리 수행용도로는 사용 X</li><li>언젠가는 인플레이스 수정(in-place modification) 방식을 지원할수는 있으나, 현재는 <strong>사용 불가</strong></li></ul></li></ul><h4 id="스파크와-하이브의-관계"><a href="#스파크와-하이브의-관계" class="headerlink" title="스파크와 하이브의 관계"></a>스파크와 하이브의 관계</h4><ul><li>스파크 SQL은 Hive 메타스토어 사용 =&gt; 하이브와 연동 good<ul><li>Hive 메타스토어는 여러 세션에서 사용할 테이블 정보 보관</li><li>(Hive 사용 시) 스파크 SQL 는 Hive Metastore에 접속해서 조회할 파일 수를 최소화 하기 위해 메타데이터 참조함<ul><li>=&gt; 기존 하둡 환경의 모든 워크로드를 스파크로 이관하기 좋음</li></ul></li></ul></li><li>Hive Metastore <ul><li>접속하기위해 필요한 속성<ul><li><code>spark.sql.hive.metastore.version</code> (default 1.2.1)</li><li><code>spark.sql.hive.metastore.jars</code> : HiveMetastoreClient 초기화 방식 변경</li></ul></li><li>스파크는 기본 버전 사용 (호환을 위해 클래스패스에 정의도 가능)</li><li>Hive Metastore가 저장된 다른 데이터베이스 접속 시<ul><li>적합한 클래스 접두사 정의 필요 (ex. MySQL =&gt; <code>com.mysql.jdbc</code>)</li><li>접두사 정의 후 <code>spark.sql.hive.metastore.sharedPrefixes</code> 속성에 설정하여 스파크, 하이브에서 공유</li></ul></li><li>사용하던 하이브 메타스토어와 연동 시, <a href="http://bit.ly/2DFlcrL">스파크 공식 문서</a> 에서 부가 정보 확인</li></ul></li></ul><h3 id="10-4-스파크-SQL-쿼리-실행-방법"><a href="#10-4-스파크-SQL-쿼리-실행-방법" class="headerlink" title="10.4 스파크 SQL 쿼리 실행 방법"></a>10.4 스파크 SQL 쿼리 실행 방법</h3><ul><li>스파크가 SQL 쿼리 실행을 위하여 제공하는 인터페이스</li><li>(1) 스파크 SQL CLI</li><li>(2) 스파크의 프로그래밍 SQL 인터페이스</li><li>(3) 스파크 SQL 쓰리프트 JDBC/ODBC 서버</li></ul><h3 id="10-5-카탈로그"><a href="#10-5-카탈로그" class="headerlink" title="10.5 카탈로그"></a>10.5 카탈로그</h3><ul><li><strong>카탈로그</strong> (catalog) : 스파크 SQL에서 가장 높은 추상화 단계<ul><li>테이블에 저장된 데이터에 대한 메타데이터 + 데이터베이스, 테이블, 함수, 뷰에 대한 정보 까지 추상화</li></ul></li><li><code>org.apache.spark.sql.catalog.Catalog</code> 패키지<ul><li>테이블, 데이터베이스, 함수 조회 등 여러 유용한 함수 제공</li></ul></li><li>스파크 SQL을 사용하는 또 다른 방식의 <U>프로그래밍 인터페이스</U> 이다<ul><li>=&gt; <code>saprk.sql</code> 함수를 사용해 관련 코드 실행 가능</li></ul></li></ul><h3 id="10-6-테이블"><a href="#10-6-테이블" class="headerlink" title="10.6 테이블"></a>10.6 테이블</h3><ul><li>스파크 SQL 사용을 위해서는 테이블 정의 부터 필요</li><li>테이블은 ‘명령을 실행할 데이터의 구조’<ul><li>DataFrame과 논리적으로 동일</li><li>=&gt; 테이블로 조인, 필터링, 집계 등의 여러 데이터 변환 작업 수행 가능</li></ul></li><li>테이블 vs DataFrame<ul><li>테이블은 데이터베이스에서 정의, DataFrame은 프로그래밍 언어로 정의</li><li>스파크에서 테이블 생성시 =&gt; <U><strong>default</strong> 데이터 베이스에 등록</U></li></ul></li><li>테이블은 <strong>‘항상 데이터를 가지고 있다’</strong> (스파크 2.x 버전)<ul><li>임시 테이블 개념 X</li><li>데이터를 가지지않은 뷰만 존재 (테이블 X)</li><li>테이블 제거 시 모든 데이터 제거됨</li></ul></li></ul><h4 id="테이블-다루기"><a href="#테이블-다루기" class="headerlink" title="테이블 다루기"></a>테이블 다루기</h4><ul><li><p>스파크 관리형 테이블</p><ul><li>테이블은 두 가지 중요 정보 저장 =&gt; 테이블의 데이터 &amp; 테이블에 대한 데이터 (<strong>메타데이터</strong>)</li><li>관리형 테이블 &amp; 외부 테이블 개념 알기<ul><li><strong>관리형 테이블</strong> : DataFrame의 <code>saveAsTable()</code> 로 생성 (스파크가 관련된 모든 정보를 추적 가능)</li><li><strong>외부 테이블</strong> : 디스크에 저장된 파일을 이용해 테이블을 정의 (파일의 메타데이터 관리)</li></ul></li><li><code>saveAsTable()</code> 메서드<ul><li>테이블을 읽고, 데이터를 스파크 포맷으로 변환 후, 새로운 경로에 저장 (Hive 기본 웨어하우스 경로)</li></ul></li><li>저장 경로에서 데이터베이스 목록 확인 가능<ul><li>기본 저장 경로 : /user/hive/warehouse</li><li>저장 경로 변경? =&gt; SparkSession 생성 시 <code>spark.sql.warehouse.dir</code> 에 설정</li></ul></li><li><code>show tables IN &#123;databaseName&#125;</code> 쿼리로 특정 데이터베이스 테이블 확인 가능</li></ul></li><li><p>테이블 생성하기</p><ul><li>다양한 데이터소스로 테이블 생성 가능</li><li>스파크는 SQL에서 전체 데이터소스 API 재사용 가능<ul><li>즉, 테이블 정의 후 테이블에 데이터 적재 필요 X (?)</li><li>실행 즉시 테이블 생성</li></ul></li><li>파일에서 데이터 읽을 때 모든 종류의 정교한 옵션 지정 가능<ul><li><code>USING</code> (Hive : <code>STORED AS</code>)</li><li>스파크는 포맷 미 지정 시, 기본적으로 하이브 SerDe 설정 사용 가능 (스파크 자체 직렬화보다 훨씬 느림)  </li></ul></li><li>테이블 특정 컬럼에 코멘트(<code>COMMENT</code>) 추가 가능</li><li>SELECT 쿼리 결과로 테이블 생성 가능<ul><li>=&gt; CTAS (Create Table .. As .. Select ..) 패턴</li></ul></li><li>파티셔닝 데이터셋을 저장해 데이터 레이아웃 제어 가능 (<a href="https://minsw.github.io/2021/02/16/Spark-The-Definitive-Guide-9%EC%9E%A5/">9장</a> 참고)</li><li>스파크에 접속한 세션에서도 생성된 테이블 사용 가능<ul><li>사용자가 임시 뷰를 만들어서 사용 가능 (스파크에는 임시 테이블 X)</li></ul></li></ul></li><li><p>외부 테이블 생성하기</p><ul><li>스파크 SQL은 HiveQL 과 완벽하게 호환 <ul><li>HiveQL 대부분 그대로 사용 가능</li></ul></li><li>스파크는 외부 테이블의 메타데이터 관리 (스파크에서 데이터 파일은 관리 X)</li><li><code>CREATE EXTERNAL TABLE</code> 구문</li><li>마찬가지로 쿼리 결과로 생성도 가능 (CTAS 패턴)</li></ul></li><li><p>테이블에 데이터 삽입하기</p><ul><li><code>INSERT INTO</code> 구문 (표준 SQL 문법 사용)</li><li>특정 파티션에만 저장하고 싶다면? =&gt; 파티션 명세 추가<ul><li>쓰기 연산은 파티셔닝 스키마에 맞게 데이터 저장 (단, 매우 느리게 동작할 수도)</li></ul></li></ul></li><li><p>테이블 메타데이터 확인하기</p><ul><li><code>DESCRIBE</code> 구문 : 테이블의 메타데이터 정보 반환<ul><li>테이블 생성 시 추가된 코멘트 확인 가능</li></ul></li><li><code>SHOW PARTITIONS</code> 으로 파티셔닝 스키마 정보 확인 가능 (파티션된 테이블인 경우)</li></ul></li><li><p>테이블 메타데이터 갱신하기</p><ul><li>가장 최신의 데이터셋을 읽고 있다는 것을 보장하려면? =&gt; “테이블 메타데이터 유지” (중요!)</li><li><code>REFRESH TABLE</code> 구문 : 테이블과 관련된 모든 캐싱된 항목 갱신 (기본적으로 파일)<ul><li>테이블이 이미 캐싱된 경우, 다음번 스캔 동작하는 시점에 다시 캐싱</li></ul></li><li><code>REPAIR TABLE</code> 구문 : 카탈로그에서 관리하는 테이블의 파티션 정보 새로고침<ul><li>새로운 파티션 정보 수집에 초점</li><li>ex. 수동으로 신규파티션을 만들면 테이블을 수리(repair) 해야함</li></ul></li></ul></li><li><p>테이블 제거하기</p><ul><li>테이블은 삭제(delete)할 수 없고, 오로지 <strong>제거(drop)</strong> 만 가능하다</li><li><code>DROP TABLE</code> 구문<ul><li>관리형 테이블 제거 시, <U><strong>데이터와 테이블 정의 모두 제거</strong>됨</U></li><li>존재하지 않는 테이블 제거 시 오류 발생 </li><li>=&gt; 테이블 존재 시에만 제거하는 <code>DROP TABLE IF EXISTS</code> 대신 사용</li></ul></li><li>외부 테이블 제거하기<ul><li>데이터는 삭제되지 않으나, 더는 외부 테이블 명으로 조회 불가</li></ul></li></ul></li><li><p>테이블 캐싱하기</p><ul><li><code>CACHE TABLE</code> 로 테이블을 캐시 가능</li><li><code>UNCACHE TABLE</code> 로 캐시에서 제거 가능</li></ul></li></ul><h3 id="10-7-뷰"><a href="#10-7-뷰" class="headerlink" title="10.7 뷰"></a>10.7 뷰</h3><ul><li>뷰는 기존 테이블에 여러 트랜스포메이션 작업 지정<ul><li>기본적으로 뷰는 ‘단순 쿼리 실행 계획’</li><li>쿼리 로직 체계화 &amp; 재사용하기 편리</li></ul></li><li>스파크가 가진 뷰에 관련된 다양한 개념<ul><li>데이터베이스에 설정하는 전역 뷰</li><li>세션별 뷰</li></ul></li><li>최종사용자 입장에서 뷰는 테이블처럼 보인다<ul><li>신규 경로에 모든 데이터를 다시 저장 X</li><li>대신 단순하게 쿼리시점에 데이터소스에 트랜스포메이션 수행</li><li>트랜스포메이션 예) filter, select, 대규모 group by, rollup</li></ul></li></ul><h4 id="뷰-다루기"><a href="#뷰-다루기" class="headerlink" title="뷰 다루기"></a>뷰 다루기</h4><ul><li><p>뷰 생성하기</p><ul><li><code>CREATE VIEW</code> 구문 사용</li><li>임시 뷰 생성 : <code>CREATE TEMP VIEW</code><ul><li>현재 세션에서만 사용할 수 있는 임시 뷰</li><li>(테이블처럼 데이터베이스에 등록 X)</li></ul></li><li>전역적 임시 뷰(global temp view) 생성 : <code>CREATE GLOBAL TEMP VIEW</code><ul><li>데이터베이스에 상관없이 사용 가능. 전체 스파크 애플리케이션에서 볼 수 O</li><li>단, 세션 종료 시 뷰도 제거</li></ul></li><li>뷰 덮어쓰기 : <code>CREATE OR REPLACE TEMP VIEW</code> (임시뷰, 일반뷰 모두)</li><li>뷰는 실질적으로 트랜스포메이션 이다<ul><li>스파크는 쿼리가 실행될때만 뷰에 정의된 트랜스포메이션 수행</li><li>즉, 테이블의 데이터를 실제로 조회하는 경우에만 필터 적용</li></ul></li><li>뷰는 <U>기존 DataFrame에서 새로운 DataFrame 만드는 것과 동일</U></li></ul></li><li><p>뷰 제거하기</p><ul><li><code>DROP VIEW (IF EXISTS)</code> 사용</li><li>뷰 제거 vs 테이블 제거<ul><li>뷰 정의는 제거되지만, <strong>어떤 데이터도 제거되지 않음</strong></li><li>(테이블 제거는 데이터도 모두 제거됨)</li></ul></li></ul></li></ul><h3 id="10-8-데이터베이스"><a href="#10-8-데이터베이스" class="headerlink" title="10.8 데이터베이스"></a>10.8 데이터베이스</h3><ul><li>데이터 베이스 = 여러 테이블을 조직화하기 위한 도구</li><li>데이터베이스 미정의 시 스파크는 기본 데이터베이스 사용 (default)</li><li>스파크에서 실행하는 모든 SQL 명령문은 실행 중인 데이터베이스 범위에서 실행<ul><li><p style="color:lightgray"> 데이터베이스 변경 시, 이전에 생성한 모든 사용자 테이블은 변경 전 데이터베이스에 속해 있으므로 다르게 쿼리해야함 (뭔 소린가 했더니..)</p> </li><li>=&gt; 즉, 다른 데이터베이스 사용 시 <code>데이터베이스 명 + 테이블 명</code> 으로 조회할 것</li></ul></li><li><code>SHOW DATABASES</code> 으로 전체 데이터베이스 목록 확인</li></ul><h4 id="데이터베이스-다루기"><a href="#데이터베이스-다루기" class="headerlink" title="데이터베이스 다루기"></a>데이터베이스 다루기</h4><ul><li>데이터베이스 생성하기<ul><li><code>CREATE DATABASE</code> 사용</li></ul></li><li>데이터베이스 설정하기<ul><li><code>USE &#123;databaseName&#125;</code> 으로 쿼리 수행할 데이터베이스 설정</li><li>다른 테이블에 쿼리 수행시 접두사 사용</li><li><code>SELECT current_database()</code> : 현재 사용중인 데이터베이스 확인</li></ul></li><li>데이터베이스 제거하기<ul><li><code>DROP DATABASE (IF EXISTS)</code> 사용</li><li>Q. 스파크 데이터베이스 삭제 시, 데이터는?</li></ul></li></ul><h3 id="10-9-select-구문"><a href="#10-9-select-구문" class="headerlink" title="10.9 select 구문"></a>10.9 select 구문</h3><ul><li>스파크 SQL은 ANSI-SQL 요건 충족<ul><li>SELECT 표현식 구조는 <code>p.296</code> 참고</li></ul></li><li>case…when…then 구문<ul><li>SQL 쿼리 값을 조건에 맞게 처리 가능</li><li>(like if-else statement)</li></ul></li></ul><h3 id="10-10-고급-주제"><a href="#10-10-고급-주제" class="headerlink" title="10.10 고급 주제"></a>10.10 고급 주제</h3><ul><li>데이터 쿼리 방법 알아보기<ul><li>SQL 쿼리 : 특정 명령 집합을 실행하도록 요청하는 SQL 구문</li><li><U>조작, 정의, 제어</U> 와 관련된 명령 정의 가능</li><li>=&gt; 본 책은 대부분 <strong>조작</strong> 관련</li></ul></li><li>복합 데이터 타입<ul><li>표준SQL에는 존재하지 않는 강력한 기능</li><li>스파크 SQL의 핵심 복합 데이터 타입 3가지 =&gt; <strong>구조체, 리스트, 맵</strong></li><li>구조체 : 중첩 데이터 생성/쿼리 방법 제공<ul><li>맵에 가까움</li></ul></li><li>리스트 : 값의 배열이나 리스트 사용<ul><li>집계 함수 <code>collection_list()</code>, <code>collect_set()</code> 으로 생성 가능 (단, 집계 연산 시에만 사용 가능) </li><li><code>ARRAY</code> 로 컬럼에 직접 배열 생성 가능</li><li><code>explode()</code> : 저장된 배열의 모든 값을 단일 로우 형태로 분해 (&lt;-&gt; <code>collect()</code>)</li></ul></li></ul></li><li>함수<ul><li>스파크 SQL은 다양한 고급 함수 제공 (<a href="http://bit.ly/2DPAycx">DataFrame 함수 문서</a> 에서 확인)</li><li><code>SHOW FUNCTIONS</code> 으로 스파크 SQL이 제공하는 전체 함수 목록 확인 가능  <ul><li><code>SHOW SYSTEM FUNCTIONS</code> : 스파크에 내장된 시스템 함수 목록</li><li><code>SHOW USER FUNCTIONS</code> : 누군가가 스파크 환경에 공개한 함수 목록 (=  <strong>사용자 정의 함수</strong>)</li><li>와일드카드 문자(*)로 필터링 가능</li><li><code>LIKE</code> 키워드 사용 가능</li></ul></li><li><code>DESCRIBE FUNCTION &#123;functionName&#125;</code> : 개별 함수의 설명과 사용법 반환 </li><li>사용자 정의 함수<ul><li>스파크는 사용자 정의 함수를 정의하여 분산 환경에서 사용할 수 있는 기능 제공</li><li>특정 언어로 함수 개발 후 등록하여 정의</li><li>함수 등록/정의 방법 =&gt; <code>spark.udf.register()</code>, Hive의 <code>CREATE TEMPORARY FUNCTION</code></li></ul></li></ul></li><li>서브 쿼리<ul><li>서브 쿼리 (subquery) : 쿼리안에 쿼리 지정<ul><li>SQL 내 정교한 로직 명시 가능</li><li>=&gt; 하나 (스칼라 서브쿼리) 이상의 결과 반환 가능?</li></ul></li><li>스파크의 기본 서브쿼리 2가지<ul><li><strong>상호연관 서브쿼리</strong> (correlated subquery) : 쿼리 외부 범위에 있는 일부 정보 사용 가능</li><li><strong>비상호연관 서브쿼리</strong> (uncorrelated subquery) : 외부 범위 정보 사용 X</li></ul></li><li><strong>조건절 서브쿼리</strong> (predicate subquery) 도 지원 =&gt; 값에 따라 필터링</li><li>비상호 스칼라 쿼리(uncorrelated scalar query) 사용 시<ul><li>기존에 없던 일부 부가 정보 가져오기 가능</li><li>(?)</li></ul></li></ul></li></ul><h3 id="10-11-다양한-기능"><a href="#10-11-다양한-기능" class="headerlink" title="10.11 다양한 기능"></a>10.11 다양한 기능</h3><ul><li>지금까지의 내용과 잘 안맞는 몇가지 특징?<ul><li>SQL 코드 성능 최적화, 디버깅 케이스 등..</li></ul></li><li>설정<ul><li>스파크 SQL 환경 설정 값 (<code>p.302-303 [표 10-1]</code> 참고)</li><li>애플리케이션 초기화 시점 or 실행 시점에 설정 가능</li></ul></li><li>SQL에서 설정값 지정하기<ul><li><code>SET</code> 사용하여 SQL을 사용하여 환경 설정 가능 (단, 스파크 SQL 관련 설정에 한함)</li><li>SQL 설정은 15장에서 자세히</li></ul></li></ul><h3 id="10-12-정리"><a href="#10-12-정리" class="headerlink" title="10.12 정리"></a>10.12 정리</h3><ul><li>스파크 SQL 관련 세부사항</li><li>스파크 SQL과 DataFrame은 매우 밀접한 연관</li></ul><h3 id="📒-단어장"><a href="#📒-단어장" class="headerlink" title="📒 단어장"></a>📒 단어장</h3><ul><li>ANSI-SQL : 미국 표준 협회 ANSI (American National Standards Institute)에서 정립한 표준 SQL문</li><li>워크로드 (workloads) : 고객 대면 애플리케이션이나 백엔드 프로세스 같이 비즈니스 가치를 창출하는 리소스 및 코드 모음. 또는 말그대로 시스템이 실행해야할 할당 작업량.</li><li>Thrift Server<ul><li>Apache Thrift (RPC Framework)? Hive의 Thrift Server?</li><li>스파크의 Thrift =&gt; 여러 사용자의 JDBC(or OCBC) 접속을 받아 사용자 쿼리를 원격으로 스파크 SQL 세션으로 실행하는 스파크 애플리케이션<ul><li><a href="https://thebook.io/006908/part02/ch05/03/03/">참고 링크</a></li></ul></li></ul></li><li>OLTP vs OLAP<ul><li>OLTP (OnLine Transaction Processing, 온라인 트랜잭션 처리) 는 DB서버에서 각각의 작업요청의 트랜잭션 처리 (CUD 무결성 보장) &amp; 결과 READ 과정</li><li>OLAP (OnLine Analytic Processing, 온라인 분석용) 는 저장된 데이터를 바탕으로 요구와 목적에 맞는 분석 정보 제공</li><li>즉, OLTP는 데이터 처리 중심, OLAP는 저장된 데이터 분석 중심</li></ul></li><li>인플레이스 수정(in-place modifiction) 방식<ul><li>in-place update?</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;img src=&quot;https://user-images.githubusercontent.com/26691216/109770707-3e9a1000-7c3f-11eb-9e63-1a6235580fa8.jpg&quot; width=300/&gt;

&lt;center&gt;&lt;h2&gt;_ </summary>
      
    
    
    
    <category term="spark" scheme="https://minsw.github.io/categories/spark/"/>
    
    
    <category term="book" scheme="https://minsw.github.io/tags/book/"/>
    
    <category term="study" scheme="https://minsw.github.io/tags/study/"/>
    
    <category term="spark" scheme="https://minsw.github.io/tags/spark/"/>
    
    <category term="apache" scheme="https://minsw.github.io/tags/apache/"/>
    
  </entry>
  
  <entry>
    <title>&amp;#039;Spark The Definitive Guide&amp;#039; 9장 - 쏘쓰는 역시 데이터소스</title>
    <link href="https://minsw.github.io/2021/02/16/Spark-The-Definitive-Guide-9%EC%9E%A5/"/>
    <id>https://minsw.github.io/2021/02/16/Spark-The-Definitive-Guide-9%EC%9E%A5/</id>
    <published>2021-02-15T15:11:36.000Z</published>
    <updated>2021-02-21T15:14:12.267Z</updated>
    
    <content type="html"><![CDATA[<br/><img src="https://user-images.githubusercontent.com/26691216/108165351-c8bd8100-7135-11eb-9cbe-6ccfa0e63155.gif" width=400/><center><h2>_ _ _</h2></center><br/><hr><h1 id="CHAPTER-9-데이터소스"><a href="#CHAPTER-9-데이터소스" class="headerlink" title="CHAPTER 9 데이터소스"></a>CHAPTER 9 데이터소스</h1><p>스파크 기본 6가지 ‘핵심’ 데이터 소스 + 커뮤니티에서 만든 외부 데이터소스 소개</p><p>핵심데이터 소스를 통해 데이터를 읽고 쓰는 방법을 터득하고<br>서드파티 데이터소스와 스파크 연동 시 고려해야할 점을 배우는 것이 목표</p><h4 id="스파크의-핵심-데이터-소스"><a href="#스파크의-핵심-데이터-소스" class="headerlink" title="스파크의 핵심 데이터 소스"></a>스파크의 핵심 데이터 소스</h4><ul><li>CSV</li><li>JSON</li><li>파케이(Parquet)</li><li>ORC</li><li>JDBC/ODBC 연결</li><li>일반 텍스트 파일</li></ul><h4 id="커뮤니티에서-만든-데이터소스"><a href="#커뮤니티에서-만든-데이터소스" class="headerlink" title="커뮤니티에서 만든 데이터소스"></a>커뮤니티에서 만든 데이터소스</h4><ul><li><a href="http://bit.ly/2DSafT8">카산드라</a></li><li><a href="http://bit.ly/2FkKN5A">HBase</a></li><li><a href="http://bit.ly/2BwA7yq">몽고DB</a></li><li><a href="http://bit.ly/2GlMsJE">AWS Redshift</a></li><li><a href="http://bit.ly/2GitGCK">XML</a></li><li>기타 수많은 데이터 소스</li></ul><h3 id="9-1-데이터소스-API의-구조"><a href="#9-1-데이터소스-API의-구조" class="headerlink" title="9.1 데이터소스 API의 구조"></a>9.1 데이터소스 API의 구조</h3><ul><li><p>데이터 소스 API 전체 구조부터 이해하기</p></li><li><p><strong><em>읽기 API</em></strong> 구조</p><ul><li>핵심 구조 (모든 데이터 소스를 읽을 때 해당 형식 사용)  <i style="color:lightgray">// 요약 표기법도 존재</i><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DataFrameReader</span>.format(...).option(<span class="string">&quot;key&quot;</span>, <span class="string">&quot;value&quot;</span>).schema(...).load()</span><br></pre></td></tr></table></figure></li><li><code>format()</code> : 포맷 설정은 Optional (default - Parquet 포멧)</li><li><code>option()</code> : 데이터 읽는 방법에 대한 파라미터 키-값 쌍으로 설정</li><li><code>schema()</code> : 데이터 소스에서 스키마를 제공하거나 추론 기능 사용 시. Optional</li></ul></li><li><p>데이터 읽기의 기초</p><ul><li><code>DataFrameReader</code> : 스파크에서 데이터를 읽을 때 기본적으로 사용<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DataFrameReader은 SparkSession의 read 속성으로 접근</span></span><br><span class="line">spark.read</span><br></pre></td></tr></table></figure></li><li>DataFrameReader에 지정해야하는 값<ul><li>포맷</li><li>스키마</li><li>읽기 모드 (필수, default 값 존재)</li><li>옵션</li><li>(+ <strong>데이터 읽을 경로</strong> 필수 지정)</li></ul></li><li><strong>읽기 모드</strong> : 스파크가 형식에 맞지않는 데이터를 만났을 때 동작 방식 지정하는 옵션<ul><li>반정형 데이터소스 다룰 시 많이 발생</li></ul></li><li>스파크의 읽기 모드 종류<ul><li><code>permissive</code> (default): 오류 레코드 모든 필드를 null로 지정하고 오류 레코드를 _corrupt_record (문자열 컬럼) 에 기록</li><li><code>dropMalformed</code> : 형식에 맞지않는 레코드가 포함된 로우 제거</li><li><code>failFast</code> : 형식에 맞지않는 레코드 만날 시 즉시 종료<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 읽기 코드 구성 예제</span></span><br><span class="line">spark.read.format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;mode&quot;</span>, <span class="string">&quot;FAILFAST&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;inferSchema&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;path&quot;</span>, <span class="string">&quot;path/to/file(s)&quot;</span>)</span><br><span class="line">  .schema(someSchema)</span><br><span class="line">  .load()</span><br></pre></td></tr></table></figure></li></ul></li></ul></li><li><p><strong><em>쓰기 API</em></strong> 구조</p><ul><li>핵심 구조 (모든 데이터 소스를 읽을 때 해당 형식 사용)<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DataFrameWriter</span>.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()</span><br></pre></td></tr></table></figure></li><li><code>format()</code> : 포맷 설정은 Optional (default - Parquet 포멧)</li><li><code>option()</code> : 데이터 쓰기 방법 설정</li><li><code>partitionBy()</code>, <code>bucketBy()</code>, <code>sortBy()</code> : 최종 파일 배치 형태(layout) 제어 가능. 파일기반 데이터소스에만 동작</li></ul></li><li><p>데이터 쓰기의 기초</p><ul><li><p>데이터 읽기와 매우 유사. Reader대신 Writer 사용</p></li><li><p><code>DataFrameWriter</code> : 데이터 소스에 항상 데이터를 기록해야하고, DataFrame 별로 DataFramewriter에 접근해야함</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DataFrame 의 write 속성을 이용해서 DataFrameWriter에 접근</span></span><br><span class="line">dataFrame.write</span><br></pre></td></tr></table></figure></li><li><p>DataFrameWriter에 지정해야하는 값</p><ul><li>포맷</li><li>옵션</li><li>저장 모드</li><li>(+ <strong>데이터 저장 경로</strong> 필수 지정)</li></ul></li><li><p><strong>저장 모드</strong> : 스파크가 지정된 위치에서 동일한 파일을 발견했을 때 동작 방식 지정하는 옵션</p></li><li><p>스파크의 저장 모드 종류</p><ul><li><code>append</code> : 해당 경로에 이미 존재하는 파일 목록에 결과 파일 추가</li><li><code>overwrite</code> : 이미 존재하는 모든 데이터 덮어쓰기</li><li><code>errorIfExists</code> (default) : 해당 경로에 데이터나 파일이 존재하면 오류 발생 및 쓰기 작업 실패</li><li><code>ignore</code> : 아무런 처리 안함</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 쓰기 코드 구성 예제</span></span><br><span class="line">spark.write.format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;mode&quot;</span>, <span class="string">&quot;OVERWRITE&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;dateFormat&quot;</span>, <span class="string">&quot;yyyy-MM-dd&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;path&quot;</span>, <span class="string">&quot;path/to/file(s)&quot;</span>)</span><br><span class="line">  .save()</span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="9-2-CSV-파일"><a href="#9-2-CSV-파일" class="headerlink" title="9.2 CSV 파일"></a>9.2 CSV 파일</h3><ul><li>CSV(comma-separated values) : <code>,</code> 로 구분된 값<ul><li>각 줄이 단일 레코드, 레코드의 각 필드는 콤마로 구분하는 텍스트 파일 포멧</li><li>구조적인 것 같아도 개 까다로운 포맷 (다양한 전제 생성 가능…)</li><li>=&gt; 따라서 CSV Reader 가 <strong>많은 옵션</strong> 제공 </li></ul></li><li>옵션<ul><li>CSV Reader, Writer 많은 옵션 제공 (<code>p.250-251 [표 9-3]</code> 참고)</li><li>maxColumns, inferSchema 등 쓰기에서는 적용되지 않는 옵션 빼고는 <strong>읽기와 쓰기는 동일한 옵션</strong> 제공</li></ul></li><li>CSV 파일 읽기<ul><li>예제 참고<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">spark.read.format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line"><span class="comment">// in Scala</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">(spark.read.format(&quot;csv&quot;)</span></span><br><span class="line"><span class="comment">  .option(&quot;header&quot;, &quot;true&quot;)</span></span><br><span class="line"><span class="comment">  .option(&quot;mode&quot;, &quot;FAILFAST&quot;)</span></span><br><span class="line"><span class="comment">  .option(&quot;inferSchema&quot;, &quot;true&quot;)</span></span><br><span class="line"><span class="comment">  .load(&quot;some/path/to/file.csv&quot;))</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure></li><li>데이터가 기대한 데이터 포맷이 아닌경우?<ul><li><strong>실제</strong> 스키마와는 일치하지 않지만 스파크는 문제 인지 X</li><li>스파크가 실제로 데이터를 읽어 들이는 시점에 문제 발생 (스파크 잡 즉시 종료)</li><li>즉, 정의하는 시점에는 문제 X. 잡 실행 시점에만 오류 발생  =&gt; <u>스파크의 <strong>지연 연산</strong> 특성</u></li></ul></li></ul></li><li>CSV 파일 쓰기<ul><li>예제 참고</li></ul></li></ul><details><summary class="point-color-can-hover">[9.2] CSV 파일 read 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// CSV 파일 읽기</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">StructField</span>, <span class="type">StructType</span>, <span class="type">StringType</span>, <span class="type">LongType</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> myManualSchema = <span class="keyword">new</span> <span class="type">StructType</span>(<span class="type">Array</span>(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">StructField</span>(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>, <span class="type">StringType</span>, <span class="literal">true</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="type">StructField</span>(<span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span>, <span class="type">StringType</span>, <span class="literal">true</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="type">StructField</span>(<span class="string">&quot;count&quot;</span>, <span class="type">LongType</span>, <span class="literal">false</span>)</span><br><span class="line">))</span><br><span class="line"></span><br><span class="line">(spark.read.format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;mode&quot;</span>, <span class="string">&quot;FAILFAST&quot;</span>)</span><br><span class="line">  .schema(myManualSchema)</span><br><span class="line">  .load(<span class="string">&quot;/data/flight-data/csv/2010-summary.csv&quot;</span>)</span><br><span class="line">  .show(<span class="number">5</span>))</span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |    United States|            Romania|    1|</span></span><br><span class="line"><span class="comment">// |    United States|            Ireland|  264|</span></span><br><span class="line"><span class="comment">// |    United States|              India|   69|</span></span><br><span class="line"><span class="comment">// |            Egypt|      United States|   24|</span></span><br><span class="line"><span class="comment">// |Equatorial Guinea|      United States|    1|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 기대한 데이터 포맷이 아니라면?</span></span><br><span class="line"><span class="comment">// =&gt; 당장 에러 발생은 X. 스파크가 실제로 데이터를 읽어들이는 시점에 에러 발생 (지연 연산)</span></span><br><span class="line"><span class="keyword">val</span> myManualSchema = <span class="keyword">new</span> <span class="type">StructType</span>(<span class="type">Array</span>(</span><br><span class="line">                     <span class="keyword">new</span> <span class="type">StructField</span>(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>, <span class="type">LongType</span>, <span class="literal">true</span>),</span><br><span class="line">                     <span class="keyword">new</span> <span class="type">StructField</span>(<span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span>, <span class="type">LongType</span>, <span class="literal">true</span>),</span><br><span class="line">                     <span class="keyword">new</span> <span class="type">StructField</span>(<span class="string">&quot;count&quot;</span>, <span class="type">LongType</span>, <span class="literal">false</span>) ))</span><br><span class="line"></span><br><span class="line">(spark.read.format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;mode&quot;</span>, <span class="string">&quot;FAILFAST&quot;</span>)</span><br><span class="line">  .schema(myManualSchema)</span><br><span class="line">  .load(<span class="string">&quot;/data/flight-data/csv/2010-summary.csv&quot;</span>)</span><br><span class="line">  .take(<span class="number">5</span>))</span><br><span class="line"><span class="comment">// org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 47.0 failed 1 times, most recent failure: Lost task 0.0 in stage 47.0 (TID 107, localhost, executor driver): org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// (참고용) 정상 스키마</span></span><br><span class="line"><span class="comment">// val myManualSchema = new StructType(Array(</span></span><br><span class="line"><span class="comment">//                      new StructField(&quot;DEST_COUNTRY_NAME&quot;, StringType, true),</span></span><br><span class="line"><span class="comment">//                      new StructField(&quot;ORIGIN_COUNTRY_NAME&quot;, StringType, true),</span></span><br><span class="line"><span class="comment">//                      new StructField(&quot;count&quot;, LongType, false) ))</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// =&gt; res122: Array[org.apache.spark.sql.Row] = Array([United States,Romania,1], [United States,Ireland,264], [United States,India,69], [Egypt,United States,24], [Equatorial Guinea,United States,1])</span></span><br></pre></td></tr></table></figure></details><details><summary class="point-color-can-hover">[9.2] CSV 파일 write 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// CSV 파일 쓰기 (CSV 파일 읽어서 TSV 파일로 내보내기)</span></span><br><span class="line"><span class="keyword">val</span> csvFile = (spark.read.format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>).option(<span class="string">&quot;mode&quot;</span>, <span class="string">&quot;FAILFAST&quot;</span>).schema(myManualSchema)</span><br><span class="line">  .load(<span class="string">&quot;/data/flight-data/csv/2010-summary.csv&quot;</span>))</span><br><span class="line"></span><br><span class="line">(csvFile.write.format(<span class="string">&quot;csv&quot;</span>).mode(<span class="string">&quot;overwrite&quot;</span>).option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;\t&quot;</span>)</span><br><span class="line">  .save(<span class="string">&quot;/tmp/my-tsv-file.tsv&quot;</span>))</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 데이터를 쓰느 시점에 DataFrame의 파티션 수를 반영</span></span><br><span class="line">$ ls /tmp/my-tsv-file.tsv/</span><br><span class="line">part-00000-183b90fb-5828-434a-b948-55dd5732c7b0-c000.csv  _SUCCESS</span><br></pre></td></tr></table></figure></details><h3 id="9-3-JSON-파일"><a href="#9-3-JSON-파일" class="headerlink" title="9.3 JSON 파일"></a>9.3 JSON 파일</h3><ul><li>JSON(JavaScript Object Notation)<ul><li>스파크는 <strong>줄로 구분된 JSON</strong> 을 기본적으로 사용<ul><li>multiLine 옵션으로 줄로 구분 vs 여러 줄로 구성된 방식 선택 가능</li><li><code>true</code> 로 설정 시 =&gt; 전체 파일을 하나의 JSON 파일로 읽기 가능</li></ul></li><li>그래도 줄로 구분된 JSON을 추천하는 이유?<ul><li>전체 파일을 읽어서 저장하는 방식이 아니므로 =&gt; 새로운 레코드 추가 가능 (안정적)</li><li>구조화되어 있고, 최소한의 기본 데이터 타입이 존재 =&gt; 적합한 데이터타입 추정 가능</li></ul></li></ul></li><li>옵션<ul><li>JSON은 객체. CSV(텍스트) 보다 옵션수 적음 (<code>p.255-256 [표 9-4]</code> 참고)</li></ul></li><li>JSON 파일 읽기<ul><li>예제 참고<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.read.format(<span class="string">&quot;json&quot;</span>)</span><br></pre></td></tr></table></figure></li></ul></li><li>JSON 파일 쓰기<ul><li>예제 참고</li><li>데이터소스와 관계없이 JSON 파일로 저장 가능<ul><li>ex. CSV DataFrame =&gt; JSON 파일</li><li>이전의 규칙을 그대로 따른다? (예제이야기인지?)</li><li>파티션당 하나의 파일을 만들고, 전체 DataFrame을 단일 폴더에 저장. JSON 객체는 한줄에 하나씩 기록.</li></ul></li></ul></li></ul><details><summary class="point-color-can-hover">[9.3] JSON 파일 read/write 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// JSON 파일 읽기</span></span><br><span class="line"><span class="comment">// spark.read.format(&quot;json&quot;)</span></span><br><span class="line">(spark.read.format(<span class="string">&quot;json&quot;</span>).option(<span class="string">&quot;mode&quot;</span>, <span class="string">&quot;FAILFAST&quot;</span>).schema(myManualSchema)</span><br><span class="line">  .load(<span class="string">&quot;/data/flight-data/json/2010-summary.json&quot;</span>).show(<span class="number">5</span>))</span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |    United States|            Romania|    1|</span></span><br><span class="line"><span class="comment">// |    United States|            Ireland|  264|</span></span><br><span class="line"><span class="comment">// |    United States|              India|   69|</span></span><br><span class="line"><span class="comment">// |            Egypt|      United States|   24|</span></span><br><span class="line"><span class="comment">// |Equatorial Guinea|      United States|    1|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// JSON 파일 쓰기 (CSV DataFrame =&gt; JSON 파일)</span></span><br><span class="line">csvFile.write.format(<span class="string">&quot;json&quot;</span>).mode(<span class="string">&quot;overwrite&quot;</span>).save(<span class="string">&quot;/tmp/my-json-file.json&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 파티션당 하나의 파일 만들고 전체 DataFrame은 단일폴더에 저장</span></span><br><span class="line">$ ls /tmp/my-json-file.json/</span><br><span class="line">part-00000-8a5f3d0c-2241-4508-ab3f-e2648f9a5ff4-c000.json  _SUCCESS</span><br><span class="line"></span><br><span class="line"><span class="comment"># JSON 객체는 한줄에 하나씩 기록</span></span><br><span class="line">$ head -2 /tmp/my-json-file.json/part-00000-8a5f3d0c-2241-4508-ab3f-e2648f9a5ff4-c000.json</span><br><span class="line">&#123;<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>:<span class="string">&quot;United States&quot;</span>,<span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span>:<span class="string">&quot;Romania&quot;</span>,<span class="string">&quot;count&quot;</span>:1&#125;</span><br><span class="line">&#123;<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>:<span class="string">&quot;United States&quot;</span>,<span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span>:<span class="string">&quot;Ireland&quot;</span>,<span class="string">&quot;count&quot;</span>:264&#125;</span><br></pre></td></tr></table></figure></details><h3 id="9-4-파케이-파일"><a href="#9-4-파케이-파일" class="headerlink" title="9.4 파케이 파일"></a>9.4 파케이 파일</h3><ul><li>파케이(Parquet) : 다양한 스토리지 최적화 기술을 제공하는 오픈소스로 만들어진 <strong>컬럼 기반의 데이터 저장 방식</strong><ul><li>분석 워크로드에 최적화</li><li>저장소 공간 절약</li><li>전체 파일 읽기 대신 개별 컬럼 읽기 가능</li><li>컬럼 기반의 압축 기능 제공</li><li>아파치 스파크와 특히 호환 good =&gt; 그래서 <strong>스파크 기본 파일 포멧</strong></li><li>복합 데이터 타입 지원 (CSV는 배열 사용 X)</li></ul></li><li>읽기 연산이 CSV, JSON보다 훨씬 효율적 =&gt; 장기저장용 데이터는 파케이 권장<ul><li><a style="color:lightgray">걍 파케이가 짱짱맨이란 소리다</a></li></ul></li><li>옵션<ul><li>파케이는 옵션이 거의 없음. 단 2개  (<code>p.259 [표 9-5]</code> 참고)<ul><li>2개만 존재하는 이유는.. 그냥 모범생 포맷이기때문… (자체 스키마 사용해서 데이터 저장)</li><li>그러나 ‘호환되지 않는 파케이 파일’ 주의 =&gt; 트기 <u>다른 버전(구버전)의 스파크 사용 시 파케이 저장</u> 에 주의</li></ul></li></ul></li><li>파케이 파일 읽기<ul><li>예제 참고<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.read.format(<span class="string">&quot;parquet&quot;</span>)</span><br></pre></td></tr></table></figure></li><li>포맷 설정만으로 충분<ul><li>DataFrame 표현을 위해 정확한 스키마가 필요할 때만 스키마 지정 </li><li>그렇지만 사실 거의 필요 X</li></ul></li><li>파케이 파일은 스키마가 파일 자체에 내장되어 추론 필요 X<ul><li>읽는 시점에 스키마를 알 수 있다 (Schema-on-read)</li><li>CSV 파일 inferSchema랑 비슷</li></ul></li></ul></li><li>파케이 파일 쓰기<ul><li>예제 참고</li><li>“읽기만큼 쉽다” =&gt; 파일의 경로만 명시하면 됨<ul><li>분할 규칙은 다른 포맷과 동일하게 적용</li></ul></li></ul></li></ul><details><summary class="point-color-can-hover">[9.4] Parquet 파일 read/write 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Parquet 파일 읽기</span></span><br><span class="line"><span class="comment">// spark.read.format(&quot;parquet&quot;)</span></span><br><span class="line">(spark.read.format(<span class="string">&quot;parquet&quot;</span>)</span><br><span class="line">  .load(<span class="string">&quot;/data/flight-data/parquet/2010-summary.parquet&quot;</span>).show(<span class="number">5</span>))</span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |    United States|            Romania|    1|</span></span><br><span class="line"><span class="comment">// |    United States|            Ireland|  264|</span></span><br><span class="line"><span class="comment">// |    United States|              India|   69|</span></span><br><span class="line"><span class="comment">// |            Egypt|      United States|   24|</span></span><br><span class="line"><span class="comment">// |Equatorial Guinea|      United States|    1|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// Parquet 파일 쓰기</span></span><br><span class="line">(csvFile.write.format(<span class="string">&quot;parquet&quot;</span>).mode(<span class="string">&quot;overwrite&quot;</span>)</span><br><span class="line">  .save(<span class="string">&quot;/tmp/my-parquet-file.parquet&quot;</span>))</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 다른 포멧과 동일한 분할 규칙</span></span><br><span class="line">$ ls /tmp/my-parquet-file.parquet/</span><br><span class="line">part-00000-7275ca33-21c1-4ce1-8e4f-93f9918a938d-c000.snappy.parquet  _SUCCESS</span><br></pre></td></tr></table></figure></details><h3 id="9-5-ORC-파일"><a href="#9-5-ORC-파일" class="headerlink" title="9.5 ORC 파일"></a>9.5 ORC 파일</h3><ul><li>ORC(Optimized Row Columnar) : 하둡 워크로드를 위해 설계된 자기 기술적(self-describing)이며 데이터 타입을 인식할 수 있는 <strong>컬럼 기반의 파일 포맷</strong><ul><li>대규모 스트리밍 읽기에 최적화</li><li>필요한 로우를 신속하게 찾을 수 있는 기능 통합</li><li>스파크에서 별도 옵션 지정 없이 데이터 읽기 가능</li></ul></li><li>ORC vs Parquet<ul><li>매우 유사하지만, 차이는 Parquet은 Spark에, ORC는 Hive에 최적화되어있음</li><li>ORC는 옵션은 따로 없는 듯?</li></ul></li><li>ORC 파일 읽기<ul><li>예제 참고</li></ul></li><li>ORC 파일 쓰기<ul><li>예제 참고</li></ul></li></ul><details><summary class="point-color-can-hover">[9.5] ORC 파일 read/write 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ORC 파일 읽기</span></span><br><span class="line">spark.read.format(<span class="string">&quot;orc&quot;</span>).load(<span class="string">&quot;/data/flight-data/orc/2010-summary.orc&quot;</span>).show(<span class="number">5</span>)</span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |    United States|            Romania|    1|</span></span><br><span class="line"><span class="comment">// |    United States|            Ireland|  264|</span></span><br><span class="line"><span class="comment">// |    United States|              India|   69|</span></span><br><span class="line"><span class="comment">// |            Egypt|      United States|   24|</span></span><br><span class="line"><span class="comment">// |Equatorial Guinea|      United States|    1|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// ORC 파일 쓰기</span></span><br><span class="line">csvFile.write.format(<span class="string">&quot;orc&quot;</span>).mode(<span class="string">&quot;overwrite&quot;</span>).save(<span class="string">&quot;/tmp/my-json-file.orc&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 다른 포멧과 동일한 분할 규칙</span></span><br><span class="line">$ ls /tmp/my-json-file.orc/</span><br><span class="line">part-00000-a45b9d23-eb06-48d1-935a-110cfdbddfdb-c000.snappy.orc  _SUCCESS</span><br></pre></td></tr></table></figure></details><h3 id="9-6-SQL-데이터베이스"><a href="#9-6-SQL-데이터베이스" class="headerlink" title="9.6 SQL 데이터베이스"></a>9.6 SQL 데이터베이스</h3><blockquote><p>예제 추가하면서 내용 보충 예정</p></blockquote><ul><li>SQLite 샘플로 예제 (DB 설정과정 생략) _ 분산환경에서 사용해서는 X</li><li>JDBC 데이터소스 옵션 (<code>p.262-263 [표 9-6]</code> 참고)</li><li>SQL 데이터베이스 읽기</li><li>쿼리 푸시다운<ul><li>데이터베이스 병렬로 읽기</li><li>슬라이딩 윈도우 기반의 파티셔닝</li></ul></li><li>SQL 데이터베이스 쓰기</li></ul><h3 id="9-7-텍스트-파일"><a href="#9-7-텍스트-파일" class="headerlink" title="9.7 텍스트 파일"></a>9.7 텍스트 파일</h3><ul><li>일반 텍스트 파일(plain-text file) 도 읽기 가능<ul><li>각 줄이 DataFrame의 레코드</li><li>변환은 마음대로 가능 (ex. 아파치 로그 파일 → 구조화된 포멧으로 파싱, 자연어 처리를 위한 일반 텍스트 파싱)</li><li>기본 데이터 타입의 유연성 활용 가능 =&gt; Dataset API 활용 👍🏻</li></ul></li><li>텍스트 파일 읽기<ul><li>예제 참고</li><li><code>textFile(텍스트 파일)</code> 사용</li><li><code>text()</code> : 파티션된 텍스트 파일을 읽고 쓸 경우 파티션 수행 결과로 생성된 디렉토리를 인식할 수 있음 (<code>textFile()</code>은 무시)</li></ul></li><li>텍스트 파일 쓰기<ul><li>예제 참고</li><li><strong>문자열 컬럼이 하나만 존재</strong>해야함 (아닐 경우 실패)</li><li>파티셔닝 작업 수행 시 더 많은 컬럼 저장 가능<ul><li>단 모든 파일에 컬럼 추가 아님</li><li>텍스트 파일이 저장되는 디렉토리에 폴더별로 컬럼 저장</li></ul></li></ul></li></ul><details><summary class="point-color-can-hover">[9.7] 텍스트 파일 read/write 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 텍스트 파일 읽기</span></span><br><span class="line">(spark.read.textFile(<span class="string">&quot;/data/flight-data/csv/2010-summary.csv&quot;</span>)</span><br><span class="line">  .selectExpr(<span class="string">&quot;split(value, &#x27;,&#x27;) as rows&quot;</span>).show())</span><br><span class="line"><span class="comment">// +--------------------+</span></span><br><span class="line"><span class="comment">// |                rows|</span></span><br><span class="line"><span class="comment">// +--------------------+</span></span><br><span class="line"><span class="comment">// |[DEST_COUNTRY_NAM...|</span></span><br><span class="line"><span class="comment">// |[United States, R...|</span></span><br><span class="line"><span class="comment">// |[United States, I...|</span></span><br><span class="line"><span class="comment">// |[United States, I...|</span></span><br><span class="line"><span class="comment">// |[Egypt, United St...|</span></span><br><span class="line"><span class="comment">// |[Equatorial Guine...|</span></span><br><span class="line"><span class="comment">// |[United States, S...|</span></span><br><span class="line"><span class="comment">// |[United States, G...|</span></span><br><span class="line"><span class="comment">// |[Costa Rica, Unit...|</span></span><br><span class="line"><span class="comment">// |[Senegal, United ...|</span></span><br><span class="line"><span class="comment">// |[United States, M...|</span></span><br><span class="line"><span class="comment">// |[Guyana, United S...|</span></span><br><span class="line"><span class="comment">// |[United States, S...|</span></span><br><span class="line"><span class="comment">// |[Malta, United St...|</span></span><br><span class="line"><span class="comment">// |[Bolivia, United ...|</span></span><br><span class="line"><span class="comment">// |[Anguilla, United...|</span></span><br><span class="line"><span class="comment">// |[Turks and Caicos...|</span></span><br><span class="line"><span class="comment">// |[United States, A...|</span></span><br><span class="line"><span class="comment">// |[Saint Vincent an...|</span></span><br><span class="line"><span class="comment">// |[Italy, United St...|</span></span><br><span class="line"><span class="comment">// +--------------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 텍스트 파일 쓰기</span></span><br><span class="line"><span class="comment">// * 문자열 컬럼이 하나만 존재해야 한다 (=&gt; 아닐 시 작업 실패)</span></span><br><span class="line">csvFile.select(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>).write.text(<span class="string">&quot;/tmp/simple-text-file.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 데이터 저장 시 파티셔닝 작업 수행하면 더 많은 컬럼 저장 가능</span></span><br><span class="line"><span class="comment">// 모든파일에 저장 X. 저장 디렉토리에 폴더 별로 컬럼 저장됨</span></span><br><span class="line">(csvFile.limit(<span class="number">10</span>).select(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>, <span class="string">&quot;count&quot;</span>)</span><br><span class="line">  .write.partitionBy(<span class="string">&quot;count&quot;</span>).text(<span class="string">&quot;/tmp/five-csv-files2.csv&quot;</span>))</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># (Result)</span></span><br><span class="line">$ ls /tmp/five-csv-files2.csv</span><br><span class="line">count=1  count=24  count=25  count=264  count=29  count=44  count=477  count=54  count=69  _SUCCESS</span><br><span class="line"></span><br><span class="line">$ ls /tmp/five-csv-files2.csv/count\=1/</span><br><span class="line">part-00000-35cf6fc6-e27f-4861-9c88-d4ce2b913f80.c000.txt</span><br></pre></td></tr></table></figure></details><h3 id="9-8-고급-I-O-개념"><a href="#9-8-고급-I-O-개념" class="headerlink" title="9.8 고급 I/O 개념"></a>9.8 고급 I/O 개념</h3><ul><li>고급 I/O<ul><li>쓰기 작업 전 파티션 수를 조절 =&gt; 병렬 처리 파일 수 제어 가능</li><li><strong>버케팅</strong> &amp; <strong>파티셔닝</strong> =&gt; 데이터 저장 구조 제어 가능</li></ul></li><li>분할 가능한 파일 타입과 압축 방식<ul><li>특정 파일 포맷은 기본적으로 분할 지원<ul><li>=&gt; 스파크가 전체 파일이 아닌 쿼리에 필요한 부분만 읽음</li><li>=&gt; 성능 향상</li></ul></li><li>하둡 분산 파일 시스템 (HDFS) 같은 시스템 : 분할된 파일을 여러 블록으로 나누어 분산 최적화<ul><li>=&gt; 더 좋고요~ 최적화 ↑</li></ul></li><li>압축 방식 : 모든 압축 방식이 분할 압축을 지원하지는 X<ul><li>데이터 저장 방식이 스파크 잡의 원활한 동작에 영향이 큼</li><li>=&gt; <strong>파케이 파일포맷 + GZIP 압축방식</strong> 추천</li></ul></li></ul></li><li>병렬로 데이터 읽기<ul><li>여러 익스큐터가 동시에 같은 파일 읽기는 불가능. 여러 파일 읽기는 가능!</li><li>ex. 다수 파일이 존재하는 폴더를 읽는 상황<ul><li>폴더의 개별 파일 = DataFrame의 파티션</li><li>=&gt; 사용 가능한 익스큐터를 이용해서 병렬로 파일 읽기 O</li></ul></li></ul></li><li>병렬로 데이터 쓰기<ul><li>파일과 데이터 수? =&gt; 데이터를 쓰는 시점의 DataFrame 파티션 수에 따라 달라질 수 있음</li><li>기본적으론 파티션 당 하나의 파일</li><li>옵션에 지정하는 파일명은 실제론 다수의 파일을 가진 <strong>디렉토리</strong><ul><li>해당 디렉토리 안에 파티션 당 하나의 파일로 데이터 저장 (1:1)</li></ul></li></ul></li></ul><h4 id="파티셔닝-amp-버케팅"><a href="#파티셔닝-amp-버케팅" class="headerlink" title="파티셔닝 &amp; 버케팅"></a>파티셔닝 &amp; 버케팅</h4><ul><li><p><strong>파티셔닝(partitioning)</strong> : 어떤 데이터를 어디에 저장할 것인지 제어</p><ul><li><p>파티셔닝된 디렉토리 or 테이블에 파일을 쓸 때, 디렉토리 별로 컬럼 데이터를 인코딩해서 저장</p></li><li><p>즉, 데이터 읽기 시 전체 데이터 스캔 없이 <strong>필요한 컬럼 데이터만 읽기</strong> 가능</p></li><li><p>특징</p><ul><li>모든 파일 기반 데이터소스에서 지원</li><li>필터링 자주 사용하는 테이블 사용 시 =&gt; 가장 손쉬운 최적화 (읽기 속도 ↑)</li></ul></li><li><p>예제</p><details><summary class="point-color-can-hover">[9.8] '파티셔닝' 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(csvFile.limit(<span class="number">10</span>).write.mode(<span class="string">&quot;overwrite&quot;</span>).partitionBy(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>)</span><br><span class="line">  .save(<span class="string">&quot;/tmp/partitioned-files.parquet&quot;</span>))</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># (Result)</span></span><br><span class="line">$ ls /tmp/partitioned-files.parquet</span><br><span class="line">DEST_COUNTRY_NAME=Costa Rica  DEST_COUNTRY_NAME=Egypt  DEST_COUNTRY_NAME=Equatorial Guinea  DEST_COUNTRY_NAME=Senegal  DEST_COUNTRY_NAME=United States  _SUCCESS</span><br><span class="line"></span><br><span class="line"><span class="comment"># 각 폴더는 조건절을 폴더명으로 사용 (조건절 만족 데이터가 저장)</span></span><br><span class="line">$ ls /tmp/partitioned-files.parquet/DEST_COUNTRY_NAME\=Senegal/</span><br><span class="line">part-00000-547b6d60-db63-4b83-90e8-005cc890f6c5.c000.snappy.parquet</span><br></pre></td></tr></table></figure></details></li></ul></li><li><p><strong>버케팅(bucketing)</strong> : 각 파일에 저장된 데이터를 제어할 수 있는 또 다른 파일 조직화 기법</p><ul><li><p>스파크 관리 테이블에서만 사용 가능</p></li><li><p>동일한 버킷 ID 가진 데이터는 동일한 물리적 파티션에 존재</p></li><li><p>즉, 데이터가 이후 사용 방식에 맞춰 사전에 파티셔닝. 조인이나 집계 시의 <strong>고비용 셔플 회피</strong> 가능</p></li><li><p>ex. 특정 컬럼을 파티셔닝해서 수억개 디렉토리 생성되면 =&gt; ‘버켓’ 단위로 데이터를 모아 일정 수 파일로 저장</p></li><li><p>버켓팅 파일 기본 경로 : <code>/user/hive/warehouse/</code></p></li><li><p>예제</p><details><summary class="point-color-can-hover">[9.8] '버케팅' 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> numberBuckets = <span class="number">10</span></span><br><span class="line"><span class="keyword">val</span> columnToBucketBy = <span class="string">&quot;count&quot;</span></span><br><span class="line"></span><br><span class="line">(csvFile.write.format(<span class="string">&quot;parquet&quot;</span>).mode(<span class="string">&quot;overwrite&quot;</span>)</span><br><span class="line">  .bucketBy(numberBuckets, columnToBucketBy).saveAsTable(<span class="string">&quot;bucketedFiles&quot;</span>))</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 기본적으로는 /user/hive/warehouse 디렉토리 하위에 버켓팅 파일 기록</span></span><br><span class="line"><span class="comment"># (=&gt; 디렉토리 먼저 생성해줘야함 `mkdir -p /user/hive/warehouse`)</span></span><br><span class="line">$ ls /user/hive/warehouse/</span><br><span class="line"></span><br><span class="line"><span class="comment"># =&gt; 근데 예제 도커 환경에서는 해당 경로로 안감.. ㅋㅋㅋ;</span></span><br><span class="line"><span class="comment"># $ find / -name bucketedfiles</span></span><br><span class="line"><span class="comment"># /zeppelin/spark-warehouse/bucketedfiles</span></span><br><span class="line"></span><br><span class="line">$ ls /zeppelin/spark-warehouse/bucketedfiles</span><br><span class="line">part-00000-3aff3e3d-fb8d-4fb6-aeba-1df413ac3e9a_00000.c000.snappy.parquet  part-00000-3aff3e3d-fb8d-4fb6-aeba-1df413ac3e9a_00006.c000.snappy.parquet</span><br><span class="line">part-00000-3aff3e3d-fb8d-4fb6-aeba-1df413ac3e9a_00001.c000.snappy.parquet  part-00000-3aff3e3d-fb8d-4fb6-aeba-1df413ac3e9a_00007.c000.snappy.parquet</span><br><span class="line">part-00000-3aff3e3d-fb8d-4fb6-aeba-1df413ac3e9a_00002.c000.snappy.parquet  part-00000-3aff3e3d-fb8d-4fb6-aeba-1df413ac3e9a_00008.c000.snappy.parquet</span><br><span class="line">part-00000-3aff3e3d-fb8d-4fb6-aeba-1df413ac3e9a_00003.c000.snappy.parquet  part-00000-3aff3e3d-fb8d-4fb6-aeba-1df413ac3e9a_00009.c000.snappy.parquet</span><br><span class="line">part-00000-3aff3e3d-fb8d-4fb6-aeba-1df413ac3e9a_00004.c000.snappy.parquet  _SUCCESS</span><br><span class="line">part-00000-3aff3e3d-fb8d-4fb6-aeba-1df413ac3e9a_00005.c000.snappy.parquet</span><br></pre></td></tr></table></figure></details></li></ul></li><li><p>더 자세한 내용은 <a href="https://bit.ly/2NJQfa2">스파크 서밋 2017</a> 참고</p></li><li><p>복합 데이터 유형 쓰기</p><ul><li>스파크의 자체 데이터 타입(<a href="https://minsw.github.io/2021/02/02/Spark-The-Definitive-Guide-6%EC%9E%A5/">6장</a> 참고)은 스파크에서는 잘 동작하지만, 모든 데이터 파일 포맷에 적합하지는 X</li><li>ex. CSV 파일은 복합 데이터 타입 미지원</li></ul></li><li><p>파일 크기 관리</p><ul><li>데이터 저장시에는 문제 없음. <strong>읽을 때는 파일 크기는 중요 요소</strong></li><li>작은 파일 多 =&gt; 메타데이터에 관리 부하 ↑↑<ul><li><code>작은 크기의 파일 문제</code> : 스파크, HDFS 등 많은 파일 시스템은 작은 크기 파일 잘 못 다룸</li></ul></li><li>그럼 큰 파일은 좋은가? =&gt; X<ul><li>몇개의 로우가 필요해도 전체 데이터 블록을 읽음. 비효율</li><li>뭐든 ‘적당’한게 베스트</li></ul></li><li><code>maxRecordsPerFile</code> : 파일당 레코드 수 지정 옵션<ul><li><strong>자동으로 파일 크기를 제어</strong>할 수 있는 새로운 방법 (since 2.2)</li><li>결과 파일 수 = 파일 쓰는 시점의 파티션 수 (파티셔닝 컬럼) 로 결정</li></ul></li></ul></li></ul><h3 id="9-9-정리"><a href="#9-9-정리" class="headerlink" title="9.9 정리"></a>9.9 정리</h3><ul><li>스파크에서 데이터를 읽고/쓸 때 사용할 수 있는 옵션</li><li>사용자 정의 데이터 소스 구현하는 방법은 개선 진행 중이므로 스킵. 궁금하다면 모범사례 참고 (<a href="https://github.com/datastax/spark-cassandra-connector">spark-cassandra-connector</a>) </li></ul><h3 id="📒-단어장"><a href="#📒-단어장" class="headerlink" title="📒 단어장"></a>📒 단어장</h3>]]></content>
    
    
      
      
    <summary type="html">&lt;br/&gt;

&lt;img src=&quot;https://user-images.githubusercontent.com/26691216/108165351-c8bd8100-7135-11eb-9cbe-6ccfa0e63155.gif&quot; width=400/&gt;

&lt;center</summary>
      
    
    
    
    <category term="spark" scheme="https://minsw.github.io/categories/spark/"/>
    
    
    <category term="book" scheme="https://minsw.github.io/tags/book/"/>
    
    <category term="study" scheme="https://minsw.github.io/tags/study/"/>
    
    <category term="spark" scheme="https://minsw.github.io/tags/spark/"/>
    
    <category term="apache" scheme="https://minsw.github.io/tags/apache/"/>
    
  </entry>
  
  <entry>
    <title>&amp;#039;Spark The Definitive Guide&amp;#039; 8장 - 조인</title>
    <link href="https://minsw.github.io/2021/02/15/Spark-The-Definitive-Guide-8%EC%9E%A5/"/>
    <id>https://minsw.github.io/2021/02/15/Spark-The-Definitive-Guide-8%EC%9E%A5/</id>
    <published>2021-02-14T17:27:10.000Z</published>
    <updated>2021-02-21T13:43:24.448Z</updated>
    
    <content type="html"><![CDATA[<br/><img src="https://user-images.githubusercontent.com/26691216/108165034-3ae19600-7135-11eb-8b40-8175f455337b.gif" width=400/><center><h2>_ _ _</h2></center><br/><hr><h1 id="CHAPTER-8-조인"><a href="#CHAPTER-8-조인" class="headerlink" title="CHAPTER 8 조인"></a>CHAPTER 8 조인</h1><p>CHAPTER 7 은 단일 데이터셋의 집계 방법 소개<br>CHPATER 8 은 다양한 데이터 셋을 합께 결합하는 <strong>조인</strong> 타입과 사용법, 실제 동작방식 소개<br><i style="color:lightgray">(=&gt; 메모리 부족 상황을 회피하는 등의 문제 상황 해결에 도움이 되는 기초 지식이 될지도)</i> </p><h3 id="8-1-조인-표현식"><a href="#8-1-조인-표현식" class="headerlink" title="8.1 조인 표현식"></a>8.1 조인 표현식</h3><ul><li><strong>조인 표현식(join expression)</strong> <ul><li>스파크의 왼쪽과 오른쪽 데이터셋의 하나 이상의 키값 을 비교하여 <strong>결합 여부를 결정</strong> (=&gt; 평과 결과)</li></ul></li><li>스파크 지원 조인 정책<ul><li>ex. 동등 조인(equal-join) : <code>왼쪽 키 == 오른쪽 키</code> 일때만 데이터셋 결합</li><li>복합 데이터 타입(배열, 리스트 ..) 사용하는 등의 복잡한 조인 정책도 가능</li></ul></li></ul><h3 id="8-2-조인-타입"><a href="#8-2-조인-타입" class="headerlink" title="8.2 조인 타입"></a>8.2 조인 타입</h3><details><summary class="point-color-can-hover">[8.2] 예제용 데이터셋 생성 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 예제용 데이터셋 생성</span></span><br><span class="line"><span class="keyword">val</span> person = (<span class="type">Seq</span>(</span><br><span class="line">    (<span class="number">0</span>, <span class="string">&quot;Bill Chambers&quot;</span>, <span class="number">0</span>, <span class="type">Seq</span>(<span class="number">100</span>)),</span><br><span class="line">    (<span class="number">1</span>, <span class="string">&quot;Matei Zaharia&quot;</span>, <span class="number">1</span>, <span class="type">Seq</span>(<span class="number">500</span>, <span class="number">250</span>, <span class="number">100</span>)),</span><br><span class="line">    (<span class="number">2</span>, <span class="string">&quot;Michael Armbrust&quot;</span>, <span class="number">1</span>, <span class="type">Seq</span>(<span class="number">250</span>, <span class="number">100</span>)))</span><br><span class="line">  .toDF(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;name&quot;</span>, <span class="string">&quot;graduate_program&quot;</span>, <span class="string">&quot;spark_status&quot;</span>))</span><br><span class="line"><span class="keyword">val</span> graduateProgram = (<span class="type">Seq</span>(</span><br><span class="line">    (<span class="number">0</span>, <span class="string">&quot;Masters&quot;</span>, <span class="string">&quot;School of Information&quot;</span>, <span class="string">&quot;UC Berkeley&quot;</span>),</span><br><span class="line">    (<span class="number">2</span>, <span class="string">&quot;Masters&quot;</span>, <span class="string">&quot;EECS&quot;</span>, <span class="string">&quot;UC Berkeley&quot;</span>),</span><br><span class="line">    (<span class="number">1</span>, <span class="string">&quot;Ph.D.&quot;</span>, <span class="string">&quot;EECS&quot;</span>, <span class="string">&quot;UC Berkeley&quot;</span>))</span><br><span class="line">  .toDF(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;degree&quot;</span>, <span class="string">&quot;department&quot;</span>, <span class="string">&quot;school&quot;</span>))</span><br><span class="line"><span class="keyword">val</span> sparkStatus = (<span class="type">Seq</span>(</span><br><span class="line">    (<span class="number">500</span>, <span class="string">&quot;Vice President&quot;</span>),</span><br><span class="line">    (<span class="number">250</span>, <span class="string">&quot;PMC Member&quot;</span>),</span><br><span class="line">    (<span class="number">100</span>, <span class="string">&quot;Contributor&quot;</span>))</span><br><span class="line">  .toDF(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;status&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 테이블로 등록</span></span><br><span class="line">person.createOrReplaceTempView(<span class="string">&quot;person&quot;</span>)</span><br><span class="line">graduateProgram.createOrReplaceTempView(<span class="string">&quot;graduateProgram&quot;</span>)</span><br><span class="line">sparkStatus.createOrReplaceTempView(<span class="string">&quot;sparkStatus&quot;</span>)</span><br></pre></td></tr></table></figure></details><ul><li><strong>조인 타입</strong> : *<em>결과 데이터셋</em>에 어떤 데이터가 있어야 하는지 결정</li><li>스파크 지원 조인 타입<ul><li>내부 조인(inner join) : 왼쪽 and 오른쪽에 키가 있는 로우 유지</li><li>외부 조인(outer join) : 왼쪽 or 오른쪽에 키가 있는 로우 유지</li><li>왼쪽 외부 조인(left outer join) : 왼쪽에 키가 있는 로우 유지</li><li>오른쪽 외부 조인(right outer join) : 오른쪽에 키가 있는 로우 유지</li><li>왼쪽 세미 조인(left semi join) : 왼쪽 키가 오른쪽에 있는 경우, 키가 일치하는 왼쪽 데이터셋만 유지</li><li>왼쪽 안티 조인(left anti join) : 왼쪽 키가 오른쪽에 없는 경우, 키가 일치하지 않는 왼쪽 데이터셋만 유지</li><li>자연 조인(natural join) : 두 데이터셋에서 동일한 이름 가진 컬럼을 암시적(Implicit)으로 결합하는 조인</li><li>교차 조인(corss join) 또는 카테시안 조인(Cartesian join) : 왼쪽 모든 로우와 오른쪽 모든 로우 조합</li></ul></li><li>조인 타입 예제는 <a href="#%EC%8A%A4%ED%8C%8C%ED%81%AC-%EC%A1%B0%EC%9D%B8-%ED%83%80%EC%9E%85-%EC%98%88%EC%A0%9C">다음</a> 참고</li></ul><h4 id="스파크-조인-타입-예제"><a href="#스파크-조인-타입-예제" class="headerlink" title="# 스파크 조인 타입 예제"></a># 스파크 조인 타입 예제</h4><details><summary class="point-color-can-hover">[8.3~10] 조인 비교 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// val joinExpression = person.col(&quot;graduate_program&quot;) === graduateProgram.col(&quot;id&quot;)</span></span><br><span class="line"><span class="comment">// joinExpression: org.apache.spark.sql.Column = (graduate_program = id)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 3. 내부 조인</span></span><br><span class="line"><span class="comment">// == INNER JOIN</span></span><br><span class="line"><span class="keyword">var</span> joinType = <span class="string">&quot;inner&quot;</span></span><br><span class="line">person.join(graduateProgram, joinExpression, joinType).show()</span><br><span class="line"><span class="comment">// +---+----------------+----------------+---------------+---+-------+--------------------+-----------+</span></span><br><span class="line"><span class="comment">// | id|            name|graduate_program|   spark_status| id| degree|          department|     school|</span></span><br><span class="line"><span class="comment">// +---+----------------+----------------+---------------+---+-------+--------------------+-----------+</span></span><br><span class="line"><span class="comment">// |  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|</span></span><br><span class="line"><span class="comment">// |  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|</span></span><br><span class="line"><span class="comment">// |  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|</span></span><br><span class="line"><span class="comment">// +---+----------------+----------------+---------------+---+-------+--------------------+-----------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 4. 외부 조인</span></span><br><span class="line"><span class="comment">// == FULL OUTER JOIN</span></span><br><span class="line">joinType = <span class="string">&quot;outer&quot;</span></span><br><span class="line">person.join(graduateProgram, joinExpression, joinType).show()</span><br><span class="line"><span class="comment">// +----+----------------+----------------+---------------+---+-------+--------------------+-----------+</span></span><br><span class="line"><span class="comment">// |  id|            name|graduate_program|   spark_status| id| degree|          department|     school|</span></span><br><span class="line"><span class="comment">// +----+----------------+----------------+---------------+---+-------+--------------------+-----------+</span></span><br><span class="line"><span class="comment">// |   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|</span></span><br><span class="line"><span class="comment">// |   2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|</span></span><br><span class="line"><span class="comment">// |null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|</span></span><br><span class="line"><span class="comment">// |   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|</span></span><br><span class="line"><span class="comment">// +----+----------------+----------------+---------------+---+-------+--------------------+-----------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 5. 왼쪽 외부 조인</span></span><br><span class="line"><span class="comment">// == LEFT OUTER JOIN</span></span><br><span class="line">joinType = <span class="string">&quot;left_outer&quot;</span></span><br><span class="line">graduateProgram.join(person, joinExpression, joinType).show()</span><br><span class="line"><span class="comment">// +---+-------+--------------------+-----------+----+----------------+----------------+---------------+</span></span><br><span class="line"><span class="comment">// | id| degree|          department|     school|  id|            name|graduate_program|   spark_status|</span></span><br><span class="line"><span class="comment">// +---+-------+--------------------+-----------+----+----------------+----------------+---------------+</span></span><br><span class="line"><span class="comment">// |  0|Masters|School of Informa...|UC Berkeley|   0|   Bill Chambers|               0|          [100]|</span></span><br><span class="line"><span class="comment">// |  2|Masters|                EECS|UC Berkeley|null|            null|            null|           null|</span></span><br><span class="line"><span class="comment">// |  1|  Ph.D.|                EECS|UC Berkeley|   2|Michael Armbrust|               1|     [250, 100]|</span></span><br><span class="line"><span class="comment">// |  1|  Ph.D.|                EECS|UC Berkeley|   1|   Matei Zaharia|               1|[500, 250, 100]|</span></span><br><span class="line"><span class="comment">// +---+-------+--------------------+-----------+----+----------------+----------------+---------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 6. 오른쪽 외부 조인</span></span><br><span class="line"><span class="comment">// == RIGHT OUTER JOIN</span></span><br><span class="line">joinType = <span class="string">&quot;right_outer&quot;</span></span><br><span class="line">person.join(graduateProgram, joinExpression, joinType).show()</span><br><span class="line"><span class="comment">// +----+----------------+----------------+---------------+---+-------+--------------------+-----------+</span></span><br><span class="line"><span class="comment">// |  id|            name|graduate_program|   spark_status| id| degree|          department|     school|</span></span><br><span class="line"><span class="comment">// +----+----------------+----------------+---------------+---+-------+--------------------+-----------+</span></span><br><span class="line"><span class="comment">// |   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|</span></span><br><span class="line"><span class="comment">// |null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|</span></span><br><span class="line"><span class="comment">// |   2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|</span></span><br><span class="line"><span class="comment">// |   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|</span></span><br><span class="line"><span class="comment">// +----+----------------+----------------+---------------+---+-------+--------------------+-----------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 7. 왼쪽 세미 조인</span></span><br><span class="line"><span class="comment">// == LEFT SEMI JOIN</span></span><br><span class="line">joinType = <span class="string">&quot;left_semi&quot;</span></span><br><span class="line">graduateProgram.join(person, joinExpression, joinType).show()</span><br><span class="line"><span class="comment">// +---+-------+--------------------+-----------+</span></span><br><span class="line"><span class="comment">// | id| degree|          department|     school|</span></span><br><span class="line"><span class="comment">// +---+-------+--------------------+-----------+</span></span><br><span class="line"><span class="comment">// |  0|Masters|School of Informa...|UC Berkeley|</span></span><br><span class="line"><span class="comment">// |  1|  Ph.D.|                EECS|UC Berkeley|</span></span><br><span class="line"><span class="comment">// +---+-------+--------------------+-----------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 세미조인은 =&gt; 값이 존재하면, 중복 키(id=0) 가 존재해도 로우 포함</span></span><br><span class="line"><span class="keyword">val</span> gradProgram2 = graduateProgram.union(<span class="type">Seq</span>(</span><br><span class="line">    (<span class="number">0</span>, <span class="string">&quot;Masters&quot;</span>, <span class="string">&quot;Duplicated Row&quot;</span>, <span class="string">&quot;Duplicated School&quot;</span>)).toDF())</span><br><span class="line">gradProgram2.createOrReplaceTempView(<span class="string">&quot;gradProgram2&quot;</span>)</span><br><span class="line"></span><br><span class="line">gradProgram2.join(person, joinExpression, joinType).show()</span><br><span class="line"><span class="comment">// +---+-------+--------------------+-----------------+</span></span><br><span class="line"><span class="comment">// | id| degree|          department|           school|</span></span><br><span class="line"><span class="comment">// +---+-------+--------------------+-----------------+</span></span><br><span class="line"><span class="comment">// |  0|Masters|School of Informa...|      UC Berkeley|</span></span><br><span class="line"><span class="comment">// |  1|  Ph.D.|                EECS|      UC Berkeley|</span></span><br><span class="line"><span class="comment">// |  0|Masters|      Duplicated Row|Duplicated School|</span></span><br><span class="line"><span class="comment">// +---+-------+--------------------+-----------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 8. 왼쪽 안티 조인</span></span><br><span class="line"><span class="comment">// == LEFT ANTI JOIN</span></span><br><span class="line">joinType = <span class="string">&quot;left_anti&quot;</span></span><br><span class="line">graduateProgram.join(person, joinExpression, joinType).show()</span><br><span class="line"><span class="comment">// +---+-------+----------+-----------+</span></span><br><span class="line"><span class="comment">// | id| degree|department|     school|</span></span><br><span class="line"><span class="comment">// +---+-------+----------+-----------+</span></span><br><span class="line"><span class="comment">// |  2|Masters|      EECS|UC Berkeley|</span></span><br><span class="line"><span class="comment">// +---+-------+----------+-----------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 9. 자연 조인</span></span><br><span class="line"><span class="comment">// == NATURAL JOIN</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 10. 교차 조인</span></span><br><span class="line"><span class="comment">// == CROSS JOIN</span></span><br><span class="line">joinType = <span class="string">&quot;cross&quot;</span></span><br><span class="line">graduateProgram.join(person, joinExpression, joinType).show()</span><br><span class="line"><span class="comment">// +---+-------+--------------------+-----------+---+----------------+----------------+---------------+</span></span><br><span class="line"><span class="comment">// | id| degree|          department|     school| id|            name|graduate_program|   spark_status|</span></span><br><span class="line"><span class="comment">// +---+-------+--------------------+-----------+---+----------------+----------------+---------------+</span></span><br><span class="line"><span class="comment">// |  0|Masters|School of Informa...|UC Berkeley|  0|   Bill Chambers|               0|          [100]|</span></span><br><span class="line"><span class="comment">// |  1|  Ph.D.|                EECS|UC Berkeley|  2|Michael Armbrust|               1|     [250, 100]|</span></span><br><span class="line"><span class="comment">// |  1|  Ph.D.|                EECS|UC Berkeley|  1|   Matei Zaharia|               1|[500, 250, 100]|</span></span><br><span class="line"><span class="comment">// +---+-------+--------------------+-----------+---+----------------+----------------+---------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 교차 조인은 =&gt; 명시적메서드 호출도 가능 (하단 예제는 키워드 없이 전체 교차 호출)</span></span><br><span class="line">person.crossJoin(graduateProgram).show()</span><br><span class="line"><span class="comment">// +---+----------------+----------------+---------------+---+-------+--------------------+-----------+</span></span><br><span class="line"><span class="comment">// | id|            name|graduate_program|   spark_status| id| degree|          department|     school|</span></span><br><span class="line"><span class="comment">// +---+----------------+----------------+---------------+---+-------+--------------------+-----------+</span></span><br><span class="line"><span class="comment">// |  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|</span></span><br><span class="line"><span class="comment">// |  1|   Matei Zaharia|               1|[500, 250, 100]|  0|Masters|School of Informa...|UC Berkeley|</span></span><br><span class="line"><span class="comment">// |  2|Michael Armbrust|               1|     [250, 100]|  0|Masters|School of Informa...|UC Berkeley|</span></span><br><span class="line"><span class="comment">// |  0|   Bill Chambers|               0|          [100]|  2|Masters|                EECS|UC Berkeley|</span></span><br><span class="line"><span class="comment">// |  1|   Matei Zaharia|               1|[500, 250, 100]|  2|Masters|                EECS|UC Berkeley|</span></span><br><span class="line"><span class="comment">// |  2|Michael Armbrust|               1|     [250, 100]|  2|Masters|                EECS|UC Berkeley|</span></span><br><span class="line"><span class="comment">// |  0|   Bill Chambers|               0|          [100]|  1|  Ph.D.|                EECS|UC Berkeley|</span></span><br><span class="line"><span class="comment">// |  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|</span></span><br><span class="line"><span class="comment">// |  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|</span></span><br><span class="line"><span class="comment">// +---+----------------+----------------+---------------+---+-------+--------------------+-----------+</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL</span></span><br><span class="line"><span class="comment">-- 3. 내부 조인</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> person <span class="keyword">JOIN</span> graduateProgram</span><br><span class="line">  <span class="keyword">ON</span> person.graduate_program <span class="operator">=</span> graduateProgram.id</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> person <span class="keyword">INNER</span> <span class="keyword">JOIN</span> graduateProgram</span><br><span class="line">  <span class="keyword">ON</span> person.graduate_program <span class="operator">=</span> graduateProgram.id</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 4. 외부 조인</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> person <span class="keyword">FULL</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> graduateProgram</span><br><span class="line">  <span class="keyword">ON</span> graduate_program <span class="operator">=</span> graduateProgram.id</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 5. 왼쪽 외부 조인</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> graduateProgram <span class="keyword">LEFT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> person</span><br><span class="line">  <span class="keyword">ON</span> person.graduate_program <span class="operator">=</span> graduateProgram.id</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 6. 오른쪽 외부 조인</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> person <span class="keyword">RIGHT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> graduateProgram</span><br><span class="line">  <span class="keyword">ON</span> person.graduate_program <span class="operator">=</span> graduateProgram.id</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 7. 왼쪽 세미 조인</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> gradProgram2 <span class="keyword">LEFT</span> SEMI <span class="keyword">JOIN</span> person</span><br><span class="line">  <span class="keyword">ON</span> gradProgram2.id <span class="operator">=</span> person.graduate_program</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 8. 왼쪽 안티 조인</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> graduateProgram <span class="keyword">LEFT</span> ANTI <span class="keyword">JOIN</span> person</span><br><span class="line">  <span class="keyword">ON</span> graduateProgram.id <span class="operator">=</span> person.graduate_program</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 9. 자연 조인</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> graduateProgram <span class="keyword">NATURAL</span> <span class="keyword">JOIN</span> person</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 10. 교차 조인</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> graduateProgram <span class="keyword">CROSS</span> <span class="keyword">JOIN</span> person</span><br><span class="line">  <span class="keyword">ON</span> graduateProgram.id <span class="operator">=</span> person.graduate_program</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> graduateProgram <span class="keyword">CROSS</span> <span class="keyword">JOIN</span> person</span><br></pre></td></tr></table></figure></details><h3 id="8-3-내부-조인"><a href="#8-3-내부-조인" class="headerlink" title="8.3 내부 조인"></a>8.3 내부 조인</h3><details><summary class="point-color-can-hover">[8.3] 내부 조인 (기본 조인) 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 내부 조인은 기본 조인 방식</span></span><br><span class="line"><span class="keyword">val</span> joinExpression = person.col(<span class="string">&quot;graduate_program&quot;</span>) === graduateProgram.col(<span class="string">&quot;id&quot;</span>)</span><br><span class="line"></span><br><span class="line">person.join(graduateProgram, joinExpression).show()</span><br><span class="line"><span class="comment">// +---+----------------+----------------+---------------+---+-------+--------------------+-----------+</span></span><br><span class="line"><span class="comment">// | id|            name|graduate_program|   spark_status| id| degree|          department|     school|</span></span><br><span class="line"><span class="comment">// +---+----------------+----------------+---------------+---+-------+--------------------+-----------+</span></span><br><span class="line"><span class="comment">// |  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|</span></span><br><span class="line"><span class="comment">// |  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|</span></span><br><span class="line"><span class="comment">// |  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|</span></span><br><span class="line"><span class="comment">// +---+----------------+----------------+---------------+---+-------+--------------------+-----------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// wrongJoinExpression (두 DF 모두에 키가 존재하지않으면 =&gt; 빈 결과 DataFrame 반환)</span></span><br><span class="line"><span class="keyword">val</span> wrongJoinExpression = person.col(<span class="string">&quot;name&quot;</span>) === graduateProgram.col(<span class="string">&quot;school&quot;</span>)</span><br><span class="line">person.join(graduateProgram, wrongJoinExpression).show()</span><br><span class="line"><span class="comment">// +---+----+----------------+------------+---+------+----------+------+</span></span><br><span class="line"><span class="comment">// | id|name|graduate_program|spark_status| id|degree|department|school|</span></span><br><span class="line"><span class="comment">// +---+----+----------------+------------+---+------+----------+------+</span></span><br><span class="line"><span class="comment">// +---+----+----------------+------------+---+------+----------+------+</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> person <span class="keyword">JOIN</span> graduateProgram</span><br><span class="line">  <span class="keyword">ON</span> person.graduate_program <span class="operator">=</span> graduateProgram.id</span><br></pre></td></tr></table></figure></details><ul><li><p>내부 조인</p><ul><li>DataFrame이나 테이블에 존재하는 키 평가, <code>True</code> 로 평가되는 로우만 결합</li><li>둘 다 키가 존재하지 않으면? =&gt; 빈 값</li><li><strong>기본 조인 방식</strong><ul><li>ex. <code>왼DF.join(오DF, 왼DF.col(&#39;A&#39;) === 오DF.col(&#39;B&#39;), inner)</code></li></ul></li><li>joinType (Optional) 으로 조인 타입 명확하게 지정도 가능</li></ul></li><li><p><code>join(DF, joinExpress, (joinType))</code></p></li></ul><h3 id="8-4-외부-조인"><a href="#8-4-외부-조인" class="headerlink" title="8.4 외부 조인"></a>8.4 외부 조인</h3><ul><li>외부 조인<ul><li>DataFrame이나 테이블에 존재하는 키를 평가, 평과 결과(<code>True</code>, <code>False</code>) 로우를 포함하여 조인</li><li>둘 다 일치하는 로우가 없으면? =&gt; null 삽입</li></ul></li></ul><h3 id="8-5-왼쪽-외부-조인"><a href="#8-5-왼쪽-외부-조인" class="headerlink" title="8.5 왼쪽 외부 조인"></a>8.5 왼쪽 외부 조인</h3><ul><li>왼쪽 외부 조인<ul><li>DataFrame이나 테이블에 존재하는 키를 평가, <u>왼쪽 DataFrame의 모든 로우</u>와 <u>왼쪽과 일치하는 오른쪽 DataFrame의 로우</u> 포함</li><li>오른쪽에 일치 로우 없으면? =&gt; null 삽입</li></ul></li></ul><h3 id="8-6-오른쪽-외부-조인"><a href="#8-6-오른쪽-외부-조인" class="headerlink" title="8.6 오른쪽 외부 조인"></a>8.6 오른쪽 외부 조인</h3><ul><li>오른쪽 외부 조인<ul><li>DataFrame이나 테이블에 존재하는 키를 평가, <u>오른쪽 DataFrame의 모든 로우</u>와 <u>오른쪽과 일치하는 왼쪽 DataFrame의 로우</u> 포함</li><li>왼쪽 일치 로우 없으면? =&gt; null 삽입</li></ul></li><li>왼쪽 외부 조인 반대지 뭐..</li></ul><h3 id="8-7-왼쪽-세미-조인"><a href="#8-7-왼쪽-세미-조인" class="headerlink" title="8.7 왼쪽 세미 조인"></a>8.7 왼쪽 세미 조인</h3><ul><li>왼쪽 세미 조인<ul><li>오른쪽 DataFrame의 어떤 값도 <strong>포함 X</strong> (단지 값이 존재하는지 확인하는 용도)</li><li>오른쪽에 값이 존재하면 =&gt; 왼쪽에 중복 키가 존재하더라도 해당 로우는 결과에 포함</li></ul></li><li>기존 조인 기능보다는 달리 <strong>DataFrame 필터</strong> 에 가깝다</li></ul><h3 id="8-8-왼쪽-안티-조인"><a href="#8-8-왼쪽-안티-조인" class="headerlink" title="8.8 왼쪽 안티 조인"></a>8.8 왼쪽 안티 조인</h3><ul><li>왼쪽 안티 조인 &lt;-&gt; 왼쪽 세미 조인 <ul><li>오른쪽 DataFrame의 어떤 값도 <strong>포함 X</strong> (== 세미)</li><li>단, 오른쪽 값이 존재하면 유지하는 대신 =&gt; 오른쪽에서 <strong>관련된 키가 없는 로우</strong>만 결과에 포함</li><li>(== SQL 의 <code>NOT IN</code>)</li></ul></li></ul><h3 id="8-9-자연-조인"><a href="#8-9-자연-조인" class="headerlink" title="8.9 자연 조인"></a>8.9 자연 조인</h3><ul><li>자연 조인<ul><li>조인하려는 컬럼을 암시적 추정 (=&gt; 일치하는 컬럼을 찾고 결과를 반환)</li><li>왼쪽, 오른쪽, 외부 자연 조인 사용 가능</li></ul></li><li>하지만 암시적 조인은 위험. 조심해서 사용할 것</li></ul><h3 id="8-10-교차-조인-카테시안-조인"><a href="#8-10-교차-조인-카테시안-조인" class="headerlink" title="8.10 교차 조인(카테시안 조인)"></a>8.10 교차 조인(카테시안 조인)</h3><ul><li>교차 조인<ul><li>조건절을 기술하지 않은 내부조인</li><li>왼쪽 모든 로우 X 오른쪽 모든 로우 결합</li></ul></li><li>교차 조인도 매우 위험. (키워드 미사용 시 왼쪽X오른쪽 로우 수 만큼 생성)<ul><li>반드시 필요한 경우에, <strong>명시적으로 정의</strong> 할 것</li><li>(참고) <code>spark.sql.crossJoin.enable = true</code> : 교차 조인 시 발생 경고 제거. 스파크가 교차조인을 다른 조인으로 처리하지 않도록 강제</li></ul></li></ul><h3 id="8-11-조인-사용-시-문제점"><a href="#8-11-조인-사용-시-문제점" class="headerlink" title="8.11 조인 사용 시 문제점"></a>8.11 조인 사용 시 문제점</h3><ul><li><p>[문제 1] 복합 데이터 타입의 조인</p><ul><li><p>불리언을 반환하는 모든 표현식 =&gt; 조인 표현식으로 사용 O</p><details><summary class="point-color-can-hover">[8.11.1] '복합 데이터 타입의 조인' 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.expr</span><br><span class="line"></span><br><span class="line">(person.withColumnRenamed(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;personId&quot;</span>)</span><br><span class="line">  .join(sparkStatus, expr(<span class="string">&quot;array_contains(spark_status, id)&quot;</span>)).show())</span><br><span class="line"><span class="comment">// +--------+----------------+----------------+---------------+---+--------------+</span></span><br><span class="line"><span class="comment">// |personId|            name|graduate_program|   spark_status| id|        status|</span></span><br><span class="line"><span class="comment">// +--------+----------------+----------------+---------------+---+--------------+</span></span><br><span class="line"><span class="comment">// |       0|   Bill Chambers|               0|          [100]|100|   Contributor|</span></span><br><span class="line"><span class="comment">// |       1|   Matei Zaharia|               1|[500, 250, 100]|500|Vice President|</span></span><br><span class="line"><span class="comment">// |       1|   Matei Zaharia|               1|[500, 250, 100]|250|    PMC Member|</span></span><br><span class="line"><span class="comment">// |       1|   Matei Zaharia|               1|[500, 250, 100]|100|   Contributor|</span></span><br><span class="line"><span class="comment">// |       2|Michael Armbrust|               1|     [250, 100]|250|    PMC Member|</span></span><br><span class="line"><span class="comment">// |       2|Michael Armbrust|               1|     [250, 100]|100|   Contributor|</span></span><br><span class="line"><span class="comment">// +--------+----------------+----------------+---------------+---+--------------+</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span></span><br><span class="line">  (<span class="keyword">select</span> id <span class="keyword">as</span> personId, name, graduate_program, spark_status <span class="keyword">FROM</span> person)</span><br><span class="line">  <span class="keyword">INNER</span> <span class="keyword">JOIN</span> sparkStatus <span class="keyword">ON</span> array_contains(spark_status, id)</span><br></pre></td></tr></table></figure></details></li></ul></li><li><p>[문제 2] 중복 컬럼명 처리</p><ul><li><p>결과 DataFrame에서 중복된 컬럼명 다루기</p><ul><li>각 컬럼은 스파크 SQL 엔진(카탈리스트) 내에 고유 ID 존재 (직접 참조는 불가)</li><li>중복 컬럼명이 존재하는 DataFrame 사용 시 특정 컬럼 참조가 어려움</li></ul></li><li><p>문제 발생 상황</p><ul><li>조인에 사용할 DataFrame의 특정 키가 동일한 이름을 가지며, 키가 제거 되지않도록 조인 표현식에 명시한 경우</li><li>조인 대상이 아닌 두 개의 컬럼이 동일 이름을 가진 경우</li></ul></li><li><p>Solution 1) 다른 조인 표현식 사용</p><ul><li><del>불리언 형태 조인 표현식</del> =&gt; 문자열이나 시퀀스 형태로 변경</li><li>조인할 때 두 컬럼 중 하나가 자동 제거됨</li></ul></li><li><p>Solution 2) 조인 후 컬럼 제거</p><ul><li>조인 시 동일한 키 이름 사용하거나 원본 DataFrame에 동일한 컬럼명 존재 시 =&gt; 원본 DataFrame을 사용해 컬럼을 참조</li><li>스파크의 SQL 분석 프로세스 특성 활용 - “명시적으로 참조된 컬럼은 검증 필요 X. 스파크 분석 단계 패스”</li><li><code>col()</code> 메서드로 컬럼 고유 ID로 해당 컬럼을 암시적 지정 가능 (예제 참고, <del><code>column()</code></del> X)</li></ul></li><li><p>Solution 3) 조인 전 컬럼명 변경</p><details><summary class="point-color-can-hover">[8.11.2] '중복 컬럼명 처리' 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 잘못된 데이터셋 생성 (두 개의 graduate_program 컬럼 존재)</span></span><br><span class="line"><span class="keyword">val</span> gradProgramDupe = graduateProgram.withColumnRenamed(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;graduate_program&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> joinExpr = gradProgramDupe.col(<span class="string">&quot;graduate_program&quot;</span>) === person.col(</span><br><span class="line">  <span class="string">&quot;graduate_program&quot;</span>)</span><br><span class="line"></span><br><span class="line">person.join(gradProgramDupe, joinExpr).show()</span><br><span class="line"><span class="comment">// +---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+</span></span><br><span class="line"><span class="comment">// | id|            name|graduate_program|   spark_status|graduate_program| degree|          department|     school|</span></span><br><span class="line"><span class="comment">// +---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+</span></span><br><span class="line"><span class="comment">// |  0|   Bill Chambers|               0|          [100]|               0|Masters|School of Informa...|UC Berkeley|</span></span><br><span class="line"><span class="comment">// |  2|Michael Armbrust|               1|     [250, 100]|               1|  Ph.D.|                EECS|UC Berkeley|</span></span><br><span class="line"><span class="comment">// |  1|   Matei Zaharia|               1|[500, 250, 100]|               1|  Ph.D.|                EECS|UC Berkeley|</span></span><br><span class="line"><span class="comment">// +---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 중복된 컬럼 중 하나를 참조하면 에러 발생 (&quot;Reference &#x27;graduate_program&#x27; is ambiguous ..&quot;)</span></span><br><span class="line">person.join(gradProgramDupe, joinExpr).select(<span class="string">&quot;graduate_program&quot;</span>).show()</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// * 중복 컬럼명 처리 Solution *</span></span><br><span class="line"><span class="comment">// Solution 1) 다른 조인 표현식 사용</span></span><br><span class="line">person.join(gradProgramDupe,<span class="string">&quot;graduate_program&quot;</span>).select(<span class="string">&quot;graduate_program&quot;</span>).show()</span><br><span class="line"><span class="comment">// +----------------+</span></span><br><span class="line"><span class="comment">// |graduate_program|</span></span><br><span class="line"><span class="comment">// +----------------+</span></span><br><span class="line"><span class="comment">// |               0|</span></span><br><span class="line"><span class="comment">// |               1|</span></span><br><span class="line"><span class="comment">// |               1|</span></span><br><span class="line"><span class="comment">// +----------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// Solution 2) 조인 후 컬럼 제거</span></span><br><span class="line">(person.join(gradProgramDupe, joinExpr).drop(person.col(<span class="string">&quot;graduate_program&quot;</span>))</span><br><span class="line">  .select(<span class="string">&quot;graduate_program&quot;</span>).show())</span><br><span class="line"><span class="comment">// +----------------+</span></span><br><span class="line"><span class="comment">// |graduate_program|</span></span><br><span class="line"><span class="comment">// +----------------+</span></span><br><span class="line"><span class="comment">// |               0|</span></span><br><span class="line"><span class="comment">// |               1|</span></span><br><span class="line"><span class="comment">// |               1|</span></span><br><span class="line"><span class="comment">// +----------------+</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> joinExpr = person.col(<span class="string">&quot;graduate_program&quot;</span>) === graduateProgram.col(<span class="string">&quot;id&quot;</span>)</span><br><span class="line">person.join(graduateProgram, joinExpr).drop(graduateProgram.col(<span class="string">&quot;id&quot;</span>)).show()</span><br><span class="line"><span class="comment">// +---+----------------+----------------+---------------+-------+--------------------+-----------+</span></span><br><span class="line"><span class="comment">// | id|            name|graduate_program|   spark_status| degree|          department|     school|</span></span><br><span class="line"><span class="comment">// +---+----------------+----------------+---------------+-------+--------------------+-----------+</span></span><br><span class="line"><span class="comment">// |  0|   Bill Chambers|               0|          [100]|Masters|School of Informa...|UC Berkeley|</span></span><br><span class="line"><span class="comment">// |  2|Michael Armbrust|               1|     [250, 100]|  Ph.D.|                EECS|UC Berkeley|</span></span><br><span class="line"><span class="comment">// |  1|   Matei Zaharia|               1|[500, 250, 100]|  Ph.D.|                EECS|UC Berkeley|</span></span><br><span class="line"><span class="comment">// +---+----------------+----------------+---------------+-------+--------------------+-----------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// Solution 3) 조인 전 컬럼명 변경</span></span><br><span class="line"><span class="keyword">val</span> gradProgram3 = graduateProgram.withColumnRenamed(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;grad_id&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> joinExpr = person.col(<span class="string">&quot;graduate_program&quot;</span>) === gradProgram3.col(<span class="string">&quot;grad_id&quot;</span>)</span><br><span class="line">person.join(gradProgram3, joinExpr).show()</span><br><span class="line"><span class="comment">// +---+----------------+----------------+---------------+-------+-------+--------------------+-----------+</span></span><br><span class="line"><span class="comment">// | id|            name|graduate_program|   spark_status|grad_id| degree|          department|     school|</span></span><br><span class="line"><span class="comment">// +---+----------------+----------------+---------------+-------+-------+--------------------+-----------+</span></span><br><span class="line"><span class="comment">// |  0|   Bill Chambers|               0|          [100]|      0|Masters|School of Informa...|UC Berkeley|</span></span><br><span class="line"><span class="comment">// |  2|Michael Armbrust|               1|     [250, 100]|      1|  Ph.D.|                EECS|UC Berkeley|</span></span><br><span class="line"><span class="comment">// |  1|   Matei Zaharia|               1|[500, 250, 100]|      1|  Ph.D.|                EECS|UC Berkeley|</span></span><br><span class="line"><span class="comment">// +---+----------------+----------------+---------------+-------+-------+--------------------+-----------+</span></span><br></pre></td></tr></table></figure></details></li></ul></li></ul><h3 id="8-12-스파크의-조인-수행-방식"><a href="#8-12-스파크의-조인-수행-방식" class="headerlink" title="8.12 스파크의 조인 수행 방식"></a>8.12 스파크의 조인 수행 방식</h3><ul><li><p>두 가지 핵심 (내부) 전략</p><ul><li>노드간 네트워크 통신 전략</li><li>노드별 연산 전략</li></ul></li><li><p>스파크 조인 수행 방식 이해하면 뭐가 좋은데?</p><ul><li>빠르게 완료되는 작업 vs 절대 완료되지 않는 작업간의 차이 이해 가능</li></ul></li><li><p><strong>네트워크 통신 전략</strong></p><ul><li>스파크는 조인 시 두 가지 클러스터 통신방식 활용<ul><li><strong>셔플 조인(shuffle join)</strong> =&gt; 전체 노드간 통신 유발</li><li><strong>브로드캐스트 조인(broadcast join)</strong> =&gt; 필요없음</li></ul></li><li>사실 이런 내부 최적화 기술은 CBO 개선 후 더 나은 통신 전략이 생기면 얼마든지 바뀔 수 있고 …</li><li>따라서 일반적 상황에서 정확히 어떤일이 일어나는지 고수준 예제로 이해해보자</li><li>예제 설정은 이해를 돕기위한 데이터 크기가 극단적인 상황 가정 (아주 크거나, 아주 작거나)</li></ul></li><li><p>(1) 큰 테이블 + 큰테이블 조인</p><ul><li><strong>셔플 조인</strong> 발생 (전체 노드간 통신)<ul><li>조인에 사용한 특정 키나 키 집합을 어떤 노드가 가졌는 지에 따라 해당 노드와 데이터 공유</li><li>=&gt; 즉 높은 네트워크 비용, 많은 자원 사용</li><li><a style="color:lightgray">(IoT 예제는 무슨 소린지 약간 애매..)</a></li></ul></li><li><em>“즉, <strong>전체</strong> 조인 프로세스가 진행되는 동안 (데이터 파티셔닝 없이) 모든 워커 노드(그리고 모든 파티션)에서 통신이 발생함을 의미”</em></li></ul></li><li><p>(2) 큰테이블 + 작은 테이블 조인</p><ul><li><p>테이블이 단일 워커 노드의 메모리에서 감당할정도로 작으면 조인 연산 최적화 가능</p></li><li><p><strong>브로드캐스트 조인</strong> 이 훨씬 효율적</p><ul><li>작은 DataFrame을 클러스터의 전체 워커 노드에 복제</li><li>자원을 많이 쓰는거 같아도, 프로세스 중 전체 노드가 통신하는 현상 방지 &amp; 다른 워커 기다림없이 작업 수행 가능</li><li>=&gt; 즉 대규모 통신은 발생하지만 노드 간 추가적인 통신 발생 X</li></ul></li><li><p>따라서 모든 단일 노드에서 개별적 조인 수행</p><ul><li>CPU가 가장 큰 병목 구간</li></ul></li><li><p>브로드 캐스트 사용</p><ul><li>DataFrame API : <code>broadcast(작은DF)</code> 로 사용 힌트 제공 가능 =&gt; but 항상 동일한 실행계획은 아님</li><li>SQL : <code>/*_ MAPJOIN() */</code> 힌트 제공 가능 (=<code>MAPJOIN</code> = <code>BROADCAST</code> = <code>BROADCASTJOIN</code>) =&gt; but <strong>강제성 X</strong> </li></ul><details><summary class="point-color-can-hover">[8.12] (2) '브로드캐스트 조인' 확인 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// [큰테이블 - 작은 테이블 조인]</span></span><br><span class="line"><span class="comment">// =&gt; 스파크가 자동으로 데이터셋을 브로드캐스트 조인으로 설정</span></span><br><span class="line"><span class="keyword">val</span> joinExpr = person.col(<span class="string">&quot;graduate_program&quot;</span>) === graduateProgram.col(<span class="string">&quot;id&quot;</span>)</span><br><span class="line"></span><br><span class="line">person.join(graduateProgram, joinExpr).explain()</span><br><span class="line"><span class="comment">// joinExpr: org.apache.spark.sql.Column = (graduate_program = id)</span></span><br><span class="line"><span class="comment">// == Physical Plan ==</span></span><br><span class="line"><span class="comment">// *(1) BroadcastHashJoin [graduate_program#11], [id#26], Inner, BuildLeft</span></span><br><span class="line"><span class="comment">// :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[2, int, false] as bigint)))</span></span><br><span class="line"><span class="comment">// :  +- LocalTableScan [id#9, name#10, graduate_program#11, spark_status#12]</span></span><br><span class="line"><span class="comment">// +- LocalTableScan [id#26, degree#27, department#28, school#29]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// DataFrame API =&gt; 옵티마이저에게 브로드캐스트 조인하도록 힌트 전달 가능</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.broadcast</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> joinExpr = person.col(<span class="string">&quot;graduate_program&quot;</span>) === graduateProgram.col(<span class="string">&quot;id&quot;</span>)</span><br><span class="line">person.join(broadcast(graduateProgram), joinExpr).explain()</span><br><span class="line"><span class="comment">// joinExpr: org.apache.spark.sql.Column = (graduate_program = id)</span></span><br><span class="line"><span class="comment">// == Physical Plan ==</span></span><br><span class="line"><span class="comment">// *(1) BroadcastHashJoin [graduate_program#11], [id#26], Inner, BuildRight</span></span><br><span class="line"><span class="comment">// :- LocalTableScan [id#9, name#10, graduate_program#11, spark_status#12]</span></span><br><span class="line"><span class="comment">// +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)))</span></span><br><span class="line"><span class="comment">//    +- LocalTableScan [id#26, degree#27, department#28, school#29]</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL : 조인 수행 힌트 주기 (강제성 X) </span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="comment">/*+ MAPJOIN(graduateProgram) */</span> <span class="operator">*</span> <span class="keyword">FROM</span> person <span class="keyword">JOIN</span> graduateProgram</span><br><span class="line">  <span class="keyword">ON</span> person.graduate_program <span class="operator">=</span> graduateProgram.id</span><br></pre></td></tr></table></figure></details></li><li><p>단점</p><ul><li>큰 데이터 브로드 캐스트 시 고비용 수집 연산으로 드라이버 노드 비정상적 종료 가능성</li></ul></li></ul></li><li><p>(3) 아주 작은 테이블 사이 조인</p><ul><li>스파크가 알아서 하게 두는게 BEST</li><li>필요하면 브로드캐스트 조인 강제 지정 가능하긴 한데..</li></ul></li></ul><h3 id="8-13-정리"><a href="#8-13-정리" class="headerlink" title="8.13 정리"></a>8.13 정리</h3><ul><li>이번엔 정리보다는 팁 느낌…</li><li>조인 전 데이터를 적절히 분할 시 셔플이 계획되어있어도 동일 머신에 두 DataFrame 의 데이터가 있을 수 있다.<ul><li>=&gt; 셔플 피하고 훨씬 효율적 조인 가능</li><li>따라서 일부 데이터를 실험용으로 사전 분할 해 조인 수행 시 성능 향상되는지 확인 해볼 것</li></ul></li><li>데이터소스는 조인 순서를 결정하는데 부가적 영향 미칠 수 있다. =&gt; 다음 챕터 참고</li><li>일부 조인은 필터 임무를 수행하므로, 네트워크 교환 데이터를 줄여 워크로드 성능 향상 쉽게 할 수 있다.</li></ul><h3 id="📒-단어장"><a href="#📒-단어장" class="headerlink" title="📒 단어장"></a>📒 단어장</h3><ul><li>비용 기반 옵티마이저 (cost-based optimizer, CBO)</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;br/&gt;

&lt;img src=&quot;https://user-images.githubusercontent.com/26691216/108165034-3ae19600-7135-11eb-8b40-8175f455337b.gif&quot; width=400/&gt;
&lt;center&gt;</summary>
      
    
    
    
    <category term="spark" scheme="https://minsw.github.io/categories/spark/"/>
    
    
    <category term="book" scheme="https://minsw.github.io/tags/book/"/>
    
    <category term="study" scheme="https://minsw.github.io/tags/study/"/>
    
    <category term="spark" scheme="https://minsw.github.io/tags/spark/"/>
    
    <category term="apache" scheme="https://minsw.github.io/tags/apache/"/>
    
  </entry>
  
  <entry>
    <title>&amp;#039;Spark The Definitive Guide&amp;#039; 7장 - 집계해라 애송이</title>
    <link href="https://minsw.github.io/2021/02/03/Spark-The-Definitive-Guide-7%EC%9E%A5/"/>
    <id>https://minsw.github.io/2021/02/03/Spark-The-Definitive-Guide-7%EC%9E%A5/</id>
    <published>2021-02-03T12:42:12.000Z</published>
    <updated>2021-02-04T08:51:49.250Z</updated>
    
    <content type="html"><![CDATA[<br/><p style="color:lightgray">벌써부터 슬슬 포스트 포멧 헷갈리기 시작하죠? 망했죠?<br/>나중에 한번에 맞춰야지 생각해놓고 절대 수정안하죠? ㅎ..</p><br/><img width="300" alt="counting" src="https://user-images.githubusercontent.com/26691216/106792995-3b405280-669a-11eb-9f64-fc4d576200cd.gif"><center><h2>_ _ _</h2></center><br/><hr><h1 id="CHAPTER-7-집계-연산"><a href="#CHAPTER-7-집계-연산" class="headerlink" title="CHAPTER 7 집계 연산"></a>CHAPTER 7 집계 연산</h1><p><em>집계(aggregation)은 무언갈 함께 모으는 행위이며 빅데이터 분석의 초석이다.</em></p><p>스파크는 모든 데이터 타입 다루는 것 + 다음과 같은 <strong>그룹화 데이터 타입</strong> 생성 가능하고,<br>지정된 집계 함수에 따라 그룹화된 결과는 RelationalGroupedDataset 을 반환.</p><blockquote><ul><li>select 구문에서 집계 수행, DataFrame 전체 데이터 요약 (가장 간단한 그룹화)</li><li>‘group by’ : 하나 이상의 키 지정. 다른 집계 함수 사용해서 값을 가진 컬럼 변환 가능</li><li>‘윈도우(window)’ : 하나 이상의 키 지정. 다른 집계 함수로 컬럼 변환 가능. + 단, 함수 입력으로 사용할 로우는 현재 로우와 <strong>연관성</strong> 있어야 함</li><li>‘그룹화 셋(grouping set)’ : 서로 다른 레벨 값 집계 (SQL, DataFrame의 롤업, 큐브)</li><li>‘롤업(rollup)’ : 하나 이상의 키 지정. 다른 집계 함수로 컬럼 변환 가능. + <strong>계층적으로 요약된 값</strong> 추출</li><li>‘큐브(cube)’ : 하나 이상의 키 지정. 다른 집계 함수로 컬럼 변환 가능. + <strong>모든 컬럼 조합에 대한 요약 값</strong> 계산</li></ul><p><i style="color:lightgray">(=&gt; 사실상 7장 요약)</i> </p></blockquote><p>📌 중요한 건, <strong>어떤 결과를 만들지</strong> 정확히 파악해야 한다는 것.<br>(정확한 답 계산 = 높은 비용 요구 → 빅데이터의 경우 근사치가 효율적일 수 있음)</p><!-- count 예제.. (데이터셋 크기 출력 및 캐싱용도)이질적으로 느껴질 수 있음. 왜나면...- 함수가 아닌 메서드 형태- 트랜스포메이션같은 지연 연산이아닌 즉시 연산(액션)--><h3 id="7-1-집계-함수"><a href="#7-1-집계-함수" class="headerlink" title="7.1 집계 함수"></a>7.1 집계 함수</h3><ul><li>모든 집계는 특별한 경우를 제외하고는 <strong>함수</strong> 사용<ul><li>=&gt; <strong>집계 함수</strong> (<a href="https://bit.ly/2tRMYus">org.apache.spark.sql.functions</a> 패키지)</li><li>예외) DataFrame의 .stat 속성 이용 (6장 참고)</li><li>스칼라, 파이썬에서 임포트 할 수 있는 함수와 SQL에서 사용가능한 함수는 약간 다름 (매 릴리즈마다 조금씩 변함)</li></ul></li></ul><h4 id="집계-함수"><a href="#집계-함수" class="headerlink" title="집계 함수"></a>집계 함수</h4><blockquote><p>집계 함수 : 키나 그룹을 지정하고 + 하나 이상의 컬럼을 변환하는 방법을 지정 (여러 입력값이 주어지면 그룹 별로 결과 생성)</p><ul><li>수치형 데이터 요약 (ex. 그룹의 평균값 구하기)</li><li>합산, 곱셈, 카운팅 등의 작업</li><li>복합 데이터 타입(배열, 리스트, 맵)을 사용한 집계 수행 가능 </li></ul></blockquote><ul><li><p><code>count(컬럼명)</code> : 전체 로우 수 카운트</p><ul><li><p>액션이 아닌 <strong>트랜스포메이션</strong>으로 동작</p></li><li><p>두가지 방식으로 사용 가능</p><ul><li><code>count(특정 컬럼)</code> : null 값 포함 X</li><li><code>count(*)</code> or <code>count(1)</code> : null 값 가진 로우 포함해서 카운트</li></ul></li><li><details><summary class="only-hover">[7.1.1] 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.count</span><br><span class="line">df.select(count(<span class="string">&quot;StockCode&quot;</span>)).show() <span class="comment">// 541909</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">COUNT</span>(<span class="operator">*</span>) <span class="keyword">FROM</span> dfTable</span><br></pre></td></tr></table></figure></details></li></ul></li><li><p><code>countDistinct(컬럼명)</code> : 고유 (distinct) 레코드 수 카운트</p><ul><li><details><summary class="only-hover">[7.1.2] 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.countDistinct</span><br><span class="line">df.select(countDistinct(<span class="string">&quot;StockCode&quot;</span>)).show() <span class="comment">// 4070</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> <span class="operator">*</span>) <span class="keyword">FROM</span> DFTABLE</span><br></pre></td></tr></table></figure></details></li></ul></li><li><p><code>approx_count_distinct(컬럼명, 최대추정오류율)</code> : 근사치 고유 레코드 수 카운트</p><ul><li><p>대규모 데이터셋 다룰 시 정확한 개수 무의미함 =&gt; 근사치로 효율</p></li><li><p><code>최대 추정 오류율 (maximum estimation error)</code> 파라미터 설정</p></li><li><details><summary class="only-hover">[7.1.3] 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.approx_count_distinct</span><br><span class="line">df.select(approx_count_distinct(<span class="string">&quot;StockCode&quot;</span>, <span class="number">0.1</span>)).show() <span class="comment">// 3364</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> approx_count_distinct(StockCode, <span class="number">0.1</span>) <span class="keyword">FROM</span> DFTABLE</span><br></pre></td></tr></table></figure></details></li></ul></li></ul><ul><li><p><code>first(컬럼명)</code>, <code>last(컬럼명)</code> : 첫 번째 값, 마지막 값 추출</p><ul><li><p>DataFrame 값이 아닌 로우 기반 동작</p></li><li><details><summary class="only-hover">[7.1.4] 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df.select(first(<span class="string">&quot;StockCode&quot;</span>), last(<span class="string">&quot;StockCode&quot;</span>)).show()</span><br><span class="line"><span class="comment">// +-----------------------+----------------------+</span></span><br><span class="line"><span class="comment">// |first(StockCode, false)|last(StockCode, false)|</span></span><br><span class="line"><span class="comment">// +-----------------------+----------------------+</span></span><br><span class="line"><span class="comment">// |                 85123A|                 22138|</span></span><br><span class="line"><span class="comment">// +-----------------------+----------------------+</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">first</span>(StockCode), <span class="keyword">last</span>(StockCode) <span class="keyword">FROM</span> dfTable</span><br></pre></td></tr></table></figure></details></li></ul></li><li><p><code>min(컬럼명)</code>, <code>max(컬럼명)</code> : 최솟값, 최댓값 추출</p><ul><li><details><summary class="only-hover">[7.1.5] 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;min, max&#125;</span><br><span class="line">df.select(min(<span class="string">&quot;Quantity&quot;</span>), max(<span class="string">&quot;Quantity&quot;</span>)).show()</span><br><span class="line"><span class="comment">// +-------------+-------------+</span></span><br><span class="line"><span class="comment">// |min(Quantity)|max(Quantity)|</span></span><br><span class="line"><span class="comment">// +-------------+-------------+</span></span><br><span class="line"><span class="comment">// |       -80995|        80995|</span></span><br><span class="line"><span class="comment">// +-------------+-------------+</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">min</span>(Quantity), <span class="built_in">max</span>(Quantity) <span class="keyword">FROM</span> dfTable</span><br></pre></td></tr></table></figure></details></li></ul></li><li><p><code>sum(컬럼명)</code> : 특정 컬럼의 모든 값 합산</p><ul><li><details><summary class="only-hover">[7.1.6] 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.sum</span><br><span class="line">df.select(sum(<span class="string">&quot;Quantity&quot;</span>)).show() <span class="comment">// 5176450</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// |sum(Quantity)|</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br><span class="line"><span class="comment">// |      5176450|</span></span><br><span class="line"><span class="comment">// +-------------+</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">sum</span>(Quantity) <span class="keyword">FROM</span> dfTable</span><br></pre></td></tr></table></figure></details></li></ul></li></ul><ul><li><p><code>sumDistinct(컬럼명)</code> : 특정 컬럼의 고유 (distinct) 값 합산</p><ul><li><details><summary class="only-hover">[7.1.7] 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.sumDistinct</span><br><span class="line">df.select(sumDistinct(<span class="string">&quot;Quantity&quot;</span>)).show() <span class="comment">// 29310</span></span><br><span class="line"><span class="comment">// +----------------------+</span></span><br><span class="line"><span class="comment">// |sum(DISTINCT Quantity)|</span></span><br><span class="line"><span class="comment">// +----------------------+</span></span><br><span class="line"><span class="comment">// |                 29310|</span></span><br><span class="line"><span class="comment">// +----------------------+</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">SUM</span>(Quantity) <span class="keyword">FROM</span> dfTable <span class="comment">-- 29310</span></span><br></pre></td></tr></table></figure></details></li></ul></li></ul><ul><li><p><code>avg(컬럼명)</code> : 평균 값</p><ul><li><p>== <code>sum()/count()</code> == <code>expr(&quot;mean(컬럼명)&quot;)</code></p></li><li><p>+ <code>distinct()</code> =&gt; 고윳값 평균 구하기도 가능</p></li><li><details><summary class="only-hover">[7.1.8] 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;sum, count, avg, expr&#125;</span><br><span class="line">(df.select(</span><br><span class="line">    count(<span class="string">&quot;Quantity&quot;</span>).alias(<span class="string">&quot;total_transactions&quot;</span>),</span><br><span class="line">    sum(<span class="string">&quot;Quantity&quot;</span>).alias(<span class="string">&quot;total_purchases&quot;</span>),</span><br><span class="line">    avg(<span class="string">&quot;Quantity&quot;</span>).alias(<span class="string">&quot;avg_purchases&quot;</span>),</span><br><span class="line">    expr(<span class="string">&quot;mean(Quantity)&quot;</span>).alias(<span class="string">&quot;mean_purchases&quot;</span>))</span><br><span class="line">  .selectExpr(</span><br><span class="line">    <span class="string">&quot;total_purchases/total_transactions&quot;</span>,</span><br><span class="line">    <span class="string">&quot;avg_purchases&quot;</span>,</span><br><span class="line">    <span class="string">&quot;mean_purchases&quot;</span>).show())</span><br><span class="line"><span class="comment">// +--------------------------------------+----------------+----------------+</span></span><br><span class="line"><span class="comment">// |(total_purchases / total_transactions)|   avg_purchases|  mean_purchases|</span></span><br><span class="line"><span class="comment">// +--------------------------------------+----------------+----------------+</span></span><br><span class="line"><span class="comment">// |                      9.55224954743324|9.55224954743324|9.55224954743324|</span></span><br><span class="line"><span class="comment">// +--------------------------------------+----------------+----------------+</span></span><br></pre></td></tr></table></figure></details></li></ul></li><li><p>분산과 표준편차</p><ul><li><p>평균(<code>m</code>) 주변에 데이터가 분포된 정도를 측정</p><ul><li>분산 : 평균과의 차이를 제곱한 결과의 평균 (<code>v = avg((x-m)^2)</code>)</li><li>표준편차 : 분산의 제곱근 (<code>σ = v^(1/2)</code>)</li></ul></li><li><p>스파크는 표본표준편차(sample standard deviation), 모표준편차(population standard deviation) 방식 지원</p><ul><li>=&gt; 아예 다르므로 <strong>잘 구분해서 사용해야함</strong></li></ul></li><li><p>표본표준분산, 표본표준편차 방식 사용 시 =&gt; <code>variance()</code>, <code>stddev()</code></p></li><li><p>모표준분산, 모표준편차 방식 사용 시 =&gt; <code>var_pop()</code>, <code>stddev_pop()</code></p></li><li><details><summary class="only-hover">[7.1.9] 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">df.select(var_pop(<span class="string">&quot;Quantity&quot;</span>), var_samp(<span class="string">&quot;Quantity&quot;</span>),</span><br><span class="line">  stddev_pop(<span class="string">&quot;Quantity&quot;</span>), stddev_samp(<span class="string">&quot;Quantity&quot;</span>)).show()</span><br><span class="line"><span class="comment">// +-----------------+------------------+--------------------+---------------------+</span></span><br><span class="line"><span class="comment">// |var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|</span></span><br><span class="line"><span class="comment">// +-----------------+------------------+--------------------+---------------------+</span></span><br><span class="line"><span class="comment">// |47559.30364660923| 47559.39140929892|  218.08095663447835|   218.08115785023455|</span></span><br><span class="line"><span class="comment">// +-----------------+------------------+--------------------+---------------------+</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">var_pop</span>(Quantity), <span class="built_in">var_samp</span>(Quantity),</span><br><span class="line"><span class="built_in">stddev_pop</span>(Quantity), <span class="built_in">stddev_samp</span>(Quantity)</span><br><span class="line"><span class="keyword">FROM</span> dfTable</span><br></pre></td></tr></table></figure></details></li></ul></li><li><p>비대칭도와 첨도</p><ul><li><p>데이터의 변곡점(extreme point) 를 측정하는 방법</p><ul><li><code>skewness(컬럼명)</code> : 비대칭도 (데이터 평균의 비대칭 정도) 측정</li><li><code>kurtosis(컬럼명)</code> : 첨도 (데이터 끝 부분의 뾰족한 정도) 측정</li></ul></li><li><p>확률변수(random variable)의 확률분포(probability distribution)로 데이터 모델링 시에 중요</p></li><li><p>수학적인 내용은 따로 알아서… 흠흠..</p></li><li><details><summary class="only-hover">[7.1.10] 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;skewness, kurtosis&#125;</span><br><span class="line">df.select(skewness(<span class="string">&quot;Quantity&quot;</span>), kurtosis(<span class="string">&quot;Quantity&quot;</span>)).show()</span><br><span class="line"><span class="comment">// +--------------------+------------------+</span></span><br><span class="line"><span class="comment">// |  skewness(Quantity)|kurtosis(Quantity)|</span></span><br><span class="line"><span class="comment">// +--------------------+------------------+</span></span><br><span class="line"><span class="comment">// |-0.26407557610528376|119768.05495530753|</span></span><br><span class="line"><span class="comment">// +--------------------+------------------+</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> skewness(Quantity), kurtosis(Quantity) <span class="keyword">FROM</span> dfTable</span><br></pre></td></tr></table></figure></details></li></ul></li><li><p>공분산과 상관관계</p><ul><li><p>두 컬럼값 사이의 영향도 비교</p></li><li><p><code>cov(컬럼1, 컬럼2)</code> : 공분산(covariance) 계산</p><ul><li>데이터 입력값에 따라 다른 범위를 가짐</li><li>var 함수처럼 표본공분산(sample covariance)이나 모공분산(population covariance) 방식으로도 계산 가능 =&gt; <code>covar_samp()</code>, <code>covar_pop()</code></li></ul></li><li><p><code>corr(컬럼1, 컬럼2)</code> : 상관관계(correlation) 계산</p><ul><li>피어슨 상관계수 (Pearson correlation coefficient) 측정 (-1 &lt;= <code>r</code> &lt;= 1)</li><li>모집단이나 표본에 대한 계산 개념 X</li></ul></li><li><details><summary class="only-hover">[7.1.11] 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;corr, covar_pop, covar_samp&#125;</span><br><span class="line">df.select(corr(<span class="string">&quot;InvoiceNo&quot;</span>, <span class="string">&quot;Quantity&quot;</span>), covar_samp(<span class="string">&quot;InvoiceNo&quot;</span>, <span class="string">&quot;Quantity&quot;</span>),</span><br><span class="line">    covar_pop(<span class="string">&quot;InvoiceNo&quot;</span>, <span class="string">&quot;Quantity&quot;</span>)).show()</span><br><span class="line"><span class="comment">// +-------------------------+-------------------------------+------------------------------+</span></span><br><span class="line"><span class="comment">// |corr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|</span></span><br><span class="line"><span class="comment">// +-------------------------+-------------------------------+------------------------------+</span></span><br><span class="line"><span class="comment">// |     4.912186085640497E-4|             1052.7280543915997|            1052.7260778754955|</span></span><br><span class="line"><span class="comment">// +-------------------------+-------------------------------+------------------------------+</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">corr</span>(InvoiceNo, Quantity), <span class="built_in">covar_samp</span>(InvoiceNo, Quantity),</span><br><span class="line"><span class="built_in">covar_pop</span>(InvoiceNo, Quantity)</span><br><span class="line"><span class="keyword">FROM</span> dfTable</span><br></pre></td></tr></table></figure></details></li></ul></li><li><p>복합 데이터 타입의 집계</p><ul><li><p>스파크는 수식을 통한 집계 외에도 <strong>복합 데이터 타입</strong>을 사용한 집계 가능 (ex. 특정 컬럼 값 =&gt; List, Set .. 등으로 수집)</p></li><li><p>수집된 데이터는 다양한 프로그래밍 방식으로 다루거나 활용 가능</p></li><li><details><summary class="only-hover">[7.1.12] 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;collect_set, collect_list&#125;</span><br><span class="line">df.agg(collect_set(<span class="string">&quot;Country&quot;</span>), collect_list(<span class="string">&quot;Country&quot;</span>)).show()</span><br><span class="line"><span class="comment">// +--------------------+---------------------+</span></span><br><span class="line"><span class="comment">// |collect_set(Country)|collect_list(Country)|</span></span><br><span class="line"><span class="comment">// +--------------------+---------------------+</span></span><br><span class="line"><span class="comment">// |[Portugal, Italy,...| [United Kingdom, ...|</span></span><br><span class="line"><span class="comment">// +--------------------+---------------------+</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> collect_set(Country), collect_set(Country) <span class="keyword">FROM</span> dfTable</span><br></pre></td></tr></table></figure></details></li></ul></li></ul><h3 id="7-2-그룹화"><a href="#7-2-그룹화" class="headerlink" title="7.2 그룹화"></a>7.2 그룹화</h3><ul><li><u>데이터 <strong>그룹</strong> 기반의 집계</u> 에 대한 내용<ul><li>([7.1] 은 DataFrame 수준의 집계 내용)</li><li>카테고리형 데이터(categorical data) 사용</li><li>=&gt; 단일 컬럼의 데이터를 그룹화, 해당 그룹의 다른 여러 컬럼을 사용해서 계산</li></ul></li><li>그룹화 작업의 2 단계<ul><li>1) 하나 이상의 컬럼 그룹화 (여러개 지정도 가능)</li><li>2) 집계 연산 수행</li></ul></li><li>표현식을 이용한 그룹화<ul><li>카운팅은 메서드, 함수 둘 다 사용 가능 🤔<ul><li>메서드 보다 <code>count()</code> 함수 사용 추천</li><li>select 구문의 표현식 지정보다 <code>agg()</code> 메서드 사용 추천</li></ul></li><li><code>agg()</code> : 여러 집계 처리 한번에 지정 &amp; 집계에 표현식 사용 가능<ul><li>트랜스포메이션 완료 컬럼에 <code>alias</code> 사용 가능</li></ul></li></ul></li><li>맵을 이용한 그룹화<ul><li>맵(map) 타입 사용 : Key = 컬럼 / Value = 수행할 집계 함수의 문자열</li><li>수행할 집계함수를 한 줄로 작성 시 =&gt; 여러 컬럼명 재사용 가능<ul><li><code>agg(Key -&gt; Value, Key -&gt; Value, ...)</code></li></ul></li></ul></li></ul><h3 id="7-3-윈도우-함수"><a href="#7-3-윈도우-함수" class="headerlink" title="7.3 윈도우 함수"></a>7.3 윈도우 함수</h3><ul><li><strong>윈도우 함수</strong> 도 집계에 사용 가능</li><li>윈도우 함수<ul><li>데이터의 특정 ‘윈도우(window)’ 대상으로 고유의 집계 연산 수행</li><li>데이터의 ‘윈도우’ =&gt; 현재 데이터에 대한 참조(reference)를 사용해 정의</li><li>윈도우 명세(window specification) =&gt; 함수에 전달될 로우 결정</li></ul></li><li>스파크가 지원하는 윈도우 함수<ul><li>랭크 함수 (ranking function)</li><li>분석 함수 (analytic function)</li><li>집계 함수 (aggragate function)</li></ul></li><li>윈도우 함수 vs group-by 함수<ul><li>윈도우 함수 : <strong>프레임</strong>에 입력되는 모든 로우에 대해 결과값 계산</li><li>group-by 함수 : 모든 로우 레코드가 단일 그룹으로만 이동</li></ul></li><li>프레임(frame) : 로우 그룹 기반의 테이블<ul><li>각 로우는 하나 이상의 프레임에 할당 가능<img width="300" alt="row - window frame" src="https://user-images.githubusercontent.com/26691216/106770674-8b5eeb00-6681-11eb-929f-09e7c373f9a2.png"></li><li>프레임 정의 방법은 예제 참고</li></ul></li><li>ex. 하루를 나타내는 값의 롤링 평균(rolling average) 구하기<ul><li>개별 로우가 7개의 다른 프레임으로 구성되어야 함</li></ul></li></ul><details><summary class="point-color-can-hover">[7.3] 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1) 주문 일자(InvoiceDate) =&gt; &#x27;date&#x27; 컬럼으로 변환 (날짜 정보만 포함)</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;col, to_date&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> dfWithDate = df.withColumn(<span class="string">&quot;date&quot;</span>, to_date(col(<span class="string">&quot;InvoiceDate&quot;</span>),</span><br><span class="line">  <span class="string">&quot;MM/d/yyyy H:mm&quot;</span>))</span><br><span class="line">dfWithDate.createOrReplaceTempView(<span class="string">&quot;dfWithDate&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 2) 윈도우 명세 만들기</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Window</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.col</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> windowSpec = (<span class="type">Window</span></span><br><span class="line">  .partitionBy(<span class="string">&quot;CustomerId&quot;</span>, <span class="string">&quot;date&quot;</span>)  <span class="comment">// 그룹을 어떻게 나눌지 결정</span></span><br><span class="line">  .orderBy(col(<span class="string">&quot;Quantity&quot;</span>).desc)    <span class="comment">// 파티션 정렬 방식</span></span><br><span class="line">  .rowsBetween(<span class="type">Window</span>.unboundedPreceding, <span class="type">Window</span>.currentRow)) <span class="comment">// 프레임 명세 (=&gt; 첫 로우 ~ 현재 로우까지 확인)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 3) 집계 함수로 분석</span></span><br><span class="line"><span class="comment">// =&gt; 컬럼 or 표현식 반환 시 DataFrame.select() 에서 사용 가능</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 예시 1. maxPurchaseQuantity = 시간대별 최대 구매 개수</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.max</span><br><span class="line"><span class="keyword">val</span> maxPurchaseQuantity = max(col(<span class="string">&quot;Quantity&quot;</span>)).over(windowSpec)</span><br><span class="line"><span class="comment">// maxPurchaseQuantity: org.apache.spark.sql.Column = max(Quantity) OVER (PARTITION BY CustomerId, date ORDER BY Quantity DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 예시 2. purchase(Dense)Rank = 구매량 순위 </span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;dense_rank, rank&#125;</span><br><span class="line"><span class="keyword">val</span> purchaseDenseRank = dense_rank().over(windowSpec) <span class="comment">// 순위가 비지않도록 dense_rank() 사용</span></span><br><span class="line"><span class="keyword">val</span> purchaseRank = rank().over(windowSpec)</span><br><span class="line"></span><br><span class="line"><span class="comment">// DataFrame.select()로 윈도우 값 확인</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.col</span><br><span class="line"></span><br><span class="line">(dfWithDate.where(<span class="string">&quot;CustomerId IS NOT NULL&quot;</span>).orderBy(<span class="string">&quot;CustomerId&quot;</span>)</span><br><span class="line">  .select(</span><br><span class="line">    col(<span class="string">&quot;CustomerId&quot;</span>),</span><br><span class="line">    col(<span class="string">&quot;date&quot;</span>),</span><br><span class="line">    col(<span class="string">&quot;Quantity&quot;</span>),</span><br><span class="line">    purchaseRank.alias(<span class="string">&quot;quantityRank&quot;</span>),</span><br><span class="line">    purchaseDenseRank.alias(<span class="string">&quot;quantityDenseRank&quot;</span>),</span><br><span class="line">    maxPurchaseQuantity.alias(<span class="string">&quot;maxPurchaseQuantity&quot;</span>)).show())</span><br><span class="line"><span class="comment">// +----------+----------+--------+------------+-----------------+-------------------+</span></span><br><span class="line"><span class="comment">// |CustomerId|      date|Quantity|quantityRank|quantityDenseRank|maxPurchaseQuantity|</span></span><br><span class="line"><span class="comment">// +----------+----------+--------+------------+-----------------+-------------------+</span></span><br><span class="line"><span class="comment">// |     12346|2011-01-18|   74215|           1|                1|              74215|</span></span><br><span class="line"><span class="comment">// |     12346|2011-01-18|  -74215|           2|                2|              74215|</span></span><br><span class="line"><span class="comment">// |     12347|2010-12-07|      36|           1|                1|                 36|</span></span><br><span class="line"><span class="comment">// |     12347|2010-12-07|      30|           2|                2|                 36|</span></span><br><span class="line"><span class="comment">// |     12347|2010-12-07|      24|           3|                3|                 36|</span></span><br><span class="line"><span class="comment">// |     12347|2010-12-07|      12|           4|                4|                 36|</span></span><br><span class="line"><span class="comment">// |     12347|2010-12-07|      12|           4|                4|                 36|</span></span><br><span class="line"><span class="comment">// |     12347|2010-12-07|      12|           4|                4|                 36|</span></span><br><span class="line"><span class="comment">// |     12347|2010-12-07|      12|           4|                4|                 36|</span></span><br><span class="line"><span class="comment">// |     12347|2010-12-07|      12|           4|                4|                 36|</span></span><br><span class="line"><span class="comment">// |     12347|2010-12-07|      12|           4|                4|                 36|</span></span><br><span class="line"><span class="comment">// |     12347|2010-12-07|      12|           4|                4|                 36|</span></span><br><span class="line"><span class="comment">// |     12347|2010-12-07|      12|           4|                4|                 36|</span></span><br><span class="line"><span class="comment">// |     12347|2010-12-07|      12|           4|                4|                 36|</span></span><br><span class="line"><span class="comment">// |     12347|2010-12-07|      12|           4|                4|                 36|</span></span><br><span class="line"><span class="comment">// |     12347|2010-12-07|      12|           4|                4|                 36|</span></span><br><span class="line"><span class="comment">// |     12347|2010-12-07|      12|           4|                4|                 36|</span></span><br><span class="line"><span class="comment">// |     12347|2010-12-07|      12|           4|                4|                 36|</span></span><br><span class="line"><span class="comment">// |     12347|2010-12-07|       6|          17|                5|                 36|</span></span><br><span class="line"><span class="comment">// |     12347|2010-12-07|       6|          17|                5|                 36|</span></span><br><span class="line"><span class="comment">// +----------+----------+--------+------------+-----------------+-------------------+</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> CustomerId, <span class="type">date</span>, Quantity,</span><br><span class="line">  <span class="built_in">rank</span>(Quantity) <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> CustomerId, <span class="type">date</span></span><br><span class="line">                       <span class="keyword">ORDER</span> <span class="keyword">BY</span> Quantity <span class="keyword">DESC</span> <span class="keyword">NULLS</span> <span class="keyword">LAST</span></span><br><span class="line">                       <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span></span><br><span class="line">                         UNBOUNDED PRECEDING <span class="keyword">AND</span></span><br><span class="line">                         <span class="keyword">CURRENT</span> <span class="type">ROW</span>) <span class="keyword">as</span> rank,</span><br><span class="line"></span><br><span class="line">  <span class="built_in">dense_rank</span>(Quantity) <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> CustomerId, <span class="type">date</span></span><br><span class="line">                             <span class="keyword">ORDER</span> <span class="keyword">BY</span> Quantity <span class="keyword">DESC</span> <span class="keyword">NULLS</span> <span class="keyword">LAST</span></span><br><span class="line">                             <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span></span><br><span class="line">                               UNBOUNDED PRECEDING <span class="keyword">AND</span></span><br><span class="line">                               <span class="keyword">CURRENT</span> <span class="type">ROW</span>) <span class="keyword">as</span> dRank,</span><br><span class="line"></span><br><span class="line">  <span class="built_in">max</span>(Quantity) <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> CustomerId, <span class="type">date</span></span><br><span class="line">                      <span class="keyword">ORDER</span> <span class="keyword">BY</span> Quantity <span class="keyword">DESC</span> <span class="keyword">NULLS</span> <span class="keyword">LAST</span></span><br><span class="line">                      <span class="keyword">ROWS</span> <span class="keyword">BETWEEN</span></span><br><span class="line">                        UNBOUNDED PRECEDING <span class="keyword">AND</span></span><br><span class="line">                        <span class="keyword">CURRENT</span> <span class="type">ROW</span>) <span class="keyword">as</span> maxPurchase</span><br><span class="line"><span class="keyword">FROM</span> dfWithDate <span class="keyword">WHERE</span> CustomerId <span class="keyword">IS</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span> <span class="keyword">ORDER</span> <span class="keyword">BY</span> CustomerId</span><br></pre></td></tr></table></figure></details><blockquote><p>Window 메서드</p><ul><li><code>partitionBy()</code> : 그룹을 어떻게 나눌지 결정 (지금까지 파티셔닝 스키마 개념이랑 관련 X)</li><li><code>orderBy()</code> : 파티션 정렬 방식 정의</li><li><code>rowsBetween(from, to)</code> : 입력된 로우의 참조 기반으로 프레임에 로우가 포함될 수 있는지 결정</li></ul><p>row_number vs <strong>rank</strong> vs <strong>dense_rank</strong></p><ul><li><code>row_number()</code> : 순서대로 넘버링 (1,2,3,4 …)</li><li><code>rank()</code> : 순서대로 넘버링 + 같은 값일 경우 같은 숫자 (1,1,3,4 …)</li><li><code>dense_rank()</code> : rank 와 동일하되, 빈값 없이 증가하게끔 넘버링 (1,1,2,3, …)</li></ul></blockquote><h3 id="7-4-그룹화-셋"><a href="#7-4-그룹화-셋" class="headerlink" title="7.4 그룹화 셋"></a>7.4 그룹화 셋</h3><ul><li>컬럼의 값을 이용해 여러 컬럽 집계 =&gt; <code>group-by</code> 표현식<ul><li>그러면 <strong>여러 그룹</strong>에 걸쳐 집계는? =&gt; <strong>그룹화셋</strong>  사용</li></ul></li><li>그룹화 셋 : 여러 집계를 결합하는 저수준 기능<ul><li><code>GROUPING SETS</code> 구문은 SQL에서만 사용 가능</li><li>DataFrame에서 동일 연산하려면? =&gt; <strong>롤업</strong>, <strong>큐브</strong> 메서드 사용</li></ul></li><li>주의 사항<ul><li>그룹화 셋, 롤업, 큐브 사용 시 <strong>null 제거 필수</strong></li><li>null에 따라 집계 수준이 달라짐 (=&gt; null 미제거시 부정확한 결과)</li></ul></li></ul><details><summary class="point-color-can-hover">[7.4.0] '그룹화 셋' 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 그룹화 셋 사용 시 null 제거 필수</span></span><br><span class="line"><span class="keyword">val</span> dfNoNull = dfWithDate.drop()</span><br><span class="line">dfNoNull.createOrReplaceTempView(<span class="string">&quot;dfNoNull&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL 예제 1) 재고 코드(StockCode)와 고객(CustomerId) 별 총 수량 구하기</span></span><br><span class="line"><span class="keyword">SELECT</span> CustomerId, stockCode, <span class="built_in">sum</span>(Quantity) <span class="keyword">FROM</span> dfNoNull</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> customerId, stockCode</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> CustomerId <span class="keyword">DESC</span>, stockCode <span class="keyword">DESC</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- (그룹화 셋 사용한 동일 표현)</span></span><br><span class="line"><span class="keyword">SELECT</span> CustomerId, stockCode, <span class="built_in">sum</span>(Quantity) <span class="keyword">FROM</span> dfNoNull</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> customerId, stockCode <span class="keyword">GROUPING</span> <span class="keyword">SETS</span>((customerId, stockCode))</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> CustomerId <span class="keyword">DESC</span>, stockCode <span class="keyword">DESC</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL 예제 2) 예제 1 + 재고 코드나 고객 상관없이 총 수량 합산 결과 추가 =&gt; group-by로 처리 불가</span></span><br><span class="line"><span class="comment">-- 그룹화 셋으로 집계 방식 지정</span></span><br><span class="line"><span class="keyword">SELECT</span> CustomerId, stockCode, <span class="built_in">sum</span>(Quantity) <span class="keyword">FROM</span> dfNoNull</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> customerId, stockCode <span class="keyword">GROUPING</span> <span class="keyword">SETS</span>((customerId, stockCode),())</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> CustomerId <span class="keyword">DESC</span>, stockCode <span class="keyword">DESC</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output</span></span><br><span class="line">+----------+---------+-------------+</span><br><span class="line">|CustomerId|stockCode|sum(Quantity)|</span><br><span class="line">+----------+---------+-------------+</span><br><span class="line">|     18287|    85173|           48|</span><br><span class="line">|     18287|   85040A|           48|</span><br><span class="line">|     18287|   85039B|          120|</span><br><span class="line">...</span><br><span class="line">|     18287|    23269|           36|</span><br><span class="line">+----------+---------+-------------+</span><br></pre></td></tr></table></figure></details><ul><li>롤업(rollup)<ul><li><code>group-by</code> 스타일의 다양한 연산을 수행할 수 있는 다차원 집계 기능</li><li><code>rollup(그룹화 키)</code> =&gt; 다양한 컬럼을 그룹화 키로 설정 가능</li><li>롤업된 컬럼값이 모두 null 인 로우 = 해당 컬럼에 속한 레코드의 전체 합계</li></ul></li></ul><details><summary class="point-color-can-hover">[7.4.1] '롤업' 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 시간(신규 Date 컬럼), 공간(Country) 을 축으로 하는 롤업</span></span><br><span class="line"><span class="comment">// =&gt; &#x27;모든 날짜 총합&#x27;, &#x27;날짜별 총합&#x27;, &#x27;날짜별 국가별 총합&#x27; 포함하는 DataFrame 생성</span></span><br><span class="line"><span class="keyword">val</span> rolledUpDF = (dfNoNull.rollup(<span class="string">&quot;Date&quot;</span>, <span class="string">&quot;Country&quot;</span>).agg(sum(<span class="string">&quot;Quantity&quot;</span>))</span><br><span class="line">  .selectExpr(<span class="string">&quot;Date&quot;</span>, <span class="string">&quot;Country&quot;</span>, <span class="string">&quot;`sum(Quantity)` as total_quantity&quot;</span>)</span><br><span class="line">  .orderBy(<span class="string">&quot;Date&quot;</span>))</span><br><span class="line">rolledUpDF.show()</span><br><span class="line"><span class="comment">// +----------+--------------+--------------+</span></span><br><span class="line"><span class="comment">// |      Date|       Country|total_quantity|</span></span><br><span class="line"><span class="comment">// +----------+--------------+--------------+</span></span><br><span class="line"><span class="comment">// |      null|          null|       5176450| =&gt; 전체 합계</span></span><br><span class="line"><span class="comment">// |2010-12-01|   Netherlands|            97|</span></span><br><span class="line"><span class="comment">// |2010-12-01|       Germany|           117|</span></span><br><span class="line"><span class="comment">// |2010-12-01|     Australia|           107|</span></span><br><span class="line"><span class="comment">// |2010-12-01|        France|           449|</span></span><br><span class="line"><span class="comment">// |2010-12-01|          EIRE|           243|</span></span><br><span class="line"><span class="comment">// |2010-12-01|United Kingdom|         23949|</span></span><br><span class="line"><span class="comment">// |2010-12-01|          null|         26814|</span></span><br><span class="line"><span class="comment">// |2010-12-01|        Norway|          1852|</span></span><br><span class="line"><span class="comment">// |2010-12-02|          EIRE|             4|</span></span><br><span class="line"><span class="comment">// |2010-12-02|          null|         21023|</span></span><br><span class="line"><span class="comment">// |2010-12-02|       Germany|           146|</span></span><br><span class="line"><span class="comment">// |2010-12-02|United Kingdom|         20873|</span></span><br><span class="line"><span class="comment">// |2010-12-03|        France|           239|</span></span><br><span class="line"><span class="comment">// |2010-12-03|      Portugal|            65|</span></span><br><span class="line"><span class="comment">// |2010-12-03|       Germany|           170|</span></span><br><span class="line"><span class="comment">// |2010-12-03|       Belgium|           528|</span></span><br><span class="line"><span class="comment">// |2010-12-03|         Spain|           400|</span></span><br><span class="line"><span class="comment">// |2010-12-03|         Italy|           164|</span></span><br><span class="line"><span class="comment">// |2010-12-03|   Switzerland|           110|</span></span><br><span class="line"><span class="comment">// +----------+--------------+--------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// Country, Date 둘 다 null 인 로우 =&gt; 전체 합계 나타냄</span></span><br><span class="line">rolledUpDF.where(<span class="string">&quot;Country IS NULL&quot;</span>).show()</span><br><span class="line">rolledUpDF.where(<span class="string">&quot;Date IS NULL&quot;</span>).show()</span><br><span class="line"><span class="comment">// +----+-------+--------------+</span></span><br><span class="line"><span class="comment">// |Date|Country|total_quantity|</span></span><br><span class="line"><span class="comment">// +----+-------+--------------+</span></span><br><span class="line"><span class="comment">// |null|   null|       5176450|</span></span><br><span class="line"><span class="comment">// +----+-------+--------------+</span></span><br></pre></td></tr></table></figure></details><ul><li>큐브(cube)<ul><li>롤업의 고차원적 사용 (호출 방식도 유사)</li><li>요소들을 계층적으로 다루는 대신 모든 차원에 대해 동일한 작업 수행</li><li>ex. 전체 기간에 대한 날짜와 국가별 결과 구하기</li><li><code>cube(그룹화 키)</code> =&gt; 요약 정보 테이블 만들기 가능</li></ul></li></ul><details><summary class="point-color-can-hover">[7.4.2] '큐브' 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 시간(신규 Date 컬럼), 공간(Country) 을 축으로 하는 큐브</span></span><br><span class="line"><span class="comment">// =&gt; &#x27;전체 기간에 대한 날짜와 국가별 결과&#x27; 포함하는 DataFrame 생성</span></span><br><span class="line"><span class="comment">//     (외에도 전체 날짜와 모든 국가에 대한 합계, 모든 국가의 날짜별 합계, 날짜별 국가별 합계, 전체 날짜의 국가별 합계, ... 가능)</span></span><br><span class="line">(dfNoNull.cube(<span class="string">&quot;Date&quot;</span>, <span class="string">&quot;Country&quot;</span>).agg(sum(col(<span class="string">&quot;Quantity&quot;</span>)))</span><br><span class="line">  .select(<span class="string">&quot;Date&quot;</span>, <span class="string">&quot;Country&quot;</span>, <span class="string">&quot;sum(Quantity)&quot;</span>).orderBy(<span class="string">&quot;Date&quot;</span>).show())</span><br><span class="line"><span class="comment">// +----+--------------------+-------------+</span></span><br><span class="line"><span class="comment">// |Date|             Country|sum(Quantity)|</span></span><br><span class="line"><span class="comment">// +----+--------------------+-------------+</span></span><br><span class="line"><span class="comment">// |null|               Japan|        25218|</span></span><br><span class="line"><span class="comment">// |null|            Portugal|        16180|</span></span><br><span class="line"><span class="comment">// |null|             Germany|       117448|</span></span><br><span class="line"><span class="comment">// |null|                 RSA|          352|</span></span><br><span class="line"><span class="comment">// |null|           Australia|        83653|</span></span><br><span class="line"><span class="comment">// |null|           Hong Kong|         4769|</span></span><br><span class="line"><span class="comment">// |null|              Cyprus|         6317|</span></span><br><span class="line"><span class="comment">// |null|             Finland|        10666|</span></span><br><span class="line"><span class="comment">// |null|United Arab Emirates|          982|</span></span><br><span class="line"><span class="comment">// |null|                null|      5176450|</span></span><br><span class="line"><span class="comment">// |null|         Unspecified|         3300|</span></span><br><span class="line"><span class="comment">// |null|               Spain|        26824|</span></span><br><span class="line"><span class="comment">// |null|           Singapore|         5234|</span></span><br><span class="line"><span class="comment">// |null|     Channel Islands|         9479|</span></span><br><span class="line"><span class="comment">// |null|             Lebanon|          386|</span></span><br><span class="line"><span class="comment">// |null|                 USA|         1034|</span></span><br><span class="line"><span class="comment">// |null|             Denmark|         8188|</span></span><br><span class="line"><span class="comment">// |null|              Norway|        19247|</span></span><br><span class="line"><span class="comment">// |null|      Czech Republic|          592|</span></span><br><span class="line"><span class="comment">// |null|  European Community|          497|</span></span><br><span class="line"><span class="comment">// +----+--------------------+-------------+</span></span><br></pre></td></tr></table></figure></details><ul><li>그룹화 메타데이터<ul><li>큐브, 롤업 사용 시 집계 수준에 따라 쉽게 필터링하고자 하면 =&gt; <strong>집계 수준 조회</strong> 필요</li><li><code>grouping_id()</code> : 결과 데이터셋의 집계 수준을 명시하는 컬럼 제공</li></ul></li></ul><details><summary class="point-color-can-hover">[7.4.3] grouping_id() 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;grouping_id, sum, expr&#125;</span><br><span class="line"></span><br><span class="line">(dfNoNull.cube(<span class="string">&quot;customerId&quot;</span>, <span class="string">&quot;stockCode&quot;</span>).agg(grouping_id(), sum(<span class="string">&quot;Quantity&quot;</span>))</span><br><span class="line">.orderBy(col(<span class="string">&quot;grouping_id()&quot;</span>).desc)</span><br><span class="line">.show())</span><br><span class="line"><span class="comment">// =&gt; 4개의 개별 그룹화 ID 값 (0,1,2,3) 반환됨</span></span><br><span class="line"><span class="comment">// +----------+---------+-------------+-------------+</span></span><br><span class="line"><span class="comment">// |customerId|stockCode|grouping_id()|sum(Quantity)|</span></span><br><span class="line"><span class="comment">// +----------+---------+-------------+-------------+</span></span><br><span class="line"><span class="comment">// |      null|     null|            3|      5176450| =&gt; 3 : 가장 높은 계층의 집계 결과. 전체 총 수량 (customerId, stockCode 관계 X)</span></span><br><span class="line"><span class="comment">// |      null|    84226|            2|           17| =&gt; 2 : 개별 stockCode 별 총 수량 (customerId 관계 X)</span></span><br><span class="line"><span class="comment">// |      null|    22856|            2|          518|</span></span><br><span class="line"><span class="comment">// |      null|    22352|            2|         3077|</span></span><br><span class="line"><span class="comment">//            ...</span></span><br><span class="line"><span class="comment">// |     14907|     null|            1|         1686| =&gt; 1 : customerId 기반 총 수량 제공 (구매 물품 관계 X)</span></span><br><span class="line"><span class="comment">// |     14543|     null|            1|          600|</span></span><br><span class="line"><span class="comment">//            ...</span></span><br><span class="line"><span class="comment">// |     13047|    22749|            0|           12| =&gt; 0 : customerId - stockCode 별 조합에 따라 총 수량</span></span><br><span class="line"><span class="comment">// |     15311|    22083|            0|          169|</span></span><br><span class="line"><span class="comment">// +----------+---------+-------------+-------------+</span></span><br></pre></td></tr></table></figure></details><ul><li>피벗(pivot)<ul><li><code>pivot()</code> : 로우 → 컬럼으로 변환 가능</li><li>=&gt; 컬럼의 모든 값을 단일 그룹화하여 계산 가능<ul><li>그러나 데이터 탐색방식에 따라 피벗 수행 결과값이 감소할 수 있음</li></ul></li><li>특정 컬럼 cardinality가 낮으면 피벗으로 다수 컬럼으로 변환 추천   =&gt; 스키마, 쿼리 대상 확인 가능</li></ul></li></ul><details><summary class="point-color-can-hover">[7.4.4] '피벗' 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> pivoted = dfWithDate.groupBy(<span class="string">&quot;date&quot;</span>).pivot(<span class="string">&quot;Country&quot;</span>).sum() <span class="comment">// 집계 수행 =&gt; 수치형 컬럼으로 나옴</span></span><br><span class="line"><span class="comment">// pivoted.printSchema()</span></span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">//  |-- date: date (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- Australia_sum(Quantity): long (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- Australia_sum(UnitPrice): double (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- Australia_sum(CustomerID): long (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- Austria_sum(Quantity): long (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- Austria_sum(UnitPrice): double (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- Austria_sum(CustomerID): long (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- Bahrain_sum(Quantity): long (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- Bahrain_sum(UnitPrice): double (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- Bahrain_sum(CustomerID): long (nullable = true)</span></span><br><span class="line"><span class="comment">//  ...</span></span><br><span class="line"></span><br><span class="line">pivoted.where(<span class="string">&quot;date &gt; &#x27;2011-12-05&#x27;&quot;</span>).select(<span class="string">&quot;date&quot;</span> ,<span class="string">&quot;`USA_sum(Quantity)`&quot;</span>).show()</span><br><span class="line"><span class="comment">// +----------+-----------------+</span></span><br><span class="line"><span class="comment">// |      date|USA_sum(Quantity)|</span></span><br><span class="line"><span class="comment">// +----------+-----------------+</span></span><br><span class="line"><span class="comment">// |2011-12-06|             null|</span></span><br><span class="line"><span class="comment">// |2011-12-09|             null|</span></span><br><span class="line"><span class="comment">// |2011-12-08|             -196|</span></span><br><span class="line"><span class="comment">// |2011-12-07|             null|</span></span><br><span class="line"><span class="comment">// +----------+-----------------+</span></span><br></pre></td></tr></table></figure></details><h3 id="7-5-사용자-정의-집계-함수"><a href="#7-5-사용자-정의-집계-함수" class="headerlink" title="7.5 사용자 정의 집계 함수"></a>7.5 사용자 정의 집계 함수</h3><ul><li>사용자 정의 집계 함수 (<strong>UDAF</strong>, user-defined aggregation function)<ul><li>직접 제작한 함수나 비즈니스 규칙에 기반한 자체 집계 함수 정의 방법</li><li>UDAF 사용 =&gt; 입력 데이터 그룹에 직접 개발한 연산 수행 가능</li><li>스파크는 입력 데이터의 모든 그룹 중간 결과를 단일 AggregationBuffer에 저장/관리</li></ul></li><li>UDAF는 현재 스칼라, 자바로만 사용 가능<ul><li>Spark 2.3 에서는 UDF/UDAF =&gt; 함수 등록 가능 ([6.12] 참고)</li></ul></li><li>생성 방법<ul><li>기본 클래스 UserDefinedAggregateFunction 상속 + 메서드 정의</li></ul></li></ul><blockquote><p>UDAF 생성 시 정의해야할 메서드</p><ul><li>inputScheme : <code>UDAF 입력 파라미터의 스키마</code>를 StructType 로 정의</li><li>bufferSchema : <code>UDAF 중간 결과의 스키마</code>를 StructType 로 정의</li><li>dataType : <code>반환될 값의 DataType</code> 정의</li><li>deterministic : <code>UDAF가 동일한 입력값에 대해 항상 동일한 결과를 반환하는지</code> Boolean 값으로 정의</li><li>initialize : <code>집계용 버퍼 값 초기화 로직</code> 정의</li><li>update : 입력받은 로우 기반으로 <code>내부 버퍼 업데이트 로직</code> 정의</li><li>merge : 두 개의 <code>집계용 버퍼 병합 로직</code> 정의</li><li>evaluate : <code>집계 최종 결과 생성 로직</code> 정의</li></ul></blockquote><details><summary class="point-color-can-hover">[7.5] 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// UDAF 예제 - &#x27;BoolAnd&#x27; Class : 입력된 모든 로우의 컬럼이 true인지 판단</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BoolAnd</span> <span class="keyword">extends</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">sql</span>.<span class="title">expressions</span>.<span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line">  <span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">StructType</span>, <span class="type">StructField</span>, <span class="type">BooleanType</span>, <span class="type">DataType</span>&#125;</span><br><span class="line">  <span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">MutableAggregationBuffer</span></span><br><span class="line">  <span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: org.apache.spark.sql.types.<span class="type">StructType</span> =</span><br><span class="line">    <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">&quot;value&quot;</span>, <span class="type">BooleanType</span>) :: <span class="type">Nil</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(</span><br><span class="line">    <span class="type">StructField</span>(<span class="string">&quot;result&quot;</span>, <span class="type">BooleanType</span>) :: <span class="type">Nil</span></span><br><span class="line">  )</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">BooleanType</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer(<span class="number">0</span>) = <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer(<span class="number">0</span>) = buffer.getAs[<span class="type">Boolean</span>](<span class="number">0</span>) &amp;&amp; input.getAs[<span class="type">Boolean</span>](<span class="number">0</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer1(<span class="number">0</span>) = buffer1.getAs[<span class="type">Boolean</span>](<span class="number">0</span>) &amp;&amp; buffer2.getAs[<span class="type">Boolean</span>](<span class="number">0</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = &#123;</span><br><span class="line">    buffer(<span class="number">0</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 함수로 등록 및 사용</span></span><br><span class="line"><span class="keyword">val</span> ba = <span class="keyword">new</span> <span class="type">BoolAnd</span></span><br><span class="line">spark.udf.register(<span class="string">&quot;booland&quot;</span>, ba)</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line">(spark.range(<span class="number">1</span>)</span><br><span class="line">  .selectExpr(<span class="string">&quot;explode(array(TRUE, TRUE, TRUE)) as t&quot;</span>)</span><br><span class="line">  .selectExpr(<span class="string">&quot;explode(array(TRUE, FALSE, TRUE)) as f&quot;</span>, <span class="string">&quot;t&quot;</span>)</span><br><span class="line">  .select(ba(col(<span class="string">&quot;t&quot;</span>)), expr(<span class="string">&quot;booland(f)&quot;</span>))</span><br><span class="line">  .show())</span><br><span class="line"><span class="comment">// +----------+----------+</span></span><br><span class="line"><span class="comment">// |booland(t)|booland(f)|</span></span><br><span class="line"><span class="comment">// +----------+----------+</span></span><br><span class="line"><span class="comment">// |      true|     false|</span></span><br><span class="line"><span class="comment">// +----------+----------+</span></span><br></pre></td></tr></table></figure></details><h3 id="7-6-정리"><a href="#7-6-정리" class="headerlink" title="7.6 정리"></a>7.6 정리</h3><ul><li>스파크에서 사용 가능한 여러 유형의 집계 연산</li><li>그룹화, 윈도우 함수, 롤업, 큐브</li></ul><h3 id="📒-단어장"><a href="#📒-단어장" class="headerlink" title="📒 단어장"></a>📒 단어장</h3><ul><li>비대칭도(skewness) : 실숫값 확률변수의 확률분포 비대칭성을 나타내는 지표 (=왜도)</li><li>첨도(kurtosis) : 확률분포의 뾰족한 정도를 나타내는 척도</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;br/&gt;

&lt;p style=&quot;color:lightgray&quot;&gt;벌써부터 슬슬 포스트 포멧 헷갈리기 시작하죠? 망했죠?&lt;br/&gt;
나중에 한번에 맞춰야지 생각해놓고 절대 수정안하죠? ㅎ..&lt;/p&gt;

&lt;br/&gt;

&lt;img width=&quot;300&quot; alt=&quot;cou</summary>
      
    
    
    
    <category term="spark" scheme="https://minsw.github.io/categories/spark/"/>
    
    
    <category term="book" scheme="https://minsw.github.io/tags/book/"/>
    
    <category term="study" scheme="https://minsw.github.io/tags/study/"/>
    
    <category term="spark" scheme="https://minsw.github.io/tags/spark/"/>
    
    <category term="apache" scheme="https://minsw.github.io/tags/apache/"/>
    
  </entry>
  
  <entry>
    <title>&amp;#039;Spark The Definitive Guide&amp;#039; 6장 - 데이터 타입 (비)공식 가이드북</title>
    <link href="https://minsw.github.io/2021/02/02/Spark-The-Definitive-Guide-6%EC%9E%A5/"/>
    <id>https://minsw.github.io/2021/02/02/Spark-The-Definitive-Guide-6%EC%9E%A5/</id>
    <published>2021-02-01T15:46:40.000Z</published>
    <updated>2021-02-07T09:43:56.505Z</updated>
    
    <content type="html"><![CDATA[<br/><center><p style="color:lightgray">라떼 시절엔,, 가이드북이 하나면 든-든했다,,, 이말이야,,, 총총 @}----</p><img width="300" alt="maple" src="https://user-images.githubusercontent.com/26691216/106801127-b149b700-66a4-11eb-9c8f-0802771ebe5f.jpg"><i>'아파치 스파크' 미인증 비공식 가이드 북<br/>[전원 증정] 50.00 페이지 포인트 (캐시 아이템 구매 가능)</i></center><br/><center><h2>_ _ _</h2></center><br/><hr><h1 id="CHAPTER-6-다양한-데이터-타입-다루기"><a href="#CHAPTER-6-다양한-데이터-타입-다루기" class="headerlink" title="CHAPTER 6 다양한 데이터 타입 다루기"></a>CHAPTER 6 다양한 데이터 타입 다루기</h1><p>CHAPTER 5 는 DataFrame의 기본 개념과 핵심 추상화 개념을 소개<br>CHAPTER 6 는 스파크의 구조적 연산에서 가장 중요한 내용인 <strong>표현식 만드는 방법</strong> 소개 + 다양한 데이터 타입 다루는 방법</p><blockquote><p>다양한 데이터 타입</p><ul><li>Boolean 타입</li><li>수치 타입</li><li>문자열 타입</li><li>date와 timestamp 타입</li><li>null 값 다루기</li><li>복합 데이터 아입</li><li>사용자 정의 함수</li></ul></blockquote><h3 id="6-1-API는-어디서-찾을까"><a href="#6-1-API는-어디서-찾을까" class="headerlink" title="6.1 API는 어디서 찾을까"></a>6.1 API는 어디서 찾을까</h3><ul><li>오늘은 언젠가 내일이 된다<ul><li>버전 바뀌면 책의 내용도 다 예전 내용이다~ 이말이야</li><li>=&gt; 따라서 <u>데이터 변환용 함수 찾는 방법</u> 을 알아야함</li></ul></li><li>어떻게 찾나?<ul><li>DataFrame (Dataset) 메서드<ul><li>DatasFrame은 Row타입을 가진 Dataset =&gt; <a href="http://bit.ly/2rKkALY">Dataset API</a></li><li>다양한 메서드를 제공하는 Dataset 하위 모듈 (ex. <a href="http://bit.ly/2DPYhJC">DataFrameStateFunctions</a> - 통계적 함수 제공, <a href="http://bit.ly/2DPAqd3">DataFrameNaFunctions</a> - null 데이터 제어)</li></ul></li><li>Column 메서드<ul><li><code>alias</code> <code>contains</code> 등의 컬럼 관련 메서드 제공 =&gt; <a href="http://bit.ly/2FloFbr">Columns API</a></li><li><code>org.apache.spark.sql.functions</code>는 데이터 타입 관련 다양한 함수 제공 (ex. <a href="http://bit.ly/2DPAycx">SQL, DataFrame 함수 등</a>)</li></ul></li></ul></li><li>모든 함수는 데이터 로우의 특정 포맷이나 구조를 다른 형태로 변환하기 위해 존재<ul><li>함수로 더 많은 로우를 만들거나 줄일 수 O</li></ul></li></ul><details><summary class="point-color-can-hover">[6.1] 예제 펼치기 - 분석용 DataFrame 생성 예제</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = (spark.read.format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;inferSchema&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">  .load(<span class="string">&quot;/data/retail-data/by-day/2010-12-01.csv&quot;</span>))</span><br><span class="line">df.printSchema()</span><br><span class="line">df.createOrReplaceTempView(<span class="string">&quot;dfTable&quot;</span>)</span><br><span class="line"><span class="comment">// df: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]</span></span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">//  |-- InvoiceNo: string (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- StockCode: string (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- Description: string (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- Quantity: integer (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- InvoiceDate: timestamp (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- UnitPrice: double (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- CustomerID: double (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- Country: string (nullable = true)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></details><h3 id="6-2-스파크-데이터-타입으로-변환하기"><a href="#6-2-스파크-데이터-타입으로-변환하기" class="headerlink" title="6.2 스파크 데이터 타입으로 변환하기"></a>6.2 스파크 데이터 타입으로 변환하기</h3><ul><li><code>lit()</code> : 데이터 타입 변환<ul><li>다른 프로그래밍 언어 고유 데이터 타입 =&gt; <strong>스파크 데이터 타입</strong> 변환</li></ul></li></ul><details><summary class="point-color-can-hover">[6.2] 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.lit</span><br><span class="line"></span><br><span class="line">df.select(lit(<span class="number">5</span>), lit(<span class="string">&quot;five&quot;</span>), lit(<span class="number">5.0</span>))</span><br><span class="line"><span class="comment">// res9: org.apache.spark.sql.DataFrame = [5: int, five: string ... 1 more field]</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL (SQL은 스파크 데이터 타입 변환 필요 X. 직접 값 입력)</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="number">5</span>, &quot;five&quot;, <span class="number">5.0</span></span><br></pre></td></tr></table></figure></details><h3 id="6-3-불리언-데이터-타입-다루기"><a href="#6-3-불리언-데이터-타입-다루기" class="headerlink" title="6.3 불리언 데이터 타입 다루기"></a>6.3 불리언 데이터 타입 다루기</h3><ul><li><p>불리언은 모든 필터링 작업의 기반 (데이터 분석에 필수)</p></li><li><p>불리언 구문 : <code>and</code>, <code>or</code>, <code>true</code>, <code>false</code></p><ul><li>불리언 구문으로 논리 문법(true/false) 생성</li></ul></li><li><p><strong>스칼라</strong> 사용 시 동등 여부</p><ul><li><code>===</code> (일치) / <code>=!=</code> (불일치)</li><li><code>not()</code>, <code>equalTO()</code> 사용 가능</li></ul></li><li><p>파이썬, 스칼라 모두 사용할 수 있는</p><ul><li><p>가장 명확한 방법? =&gt; <u>문자열 표현식에 조건절 명시</u> (ex. <code>where(&quot;InvoiceNo = 536353)</code>)</p><details><summary class="point-color-can-hover">[6.3-1] 예제 펼치기 - 문자열 표현식에 조건절 명시 </summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.col</span><br><span class="line"></span><br><span class="line"><span class="comment">// 같은 표현식 (in Scala)</span></span><br><span class="line">(df.where(col(<span class="string">&quot;InvoiceNo&quot;</span>).equalTo(<span class="number">536365</span>))</span><br><span class="line">  .select(<span class="string">&quot;InvoiceNo&quot;</span>, <span class="string">&quot;Description&quot;</span>)</span><br><span class="line">  .show(<span class="number">5</span>, <span class="literal">false</span>))</span><br><span class="line">(df.where(col(<span class="string">&quot;InvoiceNo&quot;</span>) === <span class="number">536365</span>)</span><br><span class="line">  .select(<span class="string">&quot;InvoiceNo&quot;</span>, <span class="string">&quot;Description&quot;</span>)</span><br><span class="line">  .show(<span class="number">5</span>, <span class="literal">false</span>))</span><br><span class="line"><span class="comment">// +---------+-----------------------------------+</span></span><br><span class="line"><span class="comment">// |InvoiceNo|Description                        |</span></span><br><span class="line"><span class="comment">// +---------+-----------------------------------+</span></span><br><span class="line"><span class="comment">// |536365   |WHITE HANGING HEART T-LIGHT HOLDER |</span></span><br><span class="line"><span class="comment">// |536365   |WHITE METAL LANTERN                |</span></span><br><span class="line"><span class="comment">// |536365   |CREAM CUPID HEARTS COAT HANGER     |</span></span><br><span class="line"><span class="comment">// |536365   |KNITTED UNION FLAG HOT WATER BOTTLE|</span></span><br><span class="line"><span class="comment">// |536365   |RED WOOLLY HOTTIE WHITE HEART.     |</span></span><br><span class="line"><span class="comment">// +---------+-----------------------------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 문자열 표현식에 조건절 명시 (가장 명확한 방법) 사용</span></span><br><span class="line">(df.where(<span class="string">&quot;InvoiceNo = 536365&quot;</span>)</span><br><span class="line">  .show(<span class="number">5</span>, <span class="literal">false</span>))</span><br><span class="line"></span><br><span class="line">(df.where(<span class="string">&quot;InvoiceNo &lt;&gt; 536365&quot;</span>)</span><br><span class="line">  .show(<span class="number">5</span>, <span class="literal">false</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// +---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+</span></span><br><span class="line"><span class="comment">// |InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |</span></span><br><span class="line"><span class="comment">// +---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+</span></span><br><span class="line"><span class="comment">// |536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |2010-12-01 08:26:00|2.55     |17850.0   |United Kingdom|</span></span><br><span class="line"><span class="comment">// |536365   |71053    |WHITE METAL LANTERN                |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|</span></span><br><span class="line"><span class="comment">// |536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |2010-12-01 08:26:00|2.75     |17850.0   |United Kingdom|</span></span><br><span class="line"><span class="comment">// |536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|</span></span><br><span class="line"><span class="comment">// |536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|</span></span><br><span class="line"><span class="comment">// +---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// +---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+</span></span><br><span class="line"><span class="comment">// |InvoiceNo|StockCode|Description                  |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |</span></span><br><span class="line"><span class="comment">// +---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+</span></span><br><span class="line"><span class="comment">// |536366   |22633    |HAND WARMER UNION JACK       |6       |2010-12-01 08:28:00|1.85     |17850.0   |United Kingdom|</span></span><br><span class="line"><span class="comment">// |536366   |22632    |HAND WARMER RED POLKA DOT    |6       |2010-12-01 08:28:00|1.85     |17850.0   |United Kingdom|</span></span><br><span class="line"><span class="comment">// |536367   |84879    |ASSORTED COLOUR BIRD ORNAMENT|32      |2010-12-01 08:34:00|1.69     |13047.0   |United Kingdom|</span></span><br><span class="line"><span class="comment">// |536367   |22745    |POPPY&#x27;S PLAYHOUSE BEDROOM    |6       |2010-12-01 08:34:00|2.1      |13047.0   |United Kingdom|</span></span><br><span class="line"><span class="comment">// |536367   |22748    |POPPY&#x27;S PLAYHOUSE KITCHEN    |6       |2010-12-01 08:34:00|2.1      |13047.0   |United Kingdom|</span></span><br><span class="line"><span class="comment">// +---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+</span></span><br></pre></td></tr></table></figure></details></li></ul></li><li><p>불리언 표현식 사용하는 경우</p><ul><li><p>항상 모든 표현식을 <code>and</code> 메서드로 묶어 차례대로 필터 적용 해야 함</p></li><li><p>why?</p><ul><li>스파크 내부적으로 필터 사이에 <code>and</code> 구문 추가 시</li><li>=&gt; 모든 필터를 하나의 문장으로 변환하여 <strong>동시에 모든 필터 처리</strong> 하기 때문</li></ul></li><li><p><code>and</code> 구문 사용 시</p><ul><li><code>and</code> 구문으로 조건문을 만들 수는 있으나,</li><li>차례대로 조건 나열하는게 가독성이 좋음</li></ul></li><li><p><code>or</code> 구문 사용시</p><ul><li>반드시 동일한 구문에 조건 정의해야 함</li></ul><details><summary class="point-color-can-hover">[6.3-2] 예제 펼치기 - 불리언 표현식으로 필터링 적용 </summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> priceFilter = col(<span class="string">&quot;UnitPrice&quot;</span>) &gt; <span class="number">600</span></span><br><span class="line"><span class="keyword">val</span> descripFilter = col(<span class="string">&quot;Description&quot;</span>).contains(<span class="string">&quot;POSTAGE&quot;</span>)</span><br><span class="line">(df.where(col(<span class="string">&quot;StockCode&quot;</span>).isin(<span class="string">&quot;DOT&quot;</span>)).where(priceFilter.or(descripFilter))</span><br><span class="line">  .show())</span><br><span class="line"><span class="comment">// priceFilter: org.apache.spark.sql.Column = (UnitPrice &gt; 600)</span></span><br><span class="line"><span class="comment">// descripFilter: org.apache.spark.sql.Column = contains(Description, POSTAGE)</span></span><br><span class="line"><span class="comment">// +---------+---------+--------------+--------+-------------------+---------+----------+--------------+</span></span><br><span class="line"><span class="comment">// |InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|</span></span><br><span class="line"><span class="comment">// +---------+---------+--------------+--------+-------------------+---------+----------+--------------+</span></span><br><span class="line"><span class="comment">// |   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|</span></span><br><span class="line"><span class="comment">// |   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|</span></span><br><span class="line"><span class="comment">// +---------+---------+--------------+--------+-------------------+---------+----------+--------------+</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL (동일 표현)</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> dfTable <span class="keyword">WHERE</span> StockCode <span class="keyword">in</span> (&quot;DOT&quot;) <span class="keyword">AND</span>(UnitPrice <span class="operator">&gt;</span> <span class="number">600</span> <span class="keyword">OR</span></span><br><span class="line">    instr(Description, &quot;POSTAGE&quot;) <span class="operator">&gt;=</span> <span class="number">1</span>)</span><br></pre></td></tr></table></figure></details></li></ul></li><li><p>불리언 표현식은…</p><ul><li>필터링 조건에만 사용? =&gt; 🙅🏻‍♀️. 불리언컬럼으로 DataFrame 필터링도 가능</li><li>반드시 표현식으로 정의해야? =&gt; 🙅🏻‍♀️. 별도 작업없이 컬럼명만 사용해서 정의도 가능</li><li>사실 SQL로 표현하는게 더 익숙할지도.. (성능저하 X)</li></ul></li><li><p>NULL 값 데이터 처리?</p><ul><li>=&gt; <strong>null-safe</strong> 동치(equivalence) 테스트</li><li>ex. <code>df.where(col(&quot;Description&quot;).eqNullSafe(&quot;hello&quot;)).show()</code></li></ul></li><li><p>SQL의 <code>IS [NOT] DISTINCT FROM</code> 구문</p><ul><li>과 동일한 기능이 뭘 말하나.. =&gt; <code>isNotDistinctFrom()</code> <code>isDistinctFrom()</code>? (지금도 사용하는지?)</li><li>since Spark 2.3 (<a href="https://bit.ly/2x47Obk">이슈 참고</a>)</li></ul><details><summary class="point-color-can-hover">[6.3-3] 예제 펼치기 - 불리언컬럼으로 DataFrame 필터링</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DataFrame 필터링 예제</span></span><br><span class="line"><span class="keyword">val</span> <span class="type">DOTCodeFilter</span> = col(<span class="string">&quot;StockCode&quot;</span>) === <span class="string">&quot;DOT&quot;</span></span><br><span class="line"><span class="keyword">val</span> priceFilter = col(<span class="string">&quot;UnitPrice&quot;</span>) &gt; <span class="number">600</span></span><br><span class="line"><span class="keyword">val</span> descripFilter = col(<span class="string">&quot;Description&quot;</span>).contains(<span class="string">&quot;POSTAGE&quot;</span>)</span><br><span class="line">(df.withColumn(<span class="string">&quot;isExpensive&quot;</span>, <span class="type">DOTCodeFilter</span>.and(priceFilter.or(descripFilter)))</span><br><span class="line">  .where(<span class="string">&quot;isExpensive&quot;</span>)</span><br><span class="line">  .select(<span class="string">&quot;unitPrice&quot;</span>, <span class="string">&quot;isExpensive&quot;</span>).show(<span class="number">5</span>))</span><br><span class="line"><span class="comment">// DOTCodeFilter: org.apache.spark.sql.Column = (StockCode = DOT)</span></span><br><span class="line"><span class="comment">// priceFilter: org.apache.spark.sql.Column = (UnitPrice &gt; 600)</span></span><br><span class="line"><span class="comment">// descripFilter: org.apache.spark.sql.Column = contains(Description, POSTAGE)</span></span><br><span class="line"><span class="comment">// +---------+-----------+</span></span><br><span class="line"><span class="comment">// |unitPrice|isExpensive|</span></span><br><span class="line"><span class="comment">// +---------+-----------+</span></span><br><span class="line"><span class="comment">// |   569.77|       true|</span></span><br><span class="line"><span class="comment">// |   607.49|       true|</span></span><br><span class="line"><span class="comment">// +---------+-----------+</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL (동일 표현)</span></span><br><span class="line"><span class="keyword">SELECT</span> UnitPrice, (StockCode <span class="operator">=</span> <span class="string">&#x27;DOT&#x27;</span> <span class="keyword">AND</span></span><br><span class="line">  (UnitPrice <span class="operator">&gt;</span> <span class="number">600</span> <span class="keyword">OR</span> instr(Description, &quot;POSTAGE&quot;) <span class="operator">&gt;=</span> <span class="number">1</span>)) <span class="keyword">as</span> isExpensive</span><br><span class="line"><span class="keyword">FROM</span> dfTable</span><br><span class="line"><span class="keyword">WHERE</span> (StockCode <span class="operator">=</span> <span class="string">&#x27;DOT&#x27;</span> <span class="keyword">AND</span></span><br><span class="line">       (UnitPrice <span class="operator">&gt;</span> <span class="number">600</span> <span class="keyword">OR</span> instr(Description, &quot;POSTAGE&quot;) <span class="operator">&gt;=</span> <span class="number">1</span>))</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 필터는 SQL로 사용시 더 편리할 수도. (아래 두 문장 동일하게 처리됨)</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;expr, not, col&#125;</span><br><span class="line"></span><br><span class="line">(df.withColumn(<span class="string">&quot;isExpensive&quot;</span>, not(col(<span class="string">&quot;UnitPrice&quot;</span>).leq(<span class="number">250</span>)))</span><br><span class="line">  .filter(<span class="string">&quot;isExpensive&quot;</span>)</span><br><span class="line">  .select(<span class="string">&quot;Description&quot;</span>, <span class="string">&quot;UnitPrice&quot;</span>).show(<span class="number">5</span>))</span><br><span class="line">(df.withColumn(<span class="string">&quot;isExpensive&quot;</span>, expr(<span class="string">&quot;NOT UnitPrice &lt;= 250&quot;</span>))</span><br><span class="line">  .filter(<span class="string">&quot;isExpensive&quot;</span>)</span><br><span class="line">  .select(<span class="string">&quot;Description&quot;</span>, <span class="string">&quot;UnitPrice&quot;</span>).show(<span class="number">5</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure></details></li></ul><h3 id="6-4-수치형-데이터-타입-다루기"><a href="#6-4-수치형-데이터-타입-다루기" class="headerlink" title="6.4 수치형 데이터 타입 다루기"></a>6.4 수치형 데이터 타입 다루기</h3><ul><li><code>count()</code> <ul><li>빅데이터 처리 시, 필터링 다음으로 많이 수행하는 작업</li><li>수치형 데이터 타입을 사용한 연산 방식 정의</li></ul></li><li>자주 사용하는 수치형 함수<ul><li><code>pow(밑, 지수)</code> (거듭제곱)</li><li><code>round()</code> (반올림), <code>bround()</code> (내림)</li><li><code>corr()</code> =&gt; 피어슨 상관계수 계산 (= 두 컬럼의 상관관계)</li><li><code>describe()</code> =&gt; 관련 컬럼에 대한 집계(count), 평균(mean), 표준편차(stddev), 최솟값(min), 최댓값(max) 등 계산<ul><li>하나 이상의 컬럼에대한 요약 통계 계산</li><li>그러나 콘솔 확인용으로만 사용해야함 (통계 스키마는 변경 될 수 있음)</li><li>정확한 수치 필요 시 =&gt; 해당 함수 임포트해서 적용하는 방식으로 <strong>직접 집계</strong></li></ul></li></ul></li><li><strong>StatFunction</strong> 패키지 =&gt; 다양한 통계 함수 제공<ul><li>다양한 통계값 계산에 사용하는 DataFrame 메서드 =&gt; <code>df.stat</code> 으로 접근</li><li><code>approxQuantile()</code> : 데이터 백분위수 계산 (정확하게 or 근사치로?)</li><li><code>crosstab()</code> : 교차표(cross-tabulation) 확인</li><li><code>freqItems()</code> : 자주 사용하는 항목 쌍 확인<ul><li>crosstab, freqItems 등은 결과가 너무 크면 다 출력 X</li></ul></li><li><code>monotonically_increasing_id()</code> : 모든 로우에 고유 ID 값 추가 (0 ~ )</li></ul></li><li>스파크 새로운 버전 나올 때마다 새로운 함수 생김<ul><li>=&gt; 스파크 공식 문서 참조<ul><li>ex. <code>rand()</code>, <code>randn()</code> (임의 데이터 생성 함수)</li></ul></li><li>최신 버전 StatFunction 패키지는 여러 고급 기법 관련 함수 제공하기도<ul><li>bloom 필터링, sketching algorithms ..</li><li>자세한 내용은 <a href="http://bit.ly/2ptAiY2">API docs</a></li><li>(사실 현시점 최신버전은 아니고 책기준 최신 2.2 버전인 듯)</li></ul></li></ul></li></ul><details><summary class="point-color-can-hover">[6.4] 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;expr, pow&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 두 컬럼 모두 수치형 =&gt; 곱셈 연산 가능 (+ 덧셈, 뺄셈)</span></span><br><span class="line"><span class="keyword">val</span> fabricatedQuantity = pow(col(<span class="string">&quot;Quantity&quot;</span>) * col(<span class="string">&quot;UnitPrice&quot;</span>), <span class="number">2</span>) + <span class="number">5</span></span><br><span class="line">df.select(expr(<span class="string">&quot;CustomerId&quot;</span>), fabricatedQuantity.alias(<span class="string">&quot;realQuantity&quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +----------+------------------+</span></span><br><span class="line"><span class="comment">// |CustomerId|      realQuantity|</span></span><br><span class="line"><span class="comment">// +----------+------------------+</span></span><br><span class="line"><span class="comment">// |   17850.0|239.08999999999997|</span></span><br><span class="line"><span class="comment">// |   17850.0|          418.7156|</span></span><br><span class="line"><span class="comment">// +----------+------------------+</span></span><br><span class="line"></span><br><span class="line">df.selectExpr(</span><br><span class="line">  <span class="string">&quot;CustomerId&quot;</span>,</span><br><span class="line">  <span class="string">&quot;(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity&quot;</span>).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +----------+------------------+</span></span><br><span class="line"><span class="comment">// |CustomerId|      realQuantity|</span></span><br><span class="line"><span class="comment">// +----------+------------------+</span></span><br><span class="line"><span class="comment">// |   17850.0|239.08999999999997|</span></span><br><span class="line"><span class="comment">// |   17850.0|          418.7156|</span></span><br><span class="line"><span class="comment">// +----------+------------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 반올림(round) 예제</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;round, bround&#125;</span><br><span class="line">df.select(round(col(<span class="string">&quot;UnitPrice&quot;</span>), <span class="number">1</span>).alias(<span class="string">&quot;rounded&quot;</span>), col(<span class="string">&quot;UnitPrice&quot;</span>)).show(<span class="number">5</span>)</span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"><span class="comment">// |rounded|UnitPrice|</span></span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"><span class="comment">// |    2.6|     2.55|</span></span><br><span class="line"><span class="comment">// |    3.4|     3.39|</span></span><br><span class="line"><span class="comment">// |    2.8|     2.75|</span></span><br><span class="line"><span class="comment">// |    3.4|     3.39|</span></span><br><span class="line"><span class="comment">// |    3.4|     3.39|</span></span><br><span class="line"><span class="comment">// +-------+---------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 내림(bround) 예제</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.lit</span><br><span class="line">df.select(round(lit(<span class="string">&quot;2.5&quot;</span>)), bround(lit(<span class="string">&quot;2.5&quot;</span>))).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +-------------+--------------+</span></span><br><span class="line"><span class="comment">// |round(2.5, 0)|bround(2.5, 0)|</span></span><br><span class="line"><span class="comment">// +-------------+--------------+</span></span><br><span class="line"><span class="comment">// |          3.0|           2.0|</span></span><br><span class="line"><span class="comment">// |          3.0|           2.0|</span></span><br><span class="line"><span class="comment">// +-------------+--------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 피어슨 상관계수 계산 예제</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;corr&#125;</span><br><span class="line">df.stat.corr(<span class="string">&quot;Quantity&quot;</span>, <span class="string">&quot;UnitPrice&quot;</span>)</span><br><span class="line">df.select(corr(<span class="string">&quot;Quantity&quot;</span>, <span class="string">&quot;UnitPrice&quot;</span>)).show()</span><br><span class="line"><span class="comment">// res52: Double = -0.04112314436835551</span></span><br><span class="line"><span class="comment">// +-------------------------+</span></span><br><span class="line"><span class="comment">// |corr(Quantity, UnitPrice)|</span></span><br><span class="line"><span class="comment">// +-------------------------+</span></span><br><span class="line"><span class="comment">// |     -0.04112314436835551|</span></span><br><span class="line"><span class="comment">// +-------------------------+</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL (동일 표현)</span></span><br><span class="line"><span class="keyword">SELECT</span> customerId, (<span class="built_in">POWER</span>((Quantity <span class="operator">*</span> UnitPrice), <span class="number">2.0</span>) <span class="operator">+</span> <span class="number">5</span>) <span class="keyword">as</span> realQuantity</span><br><span class="line"><span class="keyword">FROM</span> dfTable</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> round(<span class="number">2.5</span>), bround(<span class="number">2.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">corr</span>(Quantity, UnitPrice) <span class="keyword">FROM</span> dfTable</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 콘솔용 요약 통계 (describe)</span></span><br><span class="line">df.describe().show()</span><br><span class="line"><span class="comment">// +-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+</span></span><br><span class="line"><span class="comment">// |summary|        InvoiceNo|         StockCode|         Description|          Quantity|         UnitPrice|        CustomerID|       Country|</span></span><br><span class="line"><span class="comment">// +-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+</span></span><br><span class="line"><span class="comment">// |  count|             3108|              3108|                3098|              3108|              3108|              1968|          3108|</span></span><br><span class="line"><span class="comment">// |   mean| 536516.684944841|27834.304044117645|                null| 8.627413127413128| 4.151946589446603|15661.388719512195|          null|</span></span><br><span class="line"><span class="comment">// | stddev|72.89447869788873|17407.897548583845|                null|26.371821677029203|15.638659854603892|1854.4496996893627|          null|</span></span><br><span class="line"><span class="comment">// |    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|               0.0|           12431.0|     Australia|</span></span><br><span class="line"><span class="comment">// |    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|            607.49|           18229.0|United Kingdom|</span></span><br><span class="line"><span class="comment">// +-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// &#x27;직접 집계&#x27;&#x27; 필요 시 =&gt; 함수 임포트</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;count, mean, stddev_pop, min, max&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// StatFunctions package (다양한 통계 함수) 예제</span></span><br><span class="line"><span class="comment">// approxQuantile() : 데이터 백분위수 계산</span></span><br><span class="line"><span class="keyword">val</span> colName = <span class="string">&quot;UnitPrice&quot;</span></span><br><span class="line"><span class="keyword">val</span> quantileProbs = <span class="type">Array</span>(<span class="number">0.5</span>)</span><br><span class="line"><span class="keyword">val</span> relError = <span class="number">0.05</span></span><br><span class="line">df.stat.approxQuantile(<span class="string">&quot;UnitPrice&quot;</span>, quantileProbs, relError)</span><br><span class="line"><span class="comment">// res61: Array[Double] = Array(2.51)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 1) crosstab() : 교차표 확인</span></span><br><span class="line">df.stat.crosstab(<span class="string">&quot;StockCode&quot;</span>, <span class="string">&quot;Quantity&quot;</span>).show()</span><br><span class="line"><span class="comment">// +------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+</span></span><br><span class="line"><span class="comment">// |StockCode_Quantity| -1|-10|-12| -2|-24| -3| -4| -5| -6| -7|  1| 10|100| 11| 12|120|128| 13| 14|144| 15| 16| 17| 18| 19|192|  2| 20|200| 21|216| 22| 23| 24| 25|252| 27| 28|288|  3| 30| 32| 33| 34| 36|384|  4| 40|432| 47| 48|480|  5| 50| 56|  6| 60|600| 64|  7| 70| 72|  8| 80|  9| 96|</span></span><br><span class="line"><span class="comment">// +------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+</span></span><br><span class="line"><span class="comment">// |             22578|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             21327|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             22064|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             21080|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|</span></span><br><span class="line"><span class="comment">// |             22219|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             21908|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             22818|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |           15056BL|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             72817|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             22545|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             22988|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             22274|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             20750|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |            82616C|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             21703|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             22899|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             22379|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             22422|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             22769|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// |             22585|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|</span></span><br><span class="line"><span class="comment">// +------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 2) freqItems() : 자주 사용하는 항목 쌍 확인</span></span><br><span class="line">df.stat.freqItems(<span class="type">Seq</span>(<span class="string">&quot;StockCode&quot;</span>, <span class="string">&quot;Quantity&quot;</span>)).show()</span><br><span class="line"><span class="comment">// +--------------------+--------------------+</span></span><br><span class="line"><span class="comment">// | StockCode_freqItems|  Quantity_freqItems|</span></span><br><span class="line"><span class="comment">// +--------------------+--------------------+</span></span><br><span class="line"><span class="comment">// |[90214E, 20728, 2...|[200, 128, 23, 32...|</span></span><br><span class="line"><span class="comment">// +--------------------+--------------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 3) monotonically_increasing_id() : 로우에 고유 ID 값 추가</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.monotonically_increasing_id</span><br><span class="line">df.select(monotonically_increasing_id()).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +-----------------------------+</span></span><br><span class="line"><span class="comment">// |monotonically_increasing_id()|</span></span><br><span class="line"><span class="comment">// +-----------------------------+</span></span><br><span class="line"><span class="comment">// |                            0|</span></span><br><span class="line"><span class="comment">// |                            1|</span></span><br><span class="line"><span class="comment">// +-----------------------------+</span></span><br></pre></td></tr></table></figure></details><h3 id="6-5-문자열-데이터-타입-다루기"><a href="#6-5-문자열-데이터-타입-다루기" class="headerlink" title="6.5 문자열 데이터 타입 다루기"></a>6.5 문자열 데이터 타입 다루기</h3><ul><li><p>문자열 다루기 = 거의 모든 데이터 처리 과정에서 발생</p><ul><li>로그 파일에 정규 표현식을 사용한 데이터 추출, 데이터 치환, 문자열 존재 여부, 대/소문자 변환 처리 등</li></ul></li><li><p>대소문자 변환 작업</p><ul><li><code>initcap()</code> =&gt; 공백으로 구분된 모든 단어의 첫 글자 대문자로 변환</li><li><code>lower()</code> (전체 소문자로 변환) / <code>upper()</code> (전체 대문자로 변환)</li></ul></li><li><p>문자열 주변 공백 제거/추가</p><ul><li><p><code>lpad()</code>, <code>ltrim()</code>, <code>rpad()</code>, <code>rtrim()</code>, <code>trim()</code></p><details><summary class="point-color-can-hover">[6.5-1] 예제 펼치기 - 문자열 변환</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;initcap&#125;</span><br><span class="line">df.select(initcap(col(<span class="string">&quot;Description&quot;</span>))).show(<span class="number">2</span>, <span class="literal">false</span>)</span><br><span class="line"><span class="comment">// +----------------------------------+</span></span><br><span class="line"><span class="comment">// |initcap(Description)              |</span></span><br><span class="line"><span class="comment">// +----------------------------------+</span></span><br><span class="line"><span class="comment">// |White Hanging Heart T-light Holder|</span></span><br><span class="line"><span class="comment">// |White Metal Lantern               |</span></span><br><span class="line"><span class="comment">// +----------------------------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;lower, upper&#125;</span><br><span class="line">df.select(col(<span class="string">&quot;Description&quot;</span>),</span><br><span class="line">  lower(col(<span class="string">&quot;Description&quot;</span>)),</span><br><span class="line">  upper(lower(col(<span class="string">&quot;Description&quot;</span>)))).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +--------------------+--------------------+-------------------------+</span></span><br><span class="line"><span class="comment">// |         Description|  lower(Description)|upper(lower(Description))|</span></span><br><span class="line"><span class="comment">// +--------------------+--------------------+-------------------------+</span></span><br><span class="line"><span class="comment">// |WHITE HANGING HEA...|white hanging hea...|     WHITE HANGING HEA...|</span></span><br><span class="line"><span class="comment">// | WHITE METAL LANTERN| white metal lantern|      WHITE METAL LANTERN|</span></span><br><span class="line"><span class="comment">// +--------------------+--------------------+-------------------------+</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;lit, ltrim, rtrim, rpad, lpad, trim&#125;</span><br><span class="line">df.select(</span><br><span class="line">    ltrim(lit(<span class="string">&quot;    HELLO    &quot;</span>)).as(<span class="string">&quot;ltrim&quot;</span>),</span><br><span class="line">    rtrim(lit(<span class="string">&quot;    HELLO    &quot;</span>)).as(<span class="string">&quot;rtrim&quot;</span>),</span><br><span class="line">    trim(lit(<span class="string">&quot;    HELLO    &quot;</span>)).as(<span class="string">&quot;trim&quot;</span>),</span><br><span class="line">    lpad(lit(<span class="string">&quot;HELLO&quot;</span>), <span class="number">3</span>, <span class="string">&quot; &quot;</span>).as(<span class="string">&quot;lp&quot;</span>),</span><br><span class="line">    rpad(lit(<span class="string">&quot;HELLO&quot;</span>), <span class="number">10</span>, <span class="string">&quot; &quot;</span>).as(<span class="string">&quot;rp&quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +---------+---------+-----+---+----------+</span></span><br><span class="line"><span class="comment">// |    ltrim|    rtrim| trim| lp|        rp|</span></span><br><span class="line"><span class="comment">// +---------+---------+-----+---+----------+</span></span><br><span class="line"><span class="comment">// |HELLO    |    HELLO|HELLO|HEL|HELLO     |</span></span><br><span class="line"><span class="comment">// |HELLO    |    HELLO|HELLO|HEL|HELLO     |</span></span><br><span class="line"><span class="comment">// +---------+---------+-----+---+----------+</span></span><br></pre></td></tr></table></figure></details></li></ul></li><li><p>정규표현식</p><ul><li><p>스파크는 <strong>자바 정규 표현식 문법</strong> 사용</p></li><li><p><code>regexp_extract()</code>, <code>regexp_replace()</code> =&gt; 값 추출 및 치환</p></li><li><p><code>translate(column, from_string, to_string)</code> 사용한 치환 가능</p></li><li><p>값 존재 여부 확인 방법?</p><ul><li>스칼라 사용 시 <code>contains()</code> 사용</li><li>파이썬, SQL 사용 시 <code>instr()</code> 사용</li></ul></li><li><p>동적으로 인수의 개수가 변하는 상황에서는</p><ul><li>스칼라 고유 기능 <code>varargs()</code> 사용</li><li>파이썬은 <code>locate()</code> (문자열 위치를 정수로 반환. 위치는 1 ~) + 위치 정보 불리언으로 변환</li></ul><details><summary class="point-color-can-hover">[6.5-2] 예제 펼치기 - 문자열 변환</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.regexp_replace</span><br><span class="line"><span class="keyword">val</span> simpleColors = <span class="type">Seq</span>(<span class="string">&quot;black&quot;</span>, <span class="string">&quot;white&quot;</span>, <span class="string">&quot;red&quot;</span>, <span class="string">&quot;green&quot;</span>, <span class="string">&quot;blue&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> regexString = simpleColors.map(_.toUpperCase).mkString(<span class="string">&quot;|&quot;</span>)</span><br><span class="line"><span class="comment">// the | signifies `OR` in regular expression syntax</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// df.select(</span></span><br><span class="line"><span class="comment">//   regexp_replace(col(&quot;Description&quot;), regexString, &quot;COLOR&quot;).alias(&quot;color_clean&quot;),</span></span><br><span class="line"><span class="comment">//   col(&quot;Description&quot;)).show(2)</span></span><br><span class="line"><span class="comment">// regexString: String = BLACK|WHITE|RED|GREEN|BLUE</span></span><br><span class="line"><span class="comment">// +--------------------+--------------------+</span></span><br><span class="line"><span class="comment">// |         color_clean|         Description|</span></span><br><span class="line"><span class="comment">// +--------------------+--------------------+</span></span><br><span class="line"><span class="comment">// |COLOR HANGING HEA...|WHITE HANGING HEA...|</span></span><br><span class="line"><span class="comment">// | COLOR METAL LANTERN| WHITE METAL LANTERN|</span></span><br><span class="line"><span class="comment">// +--------------------+--------------------+</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.translate</span><br><span class="line">(df.select(translate(col(<span class="string">&quot;Description&quot;</span>), <span class="string">&quot;LEET&quot;</span>, <span class="string">&quot;1337&quot;</span>), col(<span class="string">&quot;Description&quot;</span>))</span><br><span class="line">  .show(<span class="number">2</span>))</span><br><span class="line"><span class="comment">// +----------------------------------+--------------------+</span></span><br><span class="line"><span class="comment">// |translate(Description, LEET, 1337)|         Description|</span></span><br><span class="line"><span class="comment">// +----------------------------------+--------------------+</span></span><br><span class="line"><span class="comment">// |              WHI73 HANGING H3A...|WHITE HANGING HEA...|</span></span><br><span class="line"><span class="comment">// |               WHI73 M37A1 1AN73RN| WHITE METAL LANTERN|</span></span><br><span class="line"><span class="comment">// +----------------------------------+--------------------+</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.regexp_extract</span><br><span class="line"><span class="keyword">val</span> regexString = simpleColors.map(_.toUpperCase).mkString(<span class="string">&quot;(&quot;</span>, <span class="string">&quot;|&quot;</span>, <span class="string">&quot;)&quot;</span>)</span><br><span class="line"><span class="comment">// the | signifies OR in regular expression syntax</span></span><br><span class="line">df.select(</span><br><span class="line">     regexp_extract(col(<span class="string">&quot;Description&quot;</span>), regexString, <span class="number">1</span>).alias(<span class="string">&quot;color_clean&quot;</span>),</span><br><span class="line">     col(<span class="string">&quot;Description&quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +-----------+--------------------+</span></span><br><span class="line"><span class="comment">// |color_clean|         Description|</span></span><br><span class="line"><span class="comment">// +-----------+--------------------+</span></span><br><span class="line"><span class="comment">// |      WHITE|WHITE HANGING HEA...|</span></span><br><span class="line"><span class="comment">// |      WHITE| WHITE METAL LANTERN|</span></span><br><span class="line"><span class="comment">// +-----------+--------------------+</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> containsBlack = col(<span class="string">&quot;Description&quot;</span>).contains(<span class="string">&quot;BLACK&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> containsWhite = col(<span class="string">&quot;DESCRIPTION&quot;</span>).contains(<span class="string">&quot;WHITE&quot;</span>)</span><br><span class="line">(df.withColumn(<span class="string">&quot;hasSimpleColor&quot;</span>, containsBlack.or(containsWhite))</span><br><span class="line">  .where(<span class="string">&quot;hasSimpleColor&quot;</span>)</span><br><span class="line">  .select(<span class="string">&quot;Description&quot;</span>).show(<span class="number">3</span>, <span class="literal">false</span>))</span><br><span class="line"><span class="comment">// +----------------------------------+</span></span><br><span class="line"><span class="comment">// |Description                       |</span></span><br><span class="line"><span class="comment">// +----------------------------------+</span></span><br><span class="line"><span class="comment">// |WHITE HANGING HEART T-LIGHT HOLDER|</span></span><br><span class="line"><span class="comment">// |WHITE METAL LANTERN               |</span></span><br><span class="line"><span class="comment">// |RED WOOLLY HOTTIE WHITE HEART.    |</span></span><br><span class="line"><span class="comment">// +----------------------------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> simpleColors = <span class="type">Seq</span>(<span class="string">&quot;black&quot;</span>, <span class="string">&quot;white&quot;</span>, <span class="string">&quot;red&quot;</span>, <span class="string">&quot;green&quot;</span>, <span class="string">&quot;blue&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> selectedColumns = simpleColors.map(color =&gt; &#123;</span><br><span class="line">   col(<span class="string">&quot;Description&quot;</span>).contains(color.toUpperCase).alias(<span class="string">s&quot;is_<span class="subst">$color</span>&quot;</span>)</span><br><span class="line">&#125;):+expr(<span class="string">&quot;*&quot;</span>) <span class="comment">// Column 타입이여야 합니다</span></span><br><span class="line">(df.select(selectedColumns:_*).where(col(<span class="string">&quot;is_white&quot;</span>).or(col(<span class="string">&quot;is_red&quot;</span>)))</span><br><span class="line">  .select(<span class="string">&quot;Description&quot;</span>).show(<span class="number">3</span>, <span class="literal">false</span>))</span><br><span class="line"><span class="comment">// +----------------------------------+</span></span><br><span class="line"><span class="comment">// |Description                       |</span></span><br><span class="line"><span class="comment">// +----------------------------------+</span></span><br><span class="line"><span class="comment">// |WHITE HANGING HEART T-LIGHT HOLDER|</span></span><br><span class="line"><span class="comment">// |WHITE METAL LANTERN               |</span></span><br><span class="line"><span class="comment">// |RED WOOLLY HOTTIE WHITE HEART.    |</span></span><br><span class="line"><span class="comment">// +----------------------------------+</span></span><br></pre></td></tr></table></figure></details></li></ul></li></ul><h3 id="6-6-날짜와-타임스탬프-데이터-타입-다루기"><a href="#6-6-날짜와-타임스탬프-데이터-타입-다루기" class="headerlink" title="6.6 날짜와 타임스탬프 데이터 타입 다루기"></a>6.6 날짜와 타임스탬프 데이터 타입 다루기</h3><ul><li><p>날짜/시간 사용 시 시간대 (timezone) 와 포맷의 유효성 확인 필요</p><ul><li>=&gt; 스파크는 두 가지 정보만 집중적으로 관리</li><li><strong>날짜</strong> (date) &amp; <strong>타임스탬프</strong> (timestamp)</li><li>inferSchema 옵션 활성화된 경우, 두 정보를 포함해 데이터 타입을 최대한 정확히 식별</li><li>스파크는 특정 날짜 포맷 명시 없이도 자체적으로 식별</li></ul></li><li><p>날짜, 시간을 문자열로 저장 &lt;-&gt; 런타임에 날짜 타입으로 변환</p><ul><li>텍스트, CSV 파일 다룰 시 많이 발생하는 방식</li><li>스파크 2.1 이하) 시간대 미지정 시, 시스템 시간대 기준으로 파싱<ul><li>시간대 설정? =&gt; <code>spark.conf.sessionLocalTimeZone</code> 속성을 로컬 시간대로 지정 - <a href="https://bit.ly/2NcW6p2">Java TimeZone 포맷</a> 따름)</li></ul></li><li>스파크 2.3 이상) <code>spark.conf.set(&quot;spark.sql.session.timeZone&quot;, &quot;UTC&quot;)</code> 으로 사용 가능</li></ul></li><li><p>TimestampType 클래스는 초 단위 정밀도까지만 지원</p><ul><li>밀리세컨드(ms), 마이크로세컨드(μs) 지원 X =&gt; 필요 시 Long 데이터타입 사용해서 우회</li><li>특이한 포맷의 날짜/시간 데이터를 다뤄야한다면 =&gt; 각 단계별 데이터타입과 포맷 정확히 파악 후 트랜스포메이션 적용 해야함</li></ul></li><li><p>스파크는 특정 시점에 데이터 포맷이 특이하게 변할 수 있다</p><ul><li>싫다면 파싱이나 변환 작업 필요</li><li>스파크는 <strong>자바의 날짜와 타임스탬프</strong> 사용 (표준 체계)</li></ul></li><li><p>자주 사용하는 함수</p><ul><li><p>오늘 기준으로 N일 전후 날짜 구하기</p><ul><li><code>date_sub(컬럼, 뺄 날짜 수)</code> (책에는 sum, 오타?)</li><li><code>date_add(컬럼, 더할 날짜 수)</code> </li></ul></li><li><p>두 날짜 사이 차이 구하기</p><ul><li><code>datediff(컬럼1, 컬럼2)</code> : 두 날짜 사이 일 수 반환</li><li><code>months_between(컬럼1, 컬럼2)</code> : 두 날짜 사이 개월 수 반환</li></ul><details><summary class="point-color-can-hover">[6.6-1] 예제 펼치기 - 날짜 구하기 및 비교</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// df.printSchema()</span></span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">//  |-- InvoiceNo: string (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- StockCode: string (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- Description: string (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- Quantity: integer (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- InvoiceDate: timestamp (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- UnitPrice: double (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- CustomerID: double (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- Country: string (nullable = true)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 예제1) 오늘 날짜 / 현재 타임스탬프 값 구하기</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;current_date, current_timestamp&#125;</span><br><span class="line"><span class="keyword">val</span> dateDF = (spark.range(<span class="number">10</span>)</span><br><span class="line">  .withColumn(<span class="string">&quot;today&quot;</span>, current_date())</span><br><span class="line">  .withColumn(<span class="string">&quot;now&quot;</span>, current_timestamp()))</span><br><span class="line">dateDF.createOrReplaceTempView(<span class="string">&quot;dateTable&quot;</span>)</span><br><span class="line"></span><br><span class="line">dateDF.printSchema()</span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">//  |-- id: long (nullable = false)</span></span><br><span class="line"><span class="comment">//  |-- today: date (nullable = false)</span></span><br><span class="line"><span class="comment">//  |-- now: timestamp (nullable = false)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 예제2) 오늘 기준으로 5일 전 날짜 구하기</span></span><br><span class="line"><span class="comment">// -- SQL : SELECT date_sub(today, 5), date_add(today, 5) FROM dateTable</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;date_add, date_sub&#125;</span><br><span class="line">dateDF.select(date_sub(col(<span class="string">&quot;today&quot;</span>), <span class="number">5</span>), date_add(col(<span class="string">&quot;today&quot;</span>), <span class="number">5</span>)).show(<span class="number">1</span>)</span><br><span class="line"><span class="comment">// +------------------+------------------+</span></span><br><span class="line"><span class="comment">// |date_sub(today, 5)|date_add(today, 5)|</span></span><br><span class="line"><span class="comment">// +------------------+------------------+</span></span><br><span class="line"><span class="comment">// |        2021-02-01|        2021-02-11|</span></span><br><span class="line"><span class="comment">// +------------------+------------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 예제3) 두 날짜 사이 차이 일수(개월수) 구하기</span></span><br><span class="line"><span class="comment">// -- SQL : SELECT to_date(&#x27;2016-01-01&#x27;), months_between(&#x27;2016-01-01&#x27;, &#x27;2017-01-01&#x27;),</span></span><br><span class="line"><span class="comment">// datediff(&#x27;2016-01-01&#x27;, &#x27;2017-01-01&#x27;)</span></span><br><span class="line"><span class="comment">// FROM dateTable</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;datediff, months_between, to_date&#125;</span><br><span class="line">(dateDF.withColumn(<span class="string">&quot;week_ago&quot;</span>, date_sub(col(<span class="string">&quot;today&quot;</span>), <span class="number">7</span>))</span><br><span class="line">  .select(datediff(col(<span class="string">&quot;week_ago&quot;</span>), col(<span class="string">&quot;today&quot;</span>))).show(<span class="number">1</span>))</span><br><span class="line">(dateDF.select(</span><br><span class="line">    to_date(lit(<span class="string">&quot;2016-01-01&quot;</span>)).alias(<span class="string">&quot;start&quot;</span>),</span><br><span class="line">    to_date(lit(<span class="string">&quot;2017-05-22&quot;</span>)).alias(<span class="string">&quot;end&quot;</span>))</span><br><span class="line">  .select(months_between(col(<span class="string">&quot;start&quot;</span>), col(<span class="string">&quot;end&quot;</span>))).show(<span class="number">1</span>))</span><br><span class="line"><span class="comment">// +-------------------------+</span></span><br><span class="line"><span class="comment">// |datediff(week_ago, today)|</span></span><br><span class="line"><span class="comment">// +-------------------------+</span></span><br><span class="line"><span class="comment">// |                       -7|</span></span><br><span class="line"><span class="comment">// +-------------------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// +--------------------------+</span></span><br><span class="line"><span class="comment">// |months_between(start, end)|</span></span><br><span class="line"><span class="comment">// +--------------------------+</span></span><br><span class="line"><span class="comment">// |              -16.67741935|</span></span><br><span class="line"><span class="comment">// +--------------------------+</span></span><br></pre></td></tr></table></figure></details></li></ul></li><li><p>날짜 변환 및 파싱</p><ul><li>올바른 포맷과 타입 사용 시 매우 쉬움</li><li>날짜나 타임스탬프 타입 사용 or ‘yyy-MM-dd’ 포맷에 맞는 문자열 지정</li><li><code>to_date()</code> : 문자열 =&gt; 날짜로 변환 (option. 날짜 포맷 지정 가능)<ul><li>날짜 포맷 : 자바의 <a href="https://bit.ly/2Mz21Qc">SimpleDateFormat 클래스 지원 포맷 </a>사용</li></ul></li><li><code>to_timestamp()</code> : 날짜 포맷 필수 (미지정시 ‘yyyy-MM-dd HH:mm:ss’ 포맷 default)</li></ul></li><li><p>날짜 파싱 실패 시?</p><ul><li><p>=&gt; <strong>null 반환</strong> (에러 X)</p></li><li><p>예상치 못한 포맷의 데이터가 나타날 수 있으므로 디버깅 어려움</p></li><li><p>문제 회피할 수 있는 방식</p><ul><li>1. 자바 <a href="https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html">SimpleDateFormat</a> 표준에 맞춰 날짜 포맷 지정</li><li>2. <code>to_date()</code>, <code>to_timestamp()</code> 사용</li></ul></li><li><p>암시적 형변환(implicit type casting)은 위험 =&gt; 명시적으로 데이터 타입 변환해서 사용할 것</p><details><summary class="point-color-can-hover">[6.6-2] 예제 펼치기 - 날짜 변환 및 파싱</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// to_date(문자열) : 문자열 -&gt; 날짜 </span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;to_date, lit&#125;</span><br><span class="line">(spark.range(<span class="number">5</span>).withColumn(<span class="string">&quot;date&quot;</span>, lit(<span class="string">&quot;2017-01-01&quot;</span>))</span><br><span class="line">  .select(to_date(col(<span class="string">&quot;date&quot;</span>))).show(<span class="number">1</span>))</span><br><span class="line"><span class="comment">// +---------------+</span></span><br><span class="line"><span class="comment">// |to_date(`date`)|</span></span><br><span class="line"><span class="comment">// +---------------+</span></span><br><span class="line"><span class="comment">// |     2017-01-01|</span></span><br><span class="line"><span class="comment">// +---------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// SimpleDateFormate 클래스 지원 포맷 사용해야</span></span><br><span class="line">dateDF.select(to_date(lit(<span class="string">&quot;2016-20-12&quot;</span>)),to_date(lit(<span class="string">&quot;2017-12-11&quot;</span>))).show(<span class="number">1</span>)</span><br><span class="line">+---------------------+---------------------+</span><br><span class="line">|to_date(<span class="symbol">&#x27;2016</span><span class="number">-20</span><span class="number">-12</span>&#x27;)|to_date(<span class="symbol">&#x27;2017</span><span class="number">-12</span><span class="number">-11</span>&#x27;)|</span><br><span class="line">+---------------------+---------------------+</span><br><span class="line">|                 <span class="literal">null</span>|           <span class="number">2017</span><span class="number">-12</span><span class="number">-11</span>|</span><br><span class="line">+---------------------+---------------------+</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// to_date(문자열, 날짜 포맷) =&gt; 날짜포맷 Option</span></span><br><span class="line"><span class="comment">// to_timestamp(문자열, 날짜 포맷) =&gt; 날짜포맷 필수</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.to_date</span><br><span class="line"><span class="keyword">val</span> dateFormat = <span class="string">&quot;yyyy-dd-MM&quot;</span></span><br><span class="line"><span class="keyword">val</span> cleanDateDF = spark.range(<span class="number">1</span>).select(</span><br><span class="line">    to_date(lit(<span class="string">&quot;2017-12-11&quot;</span>), dateFormat).alias(<span class="string">&quot;date&quot;</span>),</span><br><span class="line">    to_date(lit(<span class="string">&quot;2017-20-12&quot;</span>), dateFormat).alias(<span class="string">&quot;date2&quot;</span>))</span><br><span class="line">cleanDateDF.createOrReplaceTempView(<span class="string">&quot;dateTable2&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// cleanDateDF.show()</span></span><br><span class="line"><span class="comment">// +----------+----------+</span></span><br><span class="line"><span class="comment">// |      date|     date2|</span></span><br><span class="line"><span class="comment">// +----------+----------+</span></span><br><span class="line"><span class="comment">// |2017-11-12|2017-12-20|</span></span><br><span class="line"><span class="comment">// +----------+----------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.to_timestamp</span><br><span class="line">cleanDateDF.select(to_timestamp(col(<span class="string">&quot;date&quot;</span>), dateFormat)).show()</span><br><span class="line"><span class="comment">// +----------------------------------+</span></span><br><span class="line"><span class="comment">// |to_timestamp(`date`, &#x27;yyyy-dd-MM&#x27;)|</span></span><br><span class="line"><span class="comment">// +----------------------------------+</span></span><br><span class="line"><span class="comment">// |               2017-11-12 00:00:00|</span></span><br><span class="line"><span class="comment">// +----------------------------------+</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL (같은 표현, to_date(), to_timestamp())</span></span><br><span class="line"><span class="keyword">SELECT</span> to_date(<span class="type">date</span>, <span class="string">&#x27;yyyy-dd-MM&#x27;</span>), to_date(date2, <span class="string">&#x27;yyyy-dd-MM&#x27;</span>), to_date(<span class="type">date</span>)</span><br><span class="line"><span class="keyword">FROM</span> dateTable2</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> to_timestamp(<span class="type">date</span>, <span class="string">&#x27;yyyy-dd-MM&#x27;</span>), to_timestamp(date2, <span class="string">&#x27;yyyy-dd-MM&#x27;</span>)</span><br><span class="line"><span class="keyword">FROM</span> dateTable2</span><br></pre></td></tr></table></figure></details></li><li><p>날짜 &lt;-&gt; 타임스탬프 변환</p><ul><li>SQL (easy)<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">cast</span>(to_date(&quot;2017-01-01&quot;, &quot;yyyy-dd-MM&quot;) <span class="keyword">as</span> <span class="type">timestamp</span>)</span><br></pre></td></tr></table></figure></li><li>올바른 포맷과 타입의 날짜와 타임스탬프 사용 시에는 매우 쉽게 비교할 수 있다<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 날짜, 타임스탬프 타입 사용 or &quot;yyyy-MM-dd&quot; 포맷 문자열 사용</span></span><br><span class="line">cleanDateDF.filter(col(<span class="string">&quot;date2&quot;</span>) &gt; lit(<span class="string">&quot;2017-12-12&quot;</span>)).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 스파크가 리터럴로 인식하는 문자열 지정해서 비교도 가능</span></span><br><span class="line">cleanDateDF.filter(col(<span class="string">&quot;date2&quot;</span>) &gt; <span class="string">&quot;&#x27;2017-12-12&#x27;&quot;</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// +----------+----------+</span></span><br><span class="line"><span class="comment">// |      date|     date2|</span></span><br><span class="line"><span class="comment">// +----------+----------+</span></span><br><span class="line"><span class="comment">// |2017-11-12|2017-12-20|</span></span><br><span class="line"><span class="comment">// +----------+----------+</span></span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul><h3 id="6-7-null-값-다루기"><a href="#6-7-null-값-다루기" class="headerlink" title="6.7 null 값 다루기"></a>6.7 null 값 다루기</h3><ul><li><p>DataFrame 에서 빈 값은 <strong>NULL</strong> 로 표현하는게 좋다</p><ul><li>스파크에서는 null 을 사용해야 최적화 수행 (빈 문자열 X, 대체값 X)</li></ul></li><li><p>DataFrame 에서 null 을 다루는 기본 방식 =&gt; <code>.na</code></p><ul><li>DataFrame의 하위 패키지</li><li>연산 수행 중 null 값 제어 방식을 명시적으로 지정하는 함수는 =&gt; 5.4.15 로우정렬하기 / 6.3 불리언 데이터 타입 다루기 참조</li></ul></li><li><p>null 값을 다루는 두가지 방식</p><ul><li>1. 명시적으로 null 값 제거</li><li>2. 전역 or 컬럼 단위로 null 값을 특정 값으로 채우기<blockquote><p>null 값은 명시적으로 사용 권장.<br>그러나 null 값을 허용하지 않는 컬럼 선언해도 <strong>강제성 없음</strong></p><ul><li>즉, notnull 컬럼이여도 null 값이 있을 수 있다</li><li>nullable 속성은 스파크 SQL 옵티마이저가 해당 컬럼을 제어하는 동작을 단순하게 도울 뿐</li></ul></blockquote></li></ul></li><li><p><code>coalesce()</code></p><ul><li><p>인수의 여러 컬럼 중 null 이 아닌 첫번째 값 반환</p></li><li><p>모든 컬럼이 null이 아닌 값을 가지면 첫 번째 컬럼 값 반환</p><details><summary class="point-color-can-hover">[6.7-1] 예제 펼치기 - coalesce()</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.coalesce</span><br><span class="line"></span><br><span class="line"><span class="comment">// Description 컬럼 값 null 체크</span></span><br><span class="line"><span class="comment">//  1. null이면 CustomerId 값 반환</span></span><br><span class="line"><span class="comment">//  2. null이 아니면 Description 컬럼 값 반환</span></span><br><span class="line">df.select(coalesce(col(<span class="string">&quot;Description&quot;</span>), col(<span class="string">&quot;CustomerId&quot;</span>))).show()</span><br></pre></td></tr></table></figure></details></li></ul></li><li><p>SQL 함수</p><ul><li><p><code>ifnull()</code> : 첫 번째 값이 null이면 두 번째 값 반환, null이 아니면 첫 번째 값 반환</p></li><li><p><code>nullif()</code> : 두 값이 같으면 null 반환, 다르면 첫 번째 값 반환</p></li><li><p><code>nvl()</code> : 첫 번째 값이 null이면 두 번째 값 반환, null이 아니면 첫 번째 값 반환</p></li><li><p><code>nvl2()</code> : 첫 번째 값이 null이 아니면 두 번째 값 반환, null이면 세 번째 값 반환</p><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 이해하는 용도.. like this</span></span><br><span class="line"><span class="function"><span class="keyword">fun</span> <span class="title">ifnull</span><span class="params">(first: <span class="type">Any</span>, default: <span class="type">Any</span>)</span></span> = <span class="keyword">if</span> (first != <span class="literal">null</span>) first <span class="keyword">else</span> default</span><br><span class="line"><span class="function"><span class="keyword">fun</span> <span class="title">nullif</span><span class="params">(first: <span class="type">Any</span>, second: <span class="type">Any</span>)</span></span> = <span class="keyword">if</span> (first != second) first <span class="keyword">else</span> <span class="literal">null</span></span><br><span class="line"><span class="function"><span class="keyword">fun</span> <span class="title">nvl</span><span class="params">(first: <span class="type">Any</span>, default: <span class="type">Any</span>)</span></span> = <span class="keyword">if</span> (first != <span class="literal">null</span>) first <span class="keyword">else</span> default</span><br><span class="line"><span class="function"><span class="keyword">fun</span> <span class="title">nvl2</span><span class="params">(first: <span class="type">Any</span>, notnull_return: <span class="type">Any</span>, null_return: <span class="type">Any</span>)</span></span> = <span class="keyword">if</span> (first != <span class="literal">null</span>) notnull_return <span class="keyword">else</span> null_return</span><br></pre></td></tr></table></figure><details><summary class="point-color-can-hover">[6.7-2] 예제 펼치기 - SQL 함수</summary><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">  ifnull(<span class="keyword">null</span>, <span class="string">&#x27;return_value&#x27;</span>),</span><br><span class="line">  <span class="built_in">nullif</span>(<span class="string">&#x27;value&#x27;</span>, <span class="string">&#x27;value&#x27;</span>),</span><br><span class="line">  nvl(<span class="keyword">null</span>, <span class="string">&#x27;return_value&#x27;</span>),</span><br><span class="line">  nvl2(<span class="string">&#x27;not_null&#x27;</span>, <span class="string">&#x27;return_value&#x27;</span>, &quot;else_value&quot;)</span><br><span class="line"><span class="keyword">FROM</span> dfTable LIMIT <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- +------------+----+------------+------------+</span></span><br><span class="line"><span class="comment">-- |           a|   b|           c|           d|</span></span><br><span class="line"><span class="comment">-- +------------+----+------------+------------+</span></span><br><span class="line"><span class="comment">-- |return_value|null|return_value|return_value|</span></span><br><span class="line"><span class="comment">-- +------------+----+------------+------------+</span></span><br></pre></td></tr></table></figure></details></li></ul></li></ul><ul><li><p><code>drop()</code></p><ul><li><p>null 값을 가진 로우를 모든 로우 제거</p></li><li><p>인수</p><ul><li><code>any</code> (하나라도 null이면 제거) / <code>all</code> (모든 컬럼이 null 또는 NaN이면 제거)</li><li>배열 형태 컬럼도 인수로 전달 가능</li></ul></li><li><p>SQL 사용 시 컬럼별로 수행해야함</p><details><summary class="point-color-can-hover">[6.7-3] 예제 펼치기 - drop()</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df.na.drop()</span><br><span class="line">df.na.drop(<span class="string">&quot;any&quot;</span>) <span class="comment">// 하나라도 컬럼이 null (또는 NaN) 이면 로우 제거</span></span><br><span class="line">df.na.drop(<span class="string">&quot;all&quot;</span>) <span class="comment">// 모든 컬럼이 null (또는 NaN) 이면 로우 제거</span></span><br><span class="line"></span><br><span class="line">df.na.drop(<span class="string">&quot;all&quot;</span>, <span class="type">Seq</span>(<span class="string">&quot;StockCode&quot;</span>, <span class="string">&quot;InvoiceNo&quot;</span>)) <span class="comment">// 컬럼(배열형태) 전달 가능</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL 사용 시 컬럼 별 수행해야</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> dfTable <span class="keyword">WHERE</span> Description <span class="keyword">IS</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span></span><br></pre></td></tr></table></figure></details></li></ul></li><li><p><code>fill()</code></p><ul><li><p>하나 이상의 컬럼을 특정 값으로 채움</p></li><li><p>인수</p><ul><li>채워넣을 값, 컬럽 집합으로 구성된 맵</li><li>컬럼명 배열로 인수 사용 및 다수 컬럼에도 적용 가능 (ex. <code>df.na.fill(5:Integer)</code>, <code>df.na.fill(5:Double)</code>)</li></ul></li><li><p>스칼라 Map 타입 사용도 인수로 가능 (key:value = 컬럼명:null값을 채울 값)</p><details><summary class="point-color-can-hover">[6.7-4] 예제 펼치기 - fill()</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">df.na.fill(<span class="string">&quot;All Null values become this string&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 다수 컬럼에 적용</span></span><br><span class="line">df.na.fill(<span class="number">5</span>, <span class="type">Seq</span>(<span class="string">&quot;StockCode&quot;</span>, <span class="string">&quot;InvoiceNo&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// Map 타입으로 다수 컬럼에 적용</span></span><br><span class="line"><span class="keyword">val</span> fillColValues = <span class="type">Map</span>(<span class="string">&quot;StockCode&quot;</span> -&gt; <span class="number">5</span>, <span class="string">&quot;Description&quot;</span> -&gt; <span class="string">&quot;No Value&quot;</span>)</span><br><span class="line">df.na.fill(fillColValues)</span><br></pre></td></tr></table></figure></details></li></ul></li><li><p><code>relace()</code></p><ul><li><p>조건에 따라 다른 값으로 대체</p></li><li><p>단, 변경하고자하는 값 == 원래 값 데이터 타입 같아야</p><details><summary class="point-color-can-hover">[6.7-5] 예제 펼치기 - replace()</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.na.replace(<span class="string">&quot;Description&quot;</span>, <span class="type">Map</span>(<span class="string">&quot;&quot;</span> -&gt; <span class="string">&quot;UNKNOWN&quot;</span>))</span><br></pre></td></tr></table></figure></details></li></ul></li></ul><h3 id="6-8-정렬하기"><a href="#6-8-정렬하기" class="headerlink" title="6.8 정렬하기"></a>6.8 정렬하기</h3><ul><li><code>asc_nulls_first()</code>, <code>desc_nulls_first()</code>, <code>asc_nulls_last()</code>, <code>desc_nulls_last()</code><ul><li>DataFrame 정렬 시 null 값 표시 기준 지정 가능</li></ul></li><li>(=&gt; <a href="https://minsw.github.io/2021/01/26/Spark-The-Definitive-Guide-5%EC%9E%A5/#15-%EB%A1%9C%EC%9A%B0-%EC%A0%95%EB%A0%AC%ED%95%98%EA%B8%B0">5.4.15 - 로우정렬하기</a> 다시 참고~)</li></ul><h3 id="6-9-복합-데이터-타입-다루기"><a href="#6-9-복합-데이터-타입-다루기" class="headerlink" title="6.9 복합 데이터 타입 다루기"></a>6.9 복합 데이터 타입 다루기</h3><ul><li><p>복합 데이터 타입 : 구조체 (struct), 배열 (array), 맵(map)</p></li><li><p>구조체 = DataFrame 내부의 DataFrame</p><ul><li>쿼리문에서 다수의 컬럼을 괄호로 묶어서 =&gt; 구조체 만듦</li><li>복합 데이터 타입을 가진 DataFrame 사용<ul><li>=&gt; 다른 DataFrame 조회하는것과 동일하게 사용 가능</li><li>차이점은 문법에 점 (.) 사용 or <code>getField()</code> 사용</li><li><code>*</code> 문자로 모든 값 조회 가능 (모든 컬럼을 최상위 수준으로 끌어올리기 가능?)</li></ul></li></ul><details><summary class="point-color-can-hover">[6.8,1] 예제 펼치기 - 구조체</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">df.selectExpr(<span class="string">&quot;(Description, InvoiceNo) as complex&quot;</span>, <span class="string">&quot;*&quot;</span>)</span><br><span class="line">df.selectExpr(<span class="string">&quot;struct(Description, InvoiceNo) as complex&quot;</span>, <span class="string">&quot;*&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.struct</span><br><span class="line"><span class="keyword">val</span> complexDF = df.select(struct(<span class="string">&quot;Description&quot;</span>, <span class="string">&quot;InvoiceNo&quot;</span>).alias(<span class="string">&quot;complex&quot;</span>))</span><br><span class="line">complexDF.createOrReplaceTempView(<span class="string">&quot;complexDF&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 조회 시 점(.) 또는 getField() 사용</span></span><br><span class="line">complexDF.select(<span class="string">&quot;complex.Description&quot;</span>)</span><br><span class="line">complexDF.select(col(<span class="string">&quot;complex&quot;</span>).getField(<span class="string">&quot;Description&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// * 로 모든 값 조회 가능</span></span><br><span class="line">complexDF.select(<span class="string">&quot;complex.*&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> complex.<span class="operator">*</span> <span class="keyword">FROM</span> complexDF</span><br></pre></td></tr></table></figure></details></li><li><p>배열</p><ul><li>example) 해당하는 컬럼의 모든 단어를 하나의 로우로 변환</li><li><code>split(target, delimiter)</code> : 구분자 기준으로 나누어 배열로 변환<ul><li>복합 데이터 타입을 또 다른 컬럼처럼 다룰 수 있는 기능</li></ul></li><li>배열의 길이 : 배열 size 조회해서 길이 알 수 있음</li><li><code>array_contains()</code> : 배열에 특정 값 존재하는지 확인 가능<ul><li>하지만 시나리오 완성은 불가능</li></ul></li><li><code>explode(배열타입 칼럼)</code> : 인수의 컬럼 배열 갑셍 포함된 모든 값을 로우로 변환 (나머지 컬럼 값은 중복되어 표시)</li></ul><details><summary class="point-color-can-hover">[6.8.2] 예제 펼치기 - 배열</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// split() : 배열로 변환</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.split</span><br><span class="line">df.select(split(col(<span class="string">&quot;Description&quot;</span>), <span class="string">&quot; &quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +---------------------+</span></span><br><span class="line"><span class="comment">// |split(Description,  )|</span></span><br><span class="line"><span class="comment">// +---------------------+</span></span><br><span class="line"><span class="comment">// | [WHITE, HANGING, ...|</span></span><br><span class="line"><span class="comment">// | [WHITE, METAL, LA...|</span></span><br><span class="line"><span class="comment">// +---------------------+</span></span><br><span class="line"></span><br><span class="line">(df.select(split(col(<span class="string">&quot;Description&quot;</span>), <span class="string">&quot; &quot;</span>).alias(<span class="string">&quot;array_col&quot;</span>))</span><br><span class="line">  .selectExpr(<span class="string">&quot;array_col[0]&quot;</span>).show(<span class="number">2</span>))</span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |array_col[0]|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"><span class="comment">// |       WHITE|</span></span><br><span class="line"><span class="comment">// |       WHITE|</span></span><br><span class="line"><span class="comment">// +------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// size() : 배열의 길이</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.size</span><br><span class="line">df.select(size(split(col(<span class="string">&quot;Description&quot;</span>), <span class="string">&quot; &quot;</span>))).show(<span class="number">2</span>) <span class="comment">// shows 5 and 3</span></span><br><span class="line"><span class="comment">// +---------------------------+</span></span><br><span class="line"><span class="comment">// |size(split(Description,  ))|</span></span><br><span class="line"><span class="comment">// +---------------------------+</span></span><br><span class="line"><span class="comment">// |                          5|</span></span><br><span class="line"><span class="comment">// |                          3|</span></span><br><span class="line"><span class="comment">// +---------------------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// array_contains() : 배열에 특정값 존재하는지 확인</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.array_contains</span><br><span class="line">df.select(array_contains(split(col(<span class="string">&quot;Description&quot;</span>), <span class="string">&quot; &quot;</span>), <span class="string">&quot;WHITE&quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +--------------------------------------------+</span></span><br><span class="line"><span class="comment">// |array_contains(split(Description,  ), WHITE)|</span></span><br><span class="line"><span class="comment">// +--------------------------------------------+</span></span><br><span class="line"><span class="comment">// |                                        true|</span></span><br><span class="line"><span class="comment">// |                                        true|</span></span><br><span class="line"><span class="comment">// +--------------------------------------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// explode() : 입력된 컬럼의 배열값(split(Description) 결과물) 에 포함된 모든 값을 로우로 변환 (나머지값(InvoiceNo)은 중복)</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;split, explode&#125;</span><br><span class="line">(df.withColumn(<span class="string">&quot;splitted&quot;</span>, split(col(<span class="string">&quot;Description&quot;</span>), <span class="string">&quot; &quot;</span>))</span><br><span class="line">  .withColumn(<span class="string">&quot;exploded&quot;</span>, explode(col(<span class="string">&quot;splitted&quot;</span>)))</span><br><span class="line">  .select(<span class="string">&quot;Description&quot;</span>, <span class="string">&quot;InvoiceNo&quot;</span>, <span class="string">&quot;exploded&quot;</span>).show(<span class="number">2</span>))</span><br><span class="line"><span class="comment">// +--------------------+---------+--------+</span></span><br><span class="line"><span class="comment">// |         Description|InvoiceNo|exploded|</span></span><br><span class="line"><span class="comment">// +--------------------+---------+--------+</span></span><br><span class="line"><span class="comment">// |WHITE HANGING HEA...|   536365|   WHITE|</span></span><br><span class="line"><span class="comment">// |WHITE HANGING HEA...|   536365| HANGING|</span></span><br><span class="line"><span class="comment">// +--------------------+---------+--------+</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- split()</span></span><br><span class="line"><span class="keyword">SELECT</span> split(Description, <span class="string">&#x27; &#x27;</span>) <span class="keyword">FROM</span> dfTable</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> split(Description, <span class="string">&#x27; &#x27;</span>)[<span class="number">0</span>] <span class="keyword">FROM</span> dfTable</span><br><span class="line"></span><br><span class="line"><span class="comment">-- array_contains()</span></span><br><span class="line"><span class="keyword">SELECT</span> array_contains(split(Description, <span class="string">&#x27; &#x27;</span>), <span class="string">&#x27;WHITE&#x27;</span>) <span class="keyword">FROM</span> dfTable</span><br><span class="line"></span><br><span class="line"><span class="comment">-- explode()</span></span><br><span class="line"><span class="keyword">SELECT</span> Description, InvoiceNo, exploded</span><br><span class="line"><span class="keyword">FROM</span> (<span class="keyword">SELECT</span> <span class="operator">*</span>, split(Description, &quot; &quot;) <span class="keyword">as</span> splitted <span class="keyword">FROM</span> dfTable)</span><br><span class="line"><span class="keyword">LATERAL</span> <span class="keyword">VIEW</span> explode(splitted) <span class="keyword">as</span> exploded</span><br></pre></td></tr></table></figure></details></li><li><p>맵</p><ul><li><code>map()</code> + 키-값 쌍</li><li>적합한 키로 데이터 조회 가능, 없을 시 null 반환</li><li>map 타입 분해 -&gt; 컬럼 변환 가능</li></ul><details><summary class="point-color-can-hover">[6.8.3] 예제 펼치기 - 맵</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.map</span><br><span class="line">df.select(map(col(<span class="string">&quot;Description&quot;</span>), col(<span class="string">&quot;InvoiceNo&quot;</span>)).alias(<span class="string">&quot;complex_map&quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +--------------------+</span></span><br><span class="line"><span class="comment">// |         complex_map|</span></span><br><span class="line"><span class="comment">// +--------------------+</span></span><br><span class="line"><span class="comment">// |[WHITE HANGING HE...|</span></span><br><span class="line"><span class="comment">// |[WHITE METAL LANT...|</span></span><br><span class="line"><span class="comment">// +--------------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 키로 데이터 조회 (없을 시 null 반환)</span></span><br><span class="line">(df.select(map(col(<span class="string">&quot;Description&quot;</span>), col(<span class="string">&quot;InvoiceNo&quot;</span>)).alias(<span class="string">&quot;complex_map&quot;</span>))</span><br><span class="line">  .selectExpr(<span class="string">&quot;complex_map[&#x27;WHITE METAL LANTERN&#x27;]&quot;</span>).show(<span class="number">2</span>))</span><br><span class="line"><span class="comment">// +--------------------------------+</span></span><br><span class="line"><span class="comment">// |complex_map[WHITE METAL LANTERN]|</span></span><br><span class="line"><span class="comment">// +--------------------------------+</span></span><br><span class="line"><span class="comment">// |                            null|</span></span><br><span class="line"><span class="comment">// |                          536365|</span></span><br><span class="line"><span class="comment">// +--------------------------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// map 타입 분해 -&gt; 컬럼으로 변환 가능</span></span><br><span class="line">(df.select(map(col(<span class="string">&quot;Description&quot;</span>), col(<span class="string">&quot;InvoiceNo&quot;</span>)).alias(<span class="string">&quot;complex_map&quot;</span>))</span><br><span class="line">  .selectExpr(<span class="string">&quot;explode(complex_map)&quot;</span>).show(<span class="number">2</span>))</span><br><span class="line"><span class="comment">// +--------------------+------+</span></span><br><span class="line"><span class="comment">// |                 key| value|</span></span><br><span class="line"><span class="comment">// +--------------------+------+</span></span><br><span class="line"><span class="comment">// |WHITE HANGING HEA...|536365|</span></span><br><span class="line"><span class="comment">// | WHITE METAL LANTERN|536365|</span></span><br><span class="line"><span class="comment">// +--------------------+------+</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> map(Description, InvoiceNo) <span class="keyword">as</span> complex_map <span class="keyword">FROM</span> dfTable</span><br><span class="line"><span class="keyword">WHERE</span> Description <span class="keyword">IS</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span></span><br></pre></td></tr></table></figure></details></li></ul><h3 id="6-10-JSON-다루기"><a href="#6-10-JSON-다루기" class="headerlink" title="6.10 JSON 다루기"></a>6.10 JSON 다루기</h3><ul><li>스파크에서 JSON 데이터 다루기 위한 고유 기능 제공<ul><li>문자열 형태 JSON 조작, JSON 파싱, JSON 객체로 변환 등</li></ul></li><li><code>get_json_object()</code><ul><li>JSON 객체 (딕셔너리, 배열) 인라인 쿼리로 조회 가능</li><li>중첩 없는 단일 JSON일 시, <code>json_tuble</code> 사용 가능</li></ul></li><li><code>to_json()</code> : StructType -&gt; JSON 문자열. 데이터 소스와 동일한 형태의 딕셔너리(맵) 인자로 사용 가능</li><li><code>from_json()</code> : JSON 문자열 -&gt; 객체. 단 스키마 지정 필수 (option. 맵 데이터 타입 옵션)</li></ul><details><summary class="point-color-can-hover">[6.10] 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// JSON 컬럼 생성</span></span><br><span class="line"><span class="keyword">val</span> jsonDF = spark.range(<span class="number">1</span>).selectExpr(<span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">  &#x27;&#123;&quot;</span>myJSONK<span class="string">ey&quot; : &#123;&quot;</span>myJSONV<span class="string">alue&quot; : [1, 2, 3]&#125;&#125;&#x27; as jsonString&quot;</span><span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// get_json_object() 로 JSON 객체 조회</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;get_json_object, json_tuple&#125;</span><br><span class="line">jsonDF.select(</span><br><span class="line">    get_json_object(col(<span class="string">&quot;jsonString&quot;</span>), <span class="string">&quot;$.myJSONKey.myJSONValue[1]&quot;</span>) as <span class="string">&quot;column&quot;</span>,</span><br><span class="line">    json_tuple(col(<span class="string">&quot;jsonString&quot;</span>), <span class="string">&quot;myJSONKey&quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// =&gt; SQL 사용한 처리 (동일 표현)</span></span><br><span class="line">jsonDF.selectExpr(</span><br><span class="line">  <span class="string">&quot;get_json_object(jsonString, &#x27;$.myJSONKey.myJSONValue[1]&#x27;) as column&quot;</span>, </span><br><span class="line">  <span class="string">&quot;json_tuple(jsonString, &#x27;myJSONKey&#x27;)&quot;</span>).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +------+--------------------+</span></span><br><span class="line"><span class="comment">// |column|                  c0|</span></span><br><span class="line"><span class="comment">// +------+--------------------+</span></span><br><span class="line"><span class="comment">// |     2|&#123;&quot;myJSONValue&quot;:[1...|</span></span><br><span class="line"><span class="comment">// +------+--------------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// to_json() : StructType -&gt; JSON 문자열</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.to_json</span><br><span class="line">(df.selectExpr(<span class="string">&quot;(InvoiceNo, Description) as myStruct&quot;</span>)</span><br><span class="line">  .select(to_json(col(<span class="string">&quot;myStruct&quot;</span>))))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// from_json() : JSON 문자열 -&gt; 객체</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.from_json</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"><span class="keyword">val</span> parseSchema = <span class="keyword">new</span> <span class="type">StructType</span>(<span class="type">Array</span>(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">StructField</span>(<span class="string">&quot;InvoiceNo&quot;</span>,<span class="type">StringType</span>,<span class="literal">true</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="type">StructField</span>(<span class="string">&quot;Description&quot;</span>,<span class="type">StringType</span>,<span class="literal">true</span>)))</span><br><span class="line">(df.selectExpr(<span class="string">&quot;(InvoiceNo, Description) as myStruct&quot;</span>)</span><br><span class="line">  .select(to_json(col(<span class="string">&quot;myStruct&quot;</span>)).alias(<span class="string">&quot;newJSON&quot;</span>))</span><br><span class="line">  .select(from_json(col(<span class="string">&quot;newJSON&quot;</span>), parseSchema), col(<span class="string">&quot;newJSON&quot;</span>)).show(<span class="number">2</span>))</span><br><span class="line"><span class="comment">// +----------------------+--------------------+</span></span><br><span class="line"><span class="comment">// |jsontostructs(newJSON)|             newJSON|</span></span><br><span class="line"><span class="comment">// +----------------------+--------------------+</span></span><br><span class="line"><span class="comment">// |  [536365, WHITE HA...|&#123;&quot;InvoiceNo&quot;:&quot;536...|</span></span><br><span class="line"><span class="comment">// |  [536365, WHITE ME...|&#123;&quot;InvoiceNo&quot;:&quot;536...|</span></span><br><span class="line"><span class="comment">// +----------------------+--------------------+</span></span><br></pre></td></tr></table></figure></details><h3 id="6-11-사용자-정의-함수"><a href="#6-11-사용자-정의-함수" class="headerlink" title="6.11 사용자 정의 함수"></a>6.11 사용자 정의 함수</h3><ul><li><p><strong>사용자 정의 함수</strong> (user defined function, <strong>UDF</strong>)</p><ul><li>스파크의 가장 강력한 기능 중 하나</li><li>파이썬, 스칼라, 외부 라이브러리등 사용 =&gt; 사용자가 원하는 형태로 트랜스포메이션 생성</li></ul></li><li><p>특징</p><ul><li>하나 이상의 컬럼을 입력/반환 가능</li><li>스파크 UDF는 다양한 언어로 개발 가능</li><li>레코드 별로 데이터를 처리하는 함수이므로, 독특하거나 도메인 특화 (DSL) 언어 사용 X</li><li>=&gt; 기본적으로 특정 SparkSession이나 Context에서 사용할 수 있게 <u>임시 함수 형태로 등록</u></li></ul></li><li><p>다양한 언어로 UDF 개발 가능</p><ul><li>그러나 언어별로 성능에 영향 존재<ul><li>예제 참고</li><li>함수를 만들고 모든 워커 노드에서 해당 함수를 사용할 수 있도록 스파크에 등록<ul><li>스파크는 드라이버에서 함수 직렬화 -&gt; 네트워크 통해서 모든 익스큐터 프로세스로 전달</li><li>(언어에 관계없이 발생하는 과정)</li></ul></li><li>함수를 개발한 언어에 따라 기본적으로 동작하는 방식이 달라짐<ul><li>애초에 스칼라, 자바 사용 시 JVM 환경에서만 사용 가능<ul><li>스파크 내장함수 장점 사용 X =&gt; 성능 ↓</li><li>많은 객체 생성시에도 성능 문제</li></ul></li><li>파이썬 사용 시 모든 데이터를 직렬화하고, 파이썬 프로세스에 있는 데이터의 로우마다 함수 실행 및 JVM과 스파크에 처리 결과를 반환<ul><li>일단 직렬화 과정에서 큰 부하 발생</li><li>데이터가 파이썬으로 전달되면 스파크에서 워커 메모리 관리의 어려움</li></ul></li></ul></li><li>=&gt; 따라서 사용자 정의 함수는 <strong>자바나 스칼라로 작성</strong> 을 권장</li></ul></li></ul></li><li><p>기본은 사용자 정의 함수(UTF)는 DataFrame에서만 사용 가능 (문자열 표현식 X)</p><ul><li><strong>스파크 SQL 함수 등록하면?</strong></li><li>=&gt; 모든 프로그래밍 언어와 SQL 에서 사용자 정의 함수 사용 가능<ul><li>파이썬에서도 우회적으로 사용 가능하지만 DataFrame 함수 대신 SQL 표현식으로 사용해야함</li><li>스파크는 자체 데이터 타입(파이썬X)을 사용하므로 <strong>변환 타입 지정 권!장!</strong></li><li>반환될 타입과 다른 데이터 타입 지정시 =&gt; null 반환</li></ul></li></ul></li><li><p>사용자 정의 함수에서 선택적 값 반환</p><ul><li>파이썬 = <code>None</code> / 스칼라 = <code>Option</code> 반환</li></ul><details><summary class="point-color-can-hover">[6.11] 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> udfExampleDF = spark.range(<span class="number">5</span>).toDF(<span class="string">&quot;num&quot;</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">power3</span></span>(number:<span class="type">Double</span>):<span class="type">Double</span> = number * number * number</span><br><span class="line">power3(<span class="number">2.0</span>) <span class="comment">// 8.0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// UDF 실행 + 함수 등록 및 사용 (=&gt; DataFrame에서 사용 가능)</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.udf</span><br><span class="line"><span class="keyword">val</span> power3udf = udf(power3(_:<span class="type">Double</span>):<span class="type">Double</span>)</span><br><span class="line"></span><br><span class="line">udfExampleDF.select(power3udf(col(<span class="string">&quot;num&quot;</span>))).show()</span><br><span class="line"><span class="comment">// +--------+</span></span><br><span class="line"><span class="comment">// |UDF(num)|</span></span><br><span class="line"><span class="comment">// +--------+</span></span><br><span class="line"><span class="comment">// |     0.0|</span></span><br><span class="line"><span class="comment">// |     1.0|</span></span><br><span class="line"><span class="comment">// |     8.0|</span></span><br><span class="line"><span class="comment">// |    27.0|</span></span><br><span class="line"><span class="comment">// |    64.0|</span></span><br><span class="line"><span class="comment">// +--------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// UDF를 스파크 SQL로 등록하면 =&gt; 모든 프로그래밍 언어, SQL 에서 사용 가능 (+문자열 표현식)</span></span><br><span class="line">spark.udf.register(<span class="string">&quot;power3&quot;</span>, power3(_:<span class="type">Double</span>):<span class="type">Double</span>)</span><br><span class="line">udfExampleDF.selectExpr(<span class="string">&quot;power3(num)&quot;</span>).show(<span class="number">2</span>)</span><br><span class="line">+-------------------------------+</span><br><span class="line">|<span class="type">UDF</span>:power3(cast(num as double))|</span><br><span class="line">+-------------------------------+</span><br><span class="line">|                            <span class="number">0.0</span>|</span><br><span class="line">|                            <span class="number">1.0</span>|</span><br><span class="line">+-------------------------------+</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%spark.pyspark</span><br><span class="line">udfExampleDF.selectExpr(<span class="string">&quot;power3(num)&quot;</span>).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment"># =&gt; Scala로 등록된 UDF 사용</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 반환 데이터타입이 Integer인데 DoubeType() 으로 변환 시 =&gt; null 반환</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> IntegerType, DoubleType</span><br><span class="line">spark.udf.register(<span class="string">&quot;power3py&quot;</span>, power3, DoubleType())</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL 에서도 등록된 UDF 사용 가능</span></span><br><span class="line"><span class="keyword">SELECT</span> power3(<span class="number">12</span>), power3py(<span class="number">12</span>) <span class="comment">-- 반환 데이터 타입 문제로 동작하지 않음</span></span><br></pre></td></tr></table></figure></details></li></ul><h3 id="6-12-Hive-UDF"><a href="#6-12-Hive-UDF" class="headerlink" title="6.12 Hive UDF"></a>6.12 Hive UDF</h3><ul><li><p>하이브 문법을 사용해서 만든 함수 =&gt; <strong>UDF</strong>, <strong>UDAF</strong> 사용 가능</p><ul><li>UDF (User Defined Function)</li><li>UDAF (User Defined Aggregate Function)</li></ul></li><li><p>단, 하이브 지원 기능 활성화 필요</p><ul><li>=&gt; <code>SparkSession.builder().enableHiveSupport()</code> 명시</li><li>하이브 지원 활성화 되면 SQL로 UDF 등록 가능</li><li>사전에 컴파일된 스칼라, 자바 패키지만 지원 (라이브러리 의존성 명시 필요)</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- TEMPORARY 키워드 제거 시 =&gt; 하이브 메타스토어에 영구(permanent) 함수로 등록</span></span><br><span class="line"><span class="keyword">CREATE</span> TEMPORARY <span class="keyword">FUNCTION</span> myFunc <span class="keyword">AS</span> <span class="string">&#x27;com.organization.hive.udf.FunctionName&#x27;</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="6-13-정리"><a href="#6-13-정리" class="headerlink" title="6.13 정리"></a>6.13 정리</h3><ul><li>스파크 SQL을 사용목적에 맞게 확장하는 방식<ul><li>간단한 함수만으로도 확장 가능 (DSL X)</li></ul></li><li>스파크 SQL은 복잡한 비즈니스 로직 구현에 사용할 수 있는 강력한 기능</li></ul><h3 id="📒-단어장"><a href="#📒-단어장" class="headerlink" title="📒 단어장"></a>📒 단어장</h3>]]></content>
    
    
      
      
    <summary type="html">&lt;br/&gt;

&lt;center&gt;&lt;p style=&quot;color:lightgray&quot;&gt;라떼 시절엔,, 가이드북이 하나면 든-든했다,,, 이말이야,,, 총총 @}----&lt;/p&gt;
&lt;img width=&quot;300&quot; alt=&quot;maple&quot; src=&quot;https://user-i</summary>
      
    
    
    
    <category term="spark" scheme="https://minsw.github.io/categories/spark/"/>
    
    
    <category term="book" scheme="https://minsw.github.io/tags/book/"/>
    
    <category term="study" scheme="https://minsw.github.io/tags/study/"/>
    
    <category term="spark" scheme="https://minsw.github.io/tags/spark/"/>
    
    <category term="apache" scheme="https://minsw.github.io/tags/apache/"/>
    
  </entry>
  
  <entry>
    <title>&amp;#039;Spark The Definitive Guide&amp;#039; 5장 - 구조적 API 기본 연산</title>
    <link href="https://minsw.github.io/2021/01/26/Spark-The-Definitive-Guide-5%EC%9E%A5/"/>
    <id>https://minsw.github.io/2021/01/26/Spark-The-Definitive-Guide-5%EC%9E%A5/</id>
    <published>2021-01-25T20:26:48.000Z</published>
    <updated>2021-02-03T18:48:55.633Z</updated>
    
    <content type="html"><![CDATA[<br/><p>오늘의 교훈.<br>도커 이미지에 예제 있다고 신나게 돌리고~ 돌리고~ 하다보면<br>터진다는걸 명심하도록 하자 🥺</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.io.IOException: No space left on device</span><br></pre></td></tr></table></figure><br/><img src="https://user-images.githubusercontent.com/26691216/106086744-1b72d100-6166-11eb-8d99-1deebfa68867.jpg" width="400" alt="jongman"><center><i style="color:lightgray"> 인생은 실전이야 친구야</i></center><img width="300" alt="bomb" src="https://user-images.githubusercontent.com/26691216/106087261-fcc10a00-6166-11eb-80c2-d339e59bbc99.png"><center><h2>_ _ _</h2></center><br/><hr><h1 id="CHAPTER-5-구조적-API-기본-연산"><a href="#CHAPTER-5-구조적-API-기본-연산" class="headerlink" title="CHAPTER 5 구조적 API 기본 연산"></a>CHAPTER 5 구조적 API 기본 연산</h1><p>CHAPTER 4 는 구조적 API의 핵심 추상화 ‘개념’을 소개<br>CHAPTER 5 는 DataFrame과 그 데이터를 다루는 기본 ‘기능’ 소개</p><blockquote><p><em>‘DataFrame = Row 타입의 <strong>레코드</strong> + 여러 <strong>컬럼</strong>‘</em><br>(각 컬럼명과 데이터 타입은 <strong>스키마</strong>로 정의)</p><p>DataFrame의 <strong>파티셔닝</strong> :  DataFrame (또는 Dataset)이 클러스터에서 물리적으로 배치되는 형태를 정의</p><ul><li><strong>파티셔닝 스키마</strong> : 파티션을 배치하는 방법 정의</li><li>파티셔닝의 분할 기준? =&gt; 특정 컬럼 or 비결정론적(nondeterministically) 값 기반으로 설정</li></ul></blockquote><h3 id="5-1-스키마"><a href="#5-1-스키마" class="headerlink" title="5.1 스키마"></a>5.1 스키마</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.format(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;/data/flight-data/json/2015-summary.json&quot;</span>)</span><br><span class="line">df.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="comment">// root</span></span><br><span class="line"><span class="comment">//  |-- DEST_COUNTRY_NAME: string (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- ORIGIN_COUNTRY_NAME: string (nullable = true)</span></span><br><span class="line"><span class="comment">//  |-- count: long (nullable = true)</span></span><br></pre></td></tr></table></figure><ul><li><p>스키마는 <strong>DataFrame의 컬럼명과 데이터 타입을 정의</strong></p><ul><li>관련된 모든 것을 하나로 묶는 역할</li></ul></li><li><p>데이터 소스에서 스키마를 얻거나 (Schema-on-read), 직접 정의 가능</p><ul><li>대부분의 비정형 분석 (ad-hoc analysis)에서 schema-on-read 잘 동작</li><li>운영 환경 ETL 작업에 스파크 사용시 <strong>직접 정의 필요</strong> (샘플 데이터 타입에 따른 스키마 추론 방지)</li></ul></li><li><p>스키마는 <code>StructType</code> 객체 </p><ul><li><p>복합 데이터 타입 <code>StructType</code> (=consistOf(<code>StructField</code> 객체))</p></li><li><p>스파크는 자체 데이터 타입 정보를 사용 =&gt; 언어 별 데이터 타입으로 설정 X</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">spark.read.format(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;/data/flight-data/json/2015-summary.json&quot;</span>).schema</span><br><span class="line"><span class="comment">// res176: org.apache.spark.sql.types.StructType</span></span><br><span class="line"><span class="comment">//  = StructType(</span></span><br><span class="line"><span class="comment">//        StructField(DEST_COUNTRY_NAME,StringType,true),</span></span><br><span class="line"><span class="comment">//        StructField(ORIGIN_COUNTRY_NAME,StringType,true),</span></span><br><span class="line"><span class="comment">//        StructField(count,LongType,true)</span></span><br><span class="line"><span class="comment">//      )</span></span><br></pre></td></tr></table></figure></li></ul></li></ul><details><summary class="point-color-can-hover">[5.1] 예제 펼치기 - DataFrame에 스키마 적용 예제</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DataFrame에 스키마를 만들고 적용하는 예제</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">StructField</span>, <span class="type">StructType</span>, <span class="type">StringType</span>, <span class="type">LongType</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.<span class="type">Metadata</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> myManualSchema = <span class="type">StructType</span>(<span class="type">Array</span>(</span><br><span class="line">  <span class="type">StructField</span>(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>, <span class="type">StringType</span>, <span class="literal">true</span>),</span><br><span class="line">  <span class="type">StructField</span>(<span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span>, <span class="type">StringType</span>, <span class="literal">true</span>),</span><br><span class="line">  <span class="type">StructField</span>(<span class="string">&quot;count&quot;</span>, <span class="type">LongType</span>, <span class="literal">false</span>,</span><br><span class="line">    <span class="type">Metadata</span>.fromJson(<span class="string">&quot;&#123;\&quot;hello\&quot;:\&quot;world\&quot;&#125;&quot;</span>))</span><br><span class="line">))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> df = (spark.read.format(<span class="string">&quot;json&quot;</span>).schema(myManualSchema)</span><br><span class="line">  .load(<span class="string">&quot;/data/flight-data/json/2015-summary.json&quot;</span>))</span><br></pre></td></tr></table></figure></details><h3 id="5-2-컬럼과-표현식"><a href="#5-2-컬럼과-표현식" class="headerlink" title="5.2 컬럼과 표현식"></a>5.2 컬럼과 표현식</h3><ul><li><p>스파크의 ‘컬럼’ (=표현식)</p><ul><li>스프레드 시트, R의 dataframe, Pandas의 컬럼과 비슷</li><li>사용자는 <u><strong>표현식</strong></u>으로 DataFrame의 컬럼을 선택, 조작, 제거 가능</li><li>즉 표현식을 사용해 레코드 단위로 계산한 값을 나타내는 논리적 구조. 실제값을 얻으려면 로우 (=&gt; DataFrame) 가 필요</li><li>외부 접근시 <strong>반드시 DataFrame 을 통해야 함</strong></li></ul></li><li><p>컬럼 생성 &amp; 참조 : <code>col()</code> <code>column()</code></p><ul><li>컬럼이 DataFrame에 있는지 없는지는 모름 =&gt; <strong>분석기</strong>가 <strong>카탈로그</strong>에 저장된 정보랑 비교하기 전까지는 미확인 <a href="https://minsw.github.io/2021/01/26/Spark-The-Definitive-Guide-4%EC%9E%A5/#4-4-%EA%B5%AC%EC%A1%B0%EC%A0%81-API%EC%9D%98-%EC%8B%A4%ED%96%89-%EA%B3%BC%EC%A0%95">[4.4] 참고</a></li><li>스칼라는 고유 기능 사용 가능 : <code>$&quot;컬럼명&quot;</code> <code>&#39;컬럼명</code> (<code>&#39;</code> : 틱 마크, 심벌)</li><li>명시적 참조 : <code>DataFrame.col()</code> (조인시 유용)<br>=&gt; 명시적 컬럼 정의 시, 분석기 실행 단계에서 컬럼 확인 절차 생략</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;col, column&#125;</span><br><span class="line">col(<span class="string">&quot;someCol&quot;</span>)</span><br><span class="line">column(<span class="string">&quot;someCol&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// in Scala</span></span><br><span class="line">$<span class="string">&quot;someCol&quot;</span></span><br><span class="line"><span class="symbol">&#x27;someCol</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 명시적 참조</span></span><br><span class="line">df.col(<span class="string">&quot;someCol&quot;</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>표현식</strong> : <code>expr()</code></p><ul><li>DataFrame 레코드의 여러 값에 대한 트랜스포메이션 집합</li><li>여러 컬럼을 입력받아 식별 -&gt; 다양한 표현식을 각 레코드에 적용 -&gt; <strong>‘단일값’</strong> (복합 데이터 타입) 으로 출력 하는 함수</li><li><em>DataFrame의 컬럼은 ‘표현식’이다</em><ul><li><code>expr(&quot;someCol&quot;)</code> == <code>col(&quot;someCol&quot;)</code> (동일 동작)</li><li>컬럼은 표현식의 일부 기능 제공</li></ul></li></ul></li><li><p>스파크는 연산 순서를 지정하는 논리적 트리로 컴파일</p><ul><li>DataFrame 코드나 SQL 표현식 작성 시, 실행 시점에 동일한 논리 트리로 컴파일 되므로 동일한 성능 발휘</li><li>예시는 <code>p.129</code> [그림 5-1] 논리적 트리 DAG 참고</li><li><code>expr(&quot;someCol - 5&quot;)</code> == <code>col(&quot;someCol&quot;) - 5</code> == <code>expr(&quot;someCol&quot;) - 5</code>  (다 같은 트랜스포메이션 과정)</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 동일한 표현 - col(), expr()</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.expr</span><br><span class="line">expr(<span class="string">&quot;(((someCol + 5) * 200) - 6) &lt; otherCol&quot;</span>)</span><br><span class="line"></span><br><span class="line">(((col(<span class="string">&quot;someCol&quot;</span>) + <span class="number">5</span>) * <span class="number">200</span>) - <span class="number">6</span>) &lt; col(<span class="string">&quot;otherCol&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DataFrame 컬럼 접근 (printScheme() 아닌 프로그래밍 방식)</span></span><br><span class="line">spark.read.format(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;/data/flight-data/json/2015-summary.json&quot;</span>).columns</span><br><span class="line"><span class="comment">// res0: Array[String] = Array(DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count)</span></span><br></pre></td></tr></table></figure></li></ul><blockquote><p>‘표현식’ 과 ‘컬럼’ 사이 핵심 내용</p><ul><li>컬럼은 단지 표현식일 뿐</li><li>컬럼과 컬럼의 트랜스포메이션은 파싱된 표현식과 동일한 논리적 실행 계획으로 컴파일</li></ul></blockquote><h3 id="5-3-레코드와-로우"><a href="#5-3-레코드와-로우" class="headerlink" title="5.3 레코드와 로우"></a>5.3 레코드와 로우</h3><ul><li>스파크의 ‘로우’ (=레코드)<ul><li>스파크에서 DataFrame의 각 로우는 하나의 레코드</li><li>값을 생성하기 위해 컬럼 표현식으로 Row 객체를 다룸</li><li>Row 객체는 내부 바이트 배열을 가지는 인터페이스 =&gt; <strong>오직 컬럼 표현식으로만</strong> 다룰 수 있음 (외부 노출 X)<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Row 확인</span></span><br><span class="line">scala&gt; df.first()</span><br><span class="line"><span class="comment">// res1: org.apache.spark.sql.Row = [United States,Romania,15]</span></span><br></pre></td></tr></table></figure></li></ul></li><li>로우 생성<ul><li>각 컬럼에 해당하는 값으로 직접 Row 객체 생성 가능</li><li>그러나 Row 객체는 스키마 정보 X (=&gt; 오직 DataFrame만 가짐)</li><li>=&gt; 스키마랑 같은 순서로 값 명시해야함</li></ul></li><li>로우 데이터 접근하려면 =&gt; 원하는 위치 지정<ul><li>Python, R 은 올바른 데이터 타입으로 알아서 변환됨</li><li>Scala, Java 는 헬퍼 메서드나 데이터타입 명시적 지정 필요 (Dataset API 사용 시 jvm 객체 데이터 셋 얻을 수 있음)<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">val</span> myRow = <span class="type">Row</span>(<span class="string">&quot;Hello&quot;</span>, <span class="literal">null</span>, <span class="number">1</span>, <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">myRow(<span class="number">0</span>) <span class="comment">// type Any</span></span><br><span class="line">myRow(<span class="number">0</span>).asInstanceOf[<span class="type">String</span>] <span class="comment">// String</span></span><br><span class="line">myRow.getString(<span class="number">0</span>) <span class="comment">// String</span></span><br><span class="line">myRow.getInt(<span class="number">2</span>) <span class="comment">// Int</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># python 사용 시</span></span><br><span class="line">myRow[<span class="number">0</span>]</span><br><span class="line">myRow[<span class="number">2</span>]</span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="5-4-DataFrame의-트랜스포메이션"><a href="#5-4-DataFrame의-트랜스포메이션" class="headerlink" title="5.4 DataFrame의 트랜스포메이션"></a>5.4 DataFrame의 트랜스포메이션</h3><blockquote><p>DataFrame을 다루는 방법 (주요 작업 4가지)</p><ul><li>로우나 컬럼 추가</li><li>로우나 컬럼 제거</li><li>로우를 컬럼으로 변환하거나, 그 반대로 변환</li><li>컬럼값을 기준으로 로우 순서 변경</li></ul><p>모든 유형의 작업은 트랜스포메이션으로 변환 가능 (ex. 모든 로우의 특정 컬럼값 변경 후 결과 반환)</p></blockquote><ul><li><a href="#1-DataFrame-%EC%83%9D%EC%84%B1">(1) DataFrame 생성</a></li><li><a href="#2-select-%EC%99%80-selectExpr">(2) select 와 selectExpr</a></li><li><a href="#3-%EC%8A%A4%ED%8C%8C%ED%81%AC-%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%83%80%EC%9E%85%EC%9C%BC%EB%A1%9C-%EB%B3%80%ED%99%98%ED%95%98%EA%B8%B0">(3) 스파크 데이터 타입으로 변환하기</a></li><li><a href="#4-%EC%BB%AC%EB%9F%BC-%EC%B6%94%EA%B0%80%ED%95%98%EA%B8%B0">(4) 컬럼 추가하기</a></li><li><a href="#5-%EC%BB%AC%EB%9F%BC%EB%AA%85-%EB%B3%80%EA%B2%BD%ED%95%98%EA%B8%B0">(5) 컬럼명 변경하기</a></li><li><a href="#6-%EC%98%88%EC%95%BD-%EB%AC%B8%EC%9E%90%EC%99%80-%ED%82%A4%EC%9B%8C%EB%93%9C">(6) 예약 문자와 키워드</a></li><li><a href="#7-%EB%8C%80%EC%86%8C%EB%AC%B8%EC%9E%90-%EA%B5%AC%EB%B6%84">(7) 대소문자 구분</a></li><li><a href="#8-%EC%BB%AC%EB%9F%BC-%EC%A0%9C%EA%B1%B0%ED%95%98%EA%B8%B0">(8) 컬럼 제거하기</a></li><li><a href="#9-%EC%BB%AC%EB%9F%BC%EC%9D%98-%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%83%80%EC%9E%85-%EB%B3%80%EA%B2%BD%ED%95%98%EA%B8%B0">(9) 컬럼의 데이터 타입 변경하기</a></li><li><a href="#10-%EB%A1%9C%EC%9A%B0-%ED%95%84%ED%84%B0%EB%A7%81%ED%95%98%EA%B8%B0">(10) 로우 필터링하기</a></li><li><a href="#11-%EA%B3%A0%EC%9C%A0%ED%95%9C-%EB%A1%9C%EC%9A%B0-%EC%96%BB%EA%B8%B0">(11) 고유한 로우 얻기</a></li><li><a href="#12-%EB%AC%B4%EC%9E%91%EC%9C%84-%EC%83%98%ED%94%8C-%EB%A7%8C%EB%93%A4%EA%B8%B0">(12) 무작위 샘플 만들기</a></li><li><a href="#13-%EC%9E%84%EC%9D%98-%EB%B6%84%ED%95%A0%ED%95%98%EA%B8%B0">(13) 임의 분할하기</a></li><li><a href="#14-%EB%A1%9C%EC%9A%B0-%ED%95%A9%EC%B9%98%EA%B8%B0%EC%99%80-%EC%B6%94%EA%B0%80%ED%95%98%EA%B8%B0">(14) 로우 합치기와 추가하기</a></li><li><a href="#15-%EB%A1%9C%EC%9A%B0-%EC%A0%95%EB%A0%AC%ED%95%98%EA%B8%B0">(15) 로우 정렬하기</a></li><li><a href="#16-%EB%A1%9C%EC%9A%B0-%EC%88%98-%EC%A0%9C%ED%95%9C%ED%95%98%EA%B8%B0">(16) 로우 수 제한하기</a></li><li><a href="#17-repartition%EA%B3%BC-coalesce">(17) repartition과 coalesce</a></li><li><a href="#18-%EB%93%9C%EB%9D%BC%EC%9D%B4%EB%B2%84%EB%A1%9C-%EB%A1%9C%EC%9A%B0-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%88%98%EC%A7%91%ED%95%98%EA%B8%B0">(18) 드라이버로 로우 데이터 수집하기</a></li></ul><br/><h4 id="1-DataFrame-생성"><a href="#1-DataFrame-생성" class="headerlink" title="(1) DataFrame 생성"></a>(1) DataFrame 생성</h4><details><summary class="point-color-can-hover">[5.4-1] 예제 펼치기 </summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// A. 원시 데이터 소스 -&gt; DataFrame</span></span><br><span class="line"><span class="keyword">val</span> df = (spark.read.format(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;/data/flight-data/json/2015-summary.json&quot;</span>))</span><br><span class="line"><span class="comment">// (임시 뷰 등록)</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">&quot;dfTable&quot;</span>)</span><br><span class="line">``</span><br><span class="line"></span><br><span class="line"><span class="comment">// B. (Row 객체를 가지는) Seq 타입 -&gt; DataFrame</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">StructField</span>, <span class="type">StructType</span>, <span class="type">StringType</span>, <span class="type">LongType</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> myManualSchema = <span class="keyword">new</span> <span class="type">StructType</span>(<span class="type">Array</span>(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">StructField</span>(<span class="string">&quot;some&quot;</span>, <span class="type">StringType</span>, <span class="literal">true</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="type">StructField</span>(<span class="string">&quot;col&quot;</span>, <span class="type">StringType</span>, <span class="literal">true</span>),</span><br><span class="line">  <span class="keyword">new</span> <span class="type">StructField</span>(<span class="string">&quot;names&quot;</span>, <span class="type">LongType</span>, <span class="literal">false</span>)))</span><br><span class="line"><span class="keyword">val</span> myRows = <span class="type">Seq</span>(<span class="type">Row</span>(<span class="string">&quot;Hello&quot;</span>, <span class="literal">null</span>, <span class="number">1</span>L))</span><br><span class="line"><span class="comment">// sc 객체의 parallelize() 로 RDD 생성</span></span><br><span class="line"><span class="keyword">val</span> myRDD = spark.sparkContext.parallelize(myRows)</span><br><span class="line"><span class="comment">// createDataFrame()로 DataFrame 생성</span></span><br><span class="line"><span class="keyword">val</span> myDf = spark.createDataFrame(myRDD, myManualSchema)</span><br><span class="line">myDf.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// +-----+----+-----+</span></span><br><span class="line"><span class="comment">// | some| col|names|</span></span><br><span class="line"><span class="comment">// +-----+----+-----+</span></span><br><span class="line"><span class="comment">// |Hello|null|    1|</span></span><br><span class="line"><span class="comment">// +-----+----+-----+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// Scala 사용 시 toDF() 사용 가능 (import spark.implicits._)</span></span><br><span class="line"><span class="keyword">val</span> myDF = <span class="type">Seq</span>((<span class="string">&quot;Hello&quot;</span>, <span class="number">2</span>, <span class="number">1</span>L)).toDF(<span class="string">&quot;col1&quot;</span>, <span class="string">&quot;col2&quot;</span>, <span class="string">&quot;col3&quot;</span>)</span><br></pre></td></tr></table></figure></details><ul><li>스파크의 implicits (import 필요, <a href="http://bit.ly/2xrFpML">참고</a>)<ul><li>Scala 스파크 콘솔 사용 시 =&gt; Seq 데이터 타입에 <code>toDF()</code> 사용 가능</li><li>그러나 null 타입과는 안맞으므로 운영환경 사용은 권장 X</li></ul></li></ul><blockquote><h4 id="createDataFrame-vs-toDF"><a href="#createDataFrame-vs-toDF" class="headerlink" title="createDataFrame() vs toDF()"></a>createDataFrame() vs toDF()</h4><ul><li><code>createDataFrame(rowRDD: RDD[Row], schema: StructType) : DataFrame</code><ul><li>모든 schema customization 가능</li><li><a href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/SparkSession.html#createDataFrame(rowRDD:org.apache.spark.rdd.RDD[org.apache.spark.sql.Row],schema:org.apache.spark.sql.types.StructType):org.apache.spark.sql.DataFrame">API docs#SparkSession</a></li></ul></li><li><code>toDF()</code><ul><li>스키마 지정 없음. schema 추론 (Dataset API)</li><li><code>import spark.implicits._</code> 필요</li><li><a href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Dataset.html#toDF():org.apache.spark.sql.DataFrame">API docs#Dataset</a></li></ul></li></ul></blockquote><h4 id="2-select-와-selectExpr"><a href="#2-select-와-selectExpr" class="headerlink" title="(2) select 와 selectExpr"></a>(2) select 와 selectExpr</h4><details><summary class="point-color-can-hover">[5.4-2] 예제 펼치기 </summary><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--- SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> dataFrameTable</span><br><span class="line"><span class="keyword">SELECT</span> columnName <span class="keyword">FROM</span> dataFrameTable</span><br><span class="line"><span class="keyword">SELECT</span> columnName <span class="operator">*</span> <span class="number">10</span>, otherColumn, someOtherCol <span class="keyword">as</span> c <span class="keyword">FROM</span> dataFrameTable</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># select() == SELECT query</span></span><br><span class="line">scala&gt; df.select(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>).show(2)</span><br><span class="line">+-----------------+</span><br><span class="line">|DEST_COUNTRY_NAME|</span><br><span class="line">+-----------------+</span><br><span class="line">|    United States|</span><br><span class="line">|    United States|</span><br><span class="line">+-----------------+</span><br><span class="line"></span><br><span class="line">scala&gt; df.select(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>, <span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span>).show(2)</span><br><span class="line">+-----------------+-------------------+</span><br><span class="line">|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|</span><br><span class="line">+-----------------+-------------------+</span><br><span class="line">|    United States|            Romania|</span><br><span class="line">|    United States|            Croatia|</span><br><span class="line">+-----------------+-------------------+</span><br><span class="line"></span><br><span class="line">--- SQL</span><br><span class="line">SELECT DEST_COUNTRY_NAME FROM dfTable LIMIT 2</span><br><span class="line">SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME FROM dfTable LIMIT 2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 다양한 컬럼 참조 방법</span></span><br><span class="line"><span class="comment"># df.select(col(&quot;DEST_COUNTRY_NAME&quot;), &quot;DEST_COUNTRY_NAME&quot;) =&gt; 에러</span></span><br><span class="line">scala&gt; import org.apache.spark.sql.functions.&#123;expr, col, column&#125;</span><br><span class="line">scala&gt; (df.select(</span><br><span class="line">     |     df.col(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>),</span><br><span class="line">     |     col(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>),</span><br><span class="line">     |     column(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>),</span><br><span class="line">     |     <span class="string">&#x27;DEST_COUNTRY_NAME,</span></span><br><span class="line"><span class="string">     |     $&quot;DEST_COUNTRY_NAME&quot;,</span></span><br><span class="line"><span class="string">     |     expr(&quot;DEST_COUNTRY_NAME&quot;))</span></span><br><span class="line"><span class="string">     |   .show(2))</span></span><br><span class="line"><span class="string">+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+</span></span><br><span class="line"><span class="string">|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|</span></span><br><span class="line"><span class="string">+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+</span></span><br><span class="line"><span class="string">|    United States|    United States|    United States|    United States|    United States|    United States|</span></span><br><span class="line"><span class="string">|    United States|    United States|    United States|    United States|    United States|    United States|</span></span><br><span class="line"><span class="string">+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+</span></span><br><span class="line"><span class="string"></span></span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// expr() 예시 - 컬럼명 DEST_COUNTRY_NAME -&gt; destination -&gt; DEST_COUNTRY_NAME</span></span><br><span class="line">df.select(expr(<span class="string">&quot;DEST_COUNTRY_NAME AS destination&quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line">df.select(expr(<span class="string">&quot;DEST_COUNTRY_NAME as destination&quot;</span>).alias(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// select() + expr() =&gt; selectExpr()</span></span><br><span class="line">df.selectExpr(<span class="string">&quot;DEST_COUNTRY_NAME as newColumnName&quot;</span>, <span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +-------------+-----------------+</span></span><br><span class="line"><span class="comment">// |newColumnName|DEST_COUNTRY_NAME|</span></span><br><span class="line"><span class="comment">// +-------------+-----------------+</span></span><br><span class="line"><span class="comment">// |United States|    United States|</span></span><br><span class="line"><span class="comment">// |United States|    United States|</span></span><br><span class="line"><span class="comment">// +-------------+-----------------+</span></span><br><span class="line"></span><br><span class="line">(df.selectExpr(</span><br><span class="line">    <span class="string">&quot;*&quot;</span>, <span class="comment">// include all original columns</span></span><br><span class="line">    <span class="string">&quot;(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry&quot;</span>)</span><br><span class="line">  .show(<span class="number">2</span>))</span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+-------------+</span></span><br><span class="line"><span class="comment">// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+-------------+</span></span><br><span class="line"><span class="comment">// |    United States|            Romania|   15|        false|</span></span><br><span class="line"><span class="comment">// |    United States|            Croatia|    1|        false|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+-------------+</span></span><br><span class="line"></span><br><span class="line">df.selectExpr(<span class="string">&quot;avg(count)&quot;</span>, <span class="string">&quot;count(distinct(DEST_COUNTRY_NAME))&quot;</span>).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +-----------+---------------------------------+</span></span><br><span class="line"><span class="comment">// | avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|</span></span><br><span class="line"><span class="comment">// +-----------+---------------------------------+</span></span><br><span class="line"><span class="comment">// |1770.765625|                              132|</span></span><br><span class="line"><span class="comment">// +-----------+---------------------------------+</span></span><br></pre></td></tr></table></figure></details><blockquote><p>DataFrame을 다루기 위한 대부분의 트랜스포메이션 작업 해결 가능</p><ul><li><code>select()</code> : 컬럼이나 표현식을 사용</li><li><code>selectExpr()</code> : 문자열 표현식을 사용</li><li><code>select()</code>: 메서드로 사용할 수 없는 <code>org.apache.spark.sql.function</code> 에 포함된 다양한 함수</li></ul></blockquote><ul><li>DataFrame 컬럼 다룰 시, SQL 사용 가능</li><li>컬럼 참조 방법은 다양한 방법을 섞어서 사용할 수 있다. 5.2 재참고<ul><li>Column 객체랑 문자열을 함께 섞어쓸수는 X (ex. <code>df.select(col(&quot;DEST_COUNTRY_NAME&quot;), &quot;DEST_COUNTRY_NAME&quot;)</code> =&gt; 컴파일 에러)</li><li>가장 유연한 참조 방법 =&gt; <code>expr()</code></li></ul></li><li><code>select()</code> + <code>expr()</code> 패턴을 자주 사용 =&gt; <strong><code>selectExpr()</code></strong> (효율성 ↑)<ul><li><i style="color:gray">“?? : 크큭..스파크의 진정한 능력을 보여주지..”</i></li><li>새로운 DataFrame 생성하는 복잡한 표현식 간단하게 표현 가능</li><li>모든 유효한 비집계형 (non-aggregating) SQL 지정 가능 (단, 컬럼 식별 가능해야)</li><li>집계 함수 (avg, count 등) 사용 가능</li></ul></li></ul><h4 id="3-스파크-데이터-타입으로-변환하기"><a href="#3-스파크-데이터-타입으로-변환하기" class="headerlink" title="(3) 스파크 데이터 타입으로 변환하기"></a>(3) 스파크 데이터 타입으로 변환하기</h4><details><summary class="point-color-can-hover">[5.4-3] 예제 펼치기 </summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.lit</span><br><span class="line"></span><br><span class="line">df.select(expr(<span class="string">&quot;*&quot;</span>), lit(<span class="number">1</span>).as(<span class="string">&quot;One&quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+---+</span></span><br><span class="line"><span class="comment">// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+---+</span></span><br><span class="line"><span class="comment">// |    United States|            Romania|   15|  1|</span></span><br><span class="line"><span class="comment">// |    United States|            Croatia|    1|  1|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+---+</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL 에서 리터럴은 상숫값 (동일 표현)</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span>, <span class="number">1</span> <span class="keyword">as</span> <span class="keyword">One</span> <span class="keyword">FROM</span> dfTable LIMIT <span class="number">2</span></span><br></pre></td></tr></table></figure></details><ul><li><strong>리터럴(literal)</strong> : 프로그래밍 언어의 리터럴 값 =&gt; 스파크가 이해할 수 있는 값으로 변환<ul><li>때로는 명시적 값 (상수값, 비교에 사용할 무언가 등..) 을 스파크에 전달해야함 =&gt; 리터럴 사용</li><li>리터럴은 표현식</li><li>어떤 상수나 프로그래밍으로 생성된 변숫값을 특정 컬럼의 값과 비교할 때 사용</li></ul></li></ul><h4 id="4-컬럼-추가하기"><a href="#4-컬럼-추가하기" class="headerlink" title="(4) 컬럼 추가하기"></a>(4) 컬럼 추가하기</h4><details><summary class="point-color-can-hover">[5.4-4] 예제 펼치기 </summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">df.withColumn(<span class="string">&quot;numberOne&quot;</span>, lit(<span class="number">1</span>)).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+---------+</span></span><br><span class="line"><span class="comment">// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+---------+</span></span><br><span class="line"><span class="comment">// |    United States|            Romania|   15|        1|</span></span><br><span class="line"><span class="comment">// |    United States|            Croatia|    1|        1|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+---------+</span></span><br><span class="line"></span><br><span class="line">(df.withColumn(<span class="string">&quot;withinCountry&quot;</span>, expr(<span class="string">&quot;ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME&quot;</span>))</span><br><span class="line">  .show(<span class="number">2</span>))</span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+-------------+</span></span><br><span class="line"><span class="comment">// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+-------------+</span></span><br><span class="line"><span class="comment">// |    United States|            Romania|   15|        false|</span></span><br><span class="line"><span class="comment">// |    United States|            Croatia|    1|        false|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+-------------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 컬럼명 변경도 가능 (DEST_COUNTRY_NAME -&gt; Destination)</span></span><br><span class="line">df.withColumn(<span class="string">&quot;Destination&quot;</span>, expr(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>)).columns</span><br><span class="line"><span class="comment">// res18: Array[String] = Array(DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count, Destination)</span></span><br></pre></td></tr></table></figure></details><ul><li><code>withColumn(컬럼명, 값을 생성할 표현식)</code> 사용<ul><li>공식적 컬럼 추가 방법</li><li>컬럼명 변경하여 추가도 가능</li></ul></li></ul><h4 id="5-컬럼명-변경하기"><a href="#5-컬럼명-변경하기" class="headerlink" title="(5) 컬럼명 변경하기"></a>(5) 컬럼명 변경하기</h4><details><summary class="point-color-can-hover">[5.4-5] 예제 펼치기 </summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DEST_COUNTRY_NAME -&gt; dest 로 변경</span></span><br><span class="line">df.withColumnRenamed(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>, <span class="string">&quot;dest&quot;</span>).columns</span><br><span class="line"><span class="comment">// res21: Array[String] = Array(dest, ORIGIN_COUNTRY_NAME, count)</span></span><br></pre></td></tr></table></figure></details><ul><li><code>withColumnRenamed(컬럼명, 변경할 문자열)</code> 사용</li></ul><h4 id="6-예약-문자와-키워드"><a href="#6-예약-문자와-키워드" class="headerlink" title="(6) 예약 문자와 키워드"></a>(6) 예약 문자와 키워드</h4><details><summary class="point-color-can-hover">[5.4-6] 예제 펼치기 </summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 이스케이핑 필요 없는 예시 - 새로운 컬럼명을 나타내는 문자열</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.expr</span><br><span class="line"><span class="keyword">val</span> dfWithLongColName = df.withColumn(</span><br><span class="line">  <span class="string">&quot;This Long Column-Name&quot;</span>,</span><br><span class="line">  expr(<span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span>))</span><br><span class="line"><span class="comment">// dfWithLongColName: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 2 more fields]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 이스케이핑 필요한 예시 - 표현식으로 해당 컬럼을 참조 </span></span><br><span class="line">(dfWithLongColName.selectExpr(</span><br><span class="line">    <span class="string">&quot;`This Long Column-Name`&quot;</span>,</span><br><span class="line">    <span class="string">&quot;`This Long Column-Name` as `new col`&quot;</span>)</span><br><span class="line">  .show(<span class="number">2</span>))</span><br><span class="line"><span class="comment">// +---------------------+-------+</span></span><br><span class="line"><span class="comment">// |This Long Column-Name|new col|</span></span><br><span class="line"><span class="comment">// +---------------------+-------+</span></span><br><span class="line"><span class="comment">// |              Romania|Romania|</span></span><br><span class="line"><span class="comment">// |              Croatia|Croatia|</span></span><br><span class="line"><span class="comment">// +---------------------+-------+</span></span><br><span class="line"></span><br><span class="line">dfWithLongColName.createOrReplaceTempView(<span class="string">&quot;dfTableLong&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 같은 DataFrame 생성</span></span><br><span class="line">dfWithLongColName.select(col(<span class="string">&quot;This Long Column-Name&quot;</span>)).columns</span><br><span class="line">dfWithLongColName.select(expr(<span class="string">&quot;`This Long Column-Name`&quot;</span>)).columns</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL (동일 표현)</span></span><br><span class="line"><span class="keyword">SELECT</span> `This Long <span class="keyword">Column</span><span class="operator">-</span>Name`, `This Long <span class="keyword">Column</span><span class="operator">-</span>Name` <span class="keyword">as</span> `<span class="keyword">new</span> col`</span><br><span class="line"><span class="keyword">FROM</span> dfTableLong LIMIT <span class="number">2</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></details><ul><li>예약 문자(공백, 하이픈 (-) 등..) 는 컬럼명 사용 불가<ul><li>=&gt; 사용하려면 <strong><code>`</code> (백틱문자)</strong> 를 이용한 이스케이핑(escaping) 필요</li></ul></li><li>예약 문자나 키워드를 사용하는 ‘표현식’에는 이스케이프 처리 필요<ul><li>‘문자열’로 명시적 컬럼 참조 시에는 리터럴로 해석 =&gt; 예약문자 없이도 참조 가능</li></ul></li></ul><h4 id="7-대소문자-구분"><a href="#7-대소문자-구분" class="headerlink" title="(7) 대소문자 구분"></a>(7) 대소문자 구분</h4><details><summary class="point-color-can-hover">[5.4-7] 예제 펼치기 </summary><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> spark.sql.caseSensitive <span class="literal">true</span></span><br></pre></td></tr></table></figure></details><ul><li>기본적으로 스파크는 대소문자를 가리지 않음</li><li><code>set spark.sql.caseSenstive true</code> 설정 시 구분 가능</li></ul><h4 id="8-컬럼-제거하기"><a href="#8-컬럼-제거하기" class="headerlink" title="(8) 컬럼 제거하기"></a>(8) 컬럼 제거하기</h4><details><summary class="point-color-can-hover">[5.4-8] 예제 펼치기 </summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df.drop(<span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span>).columns</span><br><span class="line"><span class="comment">// res30: Array[String] = Array(DEST_COUNTRY_NAME, count)</span></span><br><span class="line"></span><br><span class="line">dfWithLongColName.drop(<span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span>, <span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>)</span><br><span class="line"><span class="comment">// res32: org.apache.spark.sql.DataFrame = [count: bigint, This Long Column-Name: string]</span></span><br></pre></td></tr></table></figure></details><ul><li><code>drop(컬럼명...)</code> 사용<ul><li>여러개를 인수로 넣어 다수의 컬럼을 한번에 제거 가능</li></ul></li><li><code>select()</code> 로도 제거 가능</li></ul><h4 id="9-컬럼의-데이터-타입-변경하기"><a href="#9-컬럼의-데이터-타입-변경하기" class="headerlink" title="(9) 컬럼의 데이터 타입 변경하기"></a>(9) 컬럼의 데이터 타입 변경하기</h4><details><summary class="point-color-can-hover">[5.4-9] 예제 펼치기 </summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// count 컬럼 : Integer -&gt; String 으로 형변환</span></span><br><span class="line">df.withColumn(<span class="string">&quot;count2&quot;</span>, col(<span class="string">&quot;count&quot;</span>).cast(<span class="string">&quot;long&quot;</span>))</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL (동일 표현)</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span>, <span class="built_in">cast</span>(count <span class="keyword">as</span> string) <span class="keyword">AS</span> count2 <span class="keyword">FROM</span> dfTable</span><br></pre></td></tr></table></figure></details><ul><li><code>cast()</code> 사용<ul><li>특정 데이터 타입 =&gt; 다른 데이터 타입으로 형변환</li></ul></li></ul><h4 id="10-로우-필터링하기"><a href="#10-로우-필터링하기" class="headerlink" title="(10) 로우 필터링하기"></a>(10) 로우 필터링하기</h4><details><summary class="point-color-can-hover">[5.4-10] 예제 펼치기 </summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 동일한 표현식</span></span><br><span class="line">df.filter(col(<span class="string">&quot;count&quot;</span>) &lt; <span class="number">2</span>).show(<span class="number">2</span>)</span><br><span class="line">df.where(<span class="string">&quot;count &lt; 2&quot;</span>).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |    United States|            Croatia|    1|</span></span><br><span class="line"><span class="comment">// |    United States|          Singapore|    1|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 여러 필터 적용 시 (순서 무관, 동시 적용)</span></span><br><span class="line">df.where(col(<span class="string">&quot;count&quot;</span>) &lt; <span class="number">2</span>).where(col(<span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span>) =!= <span class="string">&quot;Croatia&quot;</span>).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |    United States|          Singapore|    1|</span></span><br><span class="line"><span class="comment">// |          Moldova|      United States|    1|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL (동일 표현)</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> dfTable <span class="keyword">WHERE</span> count <span class="operator">&lt;</span> <span class="number">2</span> LIMIT <span class="number">2</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> dfTable <span class="keyword">WHERE</span> count <span class="operator">&lt;</span> <span class="number">2</span> <span class="keyword">AND</span> ORIGIN_COUNTRY_NAME <span class="operator">!=</span> &quot;Croatia&quot; LIMIT <span class="number">2</span></span><br></pre></td></tr></table></figure></details><ul><li>필터링을 하려면 참/거짓 판별 표현식 필요<ul><li>문자열 표현식, 컬럼을 다루는 기능으로 표현식 만듦</li></ul></li><li><code>where()</code> <code>filter()</code> 사용 가능<ul><li>두 메서드 모두 같은 파라미터 타입 및 같은 연산 수행</li><li><code>where()</code> 는 SQL과 유사</li><li><code>filter()</code> 는 Dataset API를 이용해서 사용하면 Dataset 각 레코드에 적용 할 함수를 사용 가능 (=&gt; 자세한건 11장)</li></ul></li><li>스파크는 필터의 순서와 상관없이 동시에 모든 필터링 작업 수행<ul><li>같은 표현식에 여러 필터 적용시</li><li>차례대로 AND 필터 연결 후 판단은 스파크에게 맡겨야 함</li></ul></li></ul><h4 id="11-고유한-로우-얻기"><a href="#11-고유한-로우-얻기" class="headerlink" title="(11) 고유한 로우 얻기"></a>(11) 고유한 로우 얻기</h4><details><summary class="point-color-can-hover">[5.4-11] 예제 펼치기 </summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df.select(<span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span>, <span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>).distinct().count()</span><br><span class="line"><span class="comment">// res41: Long = 256</span></span><br><span class="line"></span><br><span class="line">df.select(<span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span>).distinct().count()</span><br><span class="line"><span class="comment">// res44: Long = 125</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL (동일 표현)</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span>(ORIGIN_COUNTRY_NAME, DEST_COUNTRY_NAME)) <span class="keyword">FROM</span> dfTable</span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> ORIGIN_COUNTRY_NAME) <span class="keyword">FROM</span> dfTable</span><br></pre></td></tr></table></figure></details><ul><li><code>distinct()</code> 사용<ul><li>고윳값이나 중복되지 않은 값을 얻는 연산</li></ul></li></ul><h4 id="12-무작위-샘플-만들기"><a href="#12-무작위-샘플-만들기" class="headerlink" title="(12) 무작위 샘플 만들기"></a>(12) 무작위 샘플 만들기</h4><details><summary class="point-color-can-hover">[5.4-12] 예제 펼치기 </summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> seed = <span class="number">5</span></span><br><span class="line"><span class="keyword">val</span> withReplacement = <span class="literal">false</span></span><br><span class="line"><span class="keyword">val</span> fraction = <span class="number">0.5</span></span><br><span class="line">df.sample(withReplacement, fraction, seed).count()</span><br><span class="line"><span class="comment">// res46: Long = 126</span></span><br></pre></td></tr></table></figure></details><ul><li><code>sample(복원추출 여부, 추출 비율, seed)</code> 사용<ul><li>표본 데이터 추출 비율 (&lt;=1.0) 지정 가능</li><li>복원 추출 (sample with replacement), 비복원 추출 (sample without replacement) 사용 여부 지정 가능</li></ul></li></ul><h4 id="13-임의-분할하기"><a href="#13-임의-분할하기" class="headerlink" title="(13) 임의 분할하기"></a>(13) 임의 분할하기</h4><details><summary class="point-color-can-hover">[5.4-13] 예제 펼치기 </summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 총합이 1이 아닐 경우 설정되는 default</span></span><br><span class="line"><span class="keyword">val</span> dataFrames = df.randomSplit(<span class="type">Array</span>(<span class="number">0.25</span>, <span class="number">0.75</span>), seed)</span><br><span class="line">dataFrames(<span class="number">0</span>).count() &gt; dataFrames(<span class="number">1</span>).count()</span><br><span class="line"><span class="comment">// res51: Boolean = false</span></span><br></pre></td></tr></table></figure></details><ul><li>임의 분할 (random split) : 원본 DataFrame 을 임의 크기로 ‘분할’<ul><li>머신러닝 알고리즘 사용 시 학습셋, 검증셋, 테스트셋 만들때 주로 사용</li></ul></li><li><code>randomSplit(분할 가중치 Array, seed)</code><ul><li>임의성(randomized) 을 가지므로 시드값(seed) 필수</li><li>DataFrame 비율은 총합이 1이 되게 지정 (아닐 경우 예제 비율로 지정됨)</li></ul></li></ul><h4 id="14-로우-합치기와-추가하기"><a href="#14-로우-합치기와-추가하기" class="headerlink" title="(14) 로우 합치기와 추가하기"></a>(14) 로우 합치기와 추가하기</h4><details><summary class="point-color-can-hover">[5.4-14] 예제 펼치기 </summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">val</span> schema = df.schema</span><br><span class="line"><span class="keyword">val</span> newRows = <span class="type">Seq</span>(</span><br><span class="line">  <span class="type">Row</span>(<span class="string">&quot;New Country&quot;</span>, <span class="string">&quot;Other Country&quot;</span>, <span class="number">5</span>L),</span><br><span class="line">  <span class="type">Row</span>(<span class="string">&quot;New Country 2&quot;</span>, <span class="string">&quot;Other Country 3&quot;</span>, <span class="number">1</span>L)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">val</span> parallelizedRows = spark.sparkContext.parallelize(newRows)</span><br><span class="line"><span class="keyword">val</span> newDF = spark.createDataFrame(parallelizedRows, schema)</span><br><span class="line"></span><br><span class="line"><span class="comment">// df + newDF =&gt; 로우가 추가된 새로운 객체</span></span><br><span class="line">(df.union(newDF)</span><br><span class="line">  .where(<span class="string">&quot;count = 1&quot;</span>)</span><br><span class="line">  .where($<span class="string">&quot;ORIGIN_COUNTRY_NAME&quot;</span> =!= <span class="string">&quot;United States&quot;</span>)</span><br><span class="line">  .show()) <span class="comment">// get all of them and we&#x27;ll see our new rows at the end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// schema: org.apache.spark.sql.types.StructType = StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,true))</span></span><br><span class="line"><span class="comment">// newRows: Seq[org.apache.spark.sql.Row] = List([New Country,Other Country,5], [New Country 2,Other Country 3,1])</span></span><br><span class="line"><span class="comment">// parallelizedRows: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[74] at parallelize at &lt;console&gt;:29</span></span><br><span class="line"><span class="comment">// newDF: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |    United States|            Croatia|    1|</span></span><br><span class="line"><span class="comment">// |    United States|          Singapore|    1|</span></span><br><span class="line"><span class="comment">// |    United States|          Gibraltar|    1|</span></span><br><span class="line"><span class="comment">// |    United States|             Cyprus|    1|</span></span><br><span class="line"><span class="comment">// |    United States|            Estonia|    1|</span></span><br><span class="line"><span class="comment">// |    United States|          Lithuania|    1|</span></span><br><span class="line"><span class="comment">// |    United States|           Bulgaria|    1|</span></span><br><span class="line"><span class="comment">// |    United States|            Georgia|    1|</span></span><br><span class="line"><span class="comment">// |    United States|            Bahrain|    1|</span></span><br><span class="line"><span class="comment">// |    United States|   Papua New Guinea|    1|</span></span><br><span class="line"><span class="comment">// |    United States|         Montenegro|    1|</span></span><br><span class="line"><span class="comment">// |    United States|            Namibia|    1|</span></span><br><span class="line"><span class="comment">// |    New Country 2|    Other Country 3|    1|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></details><ul><li>DataFrame은 불변성 (immutability)<ul><li>DataFrame을 변경하는 레코드 추가는 불가능</li><li>=&gt; 원본 DataFrame을 새로운 DataFrame과 <strong>통합(union)</strong> (결합)</li><li>단, 통합하려는 두 DataFrame은 반드시 동일한 스키마와 컬럼 수를 가져야 함</li></ul></li><li><code>union()</code><ul><li>현재 스키마가 아닌 컬럼 위치 기반으로 동작 (자동 정렬 X)</li><li>로우가 추가된 DataFrame 을 참조하려면 새롭게 만들어진 DataFrame 사용해야하지만, <u>뷰나 테이블로 등록 시에는 동적으로 참조 가능</u></li></ul></li><li>컬럼 표현식과 문자 비교열 비교 시<ul><li>(컬럼 표현식이 아닌) 컬럼의 실제값을 비교 대상 문자열과 비교하려면</li><li>스칼라 사용 시 반드시 <strong><code>=!=</code> 함수</strong> 사용<ul><li><code>=!=</code>, <code>===</code> 는 스파크의 Column 클래스에 정의된 함수</li></ul></li><li>파이썬은 그대로 <code>!=</code>, <code>==</code></li></ul></li></ul><h4 id="15-로우-정렬하기"><a href="#15-로우-정렬하기" class="headerlink" title="(15) 로우 정렬하기"></a>(15) 로우 정렬하기</h4><details><summary class="point-color-can-hover">[5.4-15] 예제 펼치기 </summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">df.sort(<span class="string">&quot;count&quot;</span>).show(<span class="number">5</span>)</span><br><span class="line">df.orderBy(<span class="string">&quot;count&quot;</span>, <span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>).show(<span class="number">5</span>)</span><br><span class="line">df.orderBy(col(<span class="string">&quot;count&quot;</span>), col(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>)).show(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 정렬 기준 지정 (desc 오름)</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;desc, asc&#125;</span><br><span class="line"></span><br><span class="line">df.orderBy(expr(<span class="string">&quot;count desc&quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// 이거 왜 내림차순이아니라 오름차순으로 나오나... expr(&quot;count desc&quot;) 설정 안되고 default 정렬 (asc)로 설정되서 나오는 듯한데..?</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |          Moldova|      United States|    1|</span></span><br><span class="line"><span class="comment">// |    United States|            Croatia|    1|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"></span><br><span class="line">df.orderBy(desc(<span class="string">&quot;count&quot;</span>), asc(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>)).show(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// +-----------------+-------------------+------+</span></span><br><span class="line"><span class="comment">// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+------+</span></span><br><span class="line"><span class="comment">// |    United States|      United States|370002|</span></span><br><span class="line"><span class="comment">// |    United States|             Canada|  8483|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+------+</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> dfTable <span class="keyword">ORDER</span> <span class="keyword">BY</span> count <span class="keyword">DESC</span>, DEST_COUNTRY_NAME <span class="keyword">ASC</span> LIMIT <span class="number">2</span></span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// sortWithinPartitions() 로 파티션별 정렬</span></span><br><span class="line">(spark.read.format(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;/data/flight-data/json/*-summary.json&quot;</span>)</span><br><span class="line">  .sortWithinPartitions(<span class="string">&quot;count&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// explain() 시</span></span><br><span class="line"><span class="comment">// == Physical Plan ==</span></span><br><span class="line"><span class="comment">// *(1) Sort [count#450L ASC NULLS FIRST], false, 0</span></span><br><span class="line"><span class="comment">// +- *(1) FileScan json [DEST_COUNTRY_NAME#448,ORIGIN_COUNTRY_NAME#449,count#450L] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/data/flight-data/json/2015-summary.json, file:/data/flight-data/json/2012..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:bigint&gt;</span></span><br></pre></td></tr></table></figure></details><ul><li><code>sort()</code> <code>orderBy()</code> 사용<ul><li>두 메서드는 완전히 같은 방식으로 동작 (<code>orderBy()</code> 내부에서 <code>sort()</code> 사용)</li><li>다수 컬럼 지정, 컬럼 표현식, 문자열 사용 가능</li><li>정렬 기준 : <code>asc()</code>, <code>desc()</code> 로 명확한 지정 가능 (기본 동작은 오름차순)</li></ul></li><li>정렬된 DataFrame 의 NULL 값 표시 기준<ul><li><code>asc_nulls_first</code>, <code>desc_nulls_first</code>, <code>asc_nulls_last</code>, <code>desc_nulls_last</code> 로 기준 지정 가능</li></ul></li><li>파티션 별 정렬 =&gt; <code>sortWithinPartitions()</code><ul><li>트랜스포메이션 처리 전 성능 최적화를 위함</li><li>더 자세한 튜닝과 최적화 내용은 3부에서</li></ul></li></ul><blockquote><p><code>df.orderBy(expr(&quot;count desc&quot;))</code> ?</p><ul><li>count 컬럼을 desc() (내림차순) 으로 정렬되야 맞나?<ul><li>실제로는 그렇게 동작 하지 않음 (=&gt; 오름차순으로 정렬됨)</li></ul></li><li>잘못된 예제인듯한데..<ul><li>관련 stackoverflow 질문 <a href="https://stackoverflow.com/questions/63112281/pyspark-sort-dataframe-by-expression">링크 1</a> / <a href="https://stackoverflow.com/questions/63373479/sorting-2-columns-in-opposite-direction-does-not-work-using-expr-function">링크 2</a></li></ul></li></ul></blockquote><h4 id="16-로우-수-제한하기"><a href="#16-로우-수-제한하기" class="headerlink" title="(16) 로우 수 제한하기"></a>(16) 로우 수 제한하기</h4><details><summary class="point-color-can-hover">[5.4-16] 예제 펼치기 </summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">df.limit(<span class="number">5</span>).show()</span><br><span class="line"></span><br><span class="line">df.orderBy(expr(<span class="string">&quot;count desc&quot;</span>)).limit(<span class="number">6</span>).show()</span><br><span class="line"><span class="comment">// +--------------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|</span></span><br><span class="line"><span class="comment">// +--------------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |               Malta|      United States|    1|</span></span><br><span class="line"><span class="comment">// |Saint Vincent and...|      United States|    1|</span></span><br><span class="line"><span class="comment">// |       United States|            Croatia|    1|</span></span><br><span class="line"><span class="comment">// |       United States|          Gibraltar|    1|</span></span><br><span class="line"><span class="comment">// |       United States|          Singapore|    1|</span></span><br><span class="line"><span class="comment">// |             Moldova|      United States|    1|</span></span><br><span class="line"><span class="comment">// +--------------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// 뒷구르기하면서 봐도 결과가 이렇게 나와야할거같은데...</span></span><br><span class="line"><span class="comment">// df.orderBy(desc(&quot;count&quot;)).limit(6).show()</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+------+</span></span><br><span class="line"><span class="comment">// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+------+</span></span><br><span class="line"><span class="comment">// |    United States|      United States|370002|</span></span><br><span class="line"><span class="comment">// |    United States|             Canada|  8483|</span></span><br><span class="line"><span class="comment">// |           Canada|      United States|  8399|</span></span><br><span class="line"><span class="comment">// |    United States|             Mexico|  7187|</span></span><br><span class="line"><span class="comment">// |           Mexico|      United States|  7140|</span></span><br><span class="line"><span class="comment">// |   United Kingdom|      United States|  2025|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+------+</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- SQL</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> dfTable LIMIT <span class="number">6</span></span><br></pre></td></tr></table></figure></details><ul><li><code>limit(로우 수)</code> 사용<ul><li>추출할 로우 수 제한하여 추출</li></ul></li></ul><h4 id="17-repartition과-coalesce"><a href="#17-repartition과-coalesce" class="headerlink" title="(17) repartition과 coalesce"></a>(17) repartition과 coalesce</h4><details><summary class="point-color-can-hover">[5.4-17] 예제 펼치기 </summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DataFrame 현재 파티션 수 구하기</span></span><br><span class="line">df.rdd.getNumPartitions <span class="comment">// 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 전체 데이터 셔플</span></span><br><span class="line">df.repartition(<span class="number">5</span>)</span><br><span class="line"><span class="comment">// df.repartition(5).rdd.getNumPartitions =&gt; 5</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 특정 컬럼 기준 파티션 재분배</span></span><br><span class="line">df.repartition(col(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>))</span><br><span class="line"><span class="comment">// df.repartition(col(&quot;DEST_COUNTRY_NAME&quot;)).rdd.getNumPartition =&gt; 200</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 특정 컬럼 지정 + 파티션 수 지정</span></span><br><span class="line">df.repartition(<span class="number">5</span>, col(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>))</span><br><span class="line"><span class="comment">// df.repartition(5, col(&quot;DEST_COUNTRY_NAME&quot;)).rdd.getNumPartitions =&gt; 5</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// coalesce() 로 셔플없이 파티션 병합 (1 -&gt; 5 -&gt; 2)</span></span><br><span class="line">df.repartition(<span class="number">5</span>, col(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>)).coalesce(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// df.repartition(5, col(&quot;DEST_COUNTRY_NAME&quot;)).coalesce(2).rdd.getNumPartitions =&gt; 2</span></span><br></pre></td></tr></table></figure></details><ul><li>또 다른 최적화 기법? =&gt; 자주 필터링하는 컬럼 기준으로 데이터 분할<ul><li>파티셔닝 스키마와 파티션 수를 포함한 클러스터 전반의 물리적 데이터 구성 제어 가능</li></ul></li><li><code>repartition()</code> : 전체 데이터 셔플<ul><li>향후 사용할 파티션 수 &gt; 현재 파티션 수 인 경우 사용 (파티션 수 ↑)</li><li>컬럼 기준으로 파티션을 만드는 경우 사용<ul><li>자주 필터링되는 컬럼이 있다면 해당 컬럼 기준으로 파티션 재분배 추천</li></ul></li><li>선택적으로 파티션 수 지정 가능</li></ul></li><li><code>coalesce()</code> : 전체 데이터 셔플 없이 파티션 병합<ul><li><strong>파티션 수를 줄이려면</strong> coalesce 사용 (<del>repartition</del> X)</li></ul></li><li>DataFrame 파티션 수 확인은 <code>df.rdd.getNumPartitions</code> 로 확인</li></ul><h4 id="18-드라이버로-로우-데이터-수집하기"><a href="#18-드라이버로-로우-데이터-수집하기" class="headerlink" title="(18) 드라이버로 로우 데이터 수집하기"></a>(18) 드라이버로 로우 데이터 수집하기</h4><details><summary class="point-color-can-hover">[5.4-18] 예제 펼치기 </summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> collectDF = df.limit(<span class="number">10</span>)</span><br><span class="line">collectDF.take(<span class="number">5</span>) <span class="comment">// take() 는 정수형 값을 인수로 사용</span></span><br><span class="line">collectDF.show() <span class="comment">// show() =&gt; 결과를 정돈된 형태로 출력</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |    United States|            Romania|   15|</span></span><br><span class="line"><span class="comment">// |    United States|            Croatia|    1|</span></span><br><span class="line"><span class="comment">// |    United States|            Ireland|  344|</span></span><br><span class="line"><span class="comment">// |            Egypt|      United States|   15|</span></span><br><span class="line"><span class="comment">// |    United States|              India|   62|</span></span><br><span class="line"><span class="comment">// |    United States|          Singapore|    1|</span></span><br><span class="line"><span class="comment">// |    United States|            Grenada|   62|</span></span><br><span class="line"><span class="comment">// |       Costa Rica|      United States|  588|</span></span><br><span class="line"><span class="comment">// |          Senegal|      United States|   40|</span></span><br><span class="line"><span class="comment">// |          Moldova|      United States|    1|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">collectDF.show(<span class="number">5</span>, <span class="literal">false</span>)</span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"><span class="comment">// |United States    |Romania            |15   |</span></span><br><span class="line"><span class="comment">// |United States    |Croatia            |1    |</span></span><br><span class="line"><span class="comment">// |United States    |Ireland            |344  |</span></span><br><span class="line"><span class="comment">// |Egypt            |United States      |15   |</span></span><br><span class="line"><span class="comment">// |United States    |India              |62   |</span></span><br><span class="line"><span class="comment">// +-----------------+-------------------+-----+</span></span><br><span class="line"></span><br><span class="line">collectDF.collect()</span><br></pre></td></tr></table></figure></details><ul><li>스파크는 ‘드라이버’에서 클러스터 상태 정보 유지<ul><li>로컬 환경에서 데이터 다룰 때는 ‘드라이버’로 데이터 수집</li></ul></li><li>사용해본 데이터 수집 메서드 일부<ul><li><code>collect()</code> : 전체 DataFrame의 모든 데이터 수집</li><li><code>take()</code> : 상위 N개 로우 반환</li><li><code>show()</code> : 여러 로우를 보기 좋게 출력</li></ul></li><li><code>toLocalIterator()</code> : 전체 데이터셋에 대한 반복(iterate) 처리를 위해 ‘드라이버’로 로우를 모으는 방법<ul><li>iterator(반복자) 로 모든 파티션의 데이터를 ‘드라이버’에 전달</li><li>데이터셋의 파티션을 차례대로 반복 처리 가능</li></ul></li><li>드라이버로 모든 데이터 컬렉션을 수집하는 건<ul><li>=&gt; <strong>매우 큰 비용</strong> (CPU, 메모리, 네트워크)</li><li>차례대로 처리하므로 처리 비용 엄청남 (병렬 연산 X)</li></ul></li><li>따라서 대규모 데이터셋에 <code>collect()</code> 나 매우 큰 파티션에 대해 <code>toLocalIterator()</code> 사용 시 =&gt; 드라이버 비정상적 종료</li></ul><h3 id="5-5-정리"><a href="#5-5-정리" class="headerlink" title="5.5 정리"></a>5.5 정리</h3><ul><li>DataFrame 기본 연산</li><li>DataFrame 사용에 필요한 개념, 다양한 기능</li></ul><h3 id="📒-단어장"><a href="#📒-단어장" class="headerlink" title="📒 단어장"></a>📒 단어장</h3><ul><li>비결정론적(nondeterministically) : = 매번 변하는</li><li>ETL : <code>추출(Extract)</code> - <code>변환(Transform)</code> - <code>적재(Load)</code>  <i style="color:lightgray">(친숙하쥬?)</i></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;br/&gt;

&lt;p&gt;오늘의 교훈.&lt;br&gt;도커 이미지에 예제 있다고 신나게 돌리고~ 돌리고~ 하다보면&lt;br&gt;터진다는걸 명심하도록 하자 🥺&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter</summary>
      
    
    
    
    <category term="spark" scheme="https://minsw.github.io/categories/spark/"/>
    
    
    <category term="book" scheme="https://minsw.github.io/tags/book/"/>
    
    <category term="study" scheme="https://minsw.github.io/tags/study/"/>
    
    <category term="spark" scheme="https://minsw.github.io/tags/spark/"/>
    
    <category term="apache" scheme="https://minsw.github.io/tags/apache/"/>
    
  </entry>
  
  <entry>
    <title>&amp;#039;Spark The Definitive Guide&amp;#039; 4장 - 눈 떠, 구조적 API 들어간다</title>
    <link href="https://minsw.github.io/2021/01/26/Spark-The-Definitive-Guide-4%EC%9E%A5/"/>
    <id>https://minsw.github.io/2021/01/26/Spark-The-Definitive-Guide-4%EC%9E%A5/</id>
    <published>2021-01-25T17:24:59.000Z</published>
    <updated>2021-01-25T20:16:39.181Z</updated>
    
    <content type="html"><![CDATA[<br/><h4 id="📌-Part-2-구조적-API-DataFrame-SQL-Dataset"><a href="#📌-Part-2-구조적-API-DataFrame-SQL-Dataset" class="headerlink" title="📌 [Part 2] 구조적 API : DataFrame, SQL, Dataset"></a>📌 [Part 2] 구조적 API : DataFrame, SQL, Dataset</h4><h4 id="들어가기전-스파크-기본-개념-살짝-복습-👀"><a href="#들어가기전-스파크-기본-개념-살짝-복습-👀" class="headerlink" title="들어가기전, 스파크 기본 개념 살짝 복습 👀"></a>들어가기전, 스파크 기본 개념 살짝 복습 👀</h4><blockquote><p>스파크는 <code>트랜스포메이션의 처리 과정을 정의하는 분산 프로그래밍 모델</code>.</p><p>(사용자가 정의한) 다수의 <strong>트랜스포메이션</strong>은 → <strong>DAG</strong> (지향성 비순환 그래프) 로 표현되는 명령을 만들고<br><strong>액션</strong>은 하나의 잡을 클러스터에서 실행하기 위해 → 스테이지와 태스크로 나누고 DAG 처리 프로세스를 실행</p><p>이런 트랜스포메이션과 액션으로 다루는 논리적 구조? =&gt; <strong><u>DataFrame, Dataset</u></strong></p></blockquote><img width="300" alt="start" src="https://user-images.githubusercontent.com/26691216/105742251-98dbed00-5f7e-11eb-98c3-96ff8f8679dc.gif"/><center><h2>_ _ _</h2></center><br/><hr><h1 id="Chapter-4-구조적-API-개요"><a href="#Chapter-4-구조적-API-개요" class="headerlink" title="Chapter 4 구조적 API 개요"></a>Chapter 4 구조적 API 개요</h1><p>반드시 이해해야 (한다고) 하는 아래 세 가지 기본개념을 설명한다.</p><ul><li><code>타입형(typed)</code> / <code>비타입형(untyped)</code> API 개념과 차이점</li><li>핵심 용어</li><li>스파크가 구조적 API의 데이터 흐름을 해석하고 클러스터에서 실행하는 방식</li></ul><h3 id="구조적-API-의-특징"><a href="#구조적-API-의-특징" class="headerlink" title="구조적 API 의 특징"></a>구조적 API 의 특징</h3><ul><li>데이터 흐름을 정의하는 기본 추상화 개념</li><li>다양한 유형의 데이터 처리 가능<ul><li>비정형 로그파일, 반정형 CSV 파일, 정형적 Parquet (파-케이) 파일 등</li></ul></li><li>배치(batch), 스트리밍 (streaming) 처리에서 사용 가능<ul><li>배치 작업 &lt;-&gt; 스트리밍 작업 : 쉽게 변환 가능</li></ul></li><li>구조적 API 의 세 가지 분산 컬렉션 API<ul><li>Dataset</li><li>DataFrame</li><li>SQL 테이블과 뷰</li></ul></li></ul><h3 id="4-1-DataFrame과-Dataset"><a href="#4-1-DataFrame과-Dataset" class="headerlink" title="4.1 DataFrame과 Dataset"></a>4.1 DataFrame과 Dataset</h3><blockquote><p>DataFrame (코드 사용) == 테이블/뷰 (SQL 사용)</p></blockquote><ul><li>스파크의 구조화된 컬렉션 개념 : <a href="https://minsw.github.io/2021/01/24/Spark-The-Definitive-Guide-2%EC%9E%A5/#2-6-DataFrame">DataFrame [2.6]</a> / <a href="https://minsw.github.io/2021/01/24/Spark-The-Definitive-Guide-3%EC%9E%A5/#3-2-Dataset-%ED%83%80%EC%9E%85-%EC%95%88%EC%A0%95%EC%84%B1%EC%9D%84-%EC%A0%9C%EA%B3%B5%ED%95%98%EB%8A%94-%EA%B5%AC%EC%A1%B0%EC%A0%81-API">Dataset [3.2]</a></li><li>그래서 이것들이 뭔데?<ul><li>잘 정의된 로우(row), 컬럼(column)을 갖는 분산 테이블 형태의 컬렉션<ul><li>각 컬럼은 다른 컬럼과 동일한 수의 로우를 가짐 (값 없음은 null)</li><li>모든 로우는 같은 데이터 타입 정보</li></ul></li><li>지연 연산의 실행 계획<ul><li>결과를 생성하기 위해 어떤 데이터에 어떤 연산을 적용해야하는지 정의</li></ul></li><li>불변성 (Immutability)</li></ul></li></ul><h3 id="4-2-스키마"><a href="#4-2-스키마" class="headerlink" title="4.2 스키마"></a>4.2 스키마</h3><ul><li>스키마 : DataFrame의 컬럼명과 데이터 타입 정의<ul><li>직접 정의 or Schema-on-read (데이터소스에서 얻는 것)</li><li>여러 데이터 타입으로 구성</li></ul></li></ul><h3 id="4-3-스파크의-구조적-데이터-타입-개요"><a href="#4-3-스파크의-구조적-데이터-타입-개요" class="headerlink" title="4.3 스파크의 구조적 데이터 타입 개요"></a>4.3 스파크의 구조적 데이터 타입 개요</h3><ul><li><p>스파크가 사용하는 <strong>카탈리스트 (Catalyst)</strong> 엔진 </p><ul><li>다양한 실행 최적화 기능 제공</li><li>실행 계획과 처리에 사용하는 자체 데이터 타입 정보를 가짐</li></ul></li><li><p>스파크는 사실상 ‘프로그래밍 언어’</p><ul><li><p>여러 언어 API와 직접 매핑 (각 언어에 대한 매핑 테이블을 가짐)</p></li><li><p>파이썬이나 R 로 구조적 API 사용해도 =&gt; 대부분의 연산은 <del>각 언어의 자체 데이터 타입</del> 이 아닌 <strong>스파크의 데이터 타입</strong> 사용 (카탈리스트 엔진에서 변환)</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.range(<span class="number">500</span>).toDF(<span class="string">&quot;number&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// scala가 아닌 &#x27;spark&#x27;의 덧셈 연산 수행</span></span><br><span class="line">df.select(df.col(<span class="string">&quot;number&quot;</span>) + <span class="number">10</span>)</span><br></pre></td></tr></table></figure></li></ul></li><li><p><code>비타입형(untyped)</code> DataFrame vs <code>타입형(typed)</code> Dataset</p><ul><li>DataFrame, Dataset 모두 데이터 타입은 존재</li><li>그러나 스키마에 명시된 데이터 타입 <strong>일치 여부를 확인하는 시점</strong> 차이<ul><li><u>DataFrame은 <strong>런타임</strong>에</u> 일치 여부 확인 =&gt; <code>비타입형(untyped)</code></li><li><u>Dataset은 <strong>컴파일 타임</strong>에</u> 일치 여부 확인 =&gt; <code>타입형(typed)</code></li></ul></li><li>DataFrame 은 Row 타입 사용<ul><li><strong>Row 타입</strong> : 스파크가 사용하는 ‘연산에 최적화된 인메모리 포멧’ 의 내부적 표현 방식 =&gt; 언어와 무관한 동일한 효과/효율성</li><li>매우 효율적인 연산 (vs Dataset의 JVM 데이터 타입 : GC, 객체 초기화 부하 .. )</li></ul></li><li>Dataset은 지정된 데이터 타입(T) 사용<ul><li>엄격한 데이터 타입 검증 =&gt; CHAPTER 11 참고</li></ul></li></ul></li><li><p>컬럼 (column)</p><ul><li>단순 데이터 타입 (정수형, 문자열), 복합 데이터 타입 (배열, 맵), null 값 표현</li><li>스파크는 데이터 타입의 모든 정보를 추적, 다양한 컬럼 변환 방법 제공</li><li>스파크의 컬럼 == 테이블의 컬럼</li></ul></li><li><p>로우 (row)</p><ul><li>데이터 레코드 (DataFrame의 레코드는 Row 타입)</li><li>로우는 SQL, RDD, 데이터 소스에서 얻거나 직접 만들거나<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.range(<span class="number">2</span>).toDF().collect()</span><br><span class="line"><span class="comment">// res165: Array[org.apache.spark.sql.Row] = Array([0], [1])</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p>스파크의 <strong>데이터 타입</strong></p><ul><li>스파크는 여러가지 내부 데이터 타입을 가짐</li><li><code>p.116 ~ 118</code> [표 4-1 ~ 3] 스파크가 지원하는 언어별 매핑 정보 (파이썬/스칼라/자바 데이터 타입 매핑)</li><li>최신 데이터 타입은 <a href="http://bit.ly/2EdflXW">스파크 공식 문서</a> 참고</li><li>예시) 언어 별 데이터 타입 초기화 및 정의 방법<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// scala</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"><span class="keyword">val</span> b = <span class="type">ByteType</span></span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// java (Factory method)</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.DataTypes;</span><br><span class="line">ByteType x = DataTypes.ByteType;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># python</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> *</span><br><span class="line">b = ByteType()</span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="4-4-구조적-API의-실행-과정"><a href="#4-4-구조적-API의-실행-과정" class="headerlink" title="4.4 구조적 API의 실행 과정"></a>4.4 구조적 API의 실행 과정</h3><blockquote><p>구조적 API 쿼리 (사용자 코드) → 실제 실행 코드 변환 과정</p><ol><li>[👩🏻‍💻] DataFrame/Dataset/SQL을 이용해 코드 작성</li><li>[Spark ✨] 코드 =&gt; <strong>논리적 실행 계획</strong> 으로 변환 (정상적 코드일 경우)</li><li>[Spark ✨] 논리적 실행 계획 =&gt; <strong>물리적 실행 계획</strong> 으로 변환. 추가적인 최적화 가능한지 확인</li><li>[Spark ✨] 클러스터에서 <strong>물리적 실행 계획 (RDD 처리)</strong> 실행</li></ol></blockquote><ul><li><p>좀 더 가깝게 설명하자면..</p><ul><li>[본인] 이 스파크 코드 작성하고 console 이나 <code>spark-submit</code> 으로 실행</li><li>-&gt; [카탈리스트 옵티마이저] 가 코드를 받고 실제 실행 계획 (물리적) 생성</li><li>-&gt; [스파크] 는 코드 실행 &amp; 결과 반환</li></ul></li><li><p>논리적 실행 계획</p>  <img width="700" alt="mermaid-1" src="https://user-images.githubusercontent.com/26691216/105745516-b494c280-5f81-11eb-972a-827a5936fe0f.png"/>  <details><summary style="color:lightgray"> theme (neutral) </summary>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A[&#x2F;코드&#x2F;]-- 1_ --&gt;B[검증 전 ...]</span><br><span class="line">B-- 2_ Analzer --&gt;C[검증된 ...]</span><br><span class="line">C-- 3_ Optimizer --&gt;D[최적화된 논리적 실행 계획]</span><br></pre></td></tr></table></figure>  </details><ol><li>사용자 코드 → 검증 전 논리적 실행 계획 (unresolved logical plan)<ul><li>추상적 트랜스포메이션만 표현 (드라이버, 익스큐터 고려 X)</li><li>사용자의 표현식을 최적화된 표현으로 변환</li></ul></li><li>분석기 (Analyzer) : <strong>카탈로그</strong>, 모든 테이블 저장소, DataFrame 정보 =&gt; 컬럼과 테이블 검증</li><li>논리적 최적화 (Catalyst Optimizer) : 논리적 실행 계획을 최적화하는 규칙의 모음 (predicate pushing down, 선택절 구문)</li></ol></li><li><p>물리적 실행 계획</p>  <img width="700" alt="mermaid-2" src="https://user-images.githubusercontent.com/26691216/105745527-b8284980-5f81-11eb-8d36-554b97959946.png"/>  <details><summary style="color:lightgray"> theme (neutral) </summary>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A[최적화된 논리적 실행 계획] --&gt; B[물리적]</span><br><span class="line">A --&gt; C[실행계획s]</span><br><span class="line">B --&gt; D&#123;비용 모델 비교&#125;</span><br><span class="line">C --&gt; D</span><br><span class="line">D --&gt; E</span><br><span class="line">[최적의 물리적 실행 계획]</span><br><span class="line">E --&gt; F&gt;클러스터에서 처리]</span><br></pre></td></tr></table></figure>  </details><ul><li>(== 스파크의 실행 계획)</li><li>논리적 실행 계획을 클러스터 환경에서 실행하는 방법을 정의</li><li>비용 모델 비교 후 최적의 전략 선택 (ex. 조인 연산 수행 비용 계산)</li><li>=&gt; RDD, 트랜스포메이션으로 변환하고 컴파일 (<em>이래서 스파크를 ‘컴파일러’라고 부르기도</em>)</li></ul></li><li><p>실행</p><ul><li>물리적 실행 계획 선정 후 RDD (저수준 인터페이스) 대상으로 모든 코드 실행</li><li>추가 최적화 수행 : 런타임에 자바 바이트 코드 생성 (=&gt; 전체 테스크나 스테이지 제거 가능)</li><li>=&gt; 처리 결과 사용자에게 반환</li></ul></li></ul><h3 id="4-5-정리"><a href="#4-5-정리" class="headerlink" title="4.5 정리"></a>4.5 정리</h3><ul><li>Spark와 구조적 API</li><li>사용자 코드 -&gt; 물리적 실행 코드 변환 과정</li></ul><h3 id="📒-단어장"><a href="#📒-단어장" class="headerlink" title="📒 단어장"></a>📒 단어장</h3><ul><li>카탈리스트 엔진 (Catalyst Optimizer) : DataFrame DSL, SQL 을 하위 레벨의 RDD 연산으로 변환하는 Optimizer<ul><li>책에서 언급한 카탈리스트 엔진 구현 영상 - <a href="https://youtu.be/5ajs8EIPWGI">youtube1</a>, <a href="https://youtu.be/GDeePbbCz2g">youtube2</a></li><li>ref. <a href="https://thebook.io/006908/part02/ch05/05/">https://thebook.io/006908/part02/ch05/05/</a></li><li>ref. <a href="https://medium.com/@leeyh0216/spark-sql-6dc3d645cc31">https://medium.com/@leeyh0216/spark-sql-6dc3d645cc31</a></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;br/&gt;

&lt;h4 id=&quot;📌-Part-2-구조적-API-DataFrame-SQL-Dataset&quot;&gt;&lt;a href=&quot;#📌-Part-2-구조적-API-DataFrame-SQL-Dataset&quot; class=&quot;headerlink&quot; title=&quot;📌 [Par</summary>
      
    
    
    
    <category term="spark" scheme="https://minsw.github.io/categories/spark/"/>
    
    
    <category term="book" scheme="https://minsw.github.io/tags/book/"/>
    
    <category term="study" scheme="https://minsw.github.io/tags/study/"/>
    
    <category term="spark" scheme="https://minsw.github.io/tags/spark/"/>
    
    <category term="apache" scheme="https://minsw.github.io/tags/apache/"/>
    
  </entry>
  
  <entry>
    <title>&amp;#039;Spark The Definitive Guide&amp;#039; 3장 - 일단 좀 더 잡솨봐 (PART 1 끝)</title>
    <link href="https://minsw.github.io/2021/01/24/Spark-The-Definitive-Guide-3%EC%9E%A5/"/>
    <id>https://minsw.github.io/2021/01/24/Spark-The-Definitive-Guide-3%EC%9E%A5/</id>
    <published>2021-01-24T14:13:51.000Z</published>
    <updated>2021-01-24T18:11:30.116Z</updated>
    
    <content type="html"><![CDATA[<br/><h4 id="Part-1-END"><a href="#Part-1-END" class="headerlink" title="[Part 1] END"></a>[Part 1] END</h4><p>파트 1 까지 끝내고 나니 이제 조금씩 스파크 맛은 본 것 같은데,<br>이번 장은 <u>‘일단 잡솨봐’ 식 구성</u> 이라 좀 따라가기 힘들었다.</p><blockquote><p>나 : 뭔 말이에요<br>?? : <em>‘XX 장에서 자세히 알아보겠습니다.’</em></p><p>?? : <em>이렇게 A 에 B를 수행하면 Z가 나옵니다</em><br>나 : 이건 또 뭔소리여<br>?? : <em>‘이와 관련된 내용은 XX 부에서 자세히 알아보겠습니다.’</em></p></blockquote><p><img width="150" alt="angry" src="https://user-images.githubusercontent.com/26691216/105637781-85fbe680-5eb2-11eb-8956-b7de17f122de.png"></p><center><i style="color:lightgray">이toRL들이..</i></center><p>예제 기준으로 모르는 부분 찾아가면서 어찌 저찌 이해는 했지만<br>그 다음 파트가 다시 또 <code>구조적 API</code>라서 오늘 본 거 대부분은 한참 뒤에야 다시 보게 될텐데..<br>아 이거 무조건인데.. 백퍼 다 까먹는데.. 🤦🏻‍♀️  </p><center><h2>_ _ _</h2></center><br/><hr><h1 id="CHAPTER-3-스파크-기능-둘러보기"><a href="#CHAPTER-3-스파크-기능-둘러보기" class="headerlink" title="CHAPTER 3 스파크 기능 둘러보기"></a>CHAPTER 3 스파크 기능 둘러보기</h1><blockquote><p>스파크 = 기본 요소 (저수준 API + 구조적 API) + 추가 기능 (일련의 표준 라이브러리)</p><ul><li>구조적 스트리밍, 고급 분석, 라이브러리 및 에코시스템</li><li>구조적 API : Dataset, DataFrame, SQL</li><li>저수준 API : RDD, 분산형 변수</li></ul></blockquote><p>CHAPTER 2 는 구조적 API의 핵심개념을 소개했다면<br>CHAPTER 3 은 나머지 API 와 주요 라이브러리, 스파크의 다양한 기능 소개</p><h3 id="3-1-운영용-애플리케이션-실행하기"><a href="#3-1-운영용-애플리케이션-실행하기" class="headerlink" title="3.1 운영용 애플리케이션 실행하기"></a>3.1 운영용 애플리케이션 실행하기</h3><details><summary class="point-color-can-hover">[3.1] 예제 펼치기</summary><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /spark-2.4.7-bin-hadoop2.7</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># scalar example</span></span><br><span class="line">$ ./bin/spark-submit --class org.apache.spark.examples.SparkPi --master <span class="built_in">local</span> ./examples/jars/spark-examples_2.11-2.4.7.jar 10</span><br><span class="line">...</span><br><span class="line">21/01/24 13:41:15 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 0.968079 s</span><br><span class="line">Pi is roughly 3.1414071414071416 <span class="comment"># 돌릴때마다 다르게 나온다</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># python example</span></span><br><span class="line">$ ./bin/spark-submit --master <span class="built_in">local</span> ./examples/src/main/python/pi.py 10</span><br><span class="line">Pi is roughly 3.139084 <span class="comment"># 이럴거면 args 는 대체 왜 넣으란걸까</span></span><br></pre></td></tr></table></figure></details><ul><li><code>spark-submit</code> 명령<ul><li>애플리케이션 코드를 클러스터에 전송해 실행시키는 역할</li><li>대화형 쉘에서 개발한 프로그램 -&gt; 운영용 애플리케이션으로 전환 가능</li><li>스파크 애플리케이션은 standalone, Mesos, YARN 클러스터 매니저를 이용해 실행됨 (<code>--master</code> 옵션)</li></ul></li></ul><h3 id="3-2-Dataset-타입-안정성을-제공하는-구조적-API"><a href="#3-2-Dataset-타입-안정성을-제공하는-구조적-API" class="headerlink" title="3.2 Dataset : 타입 안정성을 제공하는 구조적 API"></a>3.2 Dataset : 타입 안정성을 제공하는 구조적 API</h3><details><summary class="point-color-can-hover">[3.2] 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Flight</span>(<span class="params"><span class="type">DEST_COUNTRY_NAME</span>: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                  <span class="type">ORIGIN_COUNTRY_NAME</span>: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                  count: <span class="type">BigInt</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">flightsDF</span> </span>= spark.read.parquet(<span class="string">&quot;/data/flight-data/parquet/2010-summary.parquet/&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> flights = flightsDF.as[<span class="type">Flight</span>] <span class="comment">// DataFrame -&gt; Dataset[Flight]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(flights</span><br><span class="line">  .filter(flight_row =&gt; flight_row.<span class="type">ORIGIN_COUNTRY_NAME</span> != <span class="string">&quot;Canada&quot;</span>)</span><br><span class="line">  .map(flight_row =&gt; flight_row)</span><br><span class="line">  .take(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">(flights</span><br><span class="line">  .take(<span class="number">5</span>)</span><br><span class="line">  .filter(flight_row =&gt; flight_row.<span class="type">ORIGIN_COUNTRY_NAME</span> != <span class="string">&quot;Canada&quot;</span>)</span><br><span class="line">  .map(fr =&gt; <span class="type">Flight</span>(fr.<span class="type">DEST_COUNTRY_NAME</span>, fr.<span class="type">ORIGIN_COUNTRY_NAME</span>, fr.count + <span class="number">5</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment">// res45: Array[Flight] = Array(Flight(United States,Romania,1), Flight(United States,Ireland,264), Flight(United States,India,69), Flight(Egypt,United States,24), Flight(Equatorial Guinea,United States,1))</span></span><br><span class="line"><span class="comment">// res46: Array[Flight] = Array(Flight(United States,Romania,6), Flight(United States,Ireland,269), Flight(United States,India,74), Flight(Egypt,United States,29), Flight(Equatorial Guinea,United States,6))</span></span><br></pre></td></tr></table></figure></details><ul><li><strong>Dataset</strong> : Java와 Scala의 정적 데이터 타입에 맞는 코드(statically typed code)를 지원하기 위한 스파크의 구조적 API<ul><li>Python, R 사용 X</li></ul></li><li>Dataset API 는 <strong>DataFrame 레코드 =&gt; Java나 Scala로 정의한 클래스에 할당</strong>, Collection 으로 다룰 수 있는 기능 등을 제공<ul><li>DataFrame : 다양한 데이터 타입의 테이블형 데이터를 보관할 수 있는 Row 타입 객체로 구성된 분산 컬렉션 (<a href="https://minsw.github.io/2021/01/24/Spark-The-Definitive-Guide-2%EC%9E%A5/#2-6-DataFrame">2장 참고</a>)</li><li><strong>타입 안정성을 지원</strong> 하므로 초기화에 사용한 클래스 외 다른 클래스를 사용한 접근은 X</li><li>여러 명이 개발하고 잘 정의된 인터페이스로 상호작용하는 대규모 애플리케이션 개발에 유용<br><del>잘 정의된 인터페이스 부터가 실패다 이말이야</del></li></ul></li><li>Dataset 클래스 (Java <code>Dataset&lt;T&gt;</code>, Scala <code>Dataset[T]</code>)<ul><li>내부 객체 타입을 매개변수로 사용 (T) =&gt; 해당 클래스 객체만 가질 수 있음</li><li>스파크 2.0 에서는 자바의 JavaBean 패턴, 스칼라의 케이스 클래스 유형으로 정의된 클래스 지원</li><li>타입 T를 분석해서 Dataset 스키마를 생성해야하므로 타입을 제한할 수 밖에 없음</li></ul></li><li>장점<ul><li>필요한 경우 선택적으로 사용 가능하고, map, filter 등 함수 사용 가능</li><li>코드 변경 없이 타입 안정성을 보장할 수 있고, 안전하게 데이터 다루기 가능<ul><li><code>collect()</code> 나 <code>take()</code> 호출 시 DataFrame의 row 타입 객체가 아닌 Dataset의 지정된 타입(T)의 객체로 반환</li></ul></li></ul></li><li>Dataset의 자세한 내용은 CHAPTER 11 에서 이어서</li></ul><h3 id="3-3-구조적-스트리밍"><a href="#3-3-구조적-스트리밍" class="headerlink" title="3.3 구조적 스트리밍"></a>3.3 구조적 스트리밍</h3><details><summary class="point-color-can-hover">[3.3] 예제 펼치기 (정적 DataFrame 버전) </summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 정적 DataFrame 버전</span></span><br><span class="line"><span class="keyword">val</span> staticDataFrame = (spark.read.format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;inferSchema&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">  .load(<span class="string">&quot;/data/retail-data/by-day/*.csv&quot;</span>))</span><br><span class="line"></span><br><span class="line">staticDataFrame.createOrReplaceTempView(<span class="string">&quot;retail_data&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> staticSchema = staticDataFrame.schema</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;window, column, desc, col&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// &#x27;특정 고객(CustomerId)가 대량으로 구매하는 영업 시간&#x27; 구하기</span></span><br><span class="line">(staticDataFrame</span><br><span class="line">  .selectExpr(</span><br><span class="line">    <span class="string">&quot;CustomerId&quot;</span>,</span><br><span class="line">    <span class="string">&quot;(UnitPrice * Quantity) as total_cost&quot;</span>,</span><br><span class="line">    <span class="string">&quot;InvoiceDate&quot;</span>)</span><br><span class="line">  .groupBy(</span><br><span class="line">    col(<span class="string">&quot;CustomerId&quot;</span>), window(col(<span class="string">&quot;InvoiceDate&quot;</span>), <span class="string">&quot;1 day&quot;</span>)) <span class="comment">// 관련 날짜 데이터 그룹화 &amp; 집계</span></span><br><span class="line">  .sum(<span class="string">&quot;total_cost&quot;</span>)</span><br><span class="line">  .show(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// +----------+--------------------+------------------+</span></span><br><span class="line"><span class="comment">// |CustomerId|              window|   sum(total_cost)|</span></span><br><span class="line"><span class="comment">// +----------+--------------------+------------------+</span></span><br><span class="line"><span class="comment">// |   14075.0|[2011-12-05 00:00...|316.78000000000003|</span></span><br><span class="line"><span class="comment">// |   18180.0|[2011-12-05 00:00...|            310.73|</span></span><br><span class="line"><span class="comment">// |   15358.0|[2011-12-05 00:00...| 830.0600000000003|</span></span><br><span class="line"><span class="comment">// |   15392.0|[2011-12-05 00:00...|304.40999999999997|</span></span><br><span class="line"><span class="comment">// |   15290.0|[2011-12-05 00:00...|263.02000000000004|</span></span><br><span class="line"><span class="comment">// +----------+--------------------+------------------+</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>로컬 모드 사용 시 셔플 파티션 수 (default 200) 줄이기를 권장. <code>spark.conf.set(&quot;spark.sql.shuffle.partitions&quot;, &quot;5&quot;)</code></p></blockquote></details><br/><details><summary class="point-color-can-hover">[3.3] 예제 펼치기 (Streaming 버전) </summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Streaming 버전</span></span><br><span class="line"><span class="keyword">val</span> streamingDataFrame = (spark.readStream  <span class="comment">// read =&gt; readStream</span></span><br><span class="line">    .schema(staticSchema)</span><br><span class="line">    .option(<span class="string">&quot;maxFilesPerTrigger&quot;</span>, <span class="number">1</span>)  <span class="comment">// maxFilesPerTrigger (한번에 읽을 파일 수 설정) =&gt; 파일별로 트리거 수행</span></span><br><span class="line">    .format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">    .option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">    .load(<span class="string">&quot;/data/retail-data/by-day/*.csv&quot;</span>))</span><br><span class="line"></span><br><span class="line">streamingDataFrame.isStreaming <span class="comment">// returns true</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> purchaseByCustomerPerHour = (streamingDataFrame</span><br><span class="line">  .selectExpr(</span><br><span class="line">    <span class="string">&quot;CustomerId&quot;</span>,</span><br><span class="line">    <span class="string">&quot;(UnitPrice * Quantity) as total_cost&quot;</span>,</span><br><span class="line">    <span class="string">&quot;InvoiceDate&quot;</span>)</span><br><span class="line">  .groupBy(</span><br><span class="line">    $<span class="string">&quot;CustomerId&quot;</span>, window($<span class="string">&quot;InvoiceDate&quot;</span>, <span class="string">&quot;1 day&quot;</span>))</span><br><span class="line">  .sum(<span class="string">&quot;total_cost&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1) 스트림 시작 &amp; 인메모리 테이블에 저장</span></span><br><span class="line">(purchaseByCustomerPerHour.writeStream</span><br><span class="line">    .format(<span class="string">&quot;memory&quot;</span>) <span class="comment">// memory = 인메모리 테이블에 저장</span></span><br><span class="line">    .queryName(<span class="string">&quot;customer_purchases&quot;</span>) <span class="comment">// 인메모리에 저장될 테이블명</span></span><br><span class="line">    .outputMode(<span class="string">&quot;complete&quot;</span>) <span class="comment">// complete = 모든 카운트 수행 결과를 테이블에 저장</span></span><br><span class="line">    .start())</span><br><span class="line"></span><br><span class="line"><span class="comment">// 인메모리 테이블 확인 (데이터를 많이 읽으면 읽을수록 테이블 구성이 변경)</span></span><br><span class="line">(spark.sql(<span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">  SELECT *</span></span><br><span class="line"><span class="string">  FROM customer_purchases</span></span><br><span class="line"><span class="string">  ORDER BY `sum(total_cost)` DESC</span></span><br><span class="line"><span class="string">  &quot;</span><span class="string">&quot;&quot;</span>)</span><br><span class="line">  .show(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// +----------+--------------------+------------------+</span></span><br><span class="line"><span class="comment">// |CustomerId|              window|   sum(total_cost)|</span></span><br><span class="line"><span class="comment">// +----------+--------------------+------------------+</span></span><br><span class="line"><span class="comment">// |   17450.0|[2011-09-20 00:00...|          71601.44|</span></span><br><span class="line"><span class="comment">// |      null|[2011-11-14 00:00...|          55316.08|</span></span><br><span class="line"><span class="comment">// |      null|[2011-11-07 00:00...|          42939.17|</span></span><br><span class="line"><span class="comment">// |      null|[2011-03-29 00:00...| 33521.39999999998|</span></span><br><span class="line"><span class="comment">// |      null|[2011-12-08 00:00...|31975.590000000007|</span></span><br><span class="line"><span class="comment">// +----------+--------------------+------------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 2) 처리결과 콘솔에 출력</span></span><br><span class="line">(purchaseByCustomerPerHour.writeStream</span><br><span class="line">    .format(<span class="string">&quot;console&quot;</span>) <span class="comment">// console = 콘솔에 결과 출력</span></span><br><span class="line">    .queryName(<span class="string">&quot;customer_purchases_2&quot;</span>)</span><br><span class="line">    .outputMode(<span class="string">&quot;complete&quot;</span>)</span><br><span class="line">    .start())</span><br><span class="line"></span><br></pre></td></tr></table></figure></details><ul><li><strong>구조적 스트리밍</strong> : 스트림 처리용 고수준 API<ul><li>구조적 API로 개발된 배치 모드 연산을 <strong>스트리밍 방식으로</strong> 실행 가능하며, 지연 시간을 줄이고 증분 처리 가능</li><li>즉 스트리밍 처리로 <u>빠르게 값을 얻을 수 있고</u>, 모든 작업에서 데이터를 <u>증분 처리</u>하면서 수행된다</li><li>배치 잡으로 프로토타입 개발 후에 스트리밍 잡으로 변환도 가능</li><li>스파크 2.2 버전부터 안정화 (production-ready)</li></ul></li><li>데이터를 그룹화하고 집계하는 방법 (시계열 time-series 데이터  처리)<ul><li><code>window()</code> : 집계 시에, 시계열 컬럼 기준으로 각 날짜에 대한 전체 데이터를 가지는 윈도우 구성 =&gt; 날짜, 타임스탬프 처리에 유용</li></ul></li><li>정적 DataFrame 코드 vs 스트리밍 코드<ul><li><code>read</code> vs <code>readStream</code></li><li> 일반적인 정적 액션 vs <strong>스트리밍 액션</strong></li><li>스트리밍 액션은 어딘가에 데이터를 채워넣어야함. <strong>트리거</strong>가 실행된 후 데이터를 갱신<ul><li>(인메모리 테이블에 저장 시 - 스파크는 이전 집계값보다 더 큰 값이 발생할 때만 인메모리 테이블 갱신)</li></ul></li></ul></li><li><a href="http://bit.ly/2PvOwCS">예제 retail 데이터 셋</a><ul><li>by-day 하루 치 데이터 사용</li><li>예제는 인메모리 테이블에 저장 / 파일마다 트리거 실행</li><li>예제의 두가지 방식 (메모리/콘솔 출력, 파일별 트리거 수행)은 운영 환경에서는 권장 X</li></ul></li><li>데이터 처리 시점이 아닌 이벤트 시간에 따라 윈도우를 구성하는 방식에 주목<ul><li>기존 스파크 스트리밍의 단점 =&gt; <strong>구조적 스트리밍으로 보완</strong> 가능</li><li>스트림 처리과정의 스키마 추론방법 및 구조적 스트리밍은 CHAPTER 5 에서 자세히</li></ul></li></ul><h3 id="3-4-머신러닝과-고급-분석"><a href="#3-4-머신러닝과-고급-분석" class="headerlink" title="3.4 머신러닝과 고급 분석"></a>3.4 머신러닝과 고급 분석</h3><details><summary class="point-color-can-hover">[3.4] 예제 펼치기 </summary><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># MLlib 머신러닝 알고리즘 : 수치형 데이터 필요</span></span><br><span class="line"><span class="comment"># 예제의 (정적) 데이터 =&gt; 수치형으로 변환</span></span><br><span class="line"></span><br><span class="line">staticDataFrame.printSchema()</span><br><span class="line"></span><br><span class="line">root</span><br><span class="line"> |-- InvoiceNo: string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- StockCode: string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- Description: string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- Quantity: <span class="built_in">integer</span> (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- InvoiceDate: timestamp (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- UnitPrice: double (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- CustomerID: double (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- Country: string (nullable = <span class="literal">true</span>)</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.date_format</span><br><span class="line"><span class="keyword">val</span> preppedDataFrame = (staticDataFrame</span><br><span class="line">  .na.fill(<span class="number">0</span>) <span class="comment">// 0인 경우 null로 채움</span></span><br><span class="line">  .withColumn(<span class="string">&quot;day_of_week&quot;</span>, date_format($<span class="string">&quot;InvoiceDate&quot;</span>, <span class="string">&quot;EEEE&quot;</span>)) <span class="comment">// Sunday, Monday, ..</span></span><br><span class="line">  .coalesce(<span class="number">5</span>)) <span class="comment">// 파티션 개수 줄임 (default, shuffle = false)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// (1) 데이터 =&gt; 학습 데이터셋, 테스트 데이터셋으로 분리 (2011-07-01 기준)</span></span><br><span class="line"><span class="keyword">val</span> trainDataFrame = (preppedDataFrame</span><br><span class="line">  .where(<span class="string">&quot;InvoiceDate &lt; &#x27;2011-07-01&#x27;&quot;</span>))</span><br><span class="line"><span class="keyword">val</span> testDataFrame = (preppedDataFrame</span><br><span class="line">  .where(<span class="string">&quot;InvoiceDate &gt;= &#x27;2011-07-01&#x27;&quot;</span>))</span><br><span class="line"></span><br><span class="line">trainDataFrame.count()</span><br><span class="line"><span class="comment">// res110: Long = 245903</span></span><br><span class="line">testDataFrame.count()   </span><br><span class="line"><span class="comment">// res111: Long = 296006</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// (2-1) 요일(Sunday, Monday,..)을 수치형(0,1, ..)으로 반환</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.feature.<span class="type">StringIndexer</span></span><br><span class="line"><span class="keyword">val</span> indexer = (<span class="keyword">new</span> <span class="type">StringIndexer</span>()</span><br><span class="line">  .setInputCol(<span class="string">&quot;day_of_week&quot;</span>)</span><br><span class="line">  .setOutputCol(<span class="string">&quot;day_of_week_index&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// (2-2) 숫자로 표현된 범주형 데이터 인코딩 (해당 요일인지 Boolean 타입으로 확인 가능)</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.feature.<span class="type">OneHotEncoder</span></span><br><span class="line"><span class="keyword">val</span> encoder = (<span class="keyword">new</span> <span class="type">OneHotEncoder</span>()</span><br><span class="line">  .setInputCol(<span class="string">&quot;day_of_week_index&quot;</span>)</span><br><span class="line">  .setOutputCol(<span class="string">&quot;day_of_week_encoded&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// (2-3) 수치형 벡터 타입을 입력으로 사용 (가격, 수량, 특정 날짜의 요일)</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.feature.<span class="type">VectorAssembler</span></span><br><span class="line"><span class="keyword">val</span> vectorAssembler = (<span class="keyword">new</span> <span class="type">VectorAssembler</span>()</span><br><span class="line">  .setInputCols(<span class="type">Array</span>(<span class="string">&quot;UnitPrice&quot;</span>, <span class="string">&quot;Quantity&quot;</span>, <span class="string">&quot;day_of_week_encoded&quot;</span>))</span><br><span class="line">  .setOutputCol(<span class="string">&quot;features&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// (3) 파이프라인 설정</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.<span class="type">Pipeline</span></span><br><span class="line"><span class="keyword">val</span> transformationPipeline = (<span class="keyword">new</span> <span class="type">Pipeline</span>()</span><br><span class="line">  .setStages(<span class="type">Array</span>(indexer, encoder, vectorAssembler)))</span><br><span class="line"></span><br><span class="line"><span class="comment">// (4) 변환자(transformer) 를 데이터셋에 적합(fit) =&gt; &#x27;fitted pipeline&#x27;</span></span><br><span class="line"><span class="comment">// 일관되고 반복된 방식으로 데이터 변환 가능. 학습 데이터셋 생성 완료</span></span><br><span class="line"><span class="keyword">val</span> fittedPipeline = transformationPipeline.fit(trainDataFrame)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> transformedTraining = fittedPipeline.transform(trainDataFrame)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 캐싱 사용시 중간 변화된 데이터셋의 복사본을 메모리에 저장. 전체 파이프라인 재실행보다 훨씬 빠르다</span></span><br><span class="line"><span class="comment">// 근데 왜때문에 나는 더 느린 것..? ㅎ.. CHAPTER 4 에서 다시 확인하자</span></span><br><span class="line">transformedTraining.cache()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// (5) 모델 학습 (kmeans 모델 설정 과정은 생략..)</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.clustering.<span class="type">KMeans</span></span><br><span class="line"><span class="keyword">val</span> kmeans = (<span class="keyword">new</span> <span class="type">KMeans</span>()</span><br><span class="line">  .setK(<span class="number">20</span>)</span><br><span class="line">  .setSeed(<span class="number">1</span>L))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> kmModel = kmeans.fit(transformedTraining)</span><br><span class="line"></span><br><span class="line"><span class="comment">// (6) 학습 데이터셋에 대한 비용 (군집 비용) 계산</span></span><br><span class="line">kmModel.computeCost(transformedTraining)</span><br><span class="line"><span class="comment">// res146: Double = 8.455373996537486E7</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 테스트 데이터셋과 비교</span></span><br><span class="line"><span class="comment">// 모델 개선 방법은 CHAPTER 6 에서</span></span><br><span class="line"><span class="keyword">val</span> transformedTest = fittedPipeline.transform(testDataFrame)</span><br><span class="line">kmModel.computeCost(transformedTest)</span><br><span class="line"><span class="comment">// res150: Double = 5.175070947222117E8</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></details><ul><li><p>내장된 머신러닝 알고리즘 라이브러리 MLlib 사용한 대규모 머신러닝 가능</p><ul><li>대용량 데이터 대상의 전처리(proprocessing), 멍잉(munging), 모델 학습(model training), 예측(prediction)</li><li>구조적 스트리밍에서 예측하고자 할때도 예측 모델 사용 가능</li></ul></li><li><p>스파크는 분류(classification), 회귀(regression), 군집화(clustering), 딥러닝(deep learning) 같은 머신러닝 관련 정교한 API 제공</p><ul><li>두유 노- <code>k-평균</code> ? : 군집화 표준 알고리즘. 센트로이드(centroid)라는 중심점을 사용해서.. <code>p.99 참고</code></li></ul></li><li><p>k-평균을 사용한 예제</p><ul><li>원본 데이터를 올바른 포맷으로 만드는 트렌스포메이션 정의. 실제 모델 학습 후 다음 예측 수행</li></ul></li><li><p>스파크 (MLlib DataFrame API) 에서 머신러닝 모델 학습 과정 2단계</p><ol><li>아직 학습되지 않은 모델 초기화</li><li>해당 모델을 학습</li></ol></li></ul><blockquote><p>알고리즘 명명 규칙 </p><ul><li>학습 전 알고리즘 명칭 : {Algorithm_name}</li><li>학습 후 알고리즘 명칭 : {Algorithm_name} + ‘Model’</li></ul></blockquote><h3 id="3-5-저수준-API"><a href="#3-5-저수준-API" class="headerlink" title="3.5 저수준 API"></a>3.5 저수준 API</h3><ul><li>스파크는 <strong>RDD</strong> 를 통해 자바와 파이썬 객체를 다루는데 필요한 다양한 기본 기능 (저수준 API) 제공<ul><li>DataFrame을 포함해서 스파크의 거의 모든 기능이 RDD 기반</li><li>저수준 명령으로 컴파일 =&gt; 편리하고 매우 효율적인 분산처리</li></ul></li><li>원시 데이터를 다루는 용도로도 쓸 수는 있지만, 대부분 구조적 API 사용이 더 낫다<ul><li>대신 파티션과 같은 <strong>물리적 실행 특성을 결정</strong> 할 수 있어, 세밀한 제어가 가능</li><li>비정형 데이터, 정제되지 않은 원시 데이터 처리에 사용</li></ul></li><li>언어에 따라 RDD 세부 구현에 차이가 있음<ul><li>Scala, Python 모두 사용 가능하지만 RDD가 동일하지 X</li><li>(&lt;-&gt; 언어에 관계없이 동일한 실행 특성의 DataFrame API)</li></ul></li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 메모리에 저장된 원시 데이터를 병렬 처리 (parallize) 하여 RDD[Int] 생성 후 DataFrame으로 변환</span></span><br><span class="line">spark.sparkContext.parallelize(<span class="type">Seq</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)).toDF()</span><br></pre></td></tr></table></figure><h3 id="3-6-SparkR"><a href="#3-6-SparkR" class="headerlink" title="3.6 SparkR"></a>3.6 SparkR</h3><ul><li>SparkR : 스파크를 R언어로 사용하기 위한 기능<ul><li>파이썬 API 와 유사하고, 파이썬에서 사용할 수 있는 기능은 대부분 사용 가능</li><li>R 라이브러리 사용하여 스파크 트랜스포메이션 과정을 R과 유사하게 만들 수 있음</li><li>CHAPTER 7에서 자세히 알아보자</li></ul></li></ul><h3 id="3-7-스파크의-에코시스템과-패키지"><a href="#3-7-스파크의-에코시스템과-패키지" class="headerlink" title="3.7 스파크의 에코시스템과 패키지"></a>3.7 스파크의 에코시스템과 패키지</h3><ul><li>스파크의 최고 자랑 = 커뮤니티가 만들어낸 패키지 에코시스템 &amp; 다양한 기능<ul><li>스파크 패키지 저장소 : <a href="https://spark-packages.org/">https://spark-packages.org/</a></li><li>그 외 깃헙, 기타 웹사이트 …</li></ul></li></ul><h3 id="3-8-정리"><a href="#3-8-정리" class="headerlink" title="3.8 정리"></a>3.8 정리</h3><ul><li>스파크를 비즈니스와 기술적 문제 해결에 적용할 수 있는 다양한 방법<ul><li>단순하고 강력한 프로그래밍 모델, 손쉬운 적용</li><li>다양한 패키지는 여러 비즈니스 문제를 성공적으로 해결할 수 있는 스파크의 능력에 대한 증거</li><li>더 성장하도록 더 많은 패키지가 만들어질거다~</li></ul></li></ul><h3 id="📒-단어장"><a href="#📒-단어장" class="headerlink" title="📒 단어장"></a>📒 단어장</h3><ul><li>정적 타입 코드/언어 (Statically typed) : 자료형이 고정된 언어. 컴파일 때 변수 타입이 결정 (ex. Java, Scala, C, C++ 등)<ul><li>&lt;-&gt; 동적 타입 언어 (Dynamically typed) : 런타임에 변수 타입이 결정 (ex. Python, JavaScript 등)</li></ul></li><li>멍잉 (munging) : =data wrangling. 원본 데이터를 다른 형태로 변환하거나 매핑하는 과정</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;br/&gt;

&lt;h4 id=&quot;Part-1-END&quot;&gt;&lt;a href=&quot;#Part-1-END&quot; class=&quot;headerlink&quot; title=&quot;[Part 1] END&quot;&gt;&lt;/a&gt;[Part 1] END&lt;/h4&gt;&lt;p&gt;파트 1 까지 끝내고 나니 이제 조금씩 스파크 맛</summary>
      
    
    
    
    <category term="spark" scheme="https://minsw.github.io/categories/spark/"/>
    
    
    <category term="book" scheme="https://minsw.github.io/tags/book/"/>
    
    <category term="study" scheme="https://minsw.github.io/tags/study/"/>
    
    <category term="spark" scheme="https://minsw.github.io/tags/spark/"/>
    
    <category term="apache" scheme="https://minsw.github.io/tags/apache/"/>
    
  </entry>
  
  <entry>
    <title>&amp;#039;Spark The Definitive Guide&amp;#039; 2장 - 스파크 찍어먹기</title>
    <link href="https://minsw.github.io/2021/01/24/Spark-The-Definitive-Guide-2%EC%9E%A5/"/>
    <id>https://minsw.github.io/2021/01/24/Spark-The-Definitive-Guide-2%EC%9E%A5/</id>
    <published>2021-01-24T09:42:51.000Z</published>
    <updated>2021-01-24T18:11:30.114Z</updated>
    
    <content type="html"><![CDATA[<img src="https://user-images.githubusercontent.com/26691216/105627228-e66e3200-5e78-11eb-9ea6-2e3662267b7a.jpg" width=200 /><center> 도커 이미지 사용시 Zeppelin에 예제 코드가 있다 <br/>나처럼 시력 검사&타자 연습 하느라 진빼지말고 Chapter2는 그냥 예제 코드를 쓰도록 하자... </center><center><h2>_ _ _</h2></center><br/><hr><h1 id="CHAPTER-2-스파크-간단히-살펴보기"><a href="#CHAPTER-2-스파크-간단히-살펴보기" class="headerlink" title="CHAPTER 2 스파크 간단히 살펴보기"></a>CHAPTER 2 스파크 간단히 살펴보기</h1><p>DataFrame, SQL 을 사용해 클러스터, 스파크 애플리케이션, 구조적 API 를 살펴보고<br>스파크의 핵심용어와 개념, 사용법을 익힌다.</p><h3 id="2-1-스파크의-기본-아키텍처"><a href="#2-1-스파크의-기본-아키텍처" class="headerlink" title="2.1 스파크의 기본 아키텍처"></a>2.1 스파크의 기본 아키텍처</h3><blockquote><p>스파크 애플리케이션을 이해하기 위한 핵심사항</p><ul><li>스파크는 사용가능한 자원을 파악하기 위해 <strong>클러스터 매니저</strong> 사용</li><li><strong>드라이버</strong> 프로세스는 주어직 작업을 완료하기위해, 드라이버 프로그램의 명령을 <strong>익스큐터</strong>에서 실행할 책임이 있음</li></ul></blockquote><ul><li>스파크는 클러스터의 데이터 처리 작업을 관리 / 조율<ul><li>컴퓨터 클러스터는 여러 컴퓨터의 자원을 모아 하나의 컴퓨터 처럼 사용</li><li>클러스터에서 작업을 조율할 수 있는 프레임워크 =&gt; <strong>스파크</strong></li></ul></li><li>스파크가 연산에 사용할 클러스터를 관리하는 <strong>클러스터 매니저</strong><ul><li>스파크 standalone 클러스터 매니저, 하둡 YARN, Mesos</li><li>역할<ul><li>사용자 : 스파크 애플리케이션 제출 (submit)</li><li>-&gt; 클러스터 매니저 : 애플리케이션 실행에 필요한 자원 할당 </li><li>-&gt; 할당받은 자원으로 작업 처리</li></ul></li></ul></li><li>스파크 애플리케이션 = <code>driver</code> 프로세스 + 다수의 <code>executor</code> 프로세스<ul><li><code>driver</code> 프로세스<ul><li>클러스터 노드 중 하나에서 실행. main() 함수 실행</li><li>심장과 같은 존재로, 애플리케이션 생명 주기 동안 관련 정보 모두 유지</li></ul></li><li><code>executor</code> 프로세스<ul><li>driver 가 할당한 작업 수행 &amp; 진행 상황을 driver에게 보고</li><li>대부분 스파크 코드를 실행하는 역할로, 스파크 언어 API를 통해 다양한 언어로 실행 가능</li></ul></li></ul></li></ul><h3 id="2-2-스파크의-다양한-언어-API"><a href="#2-2-스파크의-다양한-언어-API" class="headerlink" title="2.2 스파크의 다양한 언어 API"></a>2.2 스파크의 다양한 언어 API</h3><ul><li>스파크는 모든 언어에 맞는 몇몇 ‘핵심 개념’ 제공<ul><li>핵심개념 -&gt; (클러스터 머신에서 실행되는) 스파크 코드 로 변환</li><li>구조적 API만으로 작성된 코드는 언어에 무관하게 유사 성능</li></ul></li><li>언어별 요약 정보<ul><li>Scala : 스파크가 스칼라 기반. <strong>스파크의 기본 언어</strong></li><li>Java : <del>자바 지원안해주면 난리칠거니까</del> 지원은 함</li><li>Python : 스칼라가 지원하는 거의 모든 구조 지원</li><li>SQL : ANSI SQL:2003 표준 중 일부 지원</li><li>R : 스파크 코어의 sparkR, R 커뮤니티 기반의 sparklyr</li></ul></li><li>SparkSession 객체<ul><li>사용자가 스파크 코드를 실행하기위해 진입점으로 사용 가능</li><li>Python, R 사용 시에도 사용자 대신 익스큐터의 JVM에서 실행할 수 있는 코드로 변환</li></ul></li></ul><h3 id="2-3-스파크-API"><a href="#2-3-스파크-API" class="headerlink" title="2.3 스파크 API"></a>2.3 스파크 API</h3><ul><li>다양한 언어로 사용할 수 있는 이유?<ul><li>스파크가 기본적으로 제공하는 2가지 API 때문<ul><li>저수준의 비구조적(unstructured) API</li><li>고수준의 구조적(structured) API</li></ul></li></ul></li></ul><h3 id="2-4-스파크-시작하기"><a href="#2-4-스파크-시작하기" class="headerlink" title="2.4 스파크 시작하기"></a>2.4 스파크 시작하기</h3><ul><li>Q. 스파크 애플리케이션을 개발하려면<ul><li>A. 사용자 명령과 데이터를 스파크 애플리케이션에 전송하는 방법을 알아야</li></ul></li><li>SparkSession 생성 실습. 자 드가자~</li></ul><h3 id="2-5-SparkSession"><a href="#2-5-SparkSession" class="headerlink" title="2.5 SparkSession"></a>2.5 SparkSession</h3><ul><li><strong>SparkSession</strong> : 스파크 애플리케이션을 제어하는 드라이버 프로세스<ul><li>사용자가 정의한 처리명령 -&gt; 클러스터에 실행</li><li>스파크 애플리케이션에 1:1 대응</li></ul></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> scala console</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ./spark-2.4.7-bin-hadoop2.7/bin/spark-shell</span></span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> spark</span></span><br><span class="line">res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5b58f639</span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val myRange = spark.range(1000).toDF(<span class="string">&quot;number&quot;</span>)</span></span><br><span class="line">myRange: org.apache.spark.sql.DataFrame = [number: bigint]</span><br></pre></td></tr></table></figure><h3 id="2-6-DataFrame"><a href="#2-6-DataFrame" class="headerlink" title="2.6 DataFrame"></a>2.6 DataFrame</h3><ul><li><strong>DataFrame</strong> : 가장 대표적인 <strong>구조적 API</strong><ul><li>테이블 데이터를 row, column 으로 단순하게 표현<ul><li>scheme : column 과 column type 을 정의한 목록</li></ul></li><li>DataFrame 은 수천 대의 컴퓨터에 분산 가능</li><li>vs 스프레드 시트<ul><li>비슷하다고 볼 수 있지만 스프레드 시트는 단일 컴퓨터 저장</li></ul></li><li>vs Python (Pandas)의 DataFrame, R의 DataFrame<ul><li>마찬가지로 대부분 단일 컴퓨터에 존재</li><li>=&gt; 스파크 DataFrame으로 쉽게 변환 가능</li></ul></li></ul></li><li>스파크의 핵심 추상화 개념 (분산 데이터 모음)<ul><li>Dataset, DataFrame, SQL 테이블, RDD</li></ul></li><li>DataFrame의 파티션<ul><li>익스큐터가 병렬로 작업을 수행할 수 있도록 데이터를 분할하는 청크 단위</li><li>실행 중 데이터가 클러스터에서 물리적으로 분산되는 방식을 나타냄<ul><li>파티션 1 익스큐터 1000 =&gt; 병렬성 1</li><li>파티션 1000 익스큐터 1 =&gt; 병렬성 1</li></ul></li><li>물리적 파티션에 데이터 변환용 함수 지정 시 스파크가 실제 처리 방법 결정 (파티션 수동 처리 필요 X)</li></ul></li></ul><h3 id="2-7-트랜스포메이션"><a href="#2-7-트랜스포메이션" class="headerlink" title="2.7 트랜스포메이션"></a>2.7 트랜스포메이션</h3><ul><li>스파크의 핵심 데이터 구조 =&gt; <strong>불변성 (immutable)</strong><ul><li>DataFrame을 변경하려면?</li><li>원하는 변경 방법을 스파크에게 알려줘야함 =&gt; <strong>트랜스포메이션</strong></li></ul></li><li>트랜스포메이션 : 스파크에서 비즈니스 로직을 표현하는 핵심 개념<ul><li>유형<ul><li>좁은 의존성 (narrow dependency)<ul><li>입력 파티션 : 출력 파티션 = 1 : 1</li></ul></li><li>넓은 의존성 (wide dependency)<ul><li>입력 파티션 : 출력 파티션 = 1 : N</li></ul></li></ul></li></ul></li><li>지연 연산 (lazy evaluation) : 연산 그래프를 처리하기 직전까지 기다리는 동작 방식<ul><li>스파크는 연산 명령 즉시 데이터를 수정 X. 원시 데이터에 적용할 트랜스포메이션의 <strong>실행 계획</strong>을 생성</li><li>마지막까지 대기하다 DataFrame 트랜스포메이션을 간결한 물리적 실행 계획으로 컴파일 =&gt; 전체 데이터 흐름 최적화</li><li>ex. DataFrame 의 predicate pushdown</li></ul></li></ul><h3 id="2-8-액션"><a href="#2-8-액션" class="headerlink" title="2.8 액션"></a>2.8 액션</h3><ul><li>트랜스포메이션은 논리적 실행 계획<ul><li>트랜스포메이션을 선언해도 액션을 호출하지 않으면 수행 X</li></ul></li><li>액션 (action) : 실제 연산을 수행<ul><li>유형<ul><li>콘솔에서 데이터를 보는 액션</li><li>각 언어로 된 네이티브 객체에 데이터를 모으는 액션</li><li>출력 데이터소스에 저장하는 액션</li></ul></li></ul></li><li>액션 지정 시 스파크 잡 시작<ul><li><strong>스파크 잡 (job)</strong><ul><li>필터 (좁은 트랜스포메이션) 수행</li><li>-&gt; 파티션 별로 레코드 수를 카운트 (넓은 트랜스포메이션)</li><li>-&gt; 각 언어에 적합한 네이티브 객체에 결과 모음</li></ul></li><li>스파크 UI로 잡 모니터링 가능</li><li><em>스파크 잡은 개별 액션에 의해 트리거되는 다수의 트랜스포메이션으로 이루어져 있다</em></li></ul></li></ul><h3 id="2-9-스파크-UI"><a href="#2-9-스파크-UI" class="headerlink" title="2.9 스파크 UI"></a>2.9 스파크 UI</h3><ul><li>드라이버 노드의 4040 포트</li><li>스파크 잡의 상태, 환경 설정, 클러스터 상태 등의 정보 확인 가능</li></ul><h3 id="2-10-종합-예제"><a href="#2-10-종합-예제" class="headerlink" title="2.10 종합 예제"></a>2.10 종합 예제</h3><ul><li>미국 교통통계국의 항공운항 데이터 중 일부로 실습<ul><li><a href="https://bit.ly/2yw2fCx">샘플 데이터</a> : 반정형(semi-structured), csv 포맷</li><li>(=&gt; 부록 A의 도커 이미지 사용 시 이미 포함)</li></ul></li><li>스파크는 다양한 데이터소스 지원<ul><li>SparkSession의 DataFrameReader 클래스 사용해서 읽음</li><li>예제는 <strong>스키마 추론 (Schema inference)</strong> 기능 추가<ul><li>스파크는 각 컬럼의 데이터 타입 추론을 위해 적은 양의 데이터를 읽음 </li></ul></li><li>DataFrame 은 불특적 다수의 로우와 컬럼<ul><li>지연 연산 형태의 트렌스포메이션이므로 row 수 알 수 X</li></ul></li></ul></li></ul><h4 id="예제-1"><a href="#예제-1" class="headerlink" title="예제 1"></a>예제 1</h4><details><summary class="point-color-can-hover">예제 1 펼치기</summary><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ head /data/flight-data/csv/2015-summary.csv</span><br><span class="line">DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count</span><br><span class="line">United States,Romania,15</span><br><span class="line">United States,Croatia,1</span><br><span class="line">..</span><br><span class="line"></span><br><span class="line"><span class="comment"># spark-shell (scala)</span></span><br><span class="line">scala&gt; val flightData2015 = spark.read.option(<span class="string">&quot;inferSchema&quot;</span>, <span class="string">&quot;true&quot;</span>).option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>).csv(<span class="string">&quot;/data/flight-data/csv/2015-summary.csv&quot;</span>)</span><br><span class="line">flightData2015: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]</span><br><span class="line"></span><br><span class="line">scala&gt; flightData2015.take(3)</span><br><span class="line">res0: Array[org.apache.spark.sql.Row] = Array([United States,Romania,15], [United States,Croatia,1], [United States,Ireland,344])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; flightData2015.sort(<span class="string">&quot;count&quot;</span>).explain()</span><br><span class="line">== Physical Plan ==</span><br><span class="line">*(2) Sort [count<span class="comment">#12 ASC NULLS FIRST], true, 0</span></span><br><span class="line">+- Exchange rangepartitioning(count<span class="comment">#12 ASC NULLS FIRST, 200)</span></span><br><span class="line">   +- *(1) FileScan csv [DEST_COUNTRY_NAME<span class="comment">#10,ORIGIN_COUNTRY_NAME#11,count#12] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 셔플 파티션 default 200개 =&gt; 5개</span></span><br><span class="line">scala&gt; spark.conf.set(<span class="string">&quot;spark.sql.shuffle.partitions&quot;</span>, <span class="string">&quot;5&quot;</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; flightData2015.sort(<span class="string">&quot;count&quot;</span>).explain()</span><br><span class="line">== Physical Plan ==</span><br><span class="line">*(2) Sort [count<span class="comment">#12 ASC NULLS FIRST], true, 0</span></span><br><span class="line">+- Exchange rangepartitioning(count<span class="comment">#12 ASC NULLS FIRST, 5)</span></span><br><span class="line">   +- *(1) FileScan csv [DEST_COUNTRY_NAME<span class="comment">#10,ORIGIN_COUNTRY_NAME#11,count#12] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int&gt;</span></span><br><span class="line"></span><br><span class="line">scala&gt; flightData2015.sort(<span class="string">&quot;count&quot;</span>).take(2)</span><br><span class="line">res3: Array[org.apache.spark.sql.Row] = Array([United States,Singapore,1], [Moldova,United States,1])</span><br><span class="line"></span><br></pre></td></tr></table></figure></details><blockquote><ul><li><code>take(n)</code> : Action</li><li><code>sort()</code> :  Transformation (넓은) <ul><li>DataFrame 을 변경하지 않고 새로운 DataFrame을 생성해 반환</li></ul></li><li><code>explain()</code> <ul><li>DataFrame의 계보(lineage) 나 스파크 쿼리 실행 계획 출력</li></ul></li></ul></blockquote><ul><li>실행 계획? : 디버깅과 스파크의 실행과정을 이해하는데 도움을 주는 도구<ul><li>위에서 아래방향으로 읽는다</li><li>최종 결과는 가장 위, 데이터소스는 가장 아래</li></ul></li><li>DataFrame의 계보<ul><li>트랜스포메이션의 논리적 실행 계획 -&gt; DataFrame의 계보 정의</li><li>-&gt; 계보를 통해 스파크가 입력데이터에 수행한 연산을 전체 파티션에서 어떻게 재연산하는지 알 수 있음</li><li><em>함수형 프로그래밍의 핵심</em> (Pure Function, 같은 입력 -&gt; 같은 출력)</li></ul></li><li>사용자는 물리적 데이터를 직접 다루지 않고, 물리적 실행 특성을 제어<ul><li>예시 =&gt; 파티션 수 변경 <code>spark.conf.set(&quot;spark.sql.shuffle.partitions&quot;, &quot;5&quot;)</code></li><li>스파크 UI (4040 포트) 에서 스파크 잡 물리적, 논리적 실행 특성 확인 가능 <img width="500" alt="sparkui" src="https://user-images.githubusercontent.com/26691216/105624926-cfbfdf00-5e68-11eb-9407-e58a5f4688a9.png"></li></ul></li></ul><h4 id="예제-2-SQL"><a href="#예제-2-SQL" class="headerlink" title="예제 2 (SQL)"></a>예제 2 (SQL)</h4><details><summary class="point-color-can-hover">예제 2-1 펼치기</summary><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1) SQL 사용</span></span><br><span class="line">scala&gt; flightData2015.createOrReplaceTempView(<span class="string">&quot;flight_data_2015&quot;</span>)</span><br><span class="line">scala&gt; val sqlWay = spark.sql(<span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">     | SELECT DEST_COUNTRY_NAME, count(1)</span></span><br><span class="line"><span class="string">     | FROM flight_data_2015</span></span><br><span class="line"><span class="string">     | GROUP BY DEST_COUNTRY_NAME</span></span><br><span class="line"><span class="string">     | &quot;</span><span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; sqlWay.explain</span><br><span class="line">== Physical Plan ==</span><br><span class="line">*(2) HashAggregate(keys=[DEST_COUNTRY_NAME<span class="comment">#10], functions=[count(1)])</span></span><br><span class="line">+- Exchange hashpartitioning(DEST_COUNTRY_NAME<span class="comment">#10, 5)</span></span><br><span class="line">   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME<span class="comment">#10], functions=[partial_count(1)])</span></span><br><span class="line">      +- *(1) FileScan csv [DEST_COUNTRY_NAME<span class="comment">#10] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2) DataFrame 사용</span></span><br><span class="line">scala&gt; val dataFrameWay = flightData2015.groupBy(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>).count()</span><br><span class="line">dataFrameWay: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, count: bigint]</span><br><span class="line"></span><br><span class="line">scala&gt; dataFrameWay.explain</span><br><span class="line">== Physical Plan ==</span><br><span class="line">*(2) HashAggregate(keys=[DEST_COUNTRY_NAME<span class="comment">#10], functions=[count(1)])</span></span><br><span class="line">+- Exchange hashpartitioning(DEST_COUNTRY_NAME<span class="comment">#10, 5)</span></span><br><span class="line">   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME<span class="comment">#10], functions=[partial_count(1)])</span></span><br><span class="line">      +- *(1) FileScan csv [DEST_COUNTRY_NAME<span class="comment">#10] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></details><ul><li>스파크는 언어에 무관하게 같은 방식으로 트랜스포메이션 실행<ul><li>SQL, DataFrame(R, Python, Scalar, Java) 에서 비즈니스 로직 표현</li><li>스파크에서 코드 실행 전에 로직을 기본 실행계획(<code>explain</code>) 으로 컴파일</li></ul></li><li>스파크 SQL 사용시 모든 DataFrame =&gt; 테이블, 뷰 (임시 테이블) 로 등록<ul><li>위에서 설명했듯 <strong>같은 실행 계획</strong>으로 컴파일하므로 성능차이 X</li></ul></li></ul><details><summary class="point-color-can-hover">예제 2-2 펼치기</summary><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># &#x27;최대 비행 횟수&#x27; 구하기</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># SQL 쿼리</span></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;SELECT max(count) from flight_data_2015&quot;</span>).take(1)</span><br><span class="line">res9: Array[org.apache.spark.sql.Row] = Array([370002])</span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame 구문 _ max 함수 (트랜스포메이션) 사용</span></span><br><span class="line">scala&gt; import org.apache.spark.sql.functions.max</span><br><span class="line"></span><br><span class="line">scala&gt; flightData2015.select(max(<span class="string">&quot;count&quot;</span>)).take(1)</span><br><span class="line">res10: Array[org.apache.spark.sql.Row] = Array([370002])</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># &#x27;상위 5개의 도착 국가&#x27; 구하기</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># SQL 쿼리</span></span><br><span class="line">scala&gt; val maxSql = spark.sql(<span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">     | SELECT DEST_COUNTRY_NAME, sum(count) as destination_total</span></span><br><span class="line"><span class="string">     | FROM flight_data_2015</span></span><br><span class="line"><span class="string">     | GROUP BY DEST_COUNTRY_NAME</span></span><br><span class="line"><span class="string">     | ORDER BY sum(count) DESC</span></span><br><span class="line"><span class="string">     | LIMIT 5</span></span><br><span class="line"><span class="string">     | &quot;</span><span class="string">&quot;&quot;</span>)</span><br><span class="line">maxSql: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, destination_total: bigint]</span><br><span class="line"></span><br><span class="line">scala&gt; maxSql.show()</span><br><span class="line">+-----------------+-----------------+</span><br><span class="line">|DEST_COUNTRY_NAME|destination_total|</span><br><span class="line">+-----------------+-----------------+</span><br><span class="line">|    United States|           411352|</span><br><span class="line">|           Canada|             8399|</span><br><span class="line">|           Mexico|             7140|</span><br><span class="line">|   United Kingdom|             2025|</span><br><span class="line">|            Japan|             1548|</span><br><span class="line">+-----------------+-----------------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame 구문</span></span><br><span class="line">scala&gt; import org.apache.spark.sql.functions.desc</span><br><span class="line"></span><br><span class="line">scala&gt; flightData2015.groupBy(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>).sum(<span class="string">&quot;count&quot;</span>).withColumnRenamed(<span class="string">&quot;sum(count)&quot;</span>, <span class="string">&quot;destination_total&quot;</span>).sort(desc(<span class="string">&quot;destination_total&quot;</span>)).<span class="built_in">limit</span>(5).show()</span><br><span class="line">+-----------------+-----------------+</span><br><span class="line">|DEST_COUNTRY_NAME|destination_total|</span><br><span class="line">+-----------------+-----------------+</span><br><span class="line">|    United States|           411352|</span><br><span class="line">|           Canada|             8399|</span><br><span class="line">|           Mexico|             7140|</span><br><span class="line">|   United Kingdom|             2025|</span><br><span class="line">|            Japan|             1548|</span><br><span class="line">+-----------------+-----------------+</span><br><span class="line"></span><br><span class="line"><span class="comment"># 코드 수행 단계 : CSV 파일 =&gt; (1) read -&gt; (2) groupBy -&gt; (3) sum -&gt; (4) withColumnRenamed -&gt; (5) sort -&gt; (6) limit -&gt; (7) collect =&gt; Array(..)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># scala&gt; ~.explain</span></span><br><span class="line">== Physical Plan ==</span><br><span class="line">TakeOrderedAndProject(<span class="built_in">limit</span>=5, orderBy=[destination_total<span class="comment">#108L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#10,destination_total#108L])</span></span><br><span class="line">+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME<span class="comment">#10], functions=[sum(cast(count#12 as bigint))])</span></span><br><span class="line">   +- Exchange hashpartitioning(DEST_COUNTRY_NAME<span class="comment">#10, 5)</span></span><br><span class="line">      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME<span class="comment">#10], functions=[partial_sum(cast(count#12 as bigint))])</span></span><br><span class="line">         +- *(1) FileScan csv [DEST_COUNTRY_NAME<span class="comment">#10,count#12] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,count:int&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></details><ul><li>실행계획은 트랜스포메이션의 <strong>지향성 비순환 그래프 (Directed Acyclic Graph, DAG)</strong><ul><li>액션이 호출되면 결과를 만들어낸다</li><li>DAG의 각 단계는 불변성을 가진 신규 DataFrame을 생성</li></ul></li><li>예제의 전체 코드 수행 단계 (7단계) 는 p.86 [그림 2-10] 참조<ul><li>실제 실행 계획 (<code>explain</code> 이 출력하는) 은 물리적인 실행 시점에서 수행하는 최적화로 인해 다를 수 있음</li><li>직접 explain 해보면 책의 explain 과도 다르게 출력됨 </li></ul></li></ul><h3 id="2-11-정리"><a href="#2-11-정리" class="headerlink" title="2.11 정리"></a>2.11 정리</h3><ul><li>트랜스포메이션, 액션, DataFrame 실행 계획 최적화 방법<ul><li>트랜스포메이션의 지향성 비순환 그래프(DAG) 를 지연 실행하여 최적화</li></ul></li><li>예제를 통한 데이터가 파티션으로 구성되는 방법, 복잡한 트랜스포메이션 작업 실행 단계 확인</li></ul><br/><h3 id="📒-단어장"><a href="#📒-단어장" class="headerlink" title="📒 단어장"></a>📒 단어장</h3><ul><li>셔플 (Shuffle) : 스파크카 클러스터에서 파티션을 교환<ul><li>스파크는 셔플의 결과를 디스크에 저장</li></ul></li><li>가환성 (Commutative) : 두 대상의 연산 결과가 순서와 관계없이 동일 (-&gt; 교환 법칙)</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;img src=&quot;https://user-images.githubusercontent.com/26691216/105627228-e66e3200-5e78-11eb-9ea6-2e3662267b7a.jpg&quot; width=200 /&gt;
&lt;center&gt; 도커 이미</summary>
      
    
    
    
    <category term="spark" scheme="https://minsw.github.io/categories/spark/"/>
    
    
    <category term="book" scheme="https://minsw.github.io/tags/book/"/>
    
    <category term="study" scheme="https://minsw.github.io/tags/study/"/>
    
    <category term="spark" scheme="https://minsw.github.io/tags/spark/"/>
    
    <category term="apache" scheme="https://minsw.github.io/tags/apache/"/>
    
  </entry>
  
  <entry>
    <title>&amp;#039;Spark The Definitive Guide&amp;#039; 1장 - Apache Spark (스파크) 는 뭘까</title>
    <link href="https://minsw.github.io/2021/01/20/Spark-The-Definitive-Guide-1%EC%9E%A5/"/>
    <id>https://minsw.github.io/2021/01/20/Spark-The-Definitive-Guide-1%EC%9E%A5/</id>
    <published>2021-01-20T00:20:29.000Z</published>
    <updated>2021-01-24T18:11:30.113Z</updated>
    
    <content type="html"><![CDATA[<br/><h4 id="‘Spark-The-Definitive-Guide’"><a href="#‘Spark-The-Definitive-Guide’" class="headerlink" title="‘Spark The Definitive Guide’"></a>‘Spark The Definitive Guide’</h4><p><i>스파크 완벽 가이드 (&amp; 하둡 완벽 가이드), O’REILLY</i></p><p>전형적으로 묶어놔야 공부하는 타입이라..<br>어떻게든 마음의 부채를 쌓기 위해 일단 책부터 사고 일도 최대한 크게 벌리고 시작하기로 했다.</p><p>책 읽고 간단하게만 정리하는거라 내용이 별거 없긴 해도<br>또 대충 쓰다 어디 쑤셔 박아놓고서 못 찾지 말고 <i style="color:lightgray">(“엄마~ 내 글 어딨어?”)</i> 습관도 들일 겸 블로그에 남겨두는게 좋겠다.</p><p><strong>완벽</strong> 가이드라니까 일단 파트 3까지는 아묻따 따라가보자. 자 드가자~!</p><blockquote><p><strong>Apache Spark™</strong> is a unified analytics engine for large-scale data processing.</p><p>Apache Saprk 공식 페이지 <a href="https://spark.apache.org/">https://spark.apache.org/</a><br>Latest Release Spark 3.0.1</p></blockquote><br/><img src="https://user-images.githubusercontent.com/26691216/105111053-699f2900-5b03-11eb-87c1-05d1c704f2d3.jpg" width=240 /><center><h2>_ _ _</h2></center><br/><hr><h1 id="CHAPTER-1-아파치-스파크란"><a href="#CHAPTER-1-아파치-스파크란" class="headerlink" title="CHAPTER 1 아파치 스파크란"></a>CHAPTER 1 아파치 스파크란</h1><h3 id="아파치-스파크-Apache-Spark-란"><a href="#아파치-스파크-Apache-Spark-란" class="headerlink" title="아파치 스파크 (Apache Spark) 란"></a>아파치 스파크 (Apache Spark) 란</h3><p>통합 컴퓨팅 엔진이며 클러스터 환경에서 데이터를 병렬로 처리하는 라이브러리 집합.</p><ul><li>Spark 언어 4대 천왕 - Python, Java, Scala, R</li><li>SQL, Library, ML 등 라이브러리 제공</li></ul><h3 id="1-1-아파치-스파크의-철학"><a href="#1-1-아파치-스파크의-철학" class="headerlink" title="1.1 아파치 스파크의 철학"></a>1.1 아파치 스파크의 철학</h3><blockquote><p><strong>빅데이터를 위한 통합 컴퓨팅 엔진과 라이브러리 집합.</strong></p></blockquote><ul><li>통합 (unified)<ul><li>다양한 처리 유형을 지원하기 위한 자체 API</li></ul></li><li>컴퓨팅 엔진<ul><li>스파크는 데이터 저장 위치에 상관없이 처리에 집중</li><li>vs 하둡<ul><li>하둡은 저비용 저장장치를 사용하는 하둡 파일 시스템과 컴퓨팅 시스템(MR) 이 밀접하게 연관</li><li>스파크는 하둡 저장소 뿐만 아니라 하둡 아키텍처를 사용할 수 없는 환경에서도 호환 가능</li></ul></li></ul></li><li>라이브러리<ul><li>궁극적으론 데이터 분석 작업에 필요한 통합 API를 제공하는 통합 엔진 기반의 자체 라이브러리</li><li>수많은 외부 라이브러리도.. (<a href="https://spark-packages.org/">https://spark-packages.org/</a>)</li></ul></li></ul><h3 id="1-2-스파크의-등장-배경"><a href="#1-2-스파크의-등장-배경" class="headerlink" title="1.2 스파크의 등장 배경"></a>1.2 스파크의 등장 배경</h3><ul><li>더 많은 연산과 대규모 데이터 처리를 프로세서의 성능 향상에 맡겼으나, H/W 성능 향상은 2005년까지</li><li>기술의 발전으로 데이터 수집 비용은 저렴해졌지만 데이터는 클러스터에서 처리해야할 만큼 거대해짐</li><li>새로운 프로그래밍 모델이 필요 =&gt; <strong>아파치 스파크</strong></li></ul><h3 id="1-3-스파크의-역사"><a href="#1-3-스파크의-역사" class="headerlink" title="1.3 스파크의 역사"></a>1.3 스파크의 역사</h3><ul><li>UC버클리 대학에서 2009년 프로젝트로 시작<ul><li>당시 하둡 맵리듀스가 클러스터 환경용 병렬 프로그래밍 엔진의 대표주자</li></ul></li><li>‘표준 라이브러리’ 형태의 구현 방식<ul><li>조합형 API의 핵심 아이디어 진화</li><li>~ 1.0 : 함수형 연산</li><li>1.0 : 구조화된 데이터 기반의 스파크 SQL</li><li>이 후에는 더 강력한 구조체 기반의 신규 API 들 (ex. DataFrame, 머신러닝 파이프라인, 구조적 스트리밍)</li></ul></li></ul><h3 id="1-4-스파크의-현재와-미래"><a href="#1-4-스파크의-현재와-미래" class="headerlink" title="1.4 스파크의 현재와 미래"></a>1.4 스파크의 현재와 미래</h3><ul><li>거대 규모 데이터셋이나 과학적 데이터 분석에 사용 중</li><li>인기 많다는 이야기</li></ul><h3 id="1-5-스파크-실행하기-gt-부록-A"><a href="#1-5-스파크-실행하기-gt-부록-A" class="headerlink" title="1.5 스파크 실행하기 -&gt; 부록 A"></a>1.5 스파크 실행하기 -&gt; 부록 A</h3><blockquote><p><em>… 특히 도커 환경에서 예제를 실행해보고 싶다면 부록 A를 참고하시기 바랍니다.</em></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ docker pull docker.io/rheor108/spark_the_definitive_guide_practice</span><br><span class="line">$ docker run -p 8080:8080 -p 4040:4040 --name spark_ex rheor108/spark_the_definitive_guide_practice</span><br><span class="line"><span class="comment"># Zeppelin UI : http://localhost:8080 </span></span><br></pre></td></tr></table></figure><blockquote><p>[부록 A] Spark The Definitive Guide 예제 실행 환경 Docker image</p><ul><li><p><a href="https://hub.docker.com/r/rheor108/spark_the_definitive_guide_practice">https://hub.docker.com/r/rheor108/spark_the_definitive_guide_practice</a></p></li><li><p>Spark version: 2.3.2 Python version: 2.7 Zeppelin version: 0.8.0 (21.01 기준)</p></li></ul><p>Spark The Definitive Guide 저장소</p><ul><li>원서 : <a href="https://github.com/databricks/Spark-The-Definitive-Guide">https://github.com/databricks/Spark-The-Definitive-Guide</a></li><li>번역 예제 : <a href="https://github.com/FVBros/Spark-The-Definitive-Guide">https://github.com/FVBros/Spark-The-Definitive-Guide</a></li></ul></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Docker image 실행하면 기본적으로 /spark-2.3.2-bin-hadoop2.7/bin 에 대화형 콘솔 존재</span></span><br><span class="line"><span class="comment"># + 2.4.7 다운로드 받기</span></span><br><span class="line">wget https://downloads.apache.org/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz</span><br><span class="line">tar -xf spark-2.4.7-bin-hadoop2.7.tgz</span><br></pre></td></tr></table></figure><h3 id="1-6-정리"><a href="#1-6-정리" class="headerlink" title="1.6 정리"></a>1.6 정리</h3><ul><li>스파크의 개요 / 탄생 배경 / 환경 구성 방법</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;br/&gt;

&lt;h4 id=&quot;‘Spark-The-Definitive-Guide’&quot;&gt;&lt;a href=&quot;#‘Spark-The-Definitive-Guide’&quot; class=&quot;headerlink&quot; title=&quot;‘Spark The Definitive Guide’&quot;</summary>
      
    
    
    
    <category term="spark" scheme="https://minsw.github.io/categories/spark/"/>
    
    
    <category term="book" scheme="https://minsw.github.io/tags/book/"/>
    
    <category term="study" scheme="https://minsw.github.io/tags/study/"/>
    
    <category term="summarization" scheme="https://minsw.github.io/tags/summarization/"/>
    
    <category term="mwolkka" scheme="https://minsw.github.io/tags/mwolkka/"/>
    
    <category term="spark" scheme="https://minsw.github.io/tags/spark/"/>
    
    <category term="apache" scheme="https://minsw.github.io/tags/apache/"/>
    
  </entry>
  
  <entry>
    <title>Hexo 블로그 심폐소생술하기</title>
    <link href="https://minsw.github.io/2021/01/18/Hexo-%EB%B8%94%EB%A1%9C%EA%B7%B8-%EC%8B%AC%ED%8F%90%EC%86%8C%EC%83%9D%EC%88%A0%ED%95%98%EA%B8%B0/"/>
    <id>https://minsw.github.io/2021/01/18/Hexo-%EB%B8%94%EB%A1%9C%EA%B7%B8-%EC%8B%AC%ED%8F%90%EC%86%8C%EC%83%9D%EC%88%A0%ED%95%98%EA%B8%B0/</id>
    <published>2021-01-17T16:33:39.000Z</published>
    <updated>2021-01-24T18:11:30.110Z</updated>
    
    <content type="html"><![CDATA[<p>2019년에서 멈춰버린 블로그 한번 살려 보려다가<br>블로그를 통째로 날려버릴 뻔하고 (..) 그냥 날려버리려다 복구했다.</p><p>혹시라도 나처럼 N년 전에 만든 hexo 블로그를 되살려보겠다는 사람을 위해서<br> + 사실은 N년 뒤에 똑같이 삽질할 나를 위해서 정리해본다.</p><img width="287" alt="cpr" src="https://user-images.githubusercontent.com/26691216/104849937-d5c93380-592f-11eb-965a-261b6fe48519.png"><br/><h1 id="방치된-Hexo-Blog-살리기"><a href="#방치된-Hexo-Blog-살리기" class="headerlink" title="방치된 Hexo Blog 살리기"></a>방치된 Hexo Blog 살리기</h1><h2 id="Hexo-3-8-0-gt-5-3-0-업그레이드"><a href="#Hexo-3-8-0-gt-5-3-0-업그레이드" class="headerlink" title="Hexo 3.8.0 -&gt; 5.3.0 업그레이드"></a>Hexo 3.8.0 -&gt; 5.3.0 업그레이드</h2><p>2019.03 기준 <code>3.8.0</code> =&gt; 2021.01 기준 최신 버전 <code>5.3.0</code></p><h4 id="0-npm-upgrade"><a href="#0-npm-upgrade" class="headerlink" title="0. npm upgrade"></a>0. npm upgrade</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install -g npm</span><br></pre></td></tr></table></figure><h4 id="1-Hexo-재설치"><a href="#1-Hexo-재설치" class="headerlink" title="1. Hexo 재설치"></a>1. Hexo 재설치</h4><blockquote><p><a href="https://github.com/hexojs/hexo">hexojs/hexo</a> 참고</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexp-cli -g</span><br></pre></td></tr></table></figure><h4 id="2-이미-블로그가-있으니까-hexo-init-은-스킵"><a href="#2-이미-블로그가-있으니까-hexo-init-은-스킵" class="headerlink" title="2. 이미 블로그가 있으니까 hexo init 은 스킵?"></a>2. 이미 블로그가 있으니까 <code>hexo init</code> 은 스킵?</h4><p>내 레포에 있는 package.json 자체가 옛날 버전이라 그대로 npm install 하려면 실패하기도<br>⇒ <strong>최신 버전 package.json</strong> 으로 마이그레이션 필요</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ hexo init blog</span><br><span class="line">$ cp blog/package.json <span class="variable">$&#123;MY_GIT_BLOG_PATH&#125;</span> <span class="comment"># 최신 package.json 으로 덮어쓰기</span></span><br><span class="line">$ <span class="built_in">cd</span> <span class="variable">$&#123;MY_GIT_BLOG_PATH&#125;</span> &amp;&amp; npm install</span><br></pre></td></tr></table></figure><ul><li>기존 package.json, package-lock.json, node_modules/ 등 제거</li><li>최신 ‘package.json’ copy 후 <code>npm install</code> (필요한 의존성은 <code>npm install &#123;&#125; --save</code> 로 별도 추가)</li><li><code>_config.yml</code> 도 비교 후 변경 사항 있을 시 적용 (Option)</li></ul><br/><h2 id="너무-쉬운데-뭐가-문제지"><a href="#너무-쉬운데-뭐가-문제지" class="headerlink" title="너무 쉬운데 뭐가 문제지?"></a>너무 쉬운데 뭐가 문제지?</h2><blockquote><h4 id="최신-버전-Hexo-2년전에-적용한-테마-💩"><a href="#최신-버전-Hexo-2년전에-적용한-테마-💩" class="headerlink" title="최신 버전 Hexo + 2년전에 적용한 테마 = 💩"></a>최신 버전 Hexo + 2년전에 적용한 테마 = 💩</h4><p>2년전에 예쁘다고 적용해놓은 테마가 지금도 유지보수되고 있을 가능성은 0에 가깝다.<br>포기하고 새 테마를 찾거나, <strong>어거지로 적용하거나 ☠️</strong><br>둘 중 하나를 선택하도록 하자.</p></blockquote><h4 id="문제-1-서버를-올렸더니-흰-화면만-나온다"><a href="#문제-1-서버를-올렸더니-흰-화면만-나온다" class="headerlink" title="문제 1. 서버를 올렸더니 흰 화면만 나온다"></a>문제 1. 서버를 올렸더니 흰 화면만 나온다</h4><p>⇒ themes/ 에 본인이 지정한 테마가 제대로 있는지 재확인 (.gitignore 로 빼놓기도 함)</p><h4 id="문제-2-뭐가-나오긴-하는데-텍스트가-나온다"><a href="#문제-2-뭐가-나오긴-하는데-텍스트가-나온다" class="headerlink" title="문제 2. 뭐가 나오긴 하는데.. 텍스트가 나온다"></a>문제 2. 뭐가 나오긴 하는데.. 텍스트가 나온다</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># index.html</span></span><br><span class="line">extends partial/layout</span><br><span class="line"></span><br><span class="line">block container</span><br><span class="line">    include mixins/post</span><br><span class="line">    +posts()</span><br><span class="line">...</span><br></pre></td></tr></table></figure><img width="636" alt="error" src="https://user-images.githubusercontent.com/26691216/104851421-407e6d00-5938-11eb-97f5-4d041b5abdf6.png"><p>jade 또는 pug 템플릿을 사용 중인데 해당하는 renderer 가 없어서 발생.</p><p>⇒ 기존에는 <del><a href="https://www.npmjs.com/package/hexo-renderer-jade">hexo-renderer-jade</a></del> 를 썼으나 deprecated 되었으므로,<br>themes/layout 하위의 모든 <code>*.jade</code> 파일을 <code>*.pug</code> 로 변경 + <a href="https://www.npmjs.com/package/hexo-renderer-pug">hexo-renderer-pug</a> 사용</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-renderer-pug --save</span><br></pre></td></tr></table></figure><h4 id="문제-3-배포가-안된다-amp-이상하게-올라간다"><a href="#문제-3-배포가-안된다-amp-이상하게-올라간다" class="headerlink" title="문제 3. 배포가 안된다 &amp; 이상하게 올라간다"></a>문제 3. 배포가 안된다 &amp; 이상하게 올라간다</h4><p><a href="https://www.npmjs.com/package/hexo-deployer-git">hexo-deployer-git</a> 이 잘 설치되어있는지 확인.</p><p>잘 설치되어있고 <code>hexo deploy</code>가 되긴하는데<br>정적 파일말고 소스코드가 올라간다던지 뭔가 엉켰다면 <strong>.deploy_git</strong> 삭제 후 재시도 추천</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ rm -rf .deploy_git <span class="comment"># reset</span></span><br></pre></td></tr></table></figure><br/><h2 id="해치웠나"><a href="#해치웠나" class="headerlink" title="해치웠나.."></a>해치웠나..</h2><p>이 포스트가 잘 올라간다면 잘 살아난거라 볼 수 있겠다.<br>사실 어찌저찌 돌아가게만 만들어 놓은지라 언제 다시 뻗을지는 모르겠지만<br>자주 쓰게 된다면 쓰면서 조금씩 고쳐가면 되지 않을까 한다.</p><p>올해는 공부한 것도 좀 잘 정리해서 기록 해보도록 노력하자.</p><blockquote><p>하나 둘 셋 화이팅! ٩( ᐛ )و</p></blockquote><br/><h2 id="참고용-✍🏻-돌아서면-까먹는-Hexo-사용법"><a href="#참고용-✍🏻-돌아서면-까먹는-Hexo-사용법" class="headerlink" title="[참고용] ✍🏻 돌아서면 까먹는 Hexo 사용법"></a>[참고용] ✍🏻 돌아서면 까먹는 Hexo 사용법</h2><blockquote><p><a href="https://hexo.io/ko/docs/commands.html">Hexo docs - commands</a></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;NEW_POST_NAME&quot;</span> <span class="comment"># new post 작성</span></span><br><span class="line"></span><br><span class="line">$ hexo generate <span class="comment"># 정적 파일 (public) 생성</span></span><br><span class="line">$ hexo clean <span class="comment"># 캐시 및 정적 파일 삭제</span></span><br><span class="line"></span><br><span class="line">$ hexo server <span class="comment"># 로컬 서버 (localhost:4000) 구동 _ 테스트</span></span><br><span class="line">$ hexo deploy <span class="comment"># 웹 사이트 deploy</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># hexo d -g 도 가능하지만 내 블로그는 가끔 이상하게 렌더링되기도하니 아래 커맨드로 배포할 것</span></span><br><span class="line">$ hexo clean &amp;&amp; hexo generate &amp;&amp; hexo deploy</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;2019년에서 멈춰버린 블로그 한번 살려 보려다가&lt;br&gt;블로그를 통째로 날려버릴 뻔하고 (..) 그냥 날려버리려다 복구했다.&lt;/p&gt;
&lt;p&gt;혹시라도 나처럼 N년 전에 만든 hexo 블로그를 되살려보겠다는 사람을 위해서&lt;br&gt; + 사실은 N년 뒤에 </summary>
      
    
    
    
    <category term="blog" scheme="https://minsw.github.io/categories/blog/"/>
    
    
    <category term="blog" scheme="https://minsw.github.io/tags/blog/"/>
    
    <category term="hexo" scheme="https://minsw.github.io/tags/hexo/"/>
    
    <category term="CPR" scheme="https://minsw.github.io/tags/CPR/"/>
    
    <category term="guide" scheme="https://minsw.github.io/tags/guide/"/>
    
  </entry>
  
  <entry>
    <title>2019 NAVER INTERNSHIP, 그 후</title>
    <link href="https://minsw.github.io/2019/11/24/2019-NAVER-INTERNSHIP-%ED%9B%84%EA%B8%B0/"/>
    <id>https://minsw.github.io/2019/11/24/2019-NAVER-INTERNSHIP-%ED%9B%84%EA%B8%B0/</id>
    <published>2019-11-24T08:36:03.000Z</published>
    <updated>2021-03-15T08:02:40.358Z</updated>
    
    <content type="html"><![CDATA[<br/><img width="300" alt="intro" src="https://user-images.githubusercontent.com/26691216/111110886-b3a30900-85a0-11eb-98de-3ff66e7ab21e.jpg"><br/><h2 id="나는-NAVER의-인턴이-되었다"><a href="#나는-NAVER의-인턴이-되었다" class="headerlink" title="나는 NAVER의 인턴이 되었다"></a>나는 NAVER의 인턴이 되었다</h2><blockquote><p>갑자기?</p></blockquote><p><a href="https://minsw.github.io/2019/06/30/2019-NAVER-HACKDAY-SUMMER-%ED%9B%84%EA%B8%B0/">2019 NAVER HACKDAY</a> 에서 노력하는 모습을 좋게 봐주신 덕분에 면접 기회를 얻게 되었고 합격해서<br>핵데이 과제를 진행했던 네이버 쇼핑 팀에서 두 달간 인턴십을 할 수 있게 되었다.</p><p>분명 나는 여름 방학 때 캘리포니아에 있을 예정이였는데<br>정신을 차려보니 캘리포니아보다 시원하고(..) 밥과 커피가 싸고 맛있는(….) 그린팩토리였다 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ</p><img width="500" alt="ppt" src="https://user-images.githubusercontent.com/26691216/110762577-476c9080-8294-11eb-913f-02703328232b.png"><center style="color:lightgray">실제 내가 최종 발표 자료에 넣었던 장표<br/>ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ</center><p>사실 핵데이와 인턴십을 준비하면서 이것저것 찾아보던 때에 인상 깊게 봤었던 핵데이 인턴 합격 글이 두 개 있었는데,<br>들어와보니 그 두 글의 주인공이 나의 인턴 멘토님 &amp; 옆팀 분이셨던게 너무 신기했다.<br>이건 뭐랄까.. 연예인이랑 같이 일하는 기분..? (크으으으으)<br>압도적 영광…! 🙏🏻</p><br/><p>인턴 기간 두 달간 내가 진행했던 프로젝트는 <U><strong>특정 데이터의 사전검증 툴</strong></U>을 만드는 것이었는데,<br><code>Kubernetes</code>+<code>Kafka</code>+<code>ElasticSearch</code>+<code>Prometheus</code> 로 이루어진 MSA 환경에 <code>Kotlin</code>+<code>SpringBoot</code>, <code>React</code> 로 개발했다.</p><p>아 참고로 여기서 내가 경험이 있는건 <strong>Kubernetes</strong> 랑 <strong>SpringBoot</strong> 밖에 없었다.</p><img width="400" alt="death" src="https://user-images.githubusercontent.com/26691216/110765245-57d23a80-8297-11eb-8951-c1b8d323accf.jpg"><h2 id="학교로-돌아왔다"><a href="#학교로-돌아왔다" class="headerlink" title="학교로 돌아왔다"></a>학교로 돌아왔다</h2><p>인턴십을 마치자마자 나는 나의 현생 학교로 돌아왔다.</p><p>남들이 다 하길래 나도 당연히 하겠지 싶었던 졸업도 쉽지 않았고 (마지막까지 손에 땀을 쥐게함…)<br>막학기라 들을 학점도 얼마 없는데 똥손 닉값한 수강신청.. 🤦🏻‍♀️덕분에 학교도 거의 매일 갔다.</p><p>그렇게 바쁘게 살다보니 인턴십은 잊혀져 가…ㄱㅣ…<br>는 개뿔 사실 아무것도 손에 잡히지도 않았다.<br>솔직히 맨날 전환 결과 메일만 기다리고 있었음…</p><p>╭┈┈┈┈╯   ╰┈┈┈╮</p><p> ╰┳┳╯    ╰┳┳╯</p><p>  결 　    　　 언</p><p> 과  　    　　 제<br>     ╰┈┈╯<br>  메 ╭━━━━━╮　 줘<br>      ┈┈┈┈<br>　　일     　　 요</p><br/><img width="300" alt="talk" src="https://user-images.githubusercontent.com/26691216/111111690-295ba480-85a2-11eb-983d-28cb8637c99b.PNG"><center style="font-size: 150%;"><b>?</b></center><p>ㅇㅖ?</p><p>아니 멘토님.. 대체 무슨 말씀을 하시는거에요..<br>저는 합격도 못 했는데… ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ<br>합격 메일을 주세요!!!!!!!!! 합!!!!격!!!!!!!!!!!!!!!! ㅠㅠㅠㅠ</p><img width="150" alt="cry" src="https://user-images.githubusercontent.com/26691216/110768316-77b72d80-829a-11eb-81c3-276d52fe4089.jpg"><br/><h2 id="NAVER-Engineering-Day-에-발표되다"><a href="#NAVER-Engineering-Day-에-발표되다" class="headerlink" title="NAVER Engineering Day 에 발표되다"></a>NAVER Engineering Day 에 발표되다</h2><h4 id="NAVER-Engineering-Day-2019-‘Kafka-Lag-감지를-통한-Kubernetes-Autoscaling-적용기’"><a href="#NAVER-Engineering-Day-2019-‘Kafka-Lag-감지를-통한-Kubernetes-Autoscaling-적용기’" class="headerlink" title="NAVER Engineering Day 2019 - ‘Kafka Lag 감지를 통한 Kubernetes Autoscaling 적용기’"></a>NAVER Engineering Day 2019 - ‘Kafka Lag 감지를 통한 Kubernetes Autoscaling 적용기’</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">각설하고 주제에 대해 간략히 소개드리자면,</span><br><span class="line">쿠버네티스 기반의 어플리케이션들이 카프카를 통해서 요청을 받아 처리하는데,</span><br><span class="line">갑자기 요청이 급증하는 등의 부하 상황이 발생하면 HPA로 Autoscaling 이 수행되어 자동으로 Pods 개수가 늘어나고,</span><br><span class="line">이후엔 다시 줄어드는 유연한 시스템을 구축한 경험을 공유하는 것이였습니다.</span><br><span class="line"></span><br><span class="line">[출처] [후기] NAVER Engineering Day 2019 발표 후기|작성자 occidere</span><br></pre></td></tr></table></figure><blockquote><p>멘토님의 <a href="https://blog.naver.com/occidere/221758990374">[후기] NAVER Engineering Day 2019 발표 후기</a></p></blockquote><p>사실 이 발표 주제가 되는 부분은 나의 <strong>욕심</strong>이기도 했다.</p><p>핵데이 때 아무것도 모르는 채로 Kubernetes HPA를 적용해보려다 실패한 뒤로 더 고민해보고 공부하면서 생각했던 게<br><i>‘혹시라도 내가 인턴을 하게되면 이번에는 꼭 한번 적용해보자’</i> 였기 때문에.</p><p>그래서 중간 발표 이후 필수 요구 사항 개발이 끝나고 나서부터는<br>온통 <code>Kafka Lag을 모니터링하는 Custom HPA 로 Pod를 Autoscaling</code> 구현에 집중했다.</p><p>당시에는 비슷한 경험이나 자료가 많지 않아 생각보다도 더 많은 시행 착오를 거쳐야 했지만<br>든든하신 우리 멘토님들과 TL (리더) 님이 항상 관심갖고 도와주셔서 정말 많은 도움과 응원을 받았고,<br>그 덕분에 인턴 기간 내에 <strong>구현에 성공</strong>할 수 있었던 것 같다.<br><a style="color:lightgray">(놀랍게도 데모 필패 법칙도 피하고 데모도 잘 넘어감..)</a></p><p>백번 말해도 모자라지만 인턴 기간 내내 물심양면 도와주셨던 우리 두 멘토님들과 리더님, 팀원 분들,<br>그리고 특히 이 소중했던 여름의 추억과 노력을 Engineering Day 에서 정말 멋지게 발표해주신<br>갓-씨데레 멘토 @occidere 님께 다시 한번 감사의 말씀을 드리고싶다.<br>👍🏻따봉따봉 쌍따봉 👍🏻</p><br/><h2 id="NAVER로-돌아오는-길"><a href="#NAVER로-돌아오는-길" class="headerlink" title="NAVER로 돌아오는 길"></a>NAVER로 돌아오는 길</h2><p>8월 말에 인턴을 마치고 최종 결과가 나오기까지는 약 두 달정도가 걸렸던 것 같다.<br>그 사이 최종 면접과 최최종 면접(..) 도 있었고 또 다른 많은 고민들도 있었지만<br>그래도, 이 한마디면 충분했다.</p><img width="400" alt="pass" src="https://user-images.githubusercontent.com/26691216/111121457-046e2e00-85b0-11eb-89d2-9c56e9a610ac.png"><center style="color:lightgray"><del><i>무야~ 호~~!</i><del></center><br/><br/><p>인턴 기간 동안 심리적, 육체적으로 힘들지 않았다면 거짓말이다.</p><p>하지만 끝나고 나서는 좋은 기억밖에 떠오르지 않았던 이유는<br>좋은 인턴 동기들과 좋은 팀원분들이 있었기 때문이었고, 해보고 싶던 일을 해볼 수 있는 곳이었기 때문이었다.</p><p>그래서 더 돌아가고 싶었던 것 같고 돌아 올 수 있어서 좋았다.</p><br/><h4 id="minSW-NAVER-Corp-Nov-2019"><a href="#minSW-NAVER-Corp-Nov-2019" class="headerlink" title="minSW, NAVER Corp. (Nov 2019 - )"></a>minSW, NAVER Corp. (Nov 2019 - )</h4><img width="400" alt="di" src="https://user-images.githubusercontent.com/26691216/111118488-32517380-85ac-11eb-8219-23d8c9ce9cd7.jpg"><br/>]]></content>
    
    
      
      
    <summary type="html">&lt;br/&gt;

&lt;img width=&quot;300&quot; alt=&quot;intro&quot; src=&quot;https://user-images.githubusercontent.com/26691216/111110886-b3a30900-85a0-11eb-98de-3ff66e7ab21e.j</summary>
      
    
    
    
    <category term="retrospect" scheme="https://minsw.github.io/categories/retrospect/"/>
    
    
    <category term="naver" scheme="https://minsw.github.io/tags/naver/"/>
    
    <category term="kafka" scheme="https://minsw.github.io/tags/kafka/"/>
    
    <category term="internship" scheme="https://minsw.github.io/tags/internship/"/>
    
    <category term="engineeringday" scheme="https://minsw.github.io/tags/engineeringday/"/>
    
    <category term="kubernetes" scheme="https://minsw.github.io/tags/kubernetes/"/>
    
    <category term="hpa" scheme="https://minsw.github.io/tags/hpa/"/>
    
  </entry>
  
  <entry>
    <title>&amp;#039;Kotlin in Action&amp;#039; 1장 - Kotlin (코틀린) 은 뭘까</title>
    <link href="https://minsw.github.io/2019/07/02/Kotlin-in-Action-1%EC%9E%A5/"/>
    <id>https://minsw.github.io/2019/07/02/Kotlin-in-Action-1%EC%9E%A5/</id>
    <published>2019-07-02T14:07:55.000Z</published>
    <updated>2021-01-24T18:11:30.111Z</updated>
    
    <content type="html"><![CDATA[<br/><h4 id="‘Kotlin-in-Action’"><a href="#‘Kotlin-in-Action’" class="headerlink" title="‘Kotlin in Action’"></a>‘Kotlin in Action’</h4><p>공부 겸 프로젝트 준비 겸 Kotlin 책을 하나 샀다.<br><strong><em>‘Kotlin in Action’</em></strong> 은 Kotlin 언어를 개발한 JetBrains 개발자들이 직접 쓴 책으로, <u>Kotlin 다운 Kotlin 개발</u>을 하기 위해 첫 단추로 택했다.<br>내가 책 읽는 속도는 빠른데 머리에서 휘발되는 속도도 빠른 편이라(…) 시간 날 때 마다 읽은 부분은 차근차근 정리 해두려고 한다. </p><blockquote><p><strong>Kotlin</strong>은 <em>최신 멀티플랫폼 애플리케이션을 위한 정적 타입 언어</em> 로,<br>2017 Google I/O에서 안드로이드 공식언어로 선정되었고 현재 1.3 버전까지 릴리즈 되어있다.</p><p>Kotlin 공식 페이지 <a href="https://kotlinlang.org/">https://kotlinlang.org/</a></p></blockquote><center><h2>_ _ _</h2></center><br/><hr><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><h4 id="Java-를-대신할-언어에-대한-Requirement-3가지"><a href="#Java-를-대신할-언어에-대한-Requirement-3가지" class="headerlink" title="Java 를 대신할 언어에 대한 Requirement 3가지"></a>Java 를 대신할 언어에 대한 Requirement 3가지</h4><ol><li>정적 타입 지정 (static typing)</li><li>기존 자바 코드와의 완전한 호환성</li><li>해당 언어를 위한 도구 개발 복잡성 x</li></ol><p>⇒ 배우고 이해하기 쉬우며 대규모 개발/유지보수성/기존 자바와의 호환성에 적합한 강력한 언어, <u><strong>‘Kotlin’</strong></u></p><br/><h1 id="1장-코틀린이란-무엇이며-왜-필요한가"><a href="#1장-코틀린이란-무엇이며-왜-필요한가" class="headerlink" title="1장. 코틀린이란, 무엇이며 왜 필요한가?"></a>1장. 코틀린이란, 무엇이며 왜 필요한가?</h1><p>Kotlin은 자바 플랫폼에서 돌아가는 새로운 프로그래밍 언어다.<br><strong>간결하고 실용적</strong>이며 <strong>Java 코드와의 상호운용성</strong> (interoperability)을 중시한다.</p><h2 id="특성"><a href="#특성" class="headerlink" title="특성"></a>특성</h2><h4 id="1-대상-플랫폼-자바가-실행되는-Everywhere"><a href="#1-대상-플랫폼-자바가-실행되는-Everywhere" class="headerlink" title="1. 대상 플랫폼 : 자바가 실행되는 Everywhere"></a>1. 대상 플랫폼 : 자바가 실행되는 Everywhere</h4><p>일부가 아닌 개발 과정에서 수행해야하는 모든 과업에 있어 폭넓게 생산성을 향상 시킨다.<br>구체적인 영역 or 특정 프로그램 패러다임을 지원하는 여러 라이브러리와의 융합성 ↑</p><br/><h4 id="2-정적-타입-지정-언어"><a href="#2-정적-타입-지정-언어" class="headerlink" title="2. 정적 타입 지정 언어"></a>2. 정적 타입 지정 언어</h4><p>Kotlin은 <strong>정적 타입 지정 언어</strong> 이면서, <u>type inference</u> (타입추론) 과 <u>nullable type</u>을 지원한다.<br>⇒ 프로그래머의 불편함 해소 &amp; 컴파일 시점에 NPE 여부 검사 가능</p><ul><li><p><strong>정적 타입</strong> (statically typed) 지정 언어 : Java, Kotlin ..</p><blockquote><p>모든 프로그램 구성 요소 타입을 컴파일 시점에 알 수 있고, 객체의 필드나 메소드를 사용할 때마다 컴파일러가 타입을 검증한다.</p></blockquote><p>  장점 : 성능 / 신뢰성 / 유지 보수성 / 도구 지원 _ <code>p.36</code></p>  <br/></li><li><p><strong>동적 타입</strong> (dynamically typed) 지정 언어 : Groovy, JRuby …</p><blockquote><p>타입과 관계없이 모든 값을 변수에 넣을 수 있고, 필드나 메소드 접근에 대한 검증이 실행 시점에 일어난다. </p></blockquote><p>  동적이 더 유연하고 코드도 짧아지지만, 오류를 사전에 거르지 못하고 Runtime Error 발생 가능성 존재</p></li></ul><br/><h4 id="3-함수형-객체지향-프로그래밍"><a href="#3-함수형-객체지향-프로그래밍" class="headerlink" title="3. 함수형 / 객체지향 프로그래밍"></a>3. 함수형 / 객체지향 프로그래밍</h4><p>Kotlin으로 코드를 작성 할 땐, Java와 같은 <u>객체지향 프로그래밍 (OOP)</u> 과 <u>함수형 프로그래밍</u> 접근 방법을 조합해서 문제에 가장 적합한 도구를 사용하면 된다.</p><blockquote><p><strong>함수형 프로그래밍</strong> 의 핵심 개념</p><ul><li>first-class function (일급 함수)</li><li>immutability</li><li>no side effect _ pure function</li></ul></blockquote><br/><h4 id="4-무료-오픈소스"><a href="#4-무료-오픈소스" class="headerlink" title="4. 무료 오픈소스"></a>4. 무료 오픈소스</h4><p>Kotlin 언어와 이와 관련된 모든 도구는 오픈소스이다.<br>(<a href="https://github.com/jetbrains/kotlin">https://github.com/jetbrains/kotlin</a> - Apache2 License.)</p><br/><h2 id="응용"><a href="#응용" class="headerlink" title="응용"></a>응용</h2><h4 id="코틀린-서버-프로그래밍"><a href="#코틀린-서버-프로그래밍" class="headerlink" title="코틀린 서버 프로그래밍"></a>코틀린 서버 프로그래밍</h4><blockquote><p><strong>서버 프로그래밍</strong> 의 범위</p><ul><li>브라우저에 HTML 페이지를 반환하는 웹 애플리케이션</li><li>모바일 애플리케이션에게 HTTP를 통해 JSON API를  </li><li>RPC (Remote Procedure Call) 프로토콜을 통해 서로 통신하는 마이크로 서비스</li></ul></blockquote><p>Kotlin은 이러한 애플리케이션 개발에 도움을 주는 기존의 자바 프레임워크나 기술과 매끄럽게 상호운용 가능하다.</p><p>+ 새로운 기술도 적용 가능 (ex. Kotlin의 Builder Pattern, Persistence Framework …)<br>  ⇒ <code>7.5절</code> &amp; <code>11장</code> 에서 좀 더 자세히</p><br/><h4 id="코틀린-안드로이드-프로그래밍"><a href="#코틀린-안드로이드-프로그래밍" class="headerlink" title="코틀린 안드로이드 프로그래밍"></a>코틀린 안드로이드 프로그래밍</h4><p>모바일 애플리케이션은 전형적인 엔터프라이즈 애플리케이션보다 더 작고 기존과 신규 코드 통합 필요성도 더 적고, 다양한 디바이스에 대한 서비스 신뢰성 보장과 빠른 개발&amp;배포가 필요하다.</p><p>Kotlin 언어의 특성과 특별한 컴파일러 플러그인 지원을 조합하면 개발 생산성을 더 높일 수 있다.<br>뿐만 아니라 애플리케이션 신뢰성 향상, 자바6와 완전한 호환, 성능 손실 x 과 같은 장점도 취할 수 있다.</p><p>+ 참조 _ 안드로이드 API에 대한 Kotlin Adaptor를 제공하고 있는 <strong>Anko Library</strong> <a href="https://github.com/kotlin/anko">https://github.com/kotlin/anko</a></p><br/><h2 id="철학"><a href="#철학" class="headerlink" title="철학"></a>철학</h2><blockquote><p>대개 Kotlin은 Java와의 <strong><em>상호운용성</em></strong> 에 초점을 맞춘 <strong><em>실용적</em></strong> 이고 <strong><em>간결</em></strong> 하며 <strong><em>안전한</em></strong> 언어로 표현된다.</p></blockquote><h4 id="실용성"><a href="#실용성" class="headerlink" title="실용성"></a>실용성</h4><ul><li>연구를 위한 언어가 아닌, 실제 문제를 해결하기 위해 만들어진 실용적인 언어이다.</li><li>특정 프로그래밍 스타일이나 패러다임 사용을 강제하지 않는다.</li><li>도구를 강조한다. (IDE 지원)</li></ul><br/><h4 id="간결성"><a href="#간결성" class="headerlink" title="간결성"></a>간결성</h4><ul><li>기존 코드 이해가 더 쉬워진다. (→ 생산성과 개발 속도 향상)</li><li>부수적인 요소등을 묵시적으로 제공하여 코드가 깔끔하다.</li><li>람다를 지원한다.</li><li>그러나 소스코드를 가능한 짧게 만드는 것이 코틀린의 설계 목표는 아니다.</li></ul><br/><h4 id="안전성"><a href="#안전성" class="headerlink" title="안전성"></a>안전성</h4><p>  프로그램의 안전성과 생산성 사이에는 trade-off 존재</p><ul><li>JVM에서 실행한다. (메모리 안전성과 버퍼 플로우 방지등 기본적으로 높은 안전성 확보)</li><li>정적 타입 지정 언어으로, 애플리케이션의 타입 안전성을 보장한다.</li><li>실행 시점이 아닌 컴파일 시점에 검사를 통해 더 많은 오류를 방지해준다. (ex. <code>NullPointerException</code>, <code>ClassCastException</code>)</li></ul><br/><h4 id="상호운용성"><a href="#상호운용성" class="headerlink" title="상호운용성"></a>상호운용성</h4><ul><li>Java의 기존 라이브러리를 그대로 사용 가능하고, 최대한 활용하고 있다.</li><li>Java ←→ Kotlin 호출에 따로 노력이 필요하지 않다.</li><li>다중 언어 프로젝트를 완전히 지원한다.</li></ul><br/><h2 id="코틀린-도구-사용"><a href="#코틀린-도구-사용" class="headerlink" title="코틀린 도구 사용"></a>코틀린 도구 사용</h2><p>Kotlin도 Java와 마찬가지로 컴파일 언어이다.<br><img src="https://user-images.githubusercontent.com/26691216/60521297-f6b43e80-9d21-11e9-956b-4f827ed75f1b.jpg" alt="kotlin-runtime-diagram"></p><center><i>Kotlin Build Process</i></center><br/><h4 id="자바-코틀린-변환"><a href="#자바-코틀린-변환" class="headerlink" title="자바-코틀린 변환"></a>자바-코틀린 변환</h4><p>Intellij IDEA에서는 자바 코드 조각을 코틀린 파일(.kt)에 붙여넣기<br>자바 파일 자체를 변환하려면 <code>Code &gt; Convert Java File to Kotlin File</code> </p><p>도구에 대해서는 필요에 따라 쓰면 되는거라 나머지 내용 생략.</p><br/><h3 id="1장-요약-p-57"><a href="#1장-요약-p-57" class="headerlink" title="1장 요약 p.57"></a>1장 요약 <code>p.57</code></h3>]]></content>
    
    
      
      
    <summary type="html">&lt;br/&gt;

&lt;h4 id=&quot;‘Kotlin-in-Action’&quot;&gt;&lt;a href=&quot;#‘Kotlin-in-Action’&quot; class=&quot;headerlink&quot; title=&quot;‘Kotlin in Action’&quot;&gt;&lt;/a&gt;‘Kotlin in Action’&lt;/h4&gt;&lt;p</summary>
      
    
    
    
    <category term="kotlin" scheme="https://minsw.github.io/categories/kotlin/"/>
    
    
    <category term="book" scheme="https://minsw.github.io/tags/book/"/>
    
    <category term="study" scheme="https://minsw.github.io/tags/study/"/>
    
    <category term="kotlin" scheme="https://minsw.github.io/tags/kotlin/"/>
    
    <category term="summarization" scheme="https://minsw.github.io/tags/summarization/"/>
    
    <category term="mwolkka" scheme="https://minsw.github.io/tags/mwolkka/"/>
    
  </entry>
  
  <entry>
    <title>2019 NAVER CAMPUS HACKDAY SUMMER 후기</title>
    <link href="https://minsw.github.io/2019/06/30/2019-NAVER-HACKDAY-SUMMER-%ED%9B%84%EA%B8%B0/"/>
    <id>https://minsw.github.io/2019/06/30/2019-NAVER-HACKDAY-SUMMER-%ED%9B%84%EA%B8%B0/</id>
    <published>2019-06-30T07:13:28.000Z</published>
    <updated>2021-01-24T18:11:30.109Z</updated>
    
    <content type="html"><![CDATA[<p>언젠가부터 주위 동기들이 <em>해커톤 (Hackaton)</em> 에서 좋은 경험을 하고 오는 걸 보면서,<br>항상 나도 해보고 싶다는 마음은 있었지만 현생이 바쁘다는 이유로 단 한번도 도전해보지 않았었다.</p><center><img src="https://user-images.githubusercontent.com/26691216/60393712-742a5400-9b54-11e9-9771-03862d5b94df.jpg" width=30%><i>핑계는...</i></center><p>길고 길었던 지옥의 사망년을 벗어난 기념으로, 지난 4월 <U>세 개의 해커톤</U>을 지원했고 운 좋게 <strong><U>두 개</U>를 합격하였다</strong>.<br>이번 포스트에서는 그 중 <strong>NAVER CAMPUS HACKDAY</strong> 후기를 작성하고자 한다.</p><blockquote><p>나머지 하나는 추후에 업로드 예정이다. <del>아마도…</del></p></blockquote><br/><hr><br/><h1 id="2019-NAVER-CAMPUS-HACKDAY-SUMMER"><a href="#2019-NAVER-CAMPUS-HACKDAY-SUMMER" class="headerlink" title="2019 NAVER CAMPUS HACKDAY SUMMER"></a>2019 NAVER CAMPUS HACKDAY SUMMER</h1><center><img src="https://d2.naver.com/content/images/2019/03/19CHACK_S.png" width=60%></center><blockquote><p>NAVER D2 - CAMPUS HACKDAY 행사 안내 <a href="https://d2.naver.com/news/5009947">https://d2.naver.com/news/5009947</a></p><p>GITHUB Page <a href="https://github.com/NAVER-CAMPUS-HACKDAY/common">https://github.com/NAVER-CAMPUS-HACKDAY/common</a></p></blockquote><p>깃헙 레포의 이슈에 있는 37개의 주제 중 희망하는 1~2개의 주제를 골라 지원서를 작성하고, 4월 13일에 온라인 코딩 테스트를 보았다.</p><p>코딩 테스트는 원하는 시간에 접속하여 제한시간 두 시간동안 3문제를 풀어야 했고, 문제 난이도는 그리 높은 편은 아니었던 것 같다. 제출하는 시간도 기록되었기 때문에 테스트 케이스를 적당히 확인하고 한 시간 조금 넘은 시점에 제출했다.</p><p>후담이지만 <a href="https://codingcompetitions.withgoogle.com/codejam">Google code jam</a>의 ‘Round 1A 2019’ 도 같은 날에 진행되어서 스타벅스에 앉아서 하루죙일 정신없이 문제만 풀었다. 🤦🏻‍♀️</p><br/><h3 id="🎉-햅격-🎉"><a href="#🎉-햅격-🎉" class="headerlink" title="🎉 햅격~ 🎉"></a>🎉 햅격~ 🎉</h3><img src="https://user-images.githubusercontent.com/26691216/60394163-02093d80-9b5b-11e9-8129-2444f06d0741.png" width="905"><p>기대 안하고 있었지만 사실 기대하긴 했다. (ㅋㅋㅋㅋㅋㅋㅋㅋㅋ)</p><p>해커톤 중에서도 Naver hackday는 꼭 한번쯤 가보고 싶었던 행사였기 때문에 특히 좋았다. 기쁜 와중에 딱 날짜가 종설 프로젝트 중간 발표 날이라서 팀원들에게 미리 양해를 구했는데, 다행히 마음씨 좋은 우리 팀원분들은 너그럽게 이해해주셨다.</p><center><img src="https://user-images.githubusercontent.com/26691216/60394226-0124db80-9b5c-11e9-80ed-221836478984.png" width=40%></center><br/><hr><br/><h2 id="Before-Hack-day"><a href="#Before-Hack-day" class="headerlink" title="Before Hack-day"></a>Before Hack-day</h2><p>내가 수행하게 된 주제는 <strong>“컨테이너 기반 쇼핑 상품 정보 수신”</strong> 으로, 세 명이 한 팀을 이루고 멘토님 한 분이 함께 해주셨다. </p><blockquote><p><strong>[컨테이너 기반 쇼핑 상품 정보 수신]</strong></p><p><em><a href="https://github.com/NAVER-CAMPUS-HACKDAY/common/issues/7">https://github.com/NAVER-CAMPUS-HACKDAY/common/issues/7</a></em></p><p>상품정보 수집을 위하여 각 쇼핑몰에서 제공하는 상품정보 <strong>EP</strong>(Engine Page)를 주기적으로 수집하여 변경된 정보를 체크하고 서비스에 반영하고 있다. 해당 작업의 <strong>확장성 및 고가용성</strong>을 위하여 Kubernetes 등의 컨테이너 환경에서 Task Agent 들이 운용될 수 있도록 설계 및 구현이 필요하다.</p></blockquote><p>해커톤 행사 당일에 주제 선정과 개발이 모두 이루어지는 다른 해커톤들과는 다르게 Naver Campus Hackday는 본인이 지원한 주제에 따라 팀이 꾸려지고, 사전에 멘토님의 가이드에 따라 팀끼리 개발을 어느정도 진행하기도 한다. ( → 해당 부분은 팀by팀 인 듯)</p><p>프로젝트에 대해서는 멘토님께 사전에 여쭤봤을 때, 구체적인 플로우는 공개할 수 없으나 내가 짠 코드는 무관하다고 답변을 주셔서 공식 Hackday github 이슈에 노출되어있는 프로젝트의 개괄적인 내용과 함께 느낀점만 간략하게 정리하고자 한다.</p><br/><p>우리 팀의 경우 특히 인프라 구축이 필요한 주제였기 때문에 사전에 LINE과 Github을 통해 온라인 회의를 지속적으로 진행했다. 이를 통해 나는 아래와 같은 사전 준비를 하고서 Hackday에 참가하였다.</p><ol><li><p>MQ(Message Queue), 데이터 처리 방법 등에 대한 이해</p><ul><li>MQ란 무엇이고, 프로젝트에 적합한 프로젝트는 무엇인가?</li><li>데이터 처리 방식 중 Batch와 Stream의 차이와 각각의 장단점은?</li><li>확장성과 고가용성을 고려하였을 때 적합한 데이터 저장 및 분석 방법은?</li></ul></li><li><p>서버 운용 및 클러스터링 계획</p><ul><li>사전에 할당받은 10개의 서버를 어떻게 운용할 것인가?</li><li>어떤 기술 스택을 사용할 것인가?</li><li>어떤식으로 Clustering할 것인가?</li><li>어떠한 플로우로 데이터를 처리하고, 어떻게 분석할 것인가? (설계)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Container Management : Kubernetes</span><br><span class="line">Message Queue : Kafka</span><br><span class="line">Database &amp; Analytics Engine : Hbase + Spark</span><br></pre></td></tr></table></figure></li></ul></li><li><p>컨테이너 기반 (Kubernetes)</p><ul><li>어느 범위까지 컨테이너화할 것인가?</li><li>Kubernetes Clustering은 어떻게 구성 할 것인가?</li><li>Docker registry는 어떤식으로 사용할 것인가?</li></ul></li></ol><br/><p>그 외에도 대용량 데이터 처리임을 고려하여 어떻게하면 속도와 공간 효율성을 확보할 수 있을 지에 대한 고민을 정말 많이 했다.</p><p>내가 알고 있는거라곤 ‘쿠버네티스’ 다섯 글자 뿐…<br>이때까지 이정도의 대용량 데이터 처리를 해본적이 없었고 MQ 나 Hadoop 사용 경험도 없었기 때문에 처음 접하는 것들이 대부분이었다. 책도 찾아 읽고 나름대로의 공부도 많이 해서 어느정도 설계까지는 했지만 막상 실제 인프라 구축은 막막하기만 했는데, 우리 팀에 데이터 마술사님(!)이 계셔서 초반 구축을 <del>알잘딱깔센</del> 해주셔서 🐶🍯 이었다.</p><p>Hackday 준비를 하면서 공부도 많이 됐지만 능력있고 열정있는 팀원님을 보면서 특히 많이 배웠다. 최고의 팀원 최고 bbb</p><br/><h2 id="D-day-Hack-day"><a href="#D-day-Hack-day" class="headerlink" title="D-day, Hack-day"></a>D-day, Hack-day</h2><p>Hackday 당일, 춘천으로 출발 전에 미리 모여 멘토님과 간단하게 점심 식사를 했다.<br>그린 팩토리 근처 식당에서 돈까스 먹었는데 굉장히 맷-집 이더라. JMTGR.</p><center><img src="https://user-images.githubusercontent.com/26691216/60395184-bb234400-9b6a-11e9-8966-64be35892e6b.jpg" width=40%>??: 좋아해요? </center><p>멘토님을 처음 뵙는 자리라 조금 긴장도 됐었는데 생각보다 편한 분위기로 얘기를 나누고 근처 카페에서 커피도 사주셔서 감사하게 먹고 그린팩토리로 향했다.</p><p>하지만 모든게 평화롭고 순조로운 가운데, <strong>한가지 문제</strong>가 있었다.</p><p>팀원 분 중에 한 분이 몸이 아프셔서 당일 날 못 오신 것이다. 위에서 말했다 싶이 한 팀당 3명이 팀이었고, 대부분 세 명인 가운데 갑자기 우리팀만 둘이 되었다.</p><p>이 때 느낌왔다. <U>오늘 숙소 구경은 물건너 갔음을.</U></p><br/><center><img src="https://user-images.githubusercontent.com/26691216/60395094-3c79d700-9b69-11e9-9a2f-faf606655425.JPG" width=50%><i>행사 진행 장소인 NAVER CONNECT ONE은 내부 사진 공개를 금하고 있기 때문에 시설 모습이 보이는 구체적인 사진들은 공개할 수 없다.</i></center><br/><p>사실 나와 다른 팀원님은 커네트원이 두 번째 방문이라 구경은 제쳐두고 회의실에만 박혀있었어서 사진도 거의 안찍었다. 때 맞춰 나와 밥 먹고 당 떨어지면 간식 가져오는 거 외에는 회의실에 스스로를 감금했다.</p><p>왜냐고? 순조롭긴 개뿔 내가 해간거 하나도 안됐다. <del>tlqk</del></p><blockquote><p>인프라 특 _ 이유없이 갑자기 안됨</p><p>루까 특 _ 이유없긴 사실상 <strong>본인 잘못</strong>임</p></blockquote><br/><p>도착하자 마자 마음이 급했던 우리 팀은 곧 바로 회의를 시작했다. 사전에 온라인 회의를 통해 설계를 논의하긴 했었지만 확정은 아니었고, 그 이후로 각자가 고민했던 부분과 그 결과로 설계한 구조를 서로 공유하고 멘토님께 피드백을 받았다.</p><p><strong><em>결론만 얘기하자면, 나와 다른 팀원님의 디자인은 완전히 달랐다.</em></strong></p><p>같은 문제를 접했고 사용할 인프라도 같이 정했음에도 불구하고, 생각하는 해결 방식이 다른 것이다. 이런 점이 사실 놀랍기도 하고 또 각 방법의 장단점이 분명해서 어떤 설계에 따라 구현할지 토의를 하면서 많은 고민이 되었다. 결국 일단은 내 설계대로 진행하기로 결론을 냈지만, 여기의 가장 큰 이슈는 <U>Hackday 기간 안에 구현이 가능할지</U> 였다. </p><blockquote><p>✽ 프로젝트 내용은 공개 가능한 범위가 모호하여 일단 보류하고, 추후에 기회가 되는대로 짰던 코드와 함께 겪었던 문제점, 해결 과정, 느낀점 등을 따로 정리하려 한다.</p></blockquote><br/><h4 id="‘혹시’하면-‘역시’다"><a href="#‘혹시’하면-‘역시’다" class="headerlink" title="‘혹시’하면 ‘역시’다."></a>‘혹시’하면 ‘역시’다.</h4><center><img src="https://user-images.githubusercontent.com/26691216/60401215-0ebc7e80-9bb9-11e9-90bf-d301476adcce.gif" width="210"></center><center><i>서버들은 이유 없이 돌아가며 터지고, <br/>Kubernetes는 갑자기 막혔고, <br/>Hbase Cluster도 갑자기 터져 팀원님이 해결하고 계시는 와중에 <br/>우려 했던 Spark (Scalar) 멘붕까지 <strong>터/져/Ba/by</strong> 상태 <br/></i></center> <p>새로운 도메인 지식 습득과 설계에 급급했던 나머지 나는 초반 인프라 구축 참여가 적었고 거기에 대한 이해도가 다소 부족했다. 그 중 Kafka는 비교적 많이 공부를 해갔지만 DB 쪽에는 신경을 많이 쓰지 못했더니,</p><p><strong>Java로 Consumer와 Producer를 구현하긴 했는데 막상 데이터를 어떻게 다뤄야 되는지를 모르겠는 거다.</strong></p><p>다른 팀원님이 DB 문제를 해결하고 계시는 동안 혼자 Consumer 모듈에 Spark를 적용해보려니 눈앞이 캄캄했다. 밤새 Spark가 대체 뭐며 어떻게 적용해야 하는지를 찾아봐도 참고하라고 주신 Scalar 코드를 봐도 좀 처럼 각이 안나왔다.<br>이 때 사실 팀원분이 데이터 처리 경험이 나보다 많다는 이유로 나는 너무 안일하게 준비한게 아닌가라는 반성을 많이 했다.</p><p>예상했던 대로 <U>그 좋은 숙소는 주인 없는 밤을 보내게 되었다</U>. 🛏</p><br/><br/><p>다음 날 점심 식사 후, 최종 결과물을 멘토님께 공유하고 피드백을 받았다.</p><p>처음에는 팀 프로젝트였지만 진행하다보니 계획했던 결과를 내기에는 부족함이 있어 결국 팀원 각자 나름의 구현 결과물을 내게 되었다.<br>(각자의 설계를 기반으로 하되, 각자가 처한 상황에 따라 스펙을 조금씩 바꾸었다)</p><p>밤새 구현한 걸 몇 번이고 뒤집어 엎어 최종적으로 계획 했던 동작은 구현하긴 했지만 ‘대용량 데이터 처리’와 ‘컨테이너 환경’에 대한 아쉬움은 어쩔 수 없었다. 이렇게 아쉬움을 한 가득 안고 <strong>Campus Hackday</strong>는 마무리 되었지만, 또 배워가는 것도 한 가득이라 여러모로 의미가 큰 행사였다.</p><br/><h2 id="After"><a href="#After" class="headerlink" title="After .."></a>After ..</h2><p>Hackday를 다녀와서 가장 먼저 한 일은 <strong>잠</strong> 🥱 이다. 즈질 체력…;;</p><p>그러고나서 까먹기 전에 부족했던 점과 배운 점을 정리하면서<br>멘토님께 피드백 받은 부분과 스스로 아쉬웠던 부분은 인프라가 아직 남아있을 때 공부하면서 보완해보고자 했다.</p><center><img src="https://user-images.githubusercontent.com/26691216/60398767-339ffa00-9b97-11e9-9caa-05023dba1d11.png" width="210"><s>사람이 안하던 짓을 하면...</s></center><p>생각했던 것보다 서버 회수 시기가 앞당겨져서 아쉽기는 했지만, 아쉬운대로 로컬에 최대한 동일한 테스트 환경을 구축해서 Code Refactoring / Test 와 정리한 내용 바탕의 문서 작성을 마치고 팀 Github에 Issue와 PR을 올림으로써 Hackday 프로젝트를 마무리 지었다.</p><hr><br/><h4 id="느낀점"><a href="#느낀점" class="headerlink" title="느낀점"></a>느낀점</h4><p>Naver Campus Hackday와 다른 해커톤과의 차이점은 실무에 어느정도 직접적인 연관이 있는 문제들을 접하고 그 중 본인이 관심있는 주제를 정할 수 있다는 점이고, 1박 2일 행사 기간 외에도 문제에 대해 좀 더 깊게 고민하고 개발할 수 있는 시간이 있다는 것이다. </p><p>그리고 또 다른 큰 차이점은 <strong>‘경쟁이나 시험이 아니다’</strong> 라는 것이다.<br>다른 팀과의 내용 공유가 따로 없어 어떤 걸 어떻게 해결하셨는지 정말 궁금하긴 했지만, 대신에 비교도 없고 경쟁도 없다. </p><p>물론 우수참가자에게는 네이버 인턴 면접기회를 준다는 혜택이 분명 존재하지만, 멘토님도 여러 차례 강조하셨던 것 처럼 정해진 답을 찾는다기보다 <U>실제 실무에서의 문제를 접하고 해결하는 경험</U>을 할 수 있는 기회이다. 경쟁이 아닌 또래의 열정적이고 실력있는 분들을 만나 많이 배우고 자극을 받을 수 있는 점이나, 현업의 네이버 개발자님의 멘토링과 피드백을 받을 수 있는 점 모두 다른 곳에서 경험 할 수 없는 귀한 경험인 것 같다.</p><p>끝나고 가장 아쉬웠던 점 중 하나는 사전에 좀 더 많은 시간을 투자해서 당일에는 좀 더 활발한 네트워킹과 구체적인 피드백 (+ 코드 리뷰) 을 받을 수 있었으면 더 좋지 않았을까 하는 것이다. </p><blockquote><p><em>‘Hackathon’</em> 이라고 생각하고 <em>‘Hackday’</em> 를 충분히 즐기지 못했던게 아쉽긴하지만,<br>근래 조금은 지쳤던 나에게 <strong><em>좋은 자극</em></strong> 이 되었음에는 틀림 없다.  </p></blockquote><br/><p>혹시나 Naver Campus Hackday 참가를 희망하거나 이미 합격하신 분이 이 글을 읽는다면 조금의 도움이 되시길 바라며 글을 마친다.</p><br/><p>아디다디도스! 👋🏻</p><br/>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;언젠가부터 주위 동기들이 &lt;em&gt;해커톤 (Hackaton)&lt;/em&gt; 에서 좋은 경험을 하고 오는 걸 보면서,&lt;br&gt;항상 나도 해보고 싶다는 마음은 있었지만 현생이 바쁘다는 이유로 단 한번도 도전해보지 않았었다.&lt;/p&gt;
&lt;center&gt;&lt;img sr</summary>
      
    
    
    
    <category term="retrospect" scheme="https://minsw.github.io/categories/retrospect/"/>
    
    
    <category term="naver" scheme="https://minsw.github.io/tags/naver/"/>
    
    <category term="hackathon" scheme="https://minsw.github.io/tags/hackathon/"/>
    
    <category term="container" scheme="https://minsw.github.io/tags/container/"/>
    
    <category term="kafka" scheme="https://minsw.github.io/tags/kafka/"/>
    
    <category term="CNCF" scheme="https://minsw.github.io/tags/CNCF/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes는 뭘까</title>
    <link href="https://minsw.github.io/2019/01/26/Kubernetes%EB%8A%94-%EB%AD%98%EA%B9%8C/"/>
    <id>https://minsw.github.io/2019/01/26/Kubernetes%EB%8A%94-%EB%AD%98%EA%B9%8C/</id>
    <published>2019-01-26T05:38:20.000Z</published>
    <updated>2021-04-30T04:56:02.030Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Kubernetes를-시작하기-앞서"><a href="#Kubernetes를-시작하기-앞서" class="headerlink" title="Kubernetes를 시작하기 앞서"></a>Kubernetes를 시작하기 앞서</h2><blockquote><p><em>최신 개발 트렌드는 …</em></p><p><em>어플리케이션의 구조를 <u>작고, 독립적인 단위</u>로 개발하고 (Microservices),</em></p><p><em>이를 <u>경량화된 가상화 환경</u>에서 구동할 수 있는 단위 (Container)로 생성하여,</em></p><p><em>이러한 <u>컨테이너들을 관리</u>할 수 있는 환경 (Cloud Native)을 구성하는 것이다</em></p></blockquote><br><h3 id="1-Microservice-Architecture-MSA"><a href="#1-Microservice-Architecture-MSA" class="headerlink" title="1. Microservice Architecture (MSA)"></a>1. Microservice Architecture (MSA)</h3><p>과거에는 서비스를 하나의 애플리케이션으로 만들어 모든 시스템을 그 하나에 다 집어넣는 <strong>모놀리식 아키텍처(Monorithic Architecture)</strong> 로 만들었다. 이러한 일체식 구조는 개발/배포/확장을 단순하게 만드는 장점을 가지지만, 큰 규모일 수록 코드이해나 수정이 어렵다.</p><p>그래서 등장하게된 <strong>마이크로서비스 아키텍처(Microservice Architecture)</strong> 는 서비스를 <u>작고</u>, <u>독립적이고</u>, <u>느슨하게 결합</u>하는 방식의 서비스 지향 아키텍처이다. 각각의 요소를 독립적인 어플리케이션으로 만들고, API로 조합해 애플리케이션으로 만든다.</p><blockquote><p><strong>MSA 구성요소</strong></p><ul><li>Service Discovery</li><li>Circuit Breaker</li><li>Sidecar (Service Discovery + Circuit Breaker) </li><li>Service Mesh</li><li>Service Mesh’s Control Plane</li></ul></blockquote><br><br><h3 id="2-Virtualization"><a href="#2-Virtualization" class="headerlink" title="2. Virtualization"></a>2. Virtualization</h3><p>서버를 가상으로 분할하는 <strong>가상화 (Virtualization)</strong> 는, 분할된 가상의 서버 내부에서 서비스를 실행하여 리소스를 효율적으로 쓰고자하는 기술이다. 가상화는 <strong>KVM, XEN, Hyper-V</strong> 등의 하이퍼바이저 기반의 기술과, <strong>Docker, LXC</strong> 등의 컨테이너 기반의 기술이 발전하면서 상용화되고 있다.</p><br><h3 id="Container"><a href="#Container" class="headerlink" title="Container"></a>Container</h3><blockquote><p><strong>VM</strong> 은 하이퍼바이저를 통한 하드웨어의 가상화이고, </p><p><strong>Container</strong> 는 OS레벨의 가상화 (User 공간의 추상화)를 제공한다.</p></blockquote><p><strong>Container</strong>는 host 시스템의 커널을 container들끼리 공유하기때문에 가볍고 빠른 속도를 가지며 편리하다.</p><p>모듈성(modularity)와 확장성(scalability)이 좋지만 보안성이 약하다 =&gt; VM과 공존 필요</p><p>​        <em>여러대의 서버에 여러대의 어플리케이션을 쓴다면 VM이,</em></p><p>​        <em>하나의 서버에 여러대의 어플리케이션을 쓴다면 Container가 적합할 수 있다</em></p><br><h3 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h3><p>리눅스 컨테이너를 기반으로 하는 오픈소스 프로젝트</p><p>namespace, control group(cgroup)과 같은 리눅스 커널 기능을 이용해서 OS 위에 컨테이너들을 생성하는 기술이다.</p><br><br><h3 id="3-Cloud-Native-Computing-Foundation-CNCF"><a href="#3-Cloud-Native-Computing-Foundation-CNCF" class="headerlink" title="3. Cloud Native Computing Foundation (CNCF)"></a>3. Cloud Native Computing Foundation (CNCF)</h3><p><strong>Cloud Native Computing</strong> 은 클라우드 컴퓨팅 모델의 장점을 모두 활용하는 애플리케이션을 개발하고 실행하기 위한 접근 방식이다.</p><p>microservice로 앱을 배포하고, 컨테이너 별로 패키징하고, 리소스 사용량을 최적화하는 동적 조절을위해 오픈소스 소프트웨어를 사용한다. <strong>Cloud Native Computing Foundation (CNCF)</strong> 는 이러한 클라우드 기술과 관련된 표준형을 개발하려는 단체이며, <U><strong>Kubernetes</strong></U>가 유일한 중심 프로젝트로 편성되었다.</p><blockquote><p> kubernetes, prometheus, envoy, istio, …</p></blockquote><br><br><br><h1 id="Kubernetes-k8s-란"><a href="#Kubernetes-k8s-란" class="headerlink" title="Kubernetes (k8s) 란?"></a>Kubernetes (k8s) 란?</h1><blockquote><p><em>그래서 MSA형태로 개발된 서비스들을 Docker로 컨테이너화해서 띄우긴했는데..</em></p><ul><li>여러대의 물리서버에서 각각 관리하기도 어렵고</li></ul><ul><li><p><u>lifecycle management</u>도 필요하고 (문제 대응, 패치, 업데이트 등)</p></li><li><p>컨테이너 배포, 스케일링, 오퍼레이팅등도 자동으로 되면 좋겠는데…</p><p>=&gt; (“해결사가 왔어!”) <strong>Kubernetes</strong></p></li></ul></blockquote><br><h3 id="Kubernetes"><a href="#Kubernetes" class="headerlink" title="Kubernetes"></a>Kubernetes</h3><p><strong>kubernetes</strong>는 “Docker container Orchestration tool” </p><p>컨테이너화된 어플리케이션을 Automatic deployment / Scaling / Management</p><p>- 그리스어로 ‘키잡이’라는 뜻으로, 줄여서 k8s라고 부른다. (k와 s사이에 8글자)</p><p>- Google에서 최초 개발되었고 현재는 CNCF에 기증된 상태</p><blockquote><p><em>service를 host os를 공유하는 container화해서 올리는  <strong>docker</strong></em></p><p><em>docker를 관리하는 <strong>k8s</strong>  (kubernetes)</em> </p><p><em>이러한 k8s application들을 chart화 시키고 관리하는 <strong>helm</strong></em></p></blockquote><br><h3 id="k8s-Object"><a href="#k8s-Object" class="headerlink" title="k8s Object"></a>k8s Object</h3><ul><li> <strong>Pod</strong> (name + spec + containers)</li></ul><p>- k8s의 가장 기본단위이자 Container의 묶음</p><p>- pod 단위로 network namespace와 ip 가질 수 있음 (!= namespace in k8s)</p><p>- 같은 pod에서는 같은 volume 접근가능</p><br><ul><li><strong>ReplicaSet</strong> (<strong>Pod</strong> + replicas)</li></ul><p>- <u>replica</u>는 복제라는 의미로 replicas 수만큼 pod가 유지되도록 관리된다</p><p>- pod는 죽으면 다시 되살리지않지만, ReplicaSet으로 만들면 replicas 수 (=pod 수)에 맞게 계속 살린다</p><p>- (단, 모니터링이 되어 autoscaler가 작동되고 있는 상황이어야 함)</p><br><ul><li><strong>Deployment</strong> (<strong>ReplicasSet</strong> + History (revision))</li></ul><p>- deployment는 name + replicas + pod 내용으로 구성되고, 대부분은 deployment를 사용해서 배포한다</p><p>- 버전별로 설치/롤백되고 배포 관리가 가능하다</p><p>- apps/v1일때는 <u>selector</u>가 있어야 <u>labels</u>를 가져올 수 있다</p><blockquote><p>​    <code>kubectl create deployment.yaml</code> <em>로 Deployment를 생성할 때 순서를 확인해보면 ,</em></p><p>​    <em>“ Deployment -&gt; ReplicaSet -&gt; Pod “  순으로 생성된다</em></p></blockquote><br><ul><li><strong>Service</strong></li></ul><p>Load balancer를 이용하여 여러 pod들을 하나의 ip, port로 묶어서 제공하는 DNS이다</p><p>그 기준은 <u>label selector</u> 로, 특정 label을 가진 것들을 하나의 서비스로 묶는다.</p><blockquote><p>Service object 노출 방식 3가지</p><ol><li><strong>ClusterIP</strong> - default값으로, Service에 Cluster IP (내부 IP)를 할당한다. 클러스터 내부에서만 접근 가능하고 외부에서는 접근이 불가능</li><li><strong>NodePort</strong> - 각각의 Node의 IP와 static 포트를 노출하여 접근가능하게 하고, 클러스터 외부에서도 접근가능</li><li><strong>Load Balancer</strong> - Cloud provider(GCE/AWS)와 같은 외부IP를 가진 Load balancer에게 Service를 노출</li></ol></blockquote><br><ul><li>그 외 고오급 오브젝트</li></ul><p>- <strong>DaemonSet</strong> : 맵핑된 label이 있는 node가 추가되면 자동으로 해당 node에 pod 생성을 보장 (scaling)</p><p>- <strong>StatefulSet</strong> : 컨테이너가 제거/재시작되어도 상태의 영속성과 지속성을 보장    =&gt; like DB<br><br></p><blockquote><p>- <strong>Affinity</strong> : kube-schedular에게 정보 제공. 부하 분산 또는 버전관리 가능</p><p>( Session <strong>Affinity</strong> - sticky session 제공 (canary deployment) )</p></blockquote><br><br><h3 id="기타-Keyword"><a href="#기타-Keyword" class="headerlink" title="기타 Keyword"></a>기타 Keyword</h3><p><strong>Docker 배포</strong></p><blockquote><p>특징 : 확장성, 표준성, 이미지 기반, 환경변수로 제어하는 설정, 공유자원..</p></blockquote><p>배포툴 : <u><strong>kubernetes</strong></u>, docker swarm, coreos, fleet,…</p><br/><p><strong>Kubernetes 배포 프로세스 (리소스 배포)</strong></p><blockquote><p><em>binary build -&gt; containerizing(image) -&gt; push image -&gt; service define -&gt; test deploy (canary test) -&gt; prod deploy</em>  </p><p>=&gt; 어렵고 복잡. 이런 배포 프로세스를 통합/자동화하는 CI CD 파이프라인 필요</p></blockquote><br><p><strong>Kubernetes 배포 도구 (설치)</strong></p><p>배포툴 : <u>kubespray</u>, kubeadm, kops, … (CaaS 지원)</p><br><br><p>* <strong>오케스트레이션</strong> (Orchestration)</p><p>여러 서버를 운영할때, 이들을 관리하는 것</p><ul><li><p>IaC를 돕는 설정관련 도구는 chef puppet <u>Ansible</u> SaltStack…</p></li><li><p>CI/CD 관리 도구는 Travis CI, <u>Jenkins</u>, Circle CI ..</p></li><li><p>컨테이너관리 도구는 Docker swarm, <u>Kubernetes</u> …</p></li></ul><br><p>* <strong>Ansible</strong></p><p>구성관리 tool로, 인프라 관리과정을 코드로 기술한 IaC (Infra as Code)를 효율적이고 자동으로 관리할수있는 인프라 도구.</p><p>Python 기반의 개발 + YAML로 정의 + JSON으로 통신</p><p>초기설정이나 모니터링, 변경사항 추적이 불가능하다는 단점이 있지만, shell command를 제외하고는 모두 <strong>Idempotency(멱등성)</strong> 을 제공한다.</p><blockquote><p> <em>kubespray는 ansible 기반의 배포툴이다.</em></p></blockquote><br><p>* <strong>Helm</strong></p><p>Chart라는 개념으로 kubernetes의 application을 정의, 배포하고 관리</p><ul><li>Chart: app 구성하는 Kubernetes 객체들을 정의한 manifest template파일 및 설정묶음</li><li>Cient (helm client, CLI) - Server (<strong>tiller</strong>, pod형태로 배포됨) 구조</li><li>Release: client 통해 kube 위에 배포된 app</li></ul><p>=&gt; helm client 설치 후 tiller server를 kubernetes cluster위에 설치해야함</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">helm init (—upgrade)&#x2F;&#x2F; tiller 설치 </span><br><span class="line">helm install &#x2F;&#x2F; repository에 등록된 chart를 client-&gt;tiller로 보냄</span><br><span class="line">helm lint&#x2F;&#x2F; chart의 문법검사</span><br></pre></td></tr></table></figure><br><p>* <strong>CI/CD</strong></p><ul><li><p>CI (Continuous Integration): 지속적 통합, 자주 Build &amp; Packaging</p></li><li><p>CD (Continous Delivery / Deployment): 지속적 배포, 자주 Deployment</p></li></ul><br><br><h3 id="참조"><a href="#참조" class="headerlink" title="참조"></a>참조</h3><ol><li><a href="https://www.samsungsds.com/global/ko/support/insights/101917_RD_Cloudnative.html">https://www.samsungsds.com/global/ko/support/insights/101917_RD_Cloudnative.html</a>)</li><li><a href="https://engineering.linecorp.com/ko/blog/infrastructure-trends-open-infra-days-korea-2018/">https://engineering.linecorp.com/ko/blog/infrastructure-trends-open-infra-days-korea-2018/</a></li><li>갓승규님 블로그 <a href="https://ahnseungkyu.com/">https://ahnseungkyu.com/</a> </li><li>Google Cloud - JAM k8s 입문반 QWIK LAB 진행</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Kubernetes를-시작하기-앞서&quot;&gt;&lt;a href=&quot;#Kubernetes를-시작하기-앞서&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes를 시작하기 앞서&quot;&gt;&lt;/a&gt;Kubernetes를 시작하기 앞서&lt;/h2&gt;&lt;bloc</summary>
      
    
    
    
    <category term="kubernetes" scheme="https://minsw.github.io/categories/kubernetes/"/>
    
    
    <category term="container" scheme="https://minsw.github.io/tags/container/"/>
    
    <category term="CNCF" scheme="https://minsw.github.io/tags/CNCF/"/>
    
    <category term="kubernetes" scheme="https://minsw.github.io/tags/kubernetes/"/>
    
    <category term="mwolkka" scheme="https://minsw.github.io/tags/mwolkka/"/>
    
    <category term="k8s" scheme="https://minsw.github.io/tags/k8s/"/>
    
  </entry>
  
</feed>
