<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Look out</title>
  
  
  <link href="/feed.xml" rel="self"/>
  
  <link href="https://minsw.github.io/"/>
  <updated>2019-06-30T16:18:47.289Z</updated>
  <id>https://minsw.github.io/</id>
  
  <author>
    <name>Lukka Min</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2019 NAVER HACKDAY SUMMER 후기</title>
    <link href="https://minsw.github.io/2019/06/30/2019-NAVER-HACKDAY-SUMMER-%ED%9B%84%EA%B8%B0/"/>
    <id>https://minsw.github.io/2019/06/30/2019-NAVER-HACKDAY-SUMMER-후기/</id>
    <published>2019-06-30T07:13:28.000Z</published>
    <updated>2019-06-30T16:18:47.289Z</updated>
    
    <content type="html"><![CDATA[<p>언젠가부터 주위 사람들이 하나 둘 <em>해커톤 (Hackaton)</em> 에서 좋은 경험을 하고 오는 걸 보면서, 항상 해보고 싶다는 마음은 있었지만 현생이 바빠 단 한번도 도전해보지 않았었다.</p><center><img src="https://user-images.githubusercontent.com/26691216/60393712-742a5400-9b54-11e9-9771-03862d5b94df.jpg" width="30%"><br><i>핑계는…</i></center><p>길고 길었던 지옥의 사망년을 벗어난 기념으로, 지난 4월 <u>세 개의 해커톤</u>을 지원했고 운 좋게 <strong>두 개를 합격하였다</strong>.</p><p>이번 포스트에서는 그 중 <strong>NAVER CAMPUS HACKDAY</strong> 후기를 작성하고자 한다.</p><blockquote><p>나머지 하나는 추후에 업로드 예정이다. <del>아마도…</del></p></blockquote><p><br></p><hr><p><br></p><h1 id="2019-NAVER-CAMPUS-HACKDAY-SUMMER"><a href="#2019-NAVER-CAMPUS-HACKDAY-SUMMER" class="headerlink" title="2019 NAVER CAMPUS HACKDAY SUMMER"></a>2019 NAVER CAMPUS HACKDAY SUMMER</h1><center><img src="https://d2.naver.com/content/images/2019/03/19CHACK_S.png" width="60%"></center><blockquote><p>NAVER D2 - CAMPUS HACKDAY 행사 안내 <a href="https://d2.naver.com/news/5009947" target="_blank" rel="noopener">https://d2.naver.com/news/5009947</a></p><p>GITHUB Page <a href="https://github.com/NAVER-CAMPUS-HACKDAY/common" target="_blank" rel="noopener">https://github.com/NAVER-CAMPUS-HACKDAY/common</a></p></blockquote><p>깃헙 레포의 이슈에 있는 37개의 주제 중 희망하는 1~2개의 주제를 골라 지원서를 작성하고, 4월 13일에 온라인 코딩 테스트를 보았다.</p><p>코딩 테스트는 원하는 시간에 접속하여 제한시간 두 시간동안 3문제를 풀어야 했고, 문제 난이도는 그리 높은 편은 아니었던 것 같다. 제출하는 시간도 기록되었기 때문에 테스트 케이스를 적당히 확인하고 한 시간 조금 넘은 시점에 제출했다.</p><p>후담이지만 <a href="https://codingcompetitions.withgoogle.com/codejam" target="_blank" rel="noopener">Google code jam</a>의 ‘Round 1A 2019’ 도 같은 날에 진행되어서 스타벅스에 앉아서 하루죙일 정신없이 문제만 풀었다. 🤦🏻‍♀️</p><p><br></p><h3 id="🎉-햅격-🎉"><a href="#🎉-햅격-🎉" class="headerlink" title="🎉 햅격~ 🎉"></a>🎉 햅격~ 🎉</h3><p><img src="https://user-images.githubusercontent.com/26691216/60394163-02093d80-9b5b-11e9-8129-2444f06d0741.png" width="905"></p><p>기대 안하고 있었지만 사실 기대하긴 했다. (ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ)</p><p>해커톤 중에서도 Naver hackday는 꼭 한번쯤 가보고 싶었던 행사였기 때문에 특히 좋았다. 기쁜 와중에 딱 날짜가 종설 프로젝트 중간 발표 날이라서 팀원들에게 미리 양해를 구했는데, 다행히 마음씨 좋은 우리 팀원분들은 너그럽게 이해해주셨다.</p><center><img src="https://user-images.githubusercontent.com/26691216/60394226-0124db80-9b5c-11e9-80ed-221836478984.png" width="40%"></center><p><br></p><hr><p><br></p><h2 id="Before-Hack-day"><a href="#Before-Hack-day" class="headerlink" title="Before Hack-day"></a>Before Hack-day</h2><p>내가 수행하게 된 주제는 <strong>“컨테이너 기반 쇼핑 상품 정보 수신”</strong> 으로, 세 명이 한 팀을 이루고 멘토님 한 분이 함께 해주셨다. </p><blockquote><p><strong>[컨테이너 기반 쇼핑 상품 정보 수신]</strong></p><p><em><a href="https://github.com/NAVER-CAMPUS-HACKDAY/common/issues/7" target="_blank" rel="noopener">https://github.com/NAVER-CAMPUS-HACKDAY/common/issues/7</a></em></p><p>상품정보 수집을 위하여 각 쇼핑몰에서 제공하는 상품정보 <strong>EP</strong>(Engine Page)를 주기적으로 수집하여 변경된 정보를 체크하고 서비스에 반영하고 있다. 해당 작업의 <strong>확장성 및 고가용성</strong>을 위하여 Kubernetes 등의 컨테이너 환경에서 Task Agent 들이 운용될 수 있도록 설계 및 구현이 필요하다.</p></blockquote><p>해커톤 행사 당일에 주제 선정과 개발이 모두 이루어지는 다른 해커톤들과는 다르게 Naver Campus Hackday는 본인이 지원한 주제에 따라 팀이 꾸려지고, 사전에 멘토님의 가이드에 따라 팀끼리 개발을 어느정도 진행하기도 한다. ( → 해당 부분은 팀by팀 인 것 같다)</p><p>프로젝트에 대해서는 멘토님께 사전에 여쭤봤을 때, 구체적인 플로우는 공개할 수 없으나 내가 짠 코드는 무관하다고 답변을 주셔서 공식 Hackday github 이슈에 노출되어있는 프로젝트의 개괄적인 내용과 함께 느낀점만 간략하게 정리하고자 한다.</p><p><br></p><p>우리 팀의 경우 특히 인프라 구축이 필요한 주제였기 때문에 사전에 LINE과 Github을 통해 온라인 회의를 지속적으로 진행했다. 이를 통해 나는 아래와 같은 사전 준비를 하고서 Hackday에 참가하였다.</p><p><br></p><ol><li><p>MQ(Message Queue), 데이터 처리 방법 등에 대한 이해</p><ul><li>MQ란 무엇이고, 프로젝트에 적합한 프로젝트는 무엇인가?</li><li>데이터 처리 방식 중 Batch와 Stream의 차이와 각각의 장단점은?</li><li>확장성과 고가용성을 고려하였을 때 적합한 데이터 저장 및 분석 방법은?</li></ul></li><li><p>서버 운용 및 클러스터링 계획</p><ul><li>사전에 할당받은 10개의 서버를 어떻게 운용할 것인가?</li><li>어떤 기술 스택을 사용할 것인가?</li><li>어떤식으로 Clustering할 것인가?</li><li><p>어떠한 플로우로 데이터를 처리하고, 어떻게 분석할 것인가? (설계)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Container Management : Kubernetes</span><br><span class="line">Message Queue : Kafka</span><br><span class="line">Database &amp; Analytics Engine : Hbase + Spark</span><br></pre></td></tr></table></figure></li></ul></li></ol><ol start="3"><li>컨테이너 기반 (Kubernetes)<ul><li>어느 범위까지 컨테이너화할 것인가?</li><li>Kubernetes Clustering은 어떻게 구성 할 것인가?</li><li>Docker registry는 어떤식으로 사용할 것인가?</li></ul></li></ol><p><br></p><p>그 외에도 대용량 데이터 처리임을 고려하여 어떻게하면 속도와 공간 효율성을 확보할 수 있을 지에 대한 고민을 정말 많이 했다.</p><p>그리고 내가 알고 있는거라곤 ‘쿠버네티스’ 다섯 글자 뿐.. 이때까지 이정도의 대용량 데이터 처리를 해본적이 없었고 MQ 나 Hadoop 사용 경험도 없었기 때문에 처음 접하는 것들이 대부분이었다. 책도 찾아 읽고 나름대로의 공부도 많이 해서 어느정도 설계까지는 했지만 막상 실제 인프라 구축은 막막하기만 했는데, 우리 팀에 데이터 마술사님(!)이 초반 구축을 멋들어지게 해주셔서 🐶🍯 이었다.</p><p>Hackday 준비를 하면서 공부도 많이 됐지만 능력있고 열정있는 팀원님을 보면서 동기 부여도 많이 되고 정말 배울점이 많았다. 최고의 팀원 최고 bbb</p><p><br></p><h2 id="D-day-Hack-day"><a href="#D-day-Hack-day" class="headerlink" title="D-day, Hack-day"></a>D-day, Hack-day</h2><p>Hackday 당일, 춘천으로 출발 전에 미리 모여 멘토님과 간단하게 점심 식사를 했다.<br>그린 팩토리 근처 식당에서 돈까스 먹었는데 굉장히 맷-집 이더라. JMTGR.</p><center><img src="https://user-images.githubusercontent.com/26691216/60395184-bb234400-9b6a-11e9-8966-64be35892e6b.jpg" width="40%"><br>??: 좋아해요? </center><p>멘토님을 처음 뵙는 자리라 조금 긴장됐었는데 생각보다 편한 분위기로 얘기를 나누고 근처 카페에서 커피도 사주셔서 감사하게 먹고 그린팩토리로 향했다.</p><p>하지만 모든게 평화롭고 순조로운 가운데, <strong>한가지 문제</strong>가 있었다.</p><p>팀원 분 중에 한 분이 몸이 아프셔서 당일 날 못 온다고 연락을 주셨다는 것이다. 위에서 말했다 싶이 한 팀당 3명이 팀이었고, 갑자기 우리팀만 둘이 되었다.</p><p>이 때 나는 확신했다. <u>오늘 숙소 구경은 물건너 갔음을.</u></p><p><br></p><center><img src="https://user-images.githubusercontent.com/26691216/60395094-3c79d700-9b69-11e9-9a2f-faf606655425.JPG" width="50%"><br><br>행사 진행 장소인 NAVER CONNECT ONE은 내부 사진 공개를 금하고 있기 때문에 시설 모습이 보이는 구체적인 사진들은 공개할 수 없다. </center><p><br></p><p>사실 나와 다른 팀원님은 커네트원이 두 번째 방문이라 구경은 제쳐두고 회의실에만 박혀있었어서 사진도 거의 안찍었다. 때 맞춰 나와 밥 먹고 당 떨어지면 간식 가져오는 거 외에는 회의실에 스스로를 감금했다.</p><p>왜냐고? 순조롭긴 개뿔 내가 해간거 하나도 안됐다. <del>tlqk</del></p><blockquote><p>인프라 특 : 이유없이 갑자기 안됨</p><p>루까 특 : 이유없긴 사실상 99% <strong>본인 잘못</strong>임</p></blockquote><p><br></p><p>도착하자 마자 상대적으로 시간이 없었던 우리 팀은 곧 바로 회의를 시작했다. 사전에 이미 팀원 간에 온라인 회의를 통해 설계부분을 논의하긴 했었지만, 그 이후로도 각자가 고민했던 부분과 그 결과로 최종 설계한 구조를 공유하고 멘토님께 피드백을 받았다.</p><p><strong><em>결론만 얘기하자면, 나와 다른 팀원님의 디자인은 완전히 달랐다.</em></strong></p><p>같은 문제를 접했고 해결에 사용할 인프라도 같이 정했음에도 불구하고, 생각하는 해결 방식이 다르다는 것이다. 멘토님도 피드백을 주시면서 본인이 기대하셨던 두 가지의 솔루션이 나왔다고 말씀 하셨다. </p><p>이런 점이 사실 놀랍기도 하고 또 각 방법의 장단점이 분명해서 어떤 설계에 따라 구현할지 토의를 하면서 많은 고민이 되었다. 결국 내가 설계한대로 진행하기로 결론을 냈지만, 여기의 가장 큰 이슈는 <u>Hackday 기간 안에 구현이 가능할지</u> 였다. </p><blockquote><p>프로젝트 내용은 공개 가능한 범위가 모호하여 일단 보류하고 추후에 기회가 되면 짰던 코드와 함께 겪었던 문제점, 해결 과정, 느낀점 등을 따로 정리하도록 하겠다.</p></blockquote><p><br></p><h4 id="‘혹시’하면-‘역시’다"><a href="#‘혹시’하면-‘역시’다" class="headerlink" title="‘혹시’하면 ‘역시’다."></a>‘혹시’하면 ‘역시’다.</h4><center><i>서버들은 이유 없이 돌아가며 터지기 시작했고, <br><br>Kubernetes는 갑자기 막혔고, <br><br>Hbase Cluster에도 문제가 생겨 팀원님이 해결하시는 와중에 <br><br>우려 했던 나의 Spark (정확히는 Scalar)에 대한 이해 부족까지 <strong>터!져! Ba!by!</strong><br><br></i></center> <p>새로운 도메인 지식 습득과 설계에 급급해 초반 인프라 구축 참여가 적었고 이해도가 다소 부족했다. 그 중 Kafka는 비교적 많이 공부를 해갔지만 DB 쪽에는 신경을 쓰지 못해서 막상 Java로 Consumer와 Producer는 구현했는데 데이터를 실제로 어떻게 다뤄야 되는지를 모르겠는 거다.</p><p>다른 팀원님이 DB 문제를 해결하고 계시는 동안 혼자 Producer 구현 코드에서 Spark를 적용하려고 하니까 눈앞이 캄캄했다. 밤새 Spark가 대체 뭐며 어떻게 적용해야 하는지를 찾아봐도, 참고하라고 주신 Scalar Code를 봐도 좀 처럼 <strong>각이 안나왔다 각이</strong>.</p><p>이 때 내가 너무 <del>데이터 마술사</del> 팀원 님께 모든 짐을 지게 하고 정작 나는 너무 안일하지 않았나라는 반성과 부끄러움을 느꼈다.</p><p>그래서 이미 예상하긴 했지만 <u>그 좋은 숙소는 주인 없는 밤을 꼬박 샜다</u>. ☀️</p><p><br></p><p>다음 날 점심식사 후 최종 결과물을 멘토님께 공유하고 피드백을 받았다.</p><p>팀 프로젝트였지만 진행하다보니 계획했던 하나의 통합된 완성본을 만들기에는 시간과 구현의 부족함이 있어 결국 팀원 각자 나름의 결과물을 내게 되었다. (각자의 설계를 기반으로 하되 각자가 처한 상황에따라 스펙을 조금씩 바꾸었다.)</p><p>결국 아쉬움을 한 가득 안고 <strong>Campus Hackday</strong>는 마무리 되었지만, 또 배워가는 것도 참 많아 의미가 큰 행사였다.</p><p><br></p><h2 id="After"><a href="#After" class="headerlink" title="After .."></a>After ..</h2><p>Hackday를 다녀와서 가장 먼저는 <strong>잤다</strong>. 체력 저질 진짜;;</p><p>그러고나서 까먹기 전에 부족함과 배운 점을 정리하고 피드백 받은 부분과 스스로 아쉬웠던 부분은 인프라가 아직 남아있을 때 보완해보고자 했다.</p><center><img src="https://user-images.githubusercontent.com/26691216/60398767-339ffa00-9b97-11e9-9caa-05023dba1d11.png" width="210"></center><p><del>사람이 안하던 짓을 하면…</del></p><p>생각했던 것보다 서버 회수 시기가 앞당겨져서 아쉽기는 했지만, 아쉬운대로 로컬에 최대한 동일한 테스트 환경을 구축해서 Code Refactoring &amp; Test 와 정리했던 내용을 바탕으로 문서 작성을 마치고 팀 Github에 Issue와 PR을 올림으로써 Hackday 프로젝트를 마무리 지었다.</p><hr><p><br></p><p>Naver Campus Hackday와 다른 해커톤과의 차이점은 실무에 어느정도 직접적인 연관이 있는 문제들을 접할 기회와 그 중 본인이 관심있는 분야를 정할 수 있다는 점이고, 1박 2일 행사 기간 외에도 문제에 대해 좀 더 깊게 고민하고 개발할 수 있는 시간이 있다는 것이다. </p><p>그리고 가장 큰 차이는 <strong>‘경쟁이나 시험이 아니다’</strong> 라는 점이다.<br>다른 팀은 어떤 걸 어떻게 해결하는지 정말 궁금하긴 했지만, 대신에 비교도 없고 경쟁도 없다. </p><p>물론 우수참가자에게는 네이버 인턴 면접기회를 준다는 혜택이 분명 존재하지만, 멘토님도 여러 차례 강조하셨던 것 처럼 정해진 답을 찾는다기보다 <u>실제 실무에서의 문제를 접하고 해결하는 경험</u>을 할 수 있는 기회이다. 경쟁보다 또래의 열정적이고 실력있는 팀원님들을 만나 많이 배우면서 자극도 받을 수 있고, 현업의 네이버 개발자님의 멘토링을 받을 수 있는 것도 다른 곳에서 할 수 없는 정말 좋은 경험인 것 같다.</p><p>다만 끝나고 가장 아쉬웠던 점 중 하나는 행사 전에 좀 더 많은 시간을 투자해서, 당일에는 좀 더 많은 정보 공유와 구체적인 피드백 (+ 코드 리뷰) 를 받을 수 있었으면 더 좋지 않았을까 하는 것이다.</p><p>혹시나 Naver Campus Hackday 참가를 희망하거나 예정된 친구님! 이 글을 읽는다면 참고가 되시길 바라며 글을 마친다.</p><p><br></p><p>아디다디도스 👋🏻</p><p><br></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;언젠가부터 주위 사람들이 하나 둘 &lt;em&gt;해커톤 (Hackaton)&lt;/em&gt; 에서 좋은 경험을 하고 오는 걸 보면서, 항상 해보고 싶다는 마음은 있었지만 현생이 바빠 단 한번도 도전해보지 않았었다.&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;http
      
    
    </summary>
    
      <category term="retrospect" scheme="https://minsw.github.io/categories/retrospect/"/>
    
    
      <category term="naver" scheme="https://minsw.github.io/tags/naver/"/>
    
      <category term="hackathon" scheme="https://minsw.github.io/tags/hackathon/"/>
    
      <category term="container" scheme="https://minsw.github.io/tags/container/"/>
    
      <category term="kafka" scheme="https://minsw.github.io/tags/kafka/"/>
    
      <category term="CNCF" scheme="https://minsw.github.io/tags/CNCF/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes는 뭘까</title>
    <link href="https://minsw.github.io/2019/01/26/Kubernetes%EB%8A%94-%EB%AD%98%EA%B9%8C/"/>
    <id>https://minsw.github.io/2019/01/26/Kubernetes는-뭘까/</id>
    <published>2019-01-26T05:38:20.000Z</published>
    <updated>2019-06-30T07:18:36.517Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Kubernetes를-시작하기-앞서"><a href="#Kubernetes를-시작하기-앞서" class="headerlink" title="Kubernetes를 시작하기 앞서"></a>Kubernetes를 시작하기 앞서</h2><blockquote><p><em>최신 개발 트렌드는 …</em></p><p><em>어플리케이션의 구조를 <u>작고, 독립적인 단위</u>로 개발하고 (Microservices),</em></p><p><em>이를 <u>경량화된 가상화 환경</u>에서 구동할 수 있는 단위 (Container)로 생성하여,</em></p><p><em>이러한 <u>컨테이너들을 관리</u>할 수 있는 환경 (Cloud Native)을 구성하는 것이다</em></p></blockquote><p><br></p><h3 id="1-Microservice-Architecture-MSA"><a href="#1-Microservice-Architecture-MSA" class="headerlink" title="1. Microservice Architecture (MSA)"></a>1. Microservice Architecture (MSA)</h3><p>과거에는 서비스를 하나의 애플리케이션으로 만들어 모든 시스템을 그 하나에 다 집어넣는 <strong>모놀리식 아키텍처(Monorithic Architecture)</strong> 로 만들었다. 이러한 일체식 구조는 개발/배포/확장을 단순하게 만드는 장점을 가지지만, 큰 규모일 수록 코드이해나 수정이 어렵다.</p><p>그래서 등장하게된 <strong>마이크로서비스 아키텍처(Microservice Architecture)</strong> 는 서비스를 <u>작고</u>, <u>독립적이고</u>, <u>느슨하게 결합</u>하는 방식의 서비스 지향 아키텍처이다. 각각의 요소를 독립적인 어플리케이션으로 만들고, API로 조합해 애플리케이션으로 만든다.</p><blockquote><p><strong>MSA 구성요소</strong></p><ul><li>Service Discovery</li><li>Circuit Breaker</li><li>Sidecar (Service Discovery + Circuit Breaker) </li><li>Service Mesh</li><li>Service Mesh’s Control Plane</li></ul></blockquote><p><br></p><p><br></p><h3 id="2-Virtualization"><a href="#2-Virtualization" class="headerlink" title="2. Virtualization"></a>2. Virtualization</h3><p>서버를 가상으로 분할하는 <strong>가상화 (Virtualization)</strong> 는, 분할된 가상의 서버 내부에서 서비스를 실행하여 리소스를 효율적으로 쓰고자하는 기술이다. 가상화는 <strong>KVM, XEN, Hyper-V</strong> 등의 하이퍼바이저 기반의 기술과, <strong>Docker, LXC</strong> 등의 컨테이너 기반의 기술이 발전하면서 상용화되고 있다.</p><p><br></p><h3 id="Container"><a href="#Container" class="headerlink" title="Container"></a>Container</h3><blockquote><p><strong>VM</strong> 은 하이퍼바이저를 통한 하드웨어의 가상화이고, </p><p><strong>Container</strong> 는 OS레벨의 가상화 (User 공간의 추상화)를 제공한다.</p></blockquote><p><strong>Container</strong>는 host 시스템의 커널을 container들끼리 공유하기때문에 가볍고 빠른 속도를 가지며 편리하다.</p><p>모듈성(modularity)와 확장성(scalability)이 좋지만 보안성이 약하다 =&gt; VM과 공존 필요</p><p>​        <em>여러대의 서버에 여러대의 어플리케이션을 쓴다면 VM이,</em></p><p>​        <em>하나의 서버에 여러대의 어플리케이션을 쓴다면 Container가 적합할 수 있다</em></p><p><br></p><h3 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h3><p>리눅스 컨테이너를 기반으로 하는 오픈소스 프로젝트</p><p>namespace, control group(cgroup)과 같은 리눅스 커널 기능을 이용해서 OS 위에 컨테이너들을 생성하는 기술이다.</p><p><br><br><br></p><h3 id="3-Cloud-Native-Computing-Foundation-CNCF"><a href="#3-Cloud-Native-Computing-Foundation-CNCF" class="headerlink" title="3. Cloud Native Computing Foundation (CNCF)"></a>3. Cloud Native Computing Foundation (CNCF)</h3><p><strong>Cloud Native Computing</strong> 은 클라우드 컴퓨팅 모델의 장점을 모두 활용하는 애플리케이션을 개발하고 실행하기 위한 접근 방식이다.</p><p>microservice로 앱을 배포하고, 컨테이너 별로 패키징하고, 리소스 사용량을 최적화하는 동적 조절을위해 오픈소스 소프트웨어를 사용한다. <strong>Cloud Native Computing Foundation (CNCF)</strong> 는 이러한 클라우드 기술과 관련된 표준형을 개발하려는 단체이며, <strong><u>Kubernetes</u></strong>가 유일한 중심 프로젝트로 편성되었다.</p><blockquote><p> kubernetes, prometheus, envoy, istio, …</p></blockquote><p><br></p><p><br></p><p><br></p><h1 id="Kubernetes-k8s-란"><a href="#Kubernetes-k8s-란" class="headerlink" title="Kubernetes (k8s) 란?"></a>Kubernetes (k8s) 란?</h1><blockquote><p><em>그래서 MSA형태로 개발된 서비스들을 Docker로 컨테이너화해서 띄우긴했는데..</em></p><ul><li>여러대의 물리서버에서 각각 관리하기도 어렵고</li></ul><ul><li><p><u>lifecycle management</u>도 필요하고 (문제 대응, 패치, 업데이트 등)</p></li><li><p>컨테이너 배포, 스케일링, 오퍼레이팅등도 자동으로 되면 좋겠는데…</p><p>=&gt; (“해결사가 왔어!”) <strong>Kubernetes</strong></p></li></ul></blockquote><p><br></p><h3 id="Kubernetes"><a href="#Kubernetes" class="headerlink" title="Kubernetes"></a>Kubernetes</h3><p><strong>kubernetes</strong>는 “Docker container Orchestration tool” </p><p>컨테이너화된 어플리케이션을 Automatic deployment / Scaling / Management</p><p>- 그리스어로 ‘키잡이’라는 뜻으로, 줄여서 k8s라고 부른다. (k와 s사이에 8글자)</p><p>- Google에서 최초 개발되었고 현재는 CNCF에 기증된 상태</p><blockquote><p><em>service를 host os를 공유하는 container화해서 올리는  <strong>docker</strong></em></p><p><em>docker를 관리하는 <strong>k8s</strong>  (kubernetes)</em> </p><p><em>이러한 k8s application들을 chart화 시키고 관리하는 <strong>helm</strong></em></p></blockquote><p><br></p><h3 id="k8s-Object"><a href="#k8s-Object" class="headerlink" title="k8s Object"></a>k8s Object</h3><ul><li><strong>Pod</strong> (name + spec + containers)</li></ul><p>- k8s의 가장 기본단위이자 Container의 묶음</p><p>- pod 단위로 network namespace와 ip 가질 수 있음 (!= namespace in k8s)</p><p>- 같은 pod에서는 같은 volume 접근가능</p><p><br></p><ul><li><strong>ReplicaSet</strong> (<strong>Pod</strong> + replicas)</li></ul><p>- <u>replica</u>는 복제라는 의미로 replicas 수만큼 pod가 유지되도록 관리된다</p><p>- pod는 죽으면 다시 되살리지않지만, ReplicaSet으로 만들면 replicas 수 (=pod 수)에 맞게 계속 살린다</p><p>- (단, 모니터링이 되어 autoscaler가 작동되고 있는 상황이어야 함)</p><p><br></p><ul><li><strong>Deployment</strong> (<strong>ReplicasSet</strong> + History (revision))</li></ul><p>- deployment는 name + replicas + pod 내용으로 구성되고, 대부분은 deployment를 사용해서 배포한다</p><p>- 버전별로 설치/롤백되고 배포 관리가 가능하다</p><p>- apps/v1일때는 <u>selector</u>가 있어야 <u>labels</u>를 가져올 수 있다</p><blockquote><p>​    <code>kubectl create deployment.yaml</code> <em>로 Deployment를 생성할 때 순서를 확인해보면 ,</em></p><p>​    <em>“ Deployment -&gt; ReplicaSet -&gt; Pod “  순으로 생성된다</em></p></blockquote><p><br></p><ul><li><strong>Service</strong></li></ul><p>Load balancer를 이용하여 여러 pod들을 하나의 ip, port로 묶어서 제공하는 DNS이다</p><p>그 기준은 <u>label selector</u> 로, 특정 label을 가진 것들을 하나의 서비스로 묶는다.</p><blockquote><p>Service object 노출 방식 3가지</p><ol><li><strong>ClusterIP</strong> - default값으로, Service에 Cluster IP (내부 IP)를 할당한다. 클러스터 내부에서만 접근 가능하고 외부에서는 접근이 불가능</li><li><strong>NodePort</strong> - 각각의 Node의 IP와 static 포트를 노출하여 접근가능하게 하고, 클러스터 외부에서도 접근가능</li><li><strong>Load Balancer</strong> - Cloud provider(GCE/AWS)와 같은 외부IP를 가진 Load balancer에게 Service를 노출</li></ol></blockquote><p><br></p><ul><li>그 외 고오급 오브젝트</li></ul><p>- <strong>DaemonSet</strong> : 맵핑된 label이 있는 node가 추가되면 자동으로 해당 node에 pod 생성을 보장 (scaling)</p><p>- <strong>StatefulSet</strong> : 컨테이너가 제거/재시작되어도 상태의 영속성과 지속성을 보장    =&gt; like DB<br><br></p><blockquote><p>- <strong>Affinity</strong> : kube-schedular에게 정보 제공. 부하 분산 또는 버전관리 가능</p><p>( Session <strong>Affinity</strong> - sticky session 제공 (canary deployment) )</p></blockquote><p><br><br><br></p><h3 id="기타-Keyword"><a href="#기타-Keyword" class="headerlink" title="기타 Keyword"></a>기타 Keyword</h3><p><strong>Docker 배포</strong></p><blockquote><p>특징 : 확장성, 표준성, 이미지 기반, 환경변수로 제어하는 설정, 공유자원..</p></blockquote><p>배포툴 : <u><strong>kubernetes</strong></u>, docker swarm, coreos, fleet,…</p><p><br></p><p><strong>Kubernetes 배포 프로세스</strong></p><blockquote><p><em>binary build -&gt; containerizing(image) -&gt; push image -&gt; service define -&gt; test deploy (canary test) -&gt; prod deploy</em>  </p><p>=&gt; 어렵고 복잡. 이런 배포 프로세스를 통합/자동화하는 CICD (배포툴) 필요!</p></blockquote><p>배포툴 : <u>kubespray</u>, kubeadm, kops, … (CaaS 지원)</p><p><br><br><br></p><p>* <strong>오케스트레이션</strong> (Orchestration)</p><p>여러 서버를 운영할때, 이들을 관리하는 것</p><ul><li><p>IaC를 돕는 설정관련 도구는 chef puppet <u>Ansible</u> SaltStack…</p></li><li><p>CI/CD 관리 도구는 Travis CI, <u>Jenkins</u>, Circle CI ..</p></li><li><p>컨테이너관리 도구는 Docker swarm, <u>Kubernetes</u> …</p></li></ul><p><br></p><p>* <strong>Ansible</strong></p><p>구성관리 tool로, 인프라 관리과정을 코드로 기술한 IaC (Infra as Code)를 효율적이고 자동으로 관리할수있는 인프라 도구.</p><p>Python 기반의 개발 + YAML로 정의 + JSON으로 통신</p><p>초기설정이나 모니터링, 변경사항 추적이 불가능하다는 단점이 있지만, shell command를 제외하고는 모두 <strong>Idempotency(멱등성)</strong> 을 제공한다.</p><blockquote><p> <em>kubespray는 ansible 기반의 배포툴이다.</em></p></blockquote><p><br></p><p>* <strong>Helm</strong></p><p>Chart라는 개념으로 kubernetes의 application을 정의, 배포하고 관리</p><ul><li>Chart: app 구성하는 Kubernetes 객체들을 정의한 manifest template파일 및 설정묶음</li><li>Cient (helm client, CLI) - Server (<strong>tiller</strong>, pod형태로 배포됨) 구조</li><li>Release: client 통해 kube 위에 배포된 app</li></ul><p>=&gt; helm client 설치 후 tiller server를 kubernetes cluster위에 설치해야함</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">helm init (—upgrade)// tiller 설치 </span><br><span class="line">helm install // repository에 등록된 chart를 client-&gt;tiller로 보냄</span><br><span class="line">helm lint// chart의 문법검사</span><br></pre></td></tr></table></figure><p><br></p><p>* <strong>CI/CD</strong></p><ul><li><p>CI (Continuous Integration): 지속적 통합, 자주 Build &amp; Packaging</p></li><li><p>CD (Continous Delivery / Deployment): 지속적 배포, 자주 Deployment</p></li></ul><p><br><br><br></p><h3 id="참조"><a href="#참조" class="headerlink" title="참조"></a>참조</h3><ol><li><a href="https://www.samsungsds.com/global/ko/support/insights/101917_RD_Cloudnative.html" target="_blank" rel="noopener">https://www.samsungsds.com/global/ko/support/insights/101917_RD_Cloudnative.html</a>)</li><li><a href="https://engineering.linecorp.com/ko/blog/infrastructure-trends-open-infra-days-korea-2018/" target="_blank" rel="noopener">https://engineering.linecorp.com/ko/blog/infrastructure-trends-open-infra-days-korea-2018/</a></li><li>갓승규님 블로그 <a href="https://ahnseungkyu.com/" target="_blank" rel="noopener">https://ahnseungkyu.com/</a> </li><li>Google Cloud - JAM k8s 입문반 QWIK LAB 진행</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Kubernetes를-시작하기-앞서&quot;&gt;&lt;a href=&quot;#Kubernetes를-시작하기-앞서&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes를 시작하기 앞서&quot;&gt;&lt;/a&gt;Kubernetes를 시작하기 앞서&lt;/h2&gt;&lt;bloc
      
    
    </summary>
    
      <category term="kubernetes" scheme="https://minsw.github.io/categories/kubernetes/"/>
    
    
      <category term="container" scheme="https://minsw.github.io/tags/container/"/>
    
      <category term="CNCF" scheme="https://minsw.github.io/tags/CNCF/"/>
    
      <category term="kubernetes" scheme="https://minsw.github.io/tags/kubernetes/"/>
    
      <category term="k8s" scheme="https://minsw.github.io/tags/k8s/"/>
    
  </entry>
  
</feed>
