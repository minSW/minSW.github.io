<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Look out</title>
  
  
  <link href="https://minsw.github.io/feed.xml" rel="self"/>
  
  <link href="https://minsw.github.io/"/>
  <updated>2021-01-24T17:44:35.723Z</updated>
  <id>https://minsw.github.io/</id>
  
  <author>
    <name>Lukka Min</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>&amp;#039;Spark The Definitive Guide&amp;#039; 3장 - 일단 좀 더 잡솨봐 (PART 1 끝)</title>
    <link href="https://minsw.github.io/2021/01/24/Spark-The-Definitive-Guide-3%EC%9E%A5/"/>
    <id>https://minsw.github.io/2021/01/24/Spark-The-Definitive-Guide-3%EC%9E%A5/</id>
    <published>2021-01-24T14:13:51.000Z</published>
    <updated>2021-01-24T17:44:35.723Z</updated>
    
    <content type="html"><![CDATA[<br/><h4 id="Part-1-END"><a href="#Part-1-END" class="headerlink" title="[Part 1] END"></a>[Part 1] END</h4><p>파트 1 까지 끝내고 나니 이제 조금씩 스파크 맛은 본 것 같은데,<br>이번 장은 <u>‘일단 잡솨봐’ 식 구성</u> 이라 좀 따라가기 힘들었다.</p><blockquote><p>나 : 뭔 말이에요<br>?? : <em>‘XX 장에서 자세히 알아보겠습니다.’</em></p><p>?? : <em>이렇게 A 에 B를 수행하면 Z가 나옵니다</em><br>나 : 이건 또 뭔소리여<br>?? : <em>‘이와 관련된 내용은 XX 부에서 자세히 알아보겠습니다.’</em></p></blockquote><p><img width="150" alt="angry" src="https://user-images.githubusercontent.com/26691216/105637781-85fbe680-5eb2-11eb-8956-b7de17f122de.png"></p><center><i style="color:lightgray">이toRL들이..</i></center><p>예제 기준으로 모르는 부분 찾아가면서 어찌 저찌 이해는 했지만<br>그 다음 파트가 다시 또 <code>구조적 API</code>라서 오늘 본 거 대부분은 한참 뒤에야 다시 보게 될텐데..<br>아 이거 무조건인데.. 백퍼 다 까먹는데.. 🤦🏻‍♀️  </p><center><h2>_ _ _</h2></center><br/><hr><h1 id="CHAPTER-3-스파크-기능-둘러보기"><a href="#CHAPTER-3-스파크-기능-둘러보기" class="headerlink" title="CHAPTER 3 스파크 기능 둘러보기"></a>CHAPTER 3 스파크 기능 둘러보기</h1><blockquote><p>스파크 = 기본 요소 (저수준 API + 구조적 API) + 추가 기능 (일련의 표준 라이브러리)</p><ul><li>구조적 스트리밍, 고급 분석, 라이브러리 및 에코시스템</li><li>구조적 API : Dataset, DataFrame, SQL</li><li>저수준 API : RDD, 분산형 변수</li></ul></blockquote><p>CHAPTER 2 는 구조적 API의 핵심개념을 소개했다면<br>CHAPTER 3 은 나머지 API 와 주요 라이브러리, 스파크의 다양한 기능 소개</p><h3 id="3-1-운영용-애플리케이션-실행하기"><a href="#3-1-운영용-애플리케이션-실행하기" class="headerlink" title="3.1 운영용 애플리케이션 실행하기"></a>3.1 운영용 애플리케이션 실행하기</h3><details><summary class="point-color-can-hover">[3.1] 예제 펼치기</summary><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /spark-2.4.7-bin-hadoop2.7</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># scalar example</span></span><br><span class="line">$ ./bin/spark-submit --class org.apache.spark.examples.SparkPi --master <span class="built_in">local</span> ./examples/jars/spark-examples_2.11-2.4.7.jar 10</span><br><span class="line">...</span><br><span class="line">21/01/24 13:41:15 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:38, took 0.968079 s</span><br><span class="line">Pi is roughly 3.1414071414071416 <span class="comment"># 돌릴때마다 다르게 나온다</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># python example</span></span><br><span class="line">$ ./bin/spark-submit --master <span class="built_in">local</span> ./examples/src/main/python/pi.py 10</span><br><span class="line">Pi is roughly 3.139084 <span class="comment"># 이럴거면 args 는 대체 왜 넣으란걸까</span></span><br></pre></td></tr></table></figure></details><ul><li><code>spark-submit</code> 명령<ul><li>애플리케이션 코드를 클러스터에 전송해 실행시키는 역할</li><li>대화형 쉘에서 개발한 프로그램 -&gt; 운영용 애플리케이션으로 전환 가능</li><li>스파크 애플리케이션은 standalone, Mesos, YARN 클러스터 매니저를 이용해 실행됨 (<code>--master</code> 옵션)</li></ul></li></ul><h3 id="3-2-Dataset-타입-안정성을-제공하는-구조적-API"><a href="#3-2-Dataset-타입-안정성을-제공하는-구조적-API" class="headerlink" title="3.2 Dataset : 타입 안정성을 제공하는 구조적 API"></a>3.2 Dataset : 타입 안정성을 제공하는 구조적 API</h3><details><summary class="point-color-can-hover">[3.2] 예제 펼치기</summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Flight</span>(<span class="params"><span class="type">DEST_COUNTRY_NAME</span>: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                  <span class="type">ORIGIN_COUNTRY_NAME</span>: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">                  count: <span class="type">BigInt</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">flightsDF</span> </span>= spark.read.parquet(<span class="string">&quot;/data/flight-data/parquet/2010-summary.parquet/&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> flights = flightsDF.as[<span class="type">Flight</span>] <span class="comment">// DataFrame -&gt; Dataset[Flight]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(flights</span><br><span class="line">  .filter(flight_row =&gt; flight_row.<span class="type">ORIGIN_COUNTRY_NAME</span> != <span class="string">&quot;Canada&quot;</span>)</span><br><span class="line">  .map(flight_row =&gt; flight_row)</span><br><span class="line">  .take(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">(flights</span><br><span class="line">  .take(<span class="number">5</span>)</span><br><span class="line">  .filter(flight_row =&gt; flight_row.<span class="type">ORIGIN_COUNTRY_NAME</span> != <span class="string">&quot;Canada&quot;</span>)</span><br><span class="line">  .map(fr =&gt; <span class="type">Flight</span>(fr.<span class="type">DEST_COUNTRY_NAME</span>, fr.<span class="type">ORIGIN_COUNTRY_NAME</span>, fr.count + <span class="number">5</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment">// res45: Array[Flight] = Array(Flight(United States,Romania,1), Flight(United States,Ireland,264), Flight(United States,India,69), Flight(Egypt,United States,24), Flight(Equatorial Guinea,United States,1))</span></span><br><span class="line"><span class="comment">// res46: Array[Flight] = Array(Flight(United States,Romania,6), Flight(United States,Ireland,269), Flight(United States,India,74), Flight(Egypt,United States,29), Flight(Equatorial Guinea,United States,6))</span></span><br></pre></td></tr></table></figure></details><ul><li><strong>Dataset</strong> : Java와 Scala의 정적 데이터 타입에 맞는 코드(statically typed code)를 지원하기 위한 스파크의 구조적 API<ul><li>Python, R 사용 X</li></ul></li><li>Dataset API 는 <strong>DataFrame 레코드 =&gt; Java나 Scala로 정의한 클래스에 할당</strong>, Collection 으로 다룰 수 있는 기능 등을 제공<ul><li>DataFrame : 다양한 데이터 타입의 테이블형 데이터를 보관할 수 있는 Row 타입 객체로 구성된 분산 컬렉션 (<a href="https://minsw.github.io/2021/01/24/Spark-The-Definitive-Guide-2%EC%9E%A5/#2-6-DataFrame">2장 참고</a>)</li><li><strong>타입 안정성을 지원</strong> 하므로 초기화에 사용한 클래스 외 다른 클래스를 사용한 접근은 X</li><li>여러 명이 개발하고 잘 정의된 인터페이스로 상호작용하는 대규모 애플리케이션 개발에 유용<br><del>잘 정의된 인터페이스 부터가 실패다 이말이야</del></li></ul></li><li>Dataset 클래스 (Java <code>Dataset&lt;T&gt;</code>, Scala <code>Dataset[T]</code>)<ul><li>내부 객체 타입을 매개변수로 사용 (T) =&gt; 해당 클래스 객체만 가질 수 있음</li><li>스파크 2.0 에서는 자바의 JavaBean 패턴, 스칼라의 케이스 클래스 유형으로 정의된 클래스 지원</li><li>타입 T를 분석해서 Dataset 스키마를 생성해야하므로 타입을 제한할 수 밖에 없음</li></ul></li><li>장점<ul><li>필요한 경우 선택적으로 사용 가능하고, map, filter 등 함수 사용 가능</li><li>코드 변경 없이 타입 안정성을 보장할 수 있고, 안전하게 데이터 다루기 가능<ul><li><code>collect()</code> 나 <code>take()</code> 호출 시 DataFrame의 row 타입 객체가 아닌 Dataset의 지정된 타입(T)의 객체로 반환</li></ul></li></ul></li><li>Dataset의 자세한 내용은 CHAPTER 11 에서 이어서</li></ul><h3 id="3-3-구조적-스트리밍"><a href="#3-3-구조적-스트리밍" class="headerlink" title="3.3 구조적 스트리밍"></a>3.3 구조적 스트리밍</h3><details><summary class="point-color-can-hover">[3.3] 예제 펼치기 (정적 DataFrame 버전) </summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 정적 DataFrame 버전</span></span><br><span class="line"><span class="keyword">val</span> staticDataFrame = (spark.read.format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">  .option(<span class="string">&quot;inferSchema&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">  .load(<span class="string">&quot;/data/retail-data/by-day/*.csv&quot;</span>))</span><br><span class="line"></span><br><span class="line">staticDataFrame.createOrReplaceTempView(<span class="string">&quot;retail_data&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> staticSchema = staticDataFrame.schema</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;window, column, desc, col&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// &#x27;특정 고객(CustomerId)가 대량으로 구매하는 영업 시간&#x27; 구하기</span></span><br><span class="line">(staticDataFrame</span><br><span class="line">  .selectExpr(</span><br><span class="line">    <span class="string">&quot;CustomerId&quot;</span>,</span><br><span class="line">    <span class="string">&quot;(UnitPrice * Quantity) as total_cost&quot;</span>,</span><br><span class="line">    <span class="string">&quot;InvoiceDate&quot;</span>)</span><br><span class="line">  .groupBy(</span><br><span class="line">    col(<span class="string">&quot;CustomerId&quot;</span>), window(col(<span class="string">&quot;InvoiceDate&quot;</span>), <span class="string">&quot;1 day&quot;</span>)) <span class="comment">// 관련 날짜 데이터 그룹화 &amp; 집계</span></span><br><span class="line">  .sum(<span class="string">&quot;total_cost&quot;</span>)</span><br><span class="line">  .show(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// +----------+--------------------+------------------+</span></span><br><span class="line"><span class="comment">// |CustomerId|              window|   sum(total_cost)|</span></span><br><span class="line"><span class="comment">// +----------+--------------------+------------------+</span></span><br><span class="line"><span class="comment">// |   14075.0|[2011-12-05 00:00...|316.78000000000003|</span></span><br><span class="line"><span class="comment">// |   18180.0|[2011-12-05 00:00...|            310.73|</span></span><br><span class="line"><span class="comment">// |   15358.0|[2011-12-05 00:00...| 830.0600000000003|</span></span><br><span class="line"><span class="comment">// |   15392.0|[2011-12-05 00:00...|304.40999999999997|</span></span><br><span class="line"><span class="comment">// |   15290.0|[2011-12-05 00:00...|263.02000000000004|</span></span><br><span class="line"><span class="comment">// +----------+--------------------+------------------+</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>로컬 모드 사용 시 셔플 파티션 수 (default 200) 줄이기를 권장. <code>spark.conf.set(&quot;spark.sql.shuffle.partitions&quot;, &quot;5&quot;)</code></p></blockquote></details><br/><details><summary class="point-color-can-hover">[3.3] 예제 펼치기 (Streaming 버전) </summary><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Streaming 버전</span></span><br><span class="line"><span class="keyword">val</span> streamingDataFrame = (spark.readStream  <span class="comment">// read =&gt; readStream</span></span><br><span class="line">    .schema(staticSchema)</span><br><span class="line">    .option(<span class="string">&quot;maxFilesPerTrigger&quot;</span>, <span class="number">1</span>)  <span class="comment">// maxFilesPerTrigger (한번에 읽을 파일 수 설정) =&gt; 파일별로 트리거 수행</span></span><br><span class="line">    .format(<span class="string">&quot;csv&quot;</span>)</span><br><span class="line">    .option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">    .load(<span class="string">&quot;/data/retail-data/by-day/*.csv&quot;</span>))</span><br><span class="line"></span><br><span class="line">streamingDataFrame.isStreaming <span class="comment">// returns true</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> purchaseByCustomerPerHour = (streamingDataFrame</span><br><span class="line">  .selectExpr(</span><br><span class="line">    <span class="string">&quot;CustomerId&quot;</span>,</span><br><span class="line">    <span class="string">&quot;(UnitPrice * Quantity) as total_cost&quot;</span>,</span><br><span class="line">    <span class="string">&quot;InvoiceDate&quot;</span>)</span><br><span class="line">  .groupBy(</span><br><span class="line">    $<span class="string">&quot;CustomerId&quot;</span>, window($<span class="string">&quot;InvoiceDate&quot;</span>, <span class="string">&quot;1 day&quot;</span>))</span><br><span class="line">  .sum(<span class="string">&quot;total_cost&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1) 스트림 시작 &amp; 인메모리 테이블에 저장</span></span><br><span class="line">(purchaseByCustomerPerHour.writeStream</span><br><span class="line">    .format(<span class="string">&quot;memory&quot;</span>) <span class="comment">// memory = 인메모리 테이블에 저장</span></span><br><span class="line">    .queryName(<span class="string">&quot;customer_purchases&quot;</span>) <span class="comment">// 인메모리에 저장될 테이블명</span></span><br><span class="line">    .outputMode(<span class="string">&quot;complete&quot;</span>) <span class="comment">// complete = 모든 카운트 수행 결과를 테이블에 저장</span></span><br><span class="line">    .start())</span><br><span class="line"></span><br><span class="line"><span class="comment">// 인메모리 테이블 확인 (데이터를 많이 읽으면 읽을수록 테이블 구성이 변경)</span></span><br><span class="line">(spark.sql(<span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">  SELECT *</span></span><br><span class="line"><span class="string">  FROM customer_purchases</span></span><br><span class="line"><span class="string">  ORDER BY `sum(total_cost)` DESC</span></span><br><span class="line"><span class="string">  &quot;</span><span class="string">&quot;&quot;</span>)</span><br><span class="line">  .show(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// +----------+--------------------+------------------+</span></span><br><span class="line"><span class="comment">// |CustomerId|              window|   sum(total_cost)|</span></span><br><span class="line"><span class="comment">// +----------+--------------------+------------------+</span></span><br><span class="line"><span class="comment">// |   17450.0|[2011-09-20 00:00...|          71601.44|</span></span><br><span class="line"><span class="comment">// |      null|[2011-11-14 00:00...|          55316.08|</span></span><br><span class="line"><span class="comment">// |      null|[2011-11-07 00:00...|          42939.17|</span></span><br><span class="line"><span class="comment">// |      null|[2011-03-29 00:00...| 33521.39999999998|</span></span><br><span class="line"><span class="comment">// |      null|[2011-12-08 00:00...|31975.590000000007|</span></span><br><span class="line"><span class="comment">// +----------+--------------------+------------------+</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 2) 처리결과 콘솔에 출력</span></span><br><span class="line">(purchaseByCustomerPerHour.writeStream</span><br><span class="line">    .format(<span class="string">&quot;console&quot;</span>) <span class="comment">// console = 콘솔에 결과 출력</span></span><br><span class="line">    .queryName(<span class="string">&quot;customer_purchases_2&quot;</span>)</span><br><span class="line">    .outputMode(<span class="string">&quot;complete&quot;</span>)</span><br><span class="line">    .start())</span><br><span class="line"></span><br></pre></td></tr></table></figure></details><ul><li><strong>구조적 스트리밍</strong> : 스트림 처리용 고수준 API<ul><li>구조적 API로 개발된 배치 모드 연산을 <strong>스트리밍 방식으로</strong> 실행 가능하며, 지연 시간을 줄이고 증분 처리 가능</li><li>즉 스트리밍 처리로 <u>빠르게 값을 얻을 수 있고</u>, 모든 작업에서 데이터를 <u>증분 처리</u>하면서 수행된다</li><li>배치 잡으로 프로토타입 개발 후에 스트리밍 잡으로 변환도 가능</li><li>스파크 2.2 버전부터 안정화 (production-ready)</li></ul></li><li>데이터를 그룹화하고 집계하는 방법 (시계열 time-series 데이터  처리)<ul><li><code>window()</code> : 집계 시에, 시계열 컬럼 기준으로 각 날짜에 대한 전체 데이터를 가지는 윈도우 구성 =&gt; 날짜, 타임스탬프 처리에 유용</li></ul></li><li>정적 DataFrame 코드 vs 스트리밍 코드<ul><li><code>read</code> vs <code>readStream</code></li><li> 일반적인 정적 액션 vs <strong>스트리밍 액션</strong></li><li>스트리밍 액션은 어딘가에 데이터를 채워넣어야함. <strong>트리거</strong>가 실행된 후 데이터를 갱신<ul><li>(인메모리 테이블에 저장 시 - 스파크는 이전 집계값보다 더 큰 값이 발생할 때만 인메모리 테이블 갱신)</li></ul></li></ul></li><li><a href="http://bit.ly/2PvOwCS">예제 retail 데이터 셋</a><ul><li>by-day 하루 치 데이터 사용</li><li>예제는 인메모리 테이블에 저장 / 파일마다 트리거 실행</li><li>예제의 두가지 방식 (메모리/콘솔 출력, 파일별 트리거 수행)은 운영 환경에서는 권장 X</li></ul></li><li>데이터 처리 시점이 아닌 이벤트 시간에 따라 윈도우를 구성하는 방식에 주목<ul><li>기존 스파크 스트리밍의 단점 =&gt; <strong>구조적 스트리밍으로 보완</strong> 가능</li><li>스트림 처리과정의 스키마 추론방법 및 구조적 스트리밍은 CHAPTER 5 에서 자세히</li></ul></li></ul><h3 id="3-4-머신러닝과-고급-분석"><a href="#3-4-머신러닝과-고급-분석" class="headerlink" title="3.4 머신러닝과 고급 분석"></a>3.4 머신러닝과 고급 분석</h3><details><summary class="point-color-can-hover">[3.4] 예제 펼치기 </summary><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># MLlib 머신러닝 알고리즘 : 수치형 데이터 필요</span></span><br><span class="line"><span class="comment"># 예제의 (정적) 데이터 =&gt; 수치형으로 변환</span></span><br><span class="line"></span><br><span class="line">staticDataFrame.printSchema()</span><br><span class="line"></span><br><span class="line">root</span><br><span class="line"> |-- InvoiceNo: string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- StockCode: string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- Description: string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- Quantity: <span class="built_in">integer</span> (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- InvoiceDate: timestamp (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- UnitPrice: double (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- CustomerID: double (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- Country: string (nullable = <span class="literal">true</span>)</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.date_format</span><br><span class="line"><span class="keyword">val</span> preppedDataFrame = (staticDataFrame</span><br><span class="line">  .na.fill(<span class="number">0</span>) <span class="comment">// 0인 경우 null로 채움</span></span><br><span class="line">  .withColumn(<span class="string">&quot;day_of_week&quot;</span>, date_format($<span class="string">&quot;InvoiceDate&quot;</span>, <span class="string">&quot;EEEE&quot;</span>)) <span class="comment">// Sunday, Monday, ..</span></span><br><span class="line">  .coalesce(<span class="number">5</span>)) <span class="comment">// 파티션 개수 줄임 (default, shuffle = false)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// (1) 데이터 =&gt; 학습 데이터셋, 테스트 데이터셋으로 분리 (2011-07-01 기준)</span></span><br><span class="line"><span class="keyword">val</span> trainDataFrame = (preppedDataFrame</span><br><span class="line">  .where(<span class="string">&quot;InvoiceDate &lt; &#x27;2011-07-01&#x27;&quot;</span>))</span><br><span class="line"><span class="keyword">val</span> testDataFrame = (preppedDataFrame</span><br><span class="line">  .where(<span class="string">&quot;InvoiceDate &gt;= &#x27;2011-07-01&#x27;&quot;</span>))</span><br><span class="line"></span><br><span class="line">trainDataFrame.count()</span><br><span class="line"><span class="comment">// res110: Long = 245903</span></span><br><span class="line">testDataFrame.count()   </span><br><span class="line"><span class="comment">// res111: Long = 296006</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// (2-1) 요일(Sunday, Monday,..)을 수치형(0,1, ..)으로 반환</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.feature.<span class="type">StringIndexer</span></span><br><span class="line"><span class="keyword">val</span> indexer = (<span class="keyword">new</span> <span class="type">StringIndexer</span>()</span><br><span class="line">  .setInputCol(<span class="string">&quot;day_of_week&quot;</span>)</span><br><span class="line">  .setOutputCol(<span class="string">&quot;day_of_week_index&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// (2-2) 숫자로 표현된 범주형 데이터 인코딩 (해당 요일인지 Boolean 타입으로 확인 가능)</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.feature.<span class="type">OneHotEncoder</span></span><br><span class="line"><span class="keyword">val</span> encoder = (<span class="keyword">new</span> <span class="type">OneHotEncoder</span>()</span><br><span class="line">  .setInputCol(<span class="string">&quot;day_of_week_index&quot;</span>)</span><br><span class="line">  .setOutputCol(<span class="string">&quot;day_of_week_encoded&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// (2-3) 수치형 벡터 타입을 입력으로 사용 (가격, 수량, 특정 날짜의 요일)</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.feature.<span class="type">VectorAssembler</span></span><br><span class="line"><span class="keyword">val</span> vectorAssembler = (<span class="keyword">new</span> <span class="type">VectorAssembler</span>()</span><br><span class="line">  .setInputCols(<span class="type">Array</span>(<span class="string">&quot;UnitPrice&quot;</span>, <span class="string">&quot;Quantity&quot;</span>, <span class="string">&quot;day_of_week_encoded&quot;</span>))</span><br><span class="line">  .setOutputCol(<span class="string">&quot;features&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// (3) 파이프라인 설정</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.<span class="type">Pipeline</span></span><br><span class="line"><span class="keyword">val</span> transformationPipeline = (<span class="keyword">new</span> <span class="type">Pipeline</span>()</span><br><span class="line">  .setStages(<span class="type">Array</span>(indexer, encoder, vectorAssembler)))</span><br><span class="line"></span><br><span class="line"><span class="comment">// (4) 변환자(transformer) 를 데이터셋에 적합(fit) =&gt; &#x27;fitted pipeline&#x27;</span></span><br><span class="line"><span class="comment">// 일관되고 반복된 방식으로 데이터 변환 가능. 학습 데이터셋 생성 완료</span></span><br><span class="line"><span class="keyword">val</span> fittedPipeline = transformationPipeline.fit(trainDataFrame)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> transformedTraining = fittedPipeline.transform(trainDataFrame)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 캐싱 사용시 중간 변화된 데이터셋의 복사본을 메모리에 저장. 전체 파이프라인 재실행보다 훨씬 빠르다</span></span><br><span class="line"><span class="comment">// 근데 왜때문에 나는 더 느린 것..? ㅎ.. CHAPTER 4 에서 다시 확인하자</span></span><br><span class="line">transformedTraining.cache()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// (5) 모델 학습 (kmeans 모델 설정 과정은 생략..)</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.clustering.<span class="type">KMeans</span></span><br><span class="line"><span class="keyword">val</span> kmeans = (<span class="keyword">new</span> <span class="type">KMeans</span>()</span><br><span class="line">  .setK(<span class="number">20</span>)</span><br><span class="line">  .setSeed(<span class="number">1</span>L))</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> kmModel = kmeans.fit(transformedTraining)</span><br><span class="line"></span><br><span class="line"><span class="comment">// (6) 학습 데이터셋에 대한 비용 (군집 비용) 계산</span></span><br><span class="line">kmModel.computeCost(transformedTraining)</span><br><span class="line"><span class="comment">// res146: Double = 8.455373996537486E7</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 테스트 데이터셋과 비교</span></span><br><span class="line"><span class="comment">// 모델 개선 방법은 CHAPTER 6 에서</span></span><br><span class="line"><span class="keyword">val</span> transformedTest = fittedPipeline.transform(testDataFrame)</span><br><span class="line">kmModel.computeCost(transformedTest)</span><br><span class="line"><span class="comment">// res150: Double = 5.175070947222117E8</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></details><ul><li><p>내장된 머신러닝 알고리즘 라이브러리 MLlib 사용한 대규모 머신러닝 가능</p><ul><li>대용량 데이터 대상의 전처리(proprocessing), 멍잉(munging), 모델 학습(model training), 예측(prediction)</li><li>구조적 스트리밍에서 예측하고자 할때도 예측 모델 사용 가능</li></ul></li><li><p>스파크는 분류(classification), 회귀(regression), 군집화(clustering), 딥러닝(deep learning) 같은 머신러닝 관련 정교한 API 제공</p><ul><li>두유 노- <code>k-평균</code> ? : 군집화 표준 알고리즘. 센트로이드(centroid)라는 중심점을 사용해서.. <code>p.99 참고</code></li></ul></li><li><p>k-평균을 사용한 예제</p><ul><li>원본 데이터를 올바른 포맷으로 만드는 트렌스포메이션 정의. 실제 모델 학습 후 다음 예측 수행</li></ul></li><li><p>스파크 (MLlib DataFrame API) 에서 머신러닝 모델 학습 과정 2단계</p><ol><li>아직 학습되지 않은 모델 초기화</li><li>해당 모델을 학습</li></ol></li></ul><blockquote><p>알고리즘 명명 규칙 </p><ul><li>학습 전 알고리즘 명칭 : {Algorithm_name}</li><li>학습 후 알고리즘 명칭 : {Algorithm_name} + ‘Model’</li></ul></blockquote><h3 id="3-5-저수준-API"><a href="#3-5-저수준-API" class="headerlink" title="3.5 저수준 API"></a>3.5 저수준 API</h3><ul><li>스파크는 <strong>RDD</strong> 를 통해 자바와 파이썬 객체를 다루는데 필요한 다양한 기본 기능 (저수준 API) 제공<ul><li>DataFrame을 포함해서 스파크의 거의 모든 기능이 RDD 기반</li><li>저수준 명령으로 컴파일 =&gt; 편리하고 매우 효율적인 분산처리</li></ul></li><li>원시 데이터를 다루는 용도로도 쓸 수는 있지만, 대부분 구조적 API 사용이 더 낫다<ul><li>대신 파티션과 같은 <strong>물리적 실행 특성을 결정</strong> 할 수 있어, 세밀한 제어가 가능</li><li>비정형 데이터, 정제되지 않은 원시 데이터 처리에 사용</li></ul></li><li>언어에 따라 RDD 세부 구현에 차이가 있음<ul><li>Scala, Python 모두 사용 가능하지만 RDD가 동일하지 X</li><li>(&lt;-&gt; 언어에 관계없이 동일한 실행 특성의 DataFrame API)</li></ul></li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 메모리에 저장된 원시 데이터를 병렬 처리 (parallize) 하여 RDD[Int] 생성 후 DataFrame으로 변환</span></span><br><span class="line">spark.sparkContext.parallelize(<span class="type">Seq</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)).toDF()</span><br></pre></td></tr></table></figure><h3 id="3-6-SparkR"><a href="#3-6-SparkR" class="headerlink" title="3.6 SparkR"></a>3.6 SparkR</h3><ul><li>SparkR : 스파크를 R언어로 사용하기 위한 기능<ul><li>파이썬 API 와 유사하고, 파이썬에서 사용할 수 있는 기능은 대부분 사용 가능</li><li>R 라이브러리 사용하여 스파크 트랜스포메이션 과정을 R과 유사하게 만들 수 있음</li><li>CHAPTER 7에서 자세히 알아보자</li></ul></li></ul><h3 id="3-7-스파크의-에코시스템과-패키지"><a href="#3-7-스파크의-에코시스템과-패키지" class="headerlink" title="3.7 스파크의 에코시스템과 패키지"></a>3.7 스파크의 에코시스템과 패키지</h3><ul><li>스파크의 최고 자랑 = 커뮤니티가 만들어낸 패키지 에코시스템 &amp; 다양한 기능<ul><li>스파크 패키지 저장소 : <a href="https://spark-packages.org/">https://spark-packages.org/</a></li><li>그 외 깃헙, 기타 웹사이트 …</li></ul></li></ul><h3 id="3-8-정리"><a href="#3-8-정리" class="headerlink" title="3.8 정리"></a>3.8 정리</h3><ul><li>스파크를 비즈니스와 기술적 문제 해결에 적용할 수 있는 다양한 방법<ul><li>단순하고 강력한 프로그래밍 모델, 손쉬운 적용</li><li>다양한 패키지는 여러 비즈니스 문제를 성공적으로 해결할 수 있는 스파크의 능력에 대한 증거</li><li>더 성장하도록 더 많은 패키지가 만들어질거다~</li></ul></li></ul><h3 id="📒-단어장"><a href="#📒-단어장" class="headerlink" title="📒 단어장"></a>📒 단어장</h3><ul><li>정적 타입 코드/언어 (Statically typed) : 자료형이 고정된 언어. 컴파일 때 변수 타입이 결정 (ex. Java, Scala, C, C++ 등)<ul><li>&lt;-&gt; 동적 타입 언어 (Dynamically typed) : 런타임에 변수 타입이 결정 (ex. Python, JavaScript 등)</li></ul></li><li>멍잉 (munging) : =data wrangling. 원본 데이터를 다른 형태로 변환하거나 매핑하는 과정</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;br/&gt;

&lt;h4 id=&quot;Part-1-END&quot;&gt;&lt;a href=&quot;#Part-1-END&quot; class=&quot;headerlink&quot; title=&quot;[Part 1] END&quot;&gt;&lt;/a&gt;[Part 1] END&lt;/h4&gt;&lt;p&gt;파트 1 까지 끝내고 나니 이제 조금씩 스파크 맛</summary>
      
    
    
    
    <category term="spark" scheme="https://minsw.github.io/categories/spark/"/>
    
    
    <category term="study" scheme="https://minsw.github.io/tags/study/"/>
    
    <category term="book" scheme="https://minsw.github.io/tags/book/"/>
    
    <category term="spark" scheme="https://minsw.github.io/tags/spark/"/>
    
    <category term="apache" scheme="https://minsw.github.io/tags/apache/"/>
    
  </entry>
  
  <entry>
    <title>&amp;#039;Spark The Definitive Guide&amp;#039; 2장 - 스파크 찍어먹기</title>
    <link href="https://minsw.github.io/2021/01/24/Spark-The-Definitive-Guide-2%EC%9E%A5/"/>
    <id>https://minsw.github.io/2021/01/24/Spark-The-Definitive-Guide-2%EC%9E%A5/</id>
    <published>2021-01-24T09:42:51.000Z</published>
    <updated>2021-01-24T17:27:34.812Z</updated>
    
    <content type="html"><![CDATA[<img src="https://user-images.githubusercontent.com/26691216/105627228-e66e3200-5e78-11eb-9ea6-2e3662267b7a.jpg" width=200 /><center> 도커 이미지 사용시 Zeppelin에 예제 코드가 있다 <br/>나처럼 시력 검사&타자 연습 하느라 진빼지말고 Chapter2는 그냥 예제 코드를 쓰도록 하자... </center><center><h2>_ _ _</h2></center><br/><hr><h1 id="CHAPTER-2-스파크-간단히-살펴보기"><a href="#CHAPTER-2-스파크-간단히-살펴보기" class="headerlink" title="CHAPTER 2 스파크 간단히 살펴보기"></a>CHAPTER 2 스파크 간단히 살펴보기</h1><p>DataFrame, SQL 을 사용해 클러스터, 스파크 애플리케이션, 구조적 API 를 살펴보고<br>스파크의 핵심용어와 개념, 사용법을 익힌다.</p><h3 id="2-1-스파크의-기본-아키텍처"><a href="#2-1-스파크의-기본-아키텍처" class="headerlink" title="2.1 스파크의 기본 아키텍처"></a>2.1 스파크의 기본 아키텍처</h3><blockquote><p>스파크 애플리케이션을 이해하기 위한 핵심사항</p><ul><li>스파크는 사용가능한 자원을 파악하기 위해 <strong>클러스터 매니저</strong> 사용</li><li><strong>드라이버</strong> 프로세스는 주어직 작업을 완료하기위해, 드라이버 프로그램의 명령을 <strong>익스큐터</strong>에서 실행할 책임이 있음</li></ul></blockquote><ul><li>스파크는 클러스터의 데이터 처리 작업을 관리 / 조율<ul><li>컴퓨터 클러스터는 여러 컴퓨터의 자원을 모아 하나의 컴퓨터 처럼 사용</li><li>클러스터에서 작업을 조율할 수 있는 프레임워크 =&gt; <strong>스파크</strong></li></ul></li><li>스파크가 연산에 사용할 클러스터를 관리하는 <strong>클러스터 매니저</strong><ul><li>스파크 standalone 클러스터 매니저, 하둡 YARN, Mesos</li><li>역할<ul><li>사용자 : 스파크 애플리케이션 제출 (submit)</li><li>-&gt; 클러스터 매니저 : 애플리케이션 실행에 필요한 자원 할당 </li><li>-&gt; 할당받은 자원으로 작업 처리</li></ul></li></ul></li><li>스파크 애플리케이션 = <code>driver</code> 프로세스 + 다수의 <code>executor</code> 프로세스<ul><li><code>driver</code> 프로세스<ul><li>클러스터 노드 중 하나에서 실행. main() 함수 실행</li><li>심장과 같은 존재로, 애플리케이션 생명 주기 동안 관련 정보 모두 유지</li></ul></li><li><code>executor</code> 프로세스<ul><li>driver 가 할당한 작업 수행 &amp; 진행 상황을 driver에게 보고</li><li>대부분 스파크 코드를 실행하는 역할로, 스파크 언어 API를 통해 다양한 언어로 실행 가능</li></ul></li></ul></li></ul><h3 id="2-2-스파크의-다양한-언어-API"><a href="#2-2-스파크의-다양한-언어-API" class="headerlink" title="2.2 스파크의 다양한 언어 API"></a>2.2 스파크의 다양한 언어 API</h3><ul><li>스파크는 모든 언어에 맞는 몇몇 ‘핵심 개념’ 제공<ul><li>핵심개념 -&gt; (클러스터 머신에서 실행되는) 스파크 코드 로 변환</li><li>구조적 API만으로 작성된 코드는 언어에 무관하게 유사 성능</li></ul></li><li>언어별 요약 정보<ul><li>Scala : 스파크가 스칼라 기반. <strong>스파크의 기본 언어</strong></li><li>Java : <del>자바 지원안해주면 난리칠거니까</del> 지원은 함</li><li>Python : 스칼라가 지원하는 거의 모든 구조 지원</li><li>SQL : ANSI SQL:2003 표준 중 일부 지원</li><li>R : 스파크 코어의 sparkR, R 커뮤니티 기반의 sparklyr</li></ul></li><li>SparkSession 객체<ul><li>사용자가 스파크 코드를 실행하기위해 진입점으로 사용 가능</li><li>Python, R 사용 시에도 사용자 대신 익스큐터의 JVM에서 실행할 수 있는 코드로 변환</li></ul></li></ul><h3 id="2-3-스파크-API"><a href="#2-3-스파크-API" class="headerlink" title="2.3 스파크 API"></a>2.3 스파크 API</h3><ul><li>다양한 언어로 사용할 수 있는 이유?<ul><li>스파크가 기본적으로 제공하는 2가지 API 때문<ul><li>저수준의 비구조적(unstructured) API</li><li>고수준의 구조적(structured) API</li></ul></li></ul></li></ul><h3 id="2-4-스파크-시작하기"><a href="#2-4-스파크-시작하기" class="headerlink" title="2.4 스파크 시작하기"></a>2.4 스파크 시작하기</h3><ul><li>Q. 스파크 애플리케이션을 개발하려면<ul><li>A. 사용자 명령과 데이터를 스파크 애플리케이션에 전송하는 방법을 알아야</li></ul></li><li>SparkSession 생성 실습. 자 드가자~</li></ul><h3 id="2-5-SparkSession"><a href="#2-5-SparkSession" class="headerlink" title="2.5 SparkSession"></a>2.5 SparkSession</h3><ul><li><strong>SparkSession</strong> : 스파크 애플리케이션을 제어하는 드라이버 프로세스<ul><li>사용자가 정의한 처리명령 -&gt; 클러스터에 실행</li><li>스파크 애플리케이션에 1:1 대응</li></ul></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> scala console</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ./spark-2.4.7-bin-hadoop2.7/bin/spark-shell</span></span><br><span class="line"></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> spark</span></span><br><span class="line">res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5b58f639</span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val myRange = spark.range(1000).toDF(<span class="string">&quot;number&quot;</span>)</span></span><br><span class="line">myRange: org.apache.spark.sql.DataFrame = [number: bigint]</span><br></pre></td></tr></table></figure><h3 id="2-6-DataFrame"><a href="#2-6-DataFrame" class="headerlink" title="2.6 DataFrame"></a>2.6 DataFrame</h3><ul><li><strong>DataFrame</strong> : 가장 대표적인 <strong>구조적 API</strong><ul><li>테이블 데이터를 row, column 으로 단순하게 표현<ul><li>scheme : column 과 column type 을 정의한 목록</li></ul></li><li>DataFrame 은 수천 대의 컴퓨터에 분산 가능</li><li>vs 스프레드 시트<ul><li>비슷하다고 볼 수 있지만 스프레드 시트는 단일 컴퓨터 저장</li></ul></li><li>vs Python (Pandas)의 DataFrame, R의 DataFrame<ul><li>마찬가지로 대부분 단일 컴퓨터에 존재</li><li>=&gt; 스파크 DataFrame으로 쉽게 변환 가능</li></ul></li></ul></li><li>스파크의 핵심 추상화 개념 (분산 데이터 모음)<ul><li>Dataset, DataFrame, SQL 테이블, RDD</li></ul></li><li>DataFrame의 파티션<ul><li>익스큐터가 병렬로 작업을 수행할 수 있도록 데이터를 분할하는 청크 단위</li><li>실행 중 데이터가 클러스터에서 물리적으로 분산되는 방식을 나타냄<ul><li>파티션 1 익스큐터 1000 =&gt; 병렬성 1</li><li>파티션 1000 익스큐터 1 =&gt; 병렬성 1</li></ul></li><li>물리적 파티션에 데이터 변환용 함수 지정 시 스파크가 실제 처리 방법 결정 (파티션 수동 처리 필요 X)</li></ul></li></ul><h3 id="2-7-트랜스포메이션"><a href="#2-7-트랜스포메이션" class="headerlink" title="2.7 트랜스포메이션"></a>2.7 트랜스포메이션</h3><ul><li>스파크의 핵심 데이터 구조 =&gt; <strong>불변성 (immutable)</strong><ul><li>DataFrame을 변경하려면?</li><li>원하는 변경 방법을 스파크에게 알려줘야함 =&gt; <strong>트랜스포메이션</strong></li></ul></li><li>트랜스포메이션 : 스파크에서 비즈니스 로직을 표현하는 핵심 개념<ul><li>유형<ul><li>좁은 의존성 (narrow dependency)<ul><li>입력 파티션 : 출력 파티션 = 1 : 1</li></ul></li><li>넓은 의존성 (wide dependency)<ul><li>입력 파티션 : 출력 파티션 = 1 : N</li></ul></li></ul></li></ul></li><li>지연 연산 (lazy evaluation) : 연산 그래프를 처리하기 직전까지 기다리는 동작 방식<ul><li>스파크는 연산 명령 즉시 데이터를 수정 X. 원시 데이터에 적용할 트랜스포메이션의 <strong>실행 계획</strong>을 생성</li><li>마지막까지 대기하다 DataFrame 트랜스포메이션을 간결한 물리적 실행 계획으로 컴파일 =&gt; 전체 데이터 흐름 최적화</li><li>ex. DataFrame 의 predicate pushdown</li></ul></li></ul><h3 id="2-8-액션"><a href="#2-8-액션" class="headerlink" title="2.8 액션"></a>2.8 액션</h3><ul><li>트랜스포메이션은 논리적 실행 계획<ul><li>트랜스포메이션을 선언해도 액션을 호출하지 않으면 수행 X</li></ul></li><li>액션 (action) : 실제 연산을 수행<ul><li>유형<ul><li>콘솔에서 데이터를 보는 액션</li><li>각 언어로 된 네이티브 객체에 데이터를 모으는 액션</li><li>출력 데이터소스에 저장하는 액션</li></ul></li></ul></li><li>액션 지정 시 스파크 잡 시작<ul><li><strong>스파크 잡 (job)</strong><ul><li>필터 (좁은 트랜스포메이션) 수행</li><li>-&gt; 파티션 별로 레코드 수를 카운트 (넓은 트랜스포메이션)</li><li>-&gt; 각 언어에 적합한 네이티브 객체에 결과 모음</li></ul></li><li>스파크 UI로 잡 모니터링 가능</li><li><em>스파크 잡은 개별 액션에 의해 트리거되는 다수의 트랜스포메이션으로 이루어져 있다</em></li></ul></li></ul><h3 id="2-9-스파크-UI"><a href="#2-9-스파크-UI" class="headerlink" title="2.9 스파크 UI"></a>2.9 스파크 UI</h3><ul><li>드라이버 노드의 4040 포트</li><li>스파크 잡의 상태, 환경 설정, 클러스터 상태 등의 정보 확인 가능</li></ul><h3 id="2-10-종합-예제"><a href="#2-10-종합-예제" class="headerlink" title="2.10 종합 예제"></a>2.10 종합 예제</h3><ul><li>미국 교통통계국의 항공운항 데이터 중 일부로 실습<ul><li><a href="https://bit.ly/2yw2fCx">샘플 데이터</a> : 반정형(semi-structured), csv 포맷</li><li>(=&gt; 부록 A의 도커 이미지 사용 시 이미 포함)</li></ul></li><li>스파크는 다양한 데이터소스 지원<ul><li>SparkSession의 DataFrameReader 클래스 사용해서 읽음</li><li>예제는 <strong>스키마 추론 (Schema inference)</strong> 기능 추가<ul><li>스파크는 각 컬럼의 데이터 타입 추론을 위해 적은 양의 데이터를 읽음 </li></ul></li><li>DataFrame 은 불특적 다수의 로우와 컬럼<ul><li>지연 연산 형태의 트렌스포메이션이므로 row 수 알 수 X</li></ul></li></ul></li></ul><h4 id="예제-1"><a href="#예제-1" class="headerlink" title="예제 1"></a>예제 1</h4><details><summary class="point-color-can-hover">예제 1 펼치기</summary><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ head /data/flight-data/csv/2015-summary.csv</span><br><span class="line">DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count</span><br><span class="line">United States,Romania,15</span><br><span class="line">United States,Croatia,1</span><br><span class="line">..</span><br><span class="line"></span><br><span class="line"><span class="comment"># spark-shell (scala)</span></span><br><span class="line">scala&gt; val flightData2015 = spark.read.option(<span class="string">&quot;inferSchema&quot;</span>, <span class="string">&quot;true&quot;</span>).option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>).csv(<span class="string">&quot;/data/flight-data/csv/2015-summary.csv&quot;</span>)</span><br><span class="line">flightData2015: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]</span><br><span class="line"></span><br><span class="line">scala&gt; flightData2015.take(3)</span><br><span class="line">res0: Array[org.apache.spark.sql.Row] = Array([United States,Romania,15], [United States,Croatia,1], [United States,Ireland,344])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; flightData2015.sort(<span class="string">&quot;count&quot;</span>).explain()</span><br><span class="line">== Physical Plan ==</span><br><span class="line">*(2) Sort [count<span class="comment">#12 ASC NULLS FIRST], true, 0</span></span><br><span class="line">+- Exchange rangepartitioning(count<span class="comment">#12 ASC NULLS FIRST, 200)</span></span><br><span class="line">   +- *(1) FileScan csv [DEST_COUNTRY_NAME<span class="comment">#10,ORIGIN_COUNTRY_NAME#11,count#12] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 셔플 파티션 default 200개 =&gt; 5개</span></span><br><span class="line">scala&gt; spark.conf.set(<span class="string">&quot;spark.sql.shuffle.partitions&quot;</span>, <span class="string">&quot;5&quot;</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; flightData2015.sort(<span class="string">&quot;count&quot;</span>).explain()</span><br><span class="line">== Physical Plan ==</span><br><span class="line">*(2) Sort [count<span class="comment">#12 ASC NULLS FIRST], true, 0</span></span><br><span class="line">+- Exchange rangepartitioning(count<span class="comment">#12 ASC NULLS FIRST, 5)</span></span><br><span class="line">   +- *(1) FileScan csv [DEST_COUNTRY_NAME<span class="comment">#10,ORIGIN_COUNTRY_NAME#11,count#12] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int&gt;</span></span><br><span class="line"></span><br><span class="line">scala&gt; flightData2015.sort(<span class="string">&quot;count&quot;</span>).take(2)</span><br><span class="line">res3: Array[org.apache.spark.sql.Row] = Array([United States,Singapore,1], [Moldova,United States,1])</span><br><span class="line"></span><br></pre></td></tr></table></figure></details><blockquote><ul><li><code>take(n)</code> : Action</li><li><code>sort()</code> :  Transformation (넓은) <ul><li>DataFrame 을 변경하지 않고 새로운 DataFrame을 생성해 반환</li></ul></li><li><code>explain()</code> <ul><li>DataFrame의 계보(lineage) 나 스파크 쿼리 실행 계획 출력</li></ul></li></ul></blockquote><ul><li>실행 계획? : 디버깅과 스파크의 실행과정을 이해하는데 도움을 주는 도구<ul><li>위에서 아래방향으로 읽는다</li><li>최종 결과는 가장 위, 데이터소스는 가장 아래</li></ul></li><li>DataFrame의 계보<ul><li>트랜스포메이션의 논리적 실행 계획 -&gt; DataFrame의 계보 정의</li><li>-&gt; 계보를 통해 스파크가 입력데이터에 수행한 연산을 전체 파티션에서 어떻게 재연산하는지 알 수 있음</li><li><em>함수형 프로그래밍의 핵심</em> (Pure Function, 같은 입력 -&gt; 같은 출력)</li></ul></li><li>사용자는 물리적 데이터를 직접 다루지 않고, 물리적 실행 특성을 제어<ul><li>예시 =&gt; 파티션 수 변경 <code>spark.conf.set(&quot;spark.sql.shuffle.partitions&quot;, &quot;5&quot;)</code></li><li>스파크 UI (4040 포트) 에서 스파크 잡 물리적, 논리적 실행 특성 확인 가능 <img width="500" alt="sparkui" src="https://user-images.githubusercontent.com/26691216/105624926-cfbfdf00-5e68-11eb-9407-e58a5f4688a9.png"></li></ul></li></ul><h4 id="예제-2-SQL"><a href="#예제-2-SQL" class="headerlink" title="예제 2 (SQL)"></a>예제 2 (SQL)</h4><details><summary class="point-color-can-hover">예제 2-1 펼치기</summary><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1) SQL 사용</span></span><br><span class="line">scala&gt; flightData2015.createOrReplaceTempView(<span class="string">&quot;flight_data_2015&quot;</span>)</span><br><span class="line">scala&gt; val sqlWay = spark.sql(<span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">     | SELECT DEST_COUNTRY_NAME, count(1)</span></span><br><span class="line"><span class="string">     | FROM flight_data_2015</span></span><br><span class="line"><span class="string">     | GROUP BY DEST_COUNTRY_NAME</span></span><br><span class="line"><span class="string">     | &quot;</span><span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; sqlWay.explain</span><br><span class="line">== Physical Plan ==</span><br><span class="line">*(2) HashAggregate(keys=[DEST_COUNTRY_NAME<span class="comment">#10], functions=[count(1)])</span></span><br><span class="line">+- Exchange hashpartitioning(DEST_COUNTRY_NAME<span class="comment">#10, 5)</span></span><br><span class="line">   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME<span class="comment">#10], functions=[partial_count(1)])</span></span><br><span class="line">      +- *(1) FileScan csv [DEST_COUNTRY_NAME<span class="comment">#10] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2) DataFrame 사용</span></span><br><span class="line">scala&gt; val dataFrameWay = flightData2015.groupBy(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>).count()</span><br><span class="line">dataFrameWay: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, count: bigint]</span><br><span class="line"></span><br><span class="line">scala&gt; dataFrameWay.explain</span><br><span class="line">== Physical Plan ==</span><br><span class="line">*(2) HashAggregate(keys=[DEST_COUNTRY_NAME<span class="comment">#10], functions=[count(1)])</span></span><br><span class="line">+- Exchange hashpartitioning(DEST_COUNTRY_NAME<span class="comment">#10, 5)</span></span><br><span class="line">   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME<span class="comment">#10], functions=[partial_count(1)])</span></span><br><span class="line">      +- *(1) FileScan csv [DEST_COUNTRY_NAME<span class="comment">#10] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></details><ul><li>스파크는 언어에 무관하게 같은 방식으로 트랜스포메이션 실행<ul><li>SQL, DataFrame(R, Python, Scalar, Java) 에서 비즈니스 로직 표현</li><li>스파크에서 코드 실행 전에 로직을 기본 실행계획(<code>explain</code>) 으로 컴파일</li></ul></li><li>스파크 SQL 사용시 모든 DataFrame =&gt; 테이블, 뷰 (임시 테이블) 로 등록<ul><li>위에서 설명했듯 <strong>같은 실행 계획</strong>으로 컴파일하므로 성능차이 X</li></ul></li></ul><details><summary class="point-color-can-hover">예제 2-2 펼치기</summary><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># &#x27;최대 비행 횟수&#x27; 구하기</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># SQL 쿼리</span></span><br><span class="line">scala&gt; spark.sql(<span class="string">&quot;SELECT max(count) from flight_data_2015&quot;</span>).take(1)</span><br><span class="line">res9: Array[org.apache.spark.sql.Row] = Array([370002])</span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame 구문 _ max 함수 (트랜스포메이션) 사용</span></span><br><span class="line">scala&gt; import org.apache.spark.sql.functions.max</span><br><span class="line"></span><br><span class="line">scala&gt; flightData2015.select(max(<span class="string">&quot;count&quot;</span>)).take(1)</span><br><span class="line">res10: Array[org.apache.spark.sql.Row] = Array([370002])</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># &#x27;상위 5개의 도착 국가&#x27; 구하기</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># SQL 쿼리</span></span><br><span class="line">scala&gt; val maxSql = spark.sql(<span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">     | SELECT DEST_COUNTRY_NAME, sum(count) as destination_total</span></span><br><span class="line"><span class="string">     | FROM flight_data_2015</span></span><br><span class="line"><span class="string">     | GROUP BY DEST_COUNTRY_NAME</span></span><br><span class="line"><span class="string">     | ORDER BY sum(count) DESC</span></span><br><span class="line"><span class="string">     | LIMIT 5</span></span><br><span class="line"><span class="string">     | &quot;</span><span class="string">&quot;&quot;</span>)</span><br><span class="line">maxSql: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, destination_total: bigint]</span><br><span class="line"></span><br><span class="line">scala&gt; maxSql.show()</span><br><span class="line">+-----------------+-----------------+</span><br><span class="line">|DEST_COUNTRY_NAME|destination_total|</span><br><span class="line">+-----------------+-----------------+</span><br><span class="line">|    United States|           411352|</span><br><span class="line">|           Canada|             8399|</span><br><span class="line">|           Mexico|             7140|</span><br><span class="line">|   United Kingdom|             2025|</span><br><span class="line">|            Japan|             1548|</span><br><span class="line">+-----------------+-----------------+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># DataFrame 구문</span></span><br><span class="line">scala&gt; import org.apache.spark.sql.functions.desc</span><br><span class="line"></span><br><span class="line">scala&gt; flightData2015.groupBy(<span class="string">&quot;DEST_COUNTRY_NAME&quot;</span>).sum(<span class="string">&quot;count&quot;</span>).withColumnRenamed(<span class="string">&quot;sum(count)&quot;</span>, <span class="string">&quot;destination_total&quot;</span>).sort(desc(<span class="string">&quot;destination_total&quot;</span>)).<span class="built_in">limit</span>(5).show()</span><br><span class="line">+-----------------+-----------------+</span><br><span class="line">|DEST_COUNTRY_NAME|destination_total|</span><br><span class="line">+-----------------+-----------------+</span><br><span class="line">|    United States|           411352|</span><br><span class="line">|           Canada|             8399|</span><br><span class="line">|           Mexico|             7140|</span><br><span class="line">|   United Kingdom|             2025|</span><br><span class="line">|            Japan|             1548|</span><br><span class="line">+-----------------+-----------------+</span><br><span class="line"></span><br><span class="line"><span class="comment"># 코드 수행 단계 : CSV 파일 =&gt; (1) read -&gt; (2) groupBy -&gt; (3) sum -&gt; (4) withColumnRenamed -&gt; (5) sort -&gt; (6) limit -&gt; (7) collect =&gt; Array(..)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># scala&gt; ~.explain</span></span><br><span class="line">== Physical Plan ==</span><br><span class="line">TakeOrderedAndProject(<span class="built_in">limit</span>=5, orderBy=[destination_total<span class="comment">#108L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#10,destination_total#108L])</span></span><br><span class="line">+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME<span class="comment">#10], functions=[sum(cast(count#12 as bigint))])</span></span><br><span class="line">   +- Exchange hashpartitioning(DEST_COUNTRY_NAME<span class="comment">#10, 5)</span></span><br><span class="line">      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME<span class="comment">#10], functions=[partial_sum(cast(count#12 as bigint))])</span></span><br><span class="line">         +- *(1) FileScan csv [DEST_COUNTRY_NAME<span class="comment">#10,count#12] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,count:int&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></details><ul><li>실행계획은 트랜스포메이션의 <strong>지향성 비순환 그래프 (Directed Acyclic Graph, DAG)</strong><ul><li>액션이 호출되면 결과를 만들어낸다</li><li>DAG의 각 단계는 불변성을 가진 신규 DataFrame을 생성</li></ul></li><li>예제의 전체 코드 수행 단계 (7단계) 는 p.86 [그림 2-10] 참조<ul><li>실제 실행 계획 (<code>explain</code> 이 출력하는) 은 물리적인 실행 시점에서 수행하는 최적화로 인해 다를 수 있음</li><li>직접 explain 해보면 책의 explain 과도 다르게 출력됨 </li></ul></li></ul><h3 id="2-11-정리"><a href="#2-11-정리" class="headerlink" title="2.11 정리"></a>2.11 정리</h3><ul><li>트랜스포메이션, 액션, DataFrame 실행 계획 최적화 방법<ul><li>트랜스포메이션의 지향성 비순환 그래프(DAG) 를 지연 실행하여 최적화</li></ul></li><li>예제를 통한 데이터가 파티션으로 구성되는 방법, 복잡한 트랜스포메이션 작업 실행 단계 확인</li></ul><br/><h3 id="📒-단어장"><a href="#📒-단어장" class="headerlink" title="📒 단어장"></a>📒 단어장</h3><ul><li>셔플 (Shuffle) : 스파크카 클러스터에서 파티션을 교환<ul><li>스파크는 셔플의 결과를 디스크에 저장</li></ul></li><li>가환성 (Commutative) : 두 대상의 연산 결과가 순서와 관계없이 동일 (-&gt; 교환 법칙)</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;img src=&quot;https://user-images.githubusercontent.com/26691216/105627228-e66e3200-5e78-11eb-9ea6-2e3662267b7a.jpg&quot; width=200 /&gt;
&lt;center&gt; 도커 이미</summary>
      
    
    
    
    <category term="spark" scheme="https://minsw.github.io/categories/spark/"/>
    
    
    <category term="study" scheme="https://minsw.github.io/tags/study/"/>
    
    <category term="book" scheme="https://minsw.github.io/tags/book/"/>
    
    <category term="spark" scheme="https://minsw.github.io/tags/spark/"/>
    
    <category term="apache" scheme="https://minsw.github.io/tags/apache/"/>
    
  </entry>
  
  <entry>
    <title>&amp;#039;Spark The Definitive Guide&amp;#039; 1장 - Apache Spark (스파크) 는 뭘까</title>
    <link href="https://minsw.github.io/2021/01/20/Spark-The-Definitive-Guide-1%EC%9E%A5/"/>
    <id>https://minsw.github.io/2021/01/20/Spark-The-Definitive-Guide-1%EC%9E%A5/</id>
    <published>2021-01-20T00:20:29.000Z</published>
    <updated>2021-01-24T17:27:34.810Z</updated>
    
    <content type="html"><![CDATA[<br/><h4 id="‘Spark-The-Definitive-Guide’"><a href="#‘Spark-The-Definitive-Guide’" class="headerlink" title="‘Spark The Definitive Guide’"></a>‘Spark The Definitive Guide’</h4><p><i>스파크 완벽 가이드 (&amp; 하둡 완벽 가이드), O’REILLY</i></p><p>전형적으로 묶어놔야 공부하는 타입이라..<br>어떻게든 마음의 부채를 쌓기 위해 일단 책부터 사고 일도 최대한 크게 벌리고 시작하기로 했다.</p><p>책 읽고 간단하게만 정리하는거라 내용이 별거 없긴 해도<br>또 대충 쓰다 어디 쑤셔 박아놓고서 못 찾지 말고 <i style="color:lightgray">(“엄마~ 내 글 어딨어?”)</i> 습관도 들일 겸 블로그에 남겨두는게 좋겠다.</p><p><strong>완벽</strong> 가이드라니까 일단 파트 3까지는 아묻따 따라가보자. 자 드가자~!</p><blockquote><p><strong>Apache Spark™</strong> is a unified analytics engine for large-scale data processing.</p><p>Apache Saprk 공식 페이지 <a href="https://spark.apache.org/">https://spark.apache.org/</a><br>Latest Release Spark 3.0.1</p></blockquote><br/><img src="https://user-images.githubusercontent.com/26691216/105111053-699f2900-5b03-11eb-87c1-05d1c704f2d3.jpg" width=240 /><center><h2>_ _ _</h2></center><br/><hr><h1 id="CHAPTER-1-아파치-스파크란"><a href="#CHAPTER-1-아파치-스파크란" class="headerlink" title="CHAPTER 1 아파치 스파크란"></a>CHAPTER 1 아파치 스파크란</h1><h3 id="아파치-스파크-Apache-Spark-란"><a href="#아파치-스파크-Apache-Spark-란" class="headerlink" title="아파치 스파크 (Apache Spark) 란"></a>아파치 스파크 (Apache Spark) 란</h3><p>통합 컴퓨팅 엔진이며 클러스터 환경에서 데이터를 병렬로 처리하는 라이브러리 집합.</p><ul><li>Spark 언어 4대 천왕 - Python, Java, Scala, R</li><li>SQL, Library, ML 등 라이브러리 제공</li></ul><h3 id="1-1-아파치-스파크의-철학"><a href="#1-1-아파치-스파크의-철학" class="headerlink" title="1.1 아파치 스파크의 철학"></a>1.1 아파치 스파크의 철학</h3><blockquote><p><strong>빅데이터를 위한 통합 컴퓨팅 엔진과 라이브러리 집합.</strong></p></blockquote><ul><li>통합 (unified)<ul><li>다양한 처리 유형을 지원하기 위한 자체 API</li></ul></li><li>컴퓨팅 엔진<ul><li>스파크는 데이터 저장 위치에 상관없이 처리에 집중</li><li>vs 하둡<ul><li>하둡은 저비용 저장장치를 사용하는 하둡 파일 시스템과 컴퓨팅 시스템(MR) 이 밀접하게 연관</li><li>스파크는 하둡 저장소 뿐만 아니라 하둡 아키텍처를 사용할 수 없는 환경에서도 호환 가능</li></ul></li></ul></li><li>라이브러리<ul><li>궁극적으론 데이터 분석 작업에 필요한 통합 API를 제공하는 통합 엔진 기반의 자체 라이브러리</li><li>수많은 외부 라이브러리도.. (<a href="https://spark-packages.org/">https://spark-packages.org/</a>)</li></ul></li></ul><h3 id="1-2-스파크의-등장-배경"><a href="#1-2-스파크의-등장-배경" class="headerlink" title="1.2 스파크의 등장 배경"></a>1.2 스파크의 등장 배경</h3><ul><li>더 많은 연산과 대규모 데이터 처리를 프로세서의 성능 향상에 맡겼으나, H/W 성능 향상은 2005년까지</li><li>기술의 발전으로 데이터 수집 비용은 저렴해졌지만 데이터는 클러스터에서 처리해야할 만큼 거대해짐</li><li>새로운 프로그래밍 모델이 필요 =&gt; <strong>아파치 스파크</strong></li></ul><h3 id="1-3-스파크의-역사"><a href="#1-3-스파크의-역사" class="headerlink" title="1.3 스파크의 역사"></a>1.3 스파크의 역사</h3><ul><li>UC버클리 대학에서 2009년 프로젝트로 시작<ul><li>당시 하둡 맵리듀스가 클러스터 환경용 병렬 프로그래밍 엔진의 대표주자</li></ul></li><li>‘표준 라이브러리’ 형태의 구현 방식<ul><li>조합형 API의 핵심 아이디어 진화</li><li>~ 1.0 : 함수형 연산</li><li>1.0 : 구조화된 데이터 기반의 스파크 SQL</li><li>이 후에는 더 강력한 구조체 기반의 신규 API 들 (ex. DataFrame, 머신러닝 파이프라인, 구조적 스트리밍)</li></ul></li></ul><h3 id="1-4-스파크의-현재와-미래"><a href="#1-4-스파크의-현재와-미래" class="headerlink" title="1.4 스파크의 현재와 미래"></a>1.4 스파크의 현재와 미래</h3><ul><li>거대 규모 데이터셋이나 과학적 데이터 분석에 사용 중</li><li>인기 많다는 이야기</li></ul><h3 id="1-5-스파크-실행하기-gt-부록-A"><a href="#1-5-스파크-실행하기-gt-부록-A" class="headerlink" title="1.5 스파크 실행하기 -&gt; 부록 A"></a>1.5 스파크 실행하기 -&gt; 부록 A</h3><blockquote><p><em>… 특히 도커 환경에서 예제를 실행해보고 싶다면 부록 A를 참고하시기 바랍니다.</em></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ docker pull docker.io/rheor108/spark_the_definitive_guide_practice</span><br><span class="line">$ docker run -p 8080:8080 -p 4040:4040 --name spark_ex rheor108/spark_the_definitive_guide_practice</span><br><span class="line"><span class="comment"># Zeppelin UI : http://localhost:8080 </span></span><br></pre></td></tr></table></figure><blockquote><p>[부록 A] Spark The Definitive Guide 예제 실행 환경 Docker image</p><ul><li><p><a href="https://hub.docker.com/r/rheor108/spark_the_definitive_guide_practice">https://hub.docker.com/r/rheor108/spark_the_definitive_guide_practice</a></p></li><li><p>Spark version: 2.3.2 Python version: 2.7 Zeppelin version: 0.8.0 (21.01 기준)</p></li></ul><p>Spark The Definitive Guide 저장소</p><ul><li>원서 : <a href="https://github.com/databricks/Spark-The-Definitive-Guide">https://github.com/databricks/Spark-The-Definitive-Guide</a></li><li>번역 예제 : <a href="https://github.com/FVBros/Spark-The-Definitive-Guide">https://github.com/FVBros/Spark-The-Definitive-Guide</a></li></ul></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Docker image 실행하면 기본적으로 /spark-2.3.2-bin-hadoop2.7/bin 에 대화형 콘솔 존재</span></span><br><span class="line"><span class="comment"># + 2.4.7 다운로드 받기</span></span><br><span class="line">wget https://downloads.apache.org/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz</span><br><span class="line">tar -xf spark-2.4.7-bin-hadoop2.7.tgz</span><br></pre></td></tr></table></figure><h3 id="1-6-정리"><a href="#1-6-정리" class="headerlink" title="1.6 정리"></a>1.6 정리</h3><ul><li>스파크의 개요 / 탄생 배경 / 환경 구성 방법</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;br/&gt;

&lt;h4 id=&quot;‘Spark-The-Definitive-Guide’&quot;&gt;&lt;a href=&quot;#‘Spark-The-Definitive-Guide’&quot; class=&quot;headerlink&quot; title=&quot;‘Spark The Definitive Guide’&quot;</summary>
      
    
    
    
    <category term="spark" scheme="https://minsw.github.io/categories/spark/"/>
    
    
    <category term="study" scheme="https://minsw.github.io/tags/study/"/>
    
    <category term="book" scheme="https://minsw.github.io/tags/book/"/>
    
    <category term="summarization" scheme="https://minsw.github.io/tags/summarization/"/>
    
    <category term="mwolkka" scheme="https://minsw.github.io/tags/mwolkka/"/>
    
    <category term="spark" scheme="https://minsw.github.io/tags/spark/"/>
    
    <category term="apache" scheme="https://minsw.github.io/tags/apache/"/>
    
  </entry>
  
  <entry>
    <title>Hexo 블로그 심폐소생술하기</title>
    <link href="https://minsw.github.io/2021/01/18/Hexo-%EB%B8%94%EB%A1%9C%EA%B7%B8-%EC%8B%AC%ED%8F%90%EC%86%8C%EC%83%9D%EC%88%A0%ED%95%98%EA%B8%B0/"/>
    <id>https://minsw.github.io/2021/01/18/Hexo-%EB%B8%94%EB%A1%9C%EA%B7%B8-%EC%8B%AC%ED%8F%90%EC%86%8C%EC%83%9D%EC%88%A0%ED%95%98%EA%B8%B0/</id>
    <published>2021-01-17T16:33:39.000Z</published>
    <updated>2021-01-24T17:27:34.806Z</updated>
    
    <content type="html"><![CDATA[<p>2019년에서 멈춰버린 블로그 한번 살려 보려다가<br>블로그를 통째로 날려버릴 뻔하고 (..) 그냥 날려버리려다 복구했다.</p><p>혹시라도 나처럼 N년 전에 만든 hexo 블로그를 되살려보겠다는 사람을 위해서<br> + 사실은 N년 뒤에 똑같이 삽질할 나를 위해서 정리해본다.</p><img width="287" alt="cpr" src="https://user-images.githubusercontent.com/26691216/104849937-d5c93380-592f-11eb-965a-261b6fe48519.png"><br/><h1 id="방치된-Hexo-Blog-살리기"><a href="#방치된-Hexo-Blog-살리기" class="headerlink" title="방치된 Hexo Blog 살리기"></a>방치된 Hexo Blog 살리기</h1><h2 id="Hexo-3-8-0-gt-5-3-0-업그레이드"><a href="#Hexo-3-8-0-gt-5-3-0-업그레이드" class="headerlink" title="Hexo 3.8.0 -&gt; 5.3.0 업그레이드"></a>Hexo 3.8.0 -&gt; 5.3.0 업그레이드</h2><p>2019.03 기준 <code>3.8.0</code> =&gt; 2021.01 기준 최신 버전 <code>5.3.0</code></p><h4 id="0-npm-upgrade"><a href="#0-npm-upgrade" class="headerlink" title="0. npm upgrade"></a>0. npm upgrade</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install -g npm</span><br></pre></td></tr></table></figure><h4 id="1-Hexo-재설치"><a href="#1-Hexo-재설치" class="headerlink" title="1. Hexo 재설치"></a>1. Hexo 재설치</h4><blockquote><p><a href="https://github.com/hexojs/hexo">hexojs/hexo</a> 참고</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexp-cli -g</span><br></pre></td></tr></table></figure><h4 id="2-이미-블로그가-있으니까-hexo-init-은-스킵"><a href="#2-이미-블로그가-있으니까-hexo-init-은-스킵" class="headerlink" title="2. 이미 블로그가 있으니까 hexo init 은 스킵?"></a>2. 이미 블로그가 있으니까 <code>hexo init</code> 은 스킵?</h4><p>내 레포에 있는 package.json 자체가 옛날 버전이라 그대로 npm install 하려면 실패하기도<br>⇒ <strong>최신 버전 package.json</strong> 으로 마이그레이션 필요</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ hexo init blog</span><br><span class="line">$ cp blog/package.json <span class="variable">$&#123;MY_GIT_BLOG_PATH&#125;</span> <span class="comment"># 최신 package.json 으로 덮어쓰기</span></span><br><span class="line">$ <span class="built_in">cd</span> <span class="variable">$&#123;MY_GIT_BLOG_PATH&#125;</span> &amp;&amp; npm install</span><br></pre></td></tr></table></figure><ul><li>기존 package.json, package-lock.json, node_modules/ 등 제거</li><li>최신 ‘package.json’ copy 후 <code>npm install</code> (필요한 의존성은 <code>npm install &#123;&#125; --save</code> 로 별도 추가)</li><li><code>_config.yml</code> 도 비교 후 변경 사항 있을 시 적용 (Option)</li></ul><br/><h2 id="너무-쉬운데-뭐가-문제지"><a href="#너무-쉬운데-뭐가-문제지" class="headerlink" title="너무 쉬운데 뭐가 문제지?"></a>너무 쉬운데 뭐가 문제지?</h2><blockquote><h4 id="최신-버전-Hexo-2년전에-적용한-테마-💩"><a href="#최신-버전-Hexo-2년전에-적용한-테마-💩" class="headerlink" title="최신 버전 Hexo + 2년전에 적용한 테마 = 💩"></a>최신 버전 Hexo + 2년전에 적용한 테마 = 💩</h4><p>2년전에 예쁘다고 적용해놓은 테마가 지금도 유지보수되고 있을 가능성은 0에 가깝다.<br>포기하고 새 테마를 찾거나, <strong>어거지로 적용하거나 ☠️</strong><br>둘 중 하나를 선택하도록 하자.</p></blockquote><h4 id="문제-1-서버를-올렸더니-흰-화면만-나온다"><a href="#문제-1-서버를-올렸더니-흰-화면만-나온다" class="headerlink" title="문제 1. 서버를 올렸더니 흰 화면만 나온다"></a>문제 1. 서버를 올렸더니 흰 화면만 나온다</h4><p>⇒ themes/ 에 본인이 지정한 테마가 제대로 있는지 재확인 (.gitignore 로 빼놓기도 함)</p><h4 id="문제-2-뭐가-나오긴-하는데-텍스트가-나온다"><a href="#문제-2-뭐가-나오긴-하는데-텍스트가-나온다" class="headerlink" title="문제 2. 뭐가 나오긴 하는데.. 텍스트가 나온다"></a>문제 2. 뭐가 나오긴 하는데.. 텍스트가 나온다</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># index.html</span></span><br><span class="line">extends partial/layout</span><br><span class="line"></span><br><span class="line">block container</span><br><span class="line">    include mixins/post</span><br><span class="line">    +posts()</span><br><span class="line">...</span><br></pre></td></tr></table></figure><img width="636" alt="error" src="https://user-images.githubusercontent.com/26691216/104851421-407e6d00-5938-11eb-97f5-4d041b5abdf6.png"><p>jade 또는 pug 템플릿을 사용 중인데 해당하는 renderer 가 없어서 발생.</p><p>⇒ 기존에는 <del><a href="https://www.npmjs.com/package/hexo-renderer-jade">hexo-renderer-jade</a></del> 를 썼으나 deprecated 되었으므로,<br>themes/layout 하위의 모든 <code>*.jade</code> 파일을 <code>*.pug</code> 로 변경 + <a href="https://www.npmjs.com/package/hexo-renderer-pug">hexo-renderer-pug</a> 사용</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-renderer-pug --save</span><br></pre></td></tr></table></figure><h4 id="문제-3-배포가-안된다-amp-이상하게-올라간다"><a href="#문제-3-배포가-안된다-amp-이상하게-올라간다" class="headerlink" title="문제 3. 배포가 안된다 &amp; 이상하게 올라간다"></a>문제 3. 배포가 안된다 &amp; 이상하게 올라간다</h4><p><a href="https://www.npmjs.com/package/hexo-deployer-git">hexo-deployer-git</a> 이 잘 설치되어있는지 확인.</p><p>잘 설치되어있고 <code>hexo deploy</code>가 되긴하는데<br>정적 파일말고 소스코드가 올라간다던지 뭔가 엉켰다면 <strong>.deploy_git</strong> 삭제 후 재시도 추천</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ rm -rf .deploy_git <span class="comment"># reset</span></span><br></pre></td></tr></table></figure><br/><h2 id="해치웠나"><a href="#해치웠나" class="headerlink" title="해치웠나.."></a>해치웠나..</h2><p>이 포스트가 잘 올라간다면 잘 살아난거라 볼 수 있겠다.<br>사실 어찌저찌 돌아가게만 만들어 놓은지라 언제 다시 뻗을지는 모르겠지만<br>자주 쓰게 된다면 쓰면서 조금씩 고쳐가면 되지 않을까 한다.</p><p>올해는 공부한 것도 좀 잘 정리해서 기록 해보도록 노력하자.</p><blockquote><p>하나 둘 셋 화이팅! ٩( ᐛ )و</p></blockquote><br/><h2 id="참고용-✍🏻-돌아서면-까먹는-Hexo-사용법"><a href="#참고용-✍🏻-돌아서면-까먹는-Hexo-사용법" class="headerlink" title="[참고용] ✍🏻 돌아서면 까먹는 Hexo 사용법"></a>[참고용] ✍🏻 돌아서면 까먹는 Hexo 사용법</h2><blockquote><p><a href="https://hexo.io/ko/docs/commands.html">Hexo docs - commands</a></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;NEW_POST_NAME&quot;</span> <span class="comment"># new post 작성</span></span><br><span class="line"></span><br><span class="line">$ hexo generate <span class="comment"># 정적 파일 (public) 생성</span></span><br><span class="line">$ hexo clean <span class="comment"># 캐시 및 정적 파일 삭제</span></span><br><span class="line"></span><br><span class="line">$ hexo server <span class="comment"># 로컬 서버 (localhost:4000) 구동 _ 테스트</span></span><br><span class="line">$ hexo deploy <span class="comment"># 웹 사이트 deploy</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># hexo d -g 도 가능하지만 내 블로그는 가끔 이상하게 렌더링되기도하니 아래 커맨드로 배포할 것</span></span><br><span class="line">$ hexo clean &amp;&amp; hexo generate &amp;&amp; hexo deploy</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;2019년에서 멈춰버린 블로그 한번 살려 보려다가&lt;br&gt;블로그를 통째로 날려버릴 뻔하고 (..) 그냥 날려버리려다 복구했다.&lt;/p&gt;
&lt;p&gt;혹시라도 나처럼 N년 전에 만든 hexo 블로그를 되살려보겠다는 사람을 위해서&lt;br&gt; + 사실은 N년 뒤에 </summary>
      
    
    
    
    <category term="blog" scheme="https://minsw.github.io/categories/blog/"/>
    
    
    <category term="blog" scheme="https://minsw.github.io/tags/blog/"/>
    
    <category term="hexo" scheme="https://minsw.github.io/tags/hexo/"/>
    
    <category term="CPR" scheme="https://minsw.github.io/tags/CPR/"/>
    
    <category term="guide" scheme="https://minsw.github.io/tags/guide/"/>
    
  </entry>
  
  <entry>
    <title>&amp;#039;Kotlin in Action&amp;#039; 1장 - Kotlin (코틀린) 은 뭘까</title>
    <link href="https://minsw.github.io/2019/07/02/Kotlin-in-Action-1%EC%9E%A5/"/>
    <id>https://minsw.github.io/2019/07/02/Kotlin-in-Action-1%EC%9E%A5/</id>
    <published>2019-07-02T14:07:55.000Z</published>
    <updated>2021-01-24T17:27:34.808Z</updated>
    
    <content type="html"><![CDATA[<br/><h4 id="‘Kotlin-in-Action’"><a href="#‘Kotlin-in-Action’" class="headerlink" title="‘Kotlin in Action’"></a>‘Kotlin in Action’</h4><p>공부 겸 프로젝트 준비 겸 Kotlin 책을 하나 샀다.<br><strong><em>‘Kotlin in Action’</em></strong> 은 Kotlin 언어를 개발한 JetBrains 개발자들이 직접 쓴 책으로, <u>Kotlin 다운 Kotlin 개발</u>을 하기 위해 첫 단추로 택했다.<br>내가 책 읽는 속도는 빠른데 머리에서 휘발되는 속도도 빠른 편이라(…) 시간 날 때 마다 읽은 부분은 차근차근 정리 해두려고 한다. </p><blockquote><p><strong>Kotlin</strong>은 <em>최신 멀티플랫폼 애플리케이션을 위한 정적 타입 언어</em> 로,<br>2017 Google I/O에서 안드로이드 공식언어로 선정되었고 현재 1.3 버전까지 릴리즈 되어있다.</p><p>Kotlin 공식 페이지 <a href="https://kotlinlang.org/">https://kotlinlang.org/</a></p></blockquote><center><h2>_ _ _</h2></center><br/><hr><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><h4 id="Java-를-대신할-언어에-대한-Requirement-3가지"><a href="#Java-를-대신할-언어에-대한-Requirement-3가지" class="headerlink" title="Java 를 대신할 언어에 대한 Requirement 3가지"></a>Java 를 대신할 언어에 대한 Requirement 3가지</h4><ol><li>정적 타입 지정 (static typing)</li><li>기존 자바 코드와의 완전한 호환성</li><li>해당 언어를 위한 도구 개발 복잡성 x</li></ol><p>⇒ 배우고 이해하기 쉬우며 대규모 개발/유지보수성/기존 자바와의 호환성에 적합한 강력한 언어, <u><strong>‘Kotlin’</strong></u></p><br/><h1 id="1장-코틀린이란-무엇이며-왜-필요한가"><a href="#1장-코틀린이란-무엇이며-왜-필요한가" class="headerlink" title="1장. 코틀린이란, 무엇이며 왜 필요한가?"></a>1장. 코틀린이란, 무엇이며 왜 필요한가?</h1><p>Kotlin은 자바 플랫폼에서 돌아가는 새로운 프로그래밍 언어다.<br><strong>간결하고 실용적</strong>이며 <strong>Java 코드와의 상호운용성</strong> (interoperability)을 중시한다.</p><h2 id="특성"><a href="#특성" class="headerlink" title="특성"></a>특성</h2><h4 id="1-대상-플랫폼-자바가-실행되는-Everywhere"><a href="#1-대상-플랫폼-자바가-실행되는-Everywhere" class="headerlink" title="1. 대상 플랫폼 : 자바가 실행되는 Everywhere"></a>1. 대상 플랫폼 : 자바가 실행되는 Everywhere</h4><p>일부가 아닌 개발 과정에서 수행해야하는 모든 과업에 있어 폭넓게 생산성을 향상 시킨다.<br>구체적인 영역 or 특정 프로그램 패러다임을 지원하는 여러 라이브러리와의 융합성 ↑</p><br/><h4 id="2-정적-타입-지정-언어"><a href="#2-정적-타입-지정-언어" class="headerlink" title="2. 정적 타입 지정 언어"></a>2. 정적 타입 지정 언어</h4><p>Kotlin은 <strong>정적 타입 지정 언어</strong> 이면서, <u>type inference</u> (타입추론) 과 <u>nullable type</u>을 지원한다.<br>⇒ 프로그래머의 불편함 해소 &amp; 컴파일 시점에 NPE 여부 검사 가능</p><ul><li><p><strong>정적 타입</strong> (statically typed) 지정 언어 : Java, Kotlin ..</p><blockquote><p>모든 프로그램 구성 요소 타입을 컴파일 시점에 알 수 있고, 객체의 필드나 메소드를 사용할 때마다 컴파일러가 타입을 검증한다.</p></blockquote><p>  장점 : 성능 / 신뢰성 / 유지 보수성 / 도구 지원 _ <code>p.36</code></p>  <br/></li><li><p><strong>동적 타입</strong> (dynamically typed) 지정 언어 : Groovy, JRuby …</p><blockquote><p>타입과 관계없이 모든 값을 변수에 넣을 수 있고, 필드나 메소드 접근에 대한 검증이 실행 시점에 일어난다. </p></blockquote><p>  동적이 더 유연하고 코드도 짧아지지만, 오류를 사전에 거르지 못하고 Runtime Error 발생 가능성 존재</p></li></ul><br/><h4 id="3-함수형-객체지향-프로그래밍"><a href="#3-함수형-객체지향-프로그래밍" class="headerlink" title="3. 함수형 / 객체지향 프로그래밍"></a>3. 함수형 / 객체지향 프로그래밍</h4><p>Kotlin으로 코드를 작성 할 땐, Java와 같은 <u>객체지향 프로그래밍 (OOP)</u> 과 <u>함수형 프로그래밍</u> 접근 방법을 조합해서 문제에 가장 적합한 도구를 사용하면 된다.</p><blockquote><p><strong>함수형 프로그래밍</strong> 의 핵심 개념</p><ul><li>first-class function (일급 함수)</li><li>immutability</li><li>no side effect _ pure function</li></ul></blockquote><br/><h4 id="4-무료-오픈소스"><a href="#4-무료-오픈소스" class="headerlink" title="4. 무료 오픈소스"></a>4. 무료 오픈소스</h4><p>Kotlin 언어와 이와 관련된 모든 도구는 오픈소스이다.<br>(<a href="https://github.com/jetbrains/kotlin">https://github.com/jetbrains/kotlin</a> - Apache2 License.)</p><br/><h2 id="응용"><a href="#응용" class="headerlink" title="응용"></a>응용</h2><h4 id="코틀린-서버-프로그래밍"><a href="#코틀린-서버-프로그래밍" class="headerlink" title="코틀린 서버 프로그래밍"></a>코틀린 서버 프로그래밍</h4><blockquote><p><strong>서버 프로그래밍</strong> 의 범위</p><ul><li>브라우저에 HTML 페이지를 반환하는 웹 애플리케이션</li><li>모바일 애플리케이션에게 HTTP를 통해 JSON API를  </li><li>RPC (Remote Procedure Call) 프로토콜을 통해 서로 통신하는 마이크로 서비스</li></ul></blockquote><p>Kotlin은 이러한 애플리케이션 개발에 도움을 주는 기존의 자바 프레임워크나 기술과 매끄럽게 상호운용 가능하다.</p><p>+ 새로운 기술도 적용 가능 (ex. Kotlin의 Builder Pattern, Persistence Framework …)<br>  ⇒ <code>7.5절</code> &amp; <code>11장</code> 에서 좀 더 자세히</p><br/><h4 id="코틀린-안드로이드-프로그래밍"><a href="#코틀린-안드로이드-프로그래밍" class="headerlink" title="코틀린 안드로이드 프로그래밍"></a>코틀린 안드로이드 프로그래밍</h4><p>모바일 애플리케이션은 전형적인 엔터프라이즈 애플리케이션보다 더 작고 기존과 신규 코드 통합 필요성도 더 적고, 다양한 디바이스에 대한 서비스 신뢰성 보장과 빠른 개발&amp;배포가 필요하다.</p><p>Kotlin 언어의 특성과 특별한 컴파일러 플러그인 지원을 조합하면 개발 생산성을 더 높일 수 있다.<br>뿐만 아니라 애플리케이션 신뢰성 향상, 자바6와 완전한 호환, 성능 손실 x 과 같은 장점도 취할 수 있다.</p><p>+ 참조 _ 안드로이드 API에 대한 Kotlin Adaptor를 제공하고 있는 <strong>Anko Library</strong> <a href="https://github.com/kotlin/anko">https://github.com/kotlin/anko</a></p><br/><h2 id="철학"><a href="#철학" class="headerlink" title="철학"></a>철학</h2><blockquote><p>대개 Kotlin은 Java와의 <strong><em>상호운용성</em></strong> 에 초점을 맞춘 <strong><em>실용적</em></strong> 이고 <strong><em>간결</em></strong> 하며 <strong><em>안전한</em></strong> 언어로 표현된다.</p></blockquote><h4 id="실용성"><a href="#실용성" class="headerlink" title="실용성"></a>실용성</h4><ul><li>연구를 위한 언어가 아닌, 실제 문제를 해결하기 위해 만들어진 실용적인 언어이다.</li><li>특정 프로그래밍 스타일이나 패러다임 사용을 강제하지 않는다.</li><li>도구를 강조한다. (IDE 지원)</li></ul><br/><h4 id="간결성"><a href="#간결성" class="headerlink" title="간결성"></a>간결성</h4><ul><li>기존 코드 이해가 더 쉬워진다. (→ 생산성과 개발 속도 향상)</li><li>부수적인 요소등을 묵시적으로 제공하여 코드가 깔끔하다.</li><li>람다를 지원한다.</li><li>그러나 소스코드를 가능한 짧게 만드는 것이 코틀린의 설계 목표는 아니다.</li></ul><br/><h4 id="안전성"><a href="#안전성" class="headerlink" title="안전성"></a>안전성</h4><p>  프로그램의 안전성과 생산성 사이에는 trade-off 존재</p><ul><li>JVM에서 실행한다. (메모리 안전성과 버퍼 플로우 방지등 기본적으로 높은 안전성 확보)</li><li>정적 타입 지정 언어으로, 애플리케이션의 타입 안전성을 보장한다.</li><li>실행 시점이 아닌 컴파일 시점에 검사를 통해 더 많은 오류를 방지해준다. (ex. <code>NullPointerException</code>, <code>ClassCastException</code>)</li></ul><br/><h4 id="상호운용성"><a href="#상호운용성" class="headerlink" title="상호운용성"></a>상호운용성</h4><ul><li>Java의 기존 라이브러리를 그대로 사용 가능하고, 최대한 활용하고 있다.</li><li>Java ←→ Kotlin 호출에 따로 노력이 필요하지 않다.</li><li>다중 언어 프로젝트를 완전히 지원한다.</li></ul><br/><h2 id="코틀린-도구-사용"><a href="#코틀린-도구-사용" class="headerlink" title="코틀린 도구 사용"></a>코틀린 도구 사용</h2><p>Kotlin도 Java와 마찬가지로 컴파일 언어이다.<br><img src="https://user-images.githubusercontent.com/26691216/60521297-f6b43e80-9d21-11e9-956b-4f827ed75f1b.jpg" alt="kotlin-runtime-diagram"></p><center><i>Kotlin Build Process</i></center><br/><h4 id="자바-코틀린-변환"><a href="#자바-코틀린-변환" class="headerlink" title="자바-코틀린 변환"></a>자바-코틀린 변환</h4><p>Intellij IDEA에서는 자바 코드 조각을 코틀린 파일(.kt)에 붙여넣기<br>자바 파일 자체를 변환하려면 <code>Code &gt; Convert Java File to Kotlin File</code> </p><p>도구에 대해서는 필요에 따라 쓰면 되는거라 나머지 내용 생략.</p><br/><h3 id="1장-요약-p-57"><a href="#1장-요약-p-57" class="headerlink" title="1장 요약 p.57"></a>1장 요약 <code>p.57</code></h3>]]></content>
    
    
      
      
    <summary type="html">&lt;br/&gt;

&lt;h4 id=&quot;‘Kotlin-in-Action’&quot;&gt;&lt;a href=&quot;#‘Kotlin-in-Action’&quot; class=&quot;headerlink&quot; title=&quot;‘Kotlin in Action’&quot;&gt;&lt;/a&gt;‘Kotlin in Action’&lt;/h4&gt;&lt;p</summary>
      
    
    
    
    <category term="kotlin" scheme="https://minsw.github.io/categories/kotlin/"/>
    
    
    <category term="kotlin" scheme="https://minsw.github.io/tags/kotlin/"/>
    
    <category term="study" scheme="https://minsw.github.io/tags/study/"/>
    
    <category term="book" scheme="https://minsw.github.io/tags/book/"/>
    
    <category term="summarization" scheme="https://minsw.github.io/tags/summarization/"/>
    
    <category term="mwolkka" scheme="https://minsw.github.io/tags/mwolkka/"/>
    
  </entry>
  
  <entry>
    <title>2019 NAVER CAMPUS HACKDAY SUMMER 후기</title>
    <link href="https://minsw.github.io/2019/06/30/2019-NAVER-HACKDAY-SUMMER-%ED%9B%84%EA%B8%B0/"/>
    <id>https://minsw.github.io/2019/06/30/2019-NAVER-HACKDAY-SUMMER-%ED%9B%84%EA%B8%B0/</id>
    <published>2019-06-30T07:13:28.000Z</published>
    <updated>2021-01-24T17:27:34.803Z</updated>
    
    <content type="html"><![CDATA[<p>언젠가부터 주위 동기들이 <em>해커톤 (Hackaton)</em> 에서 좋은 경험을 하고 오는 걸 보면서,<br>항상 나도 해보고 싶다는 마음은 있었지만 현생이 바쁘다는 이유로 단 한번도 도전해보지 않았었다.</p><center><img src="https://user-images.githubusercontent.com/26691216/60393712-742a5400-9b54-11e9-9771-03862d5b94df.jpg" width=30%><i>핑계는...</i></center><p>길고 길었던 지옥의 사망년을 벗어난 기념으로, 지난 4월 <U>세 개의 해커톤</U>을 지원했고 운 좋게 <strong><U>두 개</U>를 합격하였다</strong>.<br>이번 포스트에서는 그 중 <strong>NAVER CAMPUS HACKDAY</strong> 후기를 작성하고자 한다.</p><blockquote><p>나머지 하나는 추후에 업로드 예정이다. <del>아마도…</del></p></blockquote><br/><hr><br/><h1 id="2019-NAVER-CAMPUS-HACKDAY-SUMMER"><a href="#2019-NAVER-CAMPUS-HACKDAY-SUMMER" class="headerlink" title="2019 NAVER CAMPUS HACKDAY SUMMER"></a>2019 NAVER CAMPUS HACKDAY SUMMER</h1><center><img src="https://d2.naver.com/content/images/2019/03/19CHACK_S.png" width=60%></center><blockquote><p>NAVER D2 - CAMPUS HACKDAY 행사 안내 <a href="https://d2.naver.com/news/5009947">https://d2.naver.com/news/5009947</a></p><p>GITHUB Page <a href="https://github.com/NAVER-CAMPUS-HACKDAY/common">https://github.com/NAVER-CAMPUS-HACKDAY/common</a></p></blockquote><p>깃헙 레포의 이슈에 있는 37개의 주제 중 희망하는 1~2개의 주제를 골라 지원서를 작성하고, 4월 13일에 온라인 코딩 테스트를 보았다.</p><p>코딩 테스트는 원하는 시간에 접속하여 제한시간 두 시간동안 3문제를 풀어야 했고, 문제 난이도는 그리 높은 편은 아니었던 것 같다. 제출하는 시간도 기록되었기 때문에 테스트 케이스를 적당히 확인하고 한 시간 조금 넘은 시점에 제출했다.</p><p>후담이지만 <a href="https://codingcompetitions.withgoogle.com/codejam">Google code jam</a>의 ‘Round 1A 2019’ 도 같은 날에 진행되어서 스타벅스에 앉아서 하루죙일 정신없이 문제만 풀었다. 🤦🏻‍♀️</p><br/><h3 id="🎉-햅격-🎉"><a href="#🎉-햅격-🎉" class="headerlink" title="🎉 햅격~ 🎉"></a>🎉 햅격~ 🎉</h3><img src="https://user-images.githubusercontent.com/26691216/60394163-02093d80-9b5b-11e9-8129-2444f06d0741.png" width="905"><p>기대 안하고 있었지만 사실 기대하긴 했다. (ㅋㅋㅋㅋㅋㅋㅋㅋㅋ)</p><p>해커톤 중에서도 Naver hackday는 꼭 한번쯤 가보고 싶었던 행사였기 때문에 특히 좋았다. 기쁜 와중에 딱 날짜가 종설 프로젝트 중간 발표 날이라서 팀원들에게 미리 양해를 구했는데, 다행히 마음씨 좋은 우리 팀원분들은 너그럽게 이해해주셨다.</p><center><img src="https://user-images.githubusercontent.com/26691216/60394226-0124db80-9b5c-11e9-80ed-221836478984.png" width=40%></center><br/><hr><br/><h2 id="Before-Hack-day"><a href="#Before-Hack-day" class="headerlink" title="Before Hack-day"></a>Before Hack-day</h2><p>내가 수행하게 된 주제는 <strong>“컨테이너 기반 쇼핑 상품 정보 수신”</strong> 으로, 세 명이 한 팀을 이루고 멘토님 한 분이 함께 해주셨다. </p><blockquote><p><strong>[컨테이너 기반 쇼핑 상품 정보 수신]</strong></p><p><em><a href="https://github.com/NAVER-CAMPUS-HACKDAY/common/issues/7">https://github.com/NAVER-CAMPUS-HACKDAY/common/issues/7</a></em></p><p>상품정보 수집을 위하여 각 쇼핑몰에서 제공하는 상품정보 <strong>EP</strong>(Engine Page)를 주기적으로 수집하여 변경된 정보를 체크하고 서비스에 반영하고 있다. 해당 작업의 <strong>확장성 및 고가용성</strong>을 위하여 Kubernetes 등의 컨테이너 환경에서 Task Agent 들이 운용될 수 있도록 설계 및 구현이 필요하다.</p></blockquote><p>해커톤 행사 당일에 주제 선정과 개발이 모두 이루어지는 다른 해커톤들과는 다르게 Naver Campus Hackday는 본인이 지원한 주제에 따라 팀이 꾸려지고, 사전에 멘토님의 가이드에 따라 팀끼리 개발을 어느정도 진행하기도 한다. ( → 해당 부분은 팀by팀 인 듯)</p><p>프로젝트에 대해서는 멘토님께 사전에 여쭤봤을 때, 구체적인 플로우는 공개할 수 없으나 내가 짠 코드는 무관하다고 답변을 주셔서 공식 Hackday github 이슈에 노출되어있는 프로젝트의 개괄적인 내용과 함께 느낀점만 간략하게 정리하고자 한다.</p><br/><p>우리 팀의 경우 특히 인프라 구축이 필요한 주제였기 때문에 사전에 LINE과 Github을 통해 온라인 회의를 지속적으로 진행했다. 이를 통해 나는 아래와 같은 사전 준비를 하고서 Hackday에 참가하였다.</p><ol><li><p>MQ(Message Queue), 데이터 처리 방법 등에 대한 이해</p><ul><li>MQ란 무엇이고, 프로젝트에 적합한 프로젝트는 무엇인가?</li><li>데이터 처리 방식 중 Batch와 Stream의 차이와 각각의 장단점은?</li><li>확장성과 고가용성을 고려하였을 때 적합한 데이터 저장 및 분석 방법은?</li></ul></li><li><p>서버 운용 및 클러스터링 계획</p><ul><li>사전에 할당받은 10개의 서버를 어떻게 운용할 것인가?</li><li>어떤 기술 스택을 사용할 것인가?</li><li>어떤식으로 Clustering할 것인가?</li><li>어떠한 플로우로 데이터를 처리하고, 어떻게 분석할 것인가? (설계)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Container Management : Kubernetes</span><br><span class="line">Message Queue : Kafka</span><br><span class="line">Database &amp; Analytics Engine : Hbase + Spark</span><br></pre></td></tr></table></figure></li></ul></li><li><p>컨테이너 기반 (Kubernetes)</p><ul><li>어느 범위까지 컨테이너화할 것인가?</li><li>Kubernetes Clustering은 어떻게 구성 할 것인가?</li><li>Docker registry는 어떤식으로 사용할 것인가?</li></ul></li></ol><br/><p>그 외에도 대용량 데이터 처리임을 고려하여 어떻게하면 속도와 공간 효율성을 확보할 수 있을 지에 대한 고민을 정말 많이 했다.</p><p>내가 알고 있는거라곤 ‘쿠버네티스’ 다섯 글자 뿐…<br>이때까지 이정도의 대용량 데이터 처리를 해본적이 없었고 MQ 나 Hadoop 사용 경험도 없었기 때문에 처음 접하는 것들이 대부분이었다. 책도 찾아 읽고 나름대로의 공부도 많이 해서 어느정도 설계까지는 했지만 막상 실제 인프라 구축은 막막하기만 했는데, 우리 팀에 데이터 마술사님(!)이 계셔서 초반 구축을 <del>알잘딱깔센</del> 해주셔서 🐶🍯 이었다.</p><p>Hackday 준비를 하면서 공부도 많이 됐지만 능력있고 열정있는 팀원님을 보면서 특히 많이 배웠다. 최고의 팀원 최고 bbb</p><br/><h2 id="D-day-Hack-day"><a href="#D-day-Hack-day" class="headerlink" title="D-day, Hack-day"></a>D-day, Hack-day</h2><p>Hackday 당일, 춘천으로 출발 전에 미리 모여 멘토님과 간단하게 점심 식사를 했다.<br>그린 팩토리 근처 식당에서 돈까스 먹었는데 굉장히 맷-집 이더라. JMTGR.</p><center><img src="https://user-images.githubusercontent.com/26691216/60395184-bb234400-9b6a-11e9-8966-64be35892e6b.jpg" width=40%>??: 좋아해요? </center><p>멘토님을 처음 뵙는 자리라 조금 긴장도 됐었는데 생각보다 편한 분위기로 얘기를 나누고 근처 카페에서 커피도 사주셔서 감사하게 먹고 그린팩토리로 향했다.</p><p>하지만 모든게 평화롭고 순조로운 가운데, <strong>한가지 문제</strong>가 있었다.</p><p>팀원 분 중에 한 분이 몸이 아프셔서 당일 날 못 오신 것이다. 위에서 말했다 싶이 한 팀당 3명이 팀이었고, 대부분 세 명인 가운데 갑자기 우리팀만 둘이 되었다.</p><p>이 때 느낌왔다. <U>오늘 숙소 구경은 물건너 갔음을.</U></p><br/><center><img src="https://user-images.githubusercontent.com/26691216/60395094-3c79d700-9b69-11e9-9a2f-faf606655425.JPG" width=50%><i>행사 진행 장소인 NAVER CONNECT ONE은 내부 사진 공개를 금하고 있기 때문에 시설 모습이 보이는 구체적인 사진들은 공개할 수 없다.</i></center><br/><p>사실 나와 다른 팀원님은 커네트원이 두 번째 방문이라 구경은 제쳐두고 회의실에만 박혀있었어서 사진도 거의 안찍었다. 때 맞춰 나와 밥 먹고 당 떨어지면 간식 가져오는 거 외에는 회의실에 스스로를 감금했다.</p><p>왜냐고? 순조롭긴 개뿔 내가 해간거 하나도 안됐다. <del>tlqk</del></p><blockquote><p>인프라 특 _ 이유없이 갑자기 안됨</p><p>루까 특 _ 이유없긴 사실상 <strong>본인 잘못</strong>임</p></blockquote><br/><p>도착하자 마자 마음이 급했던 우리 팀은 곧 바로 회의를 시작했다. 사전에 온라인 회의를 통해 설계를 논의하긴 했었지만 확정은 아니었고, 그 이후로 각자가 고민했던 부분과 그 결과로 설계한 구조를 서로 공유하고 멘토님께 피드백을 받았다.</p><p><strong><em>결론만 얘기하자면, 나와 다른 팀원님의 디자인은 완전히 달랐다.</em></strong></p><p>같은 문제를 접했고 사용할 인프라도 같이 정했음에도 불구하고, 생각하는 해결 방식이 다른 것이다. 이런 점이 사실 놀랍기도 하고 또 각 방법의 장단점이 분명해서 어떤 설계에 따라 구현할지 토의를 하면서 많은 고민이 되었다. 결국 일단은 내 설계대로 진행하기로 결론을 냈지만, 여기의 가장 큰 이슈는 <U>Hackday 기간 안에 구현이 가능할지</U> 였다. </p><blockquote><p>✽ 프로젝트 내용은 공개 가능한 범위가 모호하여 일단 보류하고, 추후에 기회가 되는대로 짰던 코드와 함께 겪었던 문제점, 해결 과정, 느낀점 등을 따로 정리하려 한다.</p></blockquote><br/><h4 id="‘혹시’하면-‘역시’다"><a href="#‘혹시’하면-‘역시’다" class="headerlink" title="‘혹시’하면 ‘역시’다."></a>‘혹시’하면 ‘역시’다.</h4><center><img src="https://user-images.githubusercontent.com/26691216/60401215-0ebc7e80-9bb9-11e9-90bf-d301476adcce.gif" width="210"></center><center><i>서버들은 이유 없이 돌아가며 터지고, <br/>Kubernetes는 갑자기 막혔고, <br/>Hbase Cluster도 갑자기 터져 팀원님이 해결하고 계시는 와중에 <br/>우려 했던 Spark (Scalar) 멘붕까지 <strong>터/져/Ba/by</strong> 상태 <br/></i></center> <p>새로운 도메인 지식 습득과 설계에 급급했던 나머지 나는 초반 인프라 구축 참여가 적었고 거기에 대한 이해도가 다소 부족했다. 그 중 Kafka는 비교적 많이 공부를 해갔지만 DB 쪽에는 신경을 많이 쓰지 못했더니,</p><p><strong>Java로 Consumer와 Producer를 구현하긴 했는데 막상 데이터를 어떻게 다뤄야 되는지를 모르겠는 거다.</strong></p><p>다른 팀원님이 DB 문제를 해결하고 계시는 동안 혼자 Consumer 모듈에 Spark를 적용해보려니 눈앞이 캄캄했다. 밤새 Spark가 대체 뭐며 어떻게 적용해야 하는지를 찾아봐도 참고하라고 주신 Scalar 코드를 봐도 좀 처럼 각이 안나왔다.<br>이 때 사실 팀원분이 데이터 처리 경험이 나보다 많다는 이유로 나는 너무 안일하게 준비한게 아닌가라는 반성을 많이 했다.</p><p>예상했던 대로 <U>그 좋은 숙소는 주인 없는 밤을 보내게 되었다</U>. 🛏</p><br/><br/><p>다음 날 점심 식사 후, 최종 결과물을 멘토님께 공유하고 피드백을 받았다.</p><p>처음에는 팀 프로젝트였지만 진행하다보니 계획했던 결과를 내기에는 부족함이 있어 결국 팀원 각자 나름의 구현 결과물을 내게 되었다.<br>(각자의 설계를 기반으로 하되, 각자가 처한 상황에 따라 스펙을 조금씩 바꾸었다)</p><p>밤새 구현한 걸 몇 번이고 뒤집어 엎어 최종적으로 계획 했던 동작은 구현하긴 했지만 ‘대용량 데이터 처리’와 ‘컨테이너 환경’에 대한 아쉬움은 어쩔 수 없었다. 이렇게 아쉬움을 한 가득 안고 <strong>Campus Hackday</strong>는 마무리 되었지만, 또 배워가는 것도 한 가득이라 여러모로 의미가 큰 행사였다.</p><br/><h2 id="After"><a href="#After" class="headerlink" title="After .."></a>After ..</h2><p>Hackday를 다녀와서 가장 먼저 한 일은 <strong>잠</strong> 🥱 이다. 즈질 체력…;;</p><p>그러고나서 까먹기 전에 부족했던 점과 배운 점을 정리하면서<br>멘토님께 피드백 받은 부분과 스스로 아쉬웠던 부분은 인프라가 아직 남아있을 때 공부하면서 보완해보고자 했다.</p><center><img src="https://user-images.githubusercontent.com/26691216/60398767-339ffa00-9b97-11e9-9caa-05023dba1d11.png" width="210"><s>사람이 안하던 짓을 하면...</s></center><p>생각했던 것보다 서버 회수 시기가 앞당겨져서 아쉽기는 했지만, 아쉬운대로 로컬에 최대한 동일한 테스트 환경을 구축해서 Code Refactoring / Test 와 정리한 내용 바탕의 문서 작성을 마치고 팀 Github에 Issue와 PR을 올림으로써 Hackday 프로젝트를 마무리 지었다.</p><hr><br/><h4 id="느낀점"><a href="#느낀점" class="headerlink" title="느낀점"></a>느낀점</h4><p>Naver Campus Hackday와 다른 해커톤과의 차이점은 실무에 어느정도 직접적인 연관이 있는 문제들을 접하고 그 중 본인이 관심있는 주제를 정할 수 있다는 점이고, 1박 2일 행사 기간 외에도 문제에 대해 좀 더 깊게 고민하고 개발할 수 있는 시간이 있다는 것이다. </p><p>그리고 또 다른 큰 차이점은 <strong>‘경쟁이나 시험이 아니다’</strong> 라는 것이다.<br>다른 팀과의 내용 공유가 따로 없어 어떤 걸 어떻게 해결하셨는지 정말 궁금하긴 했지만, 대신에 비교도 없고 경쟁도 없다. </p><p>물론 우수참가자에게는 네이버 인턴 면접기회를 준다는 혜택이 분명 존재하지만, 멘토님도 여러 차례 강조하셨던 것 처럼 정해진 답을 찾는다기보다 <U>실제 실무에서의 문제를 접하고 해결하는 경험</U>을 할 수 있는 기회이다. 경쟁이 아닌 또래의 열정적이고 실력있는 분들을 만나 많이 배우고 자극을 받을 수 있는 점이나, 현업의 네이버 개발자님의 멘토링과 피드백을 받을 수 있는 점 모두 다른 곳에서 경험 할 수 없는 귀한 경험인 것 같다.</p><p>끝나고 가장 아쉬웠던 점 중 하나는 사전에 좀 더 많은 시간을 투자해서 당일에는 좀 더 활발한 네트워킹과 구체적인 피드백 (+ 코드 리뷰) 을 받을 수 있었으면 더 좋지 않았을까 하는 것이다. </p><blockquote><p><em>‘Hackathon’</em> 이라고 생각하고 <em>‘Hackday’</em> 를 충분히 즐기지 못했던게 아쉽긴하지만,<br>근래 조금은 지쳤던 나에게 <strong><em>좋은 자극</em></strong> 이 되었음에는 틀림 없다.  </p></blockquote><br/><p>혹시나 Naver Campus Hackday 참가를 희망하거나 이미 합격하신 분이 이 글을 읽는다면 조금의 도움이 되시길 바라며 글을 마친다.</p><br/><p>아디다디도스! 👋🏻</p><br/>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;언젠가부터 주위 동기들이 &lt;em&gt;해커톤 (Hackaton)&lt;/em&gt; 에서 좋은 경험을 하고 오는 걸 보면서,&lt;br&gt;항상 나도 해보고 싶다는 마음은 있었지만 현생이 바쁘다는 이유로 단 한번도 도전해보지 않았었다.&lt;/p&gt;
&lt;center&gt;&lt;img sr</summary>
      
    
    
    
    <category term="retrospect" scheme="https://minsw.github.io/categories/retrospect/"/>
    
    
    <category term="naver" scheme="https://minsw.github.io/tags/naver/"/>
    
    <category term="hackathon" scheme="https://minsw.github.io/tags/hackathon/"/>
    
    <category term="container" scheme="https://minsw.github.io/tags/container/"/>
    
    <category term="kafka" scheme="https://minsw.github.io/tags/kafka/"/>
    
    <category term="CNCF" scheme="https://minsw.github.io/tags/CNCF/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes는 뭘까</title>
    <link href="https://minsw.github.io/2019/01/26/Kubernetes%EB%8A%94-%EB%AD%98%EA%B9%8C/"/>
    <id>https://minsw.github.io/2019/01/26/Kubernetes%EB%8A%94-%EB%AD%98%EA%B9%8C/</id>
    <published>2019-01-26T05:38:20.000Z</published>
    <updated>2021-01-24T17:27:34.809Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Kubernetes를-시작하기-앞서"><a href="#Kubernetes를-시작하기-앞서" class="headerlink" title="Kubernetes를 시작하기 앞서"></a>Kubernetes를 시작하기 앞서</h2><blockquote><p><em>최신 개발 트렌드는 …</em></p><p><em>어플리케이션의 구조를 <u>작고, 독립적인 단위</u>로 개발하고 (Microservices),</em></p><p><em>이를 <u>경량화된 가상화 환경</u>에서 구동할 수 있는 단위 (Container)로 생성하여,</em></p><p><em>이러한 <u>컨테이너들을 관리</u>할 수 있는 환경 (Cloud Native)을 구성하는 것이다</em></p></blockquote><br><h3 id="1-Microservice-Architecture-MSA"><a href="#1-Microservice-Architecture-MSA" class="headerlink" title="1. Microservice Architecture (MSA)"></a>1. Microservice Architecture (MSA)</h3><p>과거에는 서비스를 하나의 애플리케이션으로 만들어 모든 시스템을 그 하나에 다 집어넣는 <strong>모놀리식 아키텍처(Monorithic Architecture)</strong> 로 만들었다. 이러한 일체식 구조는 개발/배포/확장을 단순하게 만드는 장점을 가지지만, 큰 규모일 수록 코드이해나 수정이 어렵다.</p><p>그래서 등장하게된 <strong>마이크로서비스 아키텍처(Microservice Architecture)</strong> 는 서비스를 <u>작고</u>, <u>독립적이고</u>, <u>느슨하게 결합</u>하는 방식의 서비스 지향 아키텍처이다. 각각의 요소를 독립적인 어플리케이션으로 만들고, API로 조합해 애플리케이션으로 만든다.</p><blockquote><p><strong>MSA 구성요소</strong></p><ul><li>Service Discovery</li><li>Circuit Breaker</li><li>Sidecar (Service Discovery + Circuit Breaker) </li><li>Service Mesh</li><li>Service Mesh’s Control Plane</li></ul></blockquote><br><br><h3 id="2-Virtualization"><a href="#2-Virtualization" class="headerlink" title="2. Virtualization"></a>2. Virtualization</h3><p>서버를 가상으로 분할하는 <strong>가상화 (Virtualization)</strong> 는, 분할된 가상의 서버 내부에서 서비스를 실행하여 리소스를 효율적으로 쓰고자하는 기술이다. 가상화는 <strong>KVM, XEN, Hyper-V</strong> 등의 하이퍼바이저 기반의 기술과, <strong>Docker, LXC</strong> 등의 컨테이너 기반의 기술이 발전하면서 상용화되고 있다.</p><br><h3 id="Container"><a href="#Container" class="headerlink" title="Container"></a>Container</h3><blockquote><p><strong>VM</strong> 은 하이퍼바이저를 통한 하드웨어의 가상화이고, </p><p><strong>Container</strong> 는 OS레벨의 가상화 (User 공간의 추상화)를 제공한다.</p></blockquote><p><strong>Container</strong>는 host 시스템의 커널을 container들끼리 공유하기때문에 가볍고 빠른 속도를 가지며 편리하다.</p><p>모듈성(modularity)와 확장성(scalability)이 좋지만 보안성이 약하다 =&gt; VM과 공존 필요</p><p>​        <em>여러대의 서버에 여러대의 어플리케이션을 쓴다면 VM이,</em></p><p>​        <em>하나의 서버에 여러대의 어플리케이션을 쓴다면 Container가 적합할 수 있다</em></p><br><h3 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h3><p>리눅스 컨테이너를 기반으로 하는 오픈소스 프로젝트</p><p>namespace, control group(cgroup)과 같은 리눅스 커널 기능을 이용해서 OS 위에 컨테이너들을 생성하는 기술이다.</p><br><br><h3 id="3-Cloud-Native-Computing-Foundation-CNCF"><a href="#3-Cloud-Native-Computing-Foundation-CNCF" class="headerlink" title="3. Cloud Native Computing Foundation (CNCF)"></a>3. Cloud Native Computing Foundation (CNCF)</h3><p><strong>Cloud Native Computing</strong> 은 클라우드 컴퓨팅 모델의 장점을 모두 활용하는 애플리케이션을 개발하고 실행하기 위한 접근 방식이다.</p><p>microservice로 앱을 배포하고, 컨테이너 별로 패키징하고, 리소스 사용량을 최적화하는 동적 조절을위해 오픈소스 소프트웨어를 사용한다. <strong>Cloud Native Computing Foundation (CNCF)</strong> 는 이러한 클라우드 기술과 관련된 표준형을 개발하려는 단체이며, **<u>Kubernetes</u>**가 유일한 중심 프로젝트로 편성되었다.</p><blockquote><p> kubernetes, prometheus, envoy, istio, …</p></blockquote><br><br><br><h1 id="Kubernetes-k8s-란"><a href="#Kubernetes-k8s-란" class="headerlink" title="Kubernetes (k8s) 란?"></a>Kubernetes (k8s) 란?</h1><blockquote><p><em>그래서 MSA형태로 개발된 서비스들을 Docker로 컨테이너화해서 띄우긴했는데..</em></p><ul><li>여러대의 물리서버에서 각각 관리하기도 어렵고</li></ul><ul><li><p><u>lifecycle management</u>도 필요하고 (문제 대응, 패치, 업데이트 등)</p></li><li><p>컨테이너 배포, 스케일링, 오퍼레이팅등도 자동으로 되면 좋겠는데…</p><p>=&gt; (“해결사가 왔어!”) <strong>Kubernetes</strong></p></li></ul></blockquote><br><h3 id="Kubernetes"><a href="#Kubernetes" class="headerlink" title="Kubernetes"></a>Kubernetes</h3><p><strong>kubernetes</strong>는 “Docker container Orchestration tool” </p><p>컨테이너화된 어플리케이션을 Automatic deployment / Scaling / Management</p><p>- 그리스어로 ‘키잡이’라는 뜻으로, 줄여서 k8s라고 부른다. (k와 s사이에 8글자)</p><p>- Google에서 최초 개발되었고 현재는 CNCF에 기증된 상태</p><blockquote><p><em>service를 host os를 공유하는 container화해서 올리는  <strong>docker</strong></em></p><p><em>docker를 관리하는 <strong>k8s</strong>  (kubernetes)</em> </p><p><em>이러한 k8s application들을 chart화 시키고 관리하는 <strong>helm</strong></em></p></blockquote><br><h3 id="k8s-Object"><a href="#k8s-Object" class="headerlink" title="k8s Object"></a>k8s Object</h3><ul><li> <strong>Pod</strong> (name + spec + containers)</li></ul><p>- k8s의 가장 기본단위이자 Container의 묶음</p><p>- pod 단위로 network namespace와 ip 가질 수 있음 (!= namespace in k8s)</p><p>- 같은 pod에서는 같은 volume 접근가능</p><br><ul><li><strong>ReplicaSet</strong> (<strong>Pod</strong> + replicas)</li></ul><p>- <u>replica</u>는 복제라는 의미로 replicas 수만큼 pod가 유지되도록 관리된다</p><p>- pod는 죽으면 다시 되살리지않지만, ReplicaSet으로 만들면 replicas 수 (=pod 수)에 맞게 계속 살린다</p><p>- (단, 모니터링이 되어 autoscaler가 작동되고 있는 상황이어야 함)</p><br><ul><li><strong>Deployment</strong> (<strong>ReplicasSet</strong> + History (revision))</li></ul><p>- deployment는 name + replicas + pod 내용으로 구성되고, 대부분은 deployment를 사용해서 배포한다</p><p>- 버전별로 설치/롤백되고 배포 관리가 가능하다</p><p>- apps/v1일때는 <u>selector</u>가 있어야 <u>labels</u>를 가져올 수 있다</p><blockquote><p>​    <code>kubectl create deployment.yaml</code> <em>로 Deployment를 생성할 때 순서를 확인해보면 ,</em></p><p>​    <em>“ Deployment -&gt; ReplicaSet -&gt; Pod “  순으로 생성된다</em></p></blockquote><br><ul><li><strong>Service</strong></li></ul><p>Load balancer를 이용하여 여러 pod들을 하나의 ip, port로 묶어서 제공하는 DNS이다</p><p>그 기준은 <u>label selector</u> 로, 특정 label을 가진 것들을 하나의 서비스로 묶는다.</p><blockquote><p>Service object 노출 방식 3가지</p><ol><li><strong>ClusterIP</strong> - default값으로, Service에 Cluster IP (내부 IP)를 할당한다. 클러스터 내부에서만 접근 가능하고 외부에서는 접근이 불가능</li><li><strong>NodePort</strong> - 각각의 Node의 IP와 static 포트를 노출하여 접근가능하게 하고, 클러스터 외부에서도 접근가능</li><li><strong>Load Balancer</strong> - Cloud provider(GCE/AWS)와 같은 외부IP를 가진 Load balancer에게 Service를 노출</li></ol></blockquote><br><ul><li>그 외 고오급 오브젝트</li></ul><p>- <strong>DaemonSet</strong> : 맵핑된 label이 있는 node가 추가되면 자동으로 해당 node에 pod 생성을 보장 (scaling)</p><p>- <strong>StatefulSet</strong> : 컨테이너가 제거/재시작되어도 상태의 영속성과 지속성을 보장    =&gt; like DB<br><br></p><blockquote><p>- <strong>Affinity</strong> : kube-schedular에게 정보 제공. 부하 분산 또는 버전관리 가능</p><p>( Session <strong>Affinity</strong> - sticky session 제공 (canary deployment) )</p></blockquote><br><br><h3 id="기타-Keyword"><a href="#기타-Keyword" class="headerlink" title="기타 Keyword"></a>기타 Keyword</h3><p><strong>Docker 배포</strong></p><blockquote><p>특징 : 확장성, 표준성, 이미지 기반, 환경변수로 제어하는 설정, 공유자원..</p></blockquote><p>배포툴 : <u><strong>kubernetes</strong></u>, docker swarm, coreos, fleet,…</p><br><p><strong>Kubernetes 배포 프로세스</strong></p><blockquote><p><em>binary build -&gt; containerizing(image) -&gt; push image -&gt; service define -&gt; test deploy (canary test) -&gt; prod deploy</em>  </p><p>=&gt; 어렵고 복잡. 이런 배포 프로세스를 통합/자동화하는 CICD (배포툴) 필요!</p></blockquote><p>배포툴 : <u>kubespray</u>, kubeadm, kops, … (CaaS 지원)</p><br><br><p>* <strong>오케스트레이션</strong> (Orchestration)</p><p>여러 서버를 운영할때, 이들을 관리하는 것</p><ul><li><p>IaC를 돕는 설정관련 도구는 chef puppet <u>Ansible</u> SaltStack…</p></li><li><p>CI/CD 관리 도구는 Travis CI, <u>Jenkins</u>, Circle CI ..</p></li><li><p>컨테이너관리 도구는 Docker swarm, <u>Kubernetes</u> …</p></li></ul><br><p>* <strong>Ansible</strong></p><p>구성관리 tool로, 인프라 관리과정을 코드로 기술한 IaC (Infra as Code)를 효율적이고 자동으로 관리할수있는 인프라 도구.</p><p>Python 기반의 개발 + YAML로 정의 + JSON으로 통신</p><p>초기설정이나 모니터링, 변경사항 추적이 불가능하다는 단점이 있지만, shell command를 제외하고는 모두 <strong>Idempotency(멱등성)</strong> 을 제공한다.</p><blockquote><p> <em>kubespray는 ansible 기반의 배포툴이다.</em></p></blockquote><br><p>* <strong>Helm</strong></p><p>Chart라는 개념으로 kubernetes의 application을 정의, 배포하고 관리</p><ul><li>Chart: app 구성하는 Kubernetes 객체들을 정의한 manifest template파일 및 설정묶음</li><li>Cient (helm client, CLI) - Server (<strong>tiller</strong>, pod형태로 배포됨) 구조</li><li>Release: client 통해 kube 위에 배포된 app</li></ul><p>=&gt; helm client 설치 후 tiller server를 kubernetes cluster위에 설치해야함</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">helm init (—upgrade)&#x2F;&#x2F; tiller 설치 </span><br><span class="line">helm install &#x2F;&#x2F; repository에 등록된 chart를 client-&gt;tiller로 보냄</span><br><span class="line">helm lint&#x2F;&#x2F; chart의 문법검사</span><br></pre></td></tr></table></figure><br><p>* <strong>CI/CD</strong></p><ul><li><p>CI (Continuous Integration): 지속적 통합, 자주 Build &amp; Packaging</p></li><li><p>CD (Continous Delivery / Deployment): 지속적 배포, 자주 Deployment</p></li></ul><br><br><h3 id="참조"><a href="#참조" class="headerlink" title="참조"></a>참조</h3><ol><li><a href="https://www.samsungsds.com/global/ko/support/insights/101917_RD_Cloudnative.html">https://www.samsungsds.com/global/ko/support/insights/101917_RD_Cloudnative.html</a>)</li><li><a href="https://engineering.linecorp.com/ko/blog/infrastructure-trends-open-infra-days-korea-2018/">https://engineering.linecorp.com/ko/blog/infrastructure-trends-open-infra-days-korea-2018/</a></li><li>갓승규님 블로그 <a href="https://ahnseungkyu.com/">https://ahnseungkyu.com/</a> </li><li>Google Cloud - JAM k8s 입문반 QWIK LAB 진행</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Kubernetes를-시작하기-앞서&quot;&gt;&lt;a href=&quot;#Kubernetes를-시작하기-앞서&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes를 시작하기 앞서&quot;&gt;&lt;/a&gt;Kubernetes를 시작하기 앞서&lt;/h2&gt;&lt;bloc</summary>
      
    
    
    
    <category term="kubernetes" scheme="https://minsw.github.io/categories/kubernetes/"/>
    
    
    <category term="container" scheme="https://minsw.github.io/tags/container/"/>
    
    <category term="CNCF" scheme="https://minsw.github.io/tags/CNCF/"/>
    
    <category term="mwolkka" scheme="https://minsw.github.io/tags/mwolkka/"/>
    
    <category term="kubernetes" scheme="https://minsw.github.io/tags/kubernetes/"/>
    
    <category term="k8s" scheme="https://minsw.github.io/tags/k8s/"/>
    
  </entry>
  
</feed>
