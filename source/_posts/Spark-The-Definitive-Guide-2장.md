---
title: '&#039;Spark The Definitive Guide&#039; 2ì¥ - ìŠ¤íŒŒí¬ ì°ì–´ë¨¹ê¸°'
date: 2021-01-24 18:42:51
categories: spark
tags:
	- spark
	- apache
	- book
	- study
	- mwolkka
---


<img src="https://user-images.githubusercontent.com/26691216/105627228-e66e3200-5e78-11eb-9ea6-2e3662267b7a.jpg" width=200 />
<center> ë„ì»¤ ì´ë¯¸ì§€ ì‚¬ìš©ì‹œ Zeppelinì— ì˜ˆì œ ì½”ë“œê°€ ìˆë‹¤ <br/>
ë‚˜ì²˜ëŸ¼ ì‹œë ¥ ê²€ì‚¬&íƒ€ì ì—°ìŠµ í•˜ëŠë¼ ì§„ë¹¼ì§€ë§ê³  Chapter2ëŠ” ê·¸ëƒ¥ ì˜ˆì œ ì½”ë“œë¥¼ ì“°ë„ë¡ í•˜ì... </center>


<center><h2>_ _ _</h2></center>

<br/>

---


# CHAPTER 2 ìŠ¤íŒŒí¬ ê°„ë‹¨íˆ ì‚´í´ë³´ê¸°

DataFrame, SQL ì„ ì‚¬ìš©í•´ í´ëŸ¬ìŠ¤í„°, ìŠ¤íŒŒí¬ ì• í”Œë¦¬ì¼€ì´ì…˜, êµ¬ì¡°ì  API ë¥¼ ì‚´í´ë³´ê³ 
ìŠ¤íŒŒí¬ì˜ í•µì‹¬ìš©ì–´ì™€ ê°œë…, ì‚¬ìš©ë²•ì„ ìµíŒë‹¤.


### 2.1 ìŠ¤íŒŒí¬ì˜ ê¸°ë³¸ ì•„í‚¤í…ì²˜
> ìŠ¤íŒŒí¬ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì´í•´í•˜ê¸° ìœ„í•œ í•µì‹¬ì‚¬í•­
> - ìŠ¤íŒŒí¬ëŠ” ì‚¬ìš©ê°€ëŠ¥í•œ ìì›ì„ íŒŒì•…í•˜ê¸° ìœ„í•´ **í´ëŸ¬ìŠ¤í„° ë§¤ë‹ˆì €** ì‚¬ìš©
> - **ë“œë¼ì´ë²„** í”„ë¡œì„¸ìŠ¤ëŠ” ì£¼ì–´ì§ ì‘ì—…ì„ ì™„ë£Œí•˜ê¸°ìœ„í•´, ë“œë¼ì´ë²„ í”„ë¡œê·¸ë¨ì˜ ëª…ë ¹ì„ **ìµìŠ¤íí„°**ì—ì„œ ì‹¤í–‰í•  ì±…ì„ì´ ìˆìŒ
>

- ìŠ¤íŒŒí¬ëŠ” í´ëŸ¬ìŠ¤í„°ì˜ ë°ì´í„° ì²˜ë¦¬ ì‘ì—…ì„ ê´€ë¦¬ / ì¡°ìœ¨
  - ì»´í“¨í„° í´ëŸ¬ìŠ¤í„°ëŠ” ì—¬ëŸ¬ ì»´í“¨í„°ì˜ ìì›ì„ ëª¨ì•„ í•˜ë‚˜ì˜ ì»´í“¨í„° ì²˜ëŸ¼ ì‚¬ìš©
  - í´ëŸ¬ìŠ¤í„°ì—ì„œ ì‘ì—…ì„ ì¡°ìœ¨í•  ìˆ˜ ìˆëŠ” í”„ë ˆì„ì›Œí¬ => **ìŠ¤íŒŒí¬**
- ìŠ¤íŒŒí¬ê°€ ì—°ì‚°ì— ì‚¬ìš©í•  í´ëŸ¬ìŠ¤í„°ë¥¼ ê´€ë¦¬í•˜ëŠ” **í´ëŸ¬ìŠ¤í„° ë§¤ë‹ˆì €**
  - ìŠ¤íŒŒí¬ standalone í´ëŸ¬ìŠ¤í„° ë§¤ë‹ˆì €, í•˜ë‘¡ YARN, Mesos
  - ì—­í• 
    - ì‚¬ìš©ì : ìŠ¤íŒŒí¬ ì• í”Œë¦¬ì¼€ì´ì…˜ ì œì¶œ (submit)
    - -> í´ëŸ¬ìŠ¤í„° ë§¤ë‹ˆì € : ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ì— í•„ìš”í•œ ìì› í• ë‹¹ 
    - -> í• ë‹¹ë°›ì€ ìì›ìœ¼ë¡œ ì‘ì—… ì²˜ë¦¬
- ìŠ¤íŒŒí¬ ì• í”Œë¦¬ì¼€ì´ì…˜ = `driver` í”„ë¡œì„¸ìŠ¤ + ë‹¤ìˆ˜ì˜ `executor` í”„ë¡œì„¸ìŠ¤
  - `driver` í”„ë¡œì„¸ìŠ¤
    - í´ëŸ¬ìŠ¤í„° ë…¸ë“œ ì¤‘ í•˜ë‚˜ì—ì„œ ì‹¤í–‰. main() í•¨ìˆ˜ ì‹¤í–‰
    - ì‹¬ì¥ê³¼ ê°™ì€ ì¡´ì¬ë¡œ, ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒëª… ì£¼ê¸° ë™ì•ˆ ê´€ë ¨ ì •ë³´ ëª¨ë‘ ìœ ì§€
  - `executor` í”„ë¡œì„¸ìŠ¤
    - driver ê°€ í• ë‹¹í•œ ì‘ì—… ìˆ˜í–‰ & ì§„í–‰ ìƒí™©ì„ driverì—ê²Œ ë³´ê³ 
    - ëŒ€ë¶€ë¶„ ìŠ¤íŒŒí¬ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ì—­í• ë¡œ, ìŠ¤íŒŒí¬ ì–¸ì–´ APIë¥¼ í†µí•´ ë‹¤ì–‘í•œ ì–¸ì–´ë¡œ ì‹¤í–‰ ê°€ëŠ¥

### 2.2 ìŠ¤íŒŒí¬ì˜ ë‹¤ì–‘í•œ ì–¸ì–´ API
- ìŠ¤íŒŒí¬ëŠ” ëª¨ë“  ì–¸ì–´ì— ë§ëŠ” ëª‡ëª‡ 'í•µì‹¬ ê°œë…' ì œê³µ
  - í•µì‹¬ê°œë… -> (í´ëŸ¬ìŠ¤í„° ë¨¸ì‹ ì—ì„œ ì‹¤í–‰ë˜ëŠ”) ìŠ¤íŒŒí¬ ì½”ë“œ ë¡œ ë³€í™˜
  - êµ¬ì¡°ì  APIë§Œìœ¼ë¡œ ì‘ì„±ëœ ì½”ë“œëŠ” ì–¸ì–´ì— ë¬´ê´€í•˜ê²Œ ìœ ì‚¬ ì„±ëŠ¥
- ì–¸ì–´ë³„ ìš”ì•½ ì •ë³´
  - Scala : ìŠ¤íŒŒí¬ê°€ ìŠ¤ì¹¼ë¼ ê¸°ë°˜. **ìŠ¤íŒŒí¬ì˜ ê¸°ë³¸ ì–¸ì–´**
  - Java : ~~ìë°” ì§€ì›ì•ˆí•´ì£¼ë©´ ë‚œë¦¬ì¹ ê±°ë‹ˆê¹Œ~~ ì§€ì›ì€ í•¨
  - Python : ìŠ¤ì¹¼ë¼ê°€ ì§€ì›í•˜ëŠ” ê±°ì˜ ëª¨ë“  êµ¬ì¡° ì§€ì›
  - SQL : ANSI SQL:2003 í‘œì¤€ ì¤‘ ì¼ë¶€ ì§€ì›
  - R : ìŠ¤íŒŒí¬ ì½”ì–´ì˜ sparkR, R ì»¤ë®¤ë‹ˆí‹° ê¸°ë°˜ì˜ sparklyr
- SparkSession ê°ì²´
  - ì‚¬ìš©ìê°€ ìŠ¤íŒŒí¬ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê¸°ìœ„í•´ ì§„ì…ì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥
  - Python, R ì‚¬ìš© ì‹œì—ë„ ì‚¬ìš©ì ëŒ€ì‹  ìµìŠ¤íí„°ì˜ JVMì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ì½”ë“œë¡œ ë³€í™˜

### 2.3 ìŠ¤íŒŒí¬ API
- ë‹¤ì–‘í•œ ì–¸ì–´ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì´ìœ ?
  - ìŠ¤íŒŒí¬ê°€ ê¸°ë³¸ì ìœ¼ë¡œ ì œê³µí•˜ëŠ” 2ê°€ì§€ API ë•Œë¬¸
    - ì €ìˆ˜ì¤€ì˜ ë¹„êµ¬ì¡°ì (unstructured) API
    - ê³ ìˆ˜ì¤€ì˜ êµ¬ì¡°ì (structured) API

### 2.4 ìŠ¤íŒŒí¬ ì‹œì‘í•˜ê¸°
- Q. ìŠ¤íŒŒí¬ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•˜ë ¤ë©´
  - A. ì‚¬ìš©ì ëª…ë ¹ê³¼ ë°ì´í„°ë¥¼ ìŠ¤íŒŒí¬ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì „ì†¡í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ì•¼
- SparkSession ìƒì„± ì‹¤ìŠµ. ì ë“œê°€ì~

### 2.5 SparkSession
- **SparkSession** : ìŠ¤íŒŒí¬ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì œì–´í•˜ëŠ” ë“œë¼ì´ë²„ í”„ë¡œì„¸ìŠ¤
  - ì‚¬ìš©ìê°€ ì •ì˜í•œ ì²˜ë¦¬ëª…ë ¹ -> í´ëŸ¬ìŠ¤í„°ì— ì‹¤í–‰
  - ìŠ¤íŒŒí¬ ì• í”Œë¦¬ì¼€ì´ì…˜ì— 1:1 ëŒ€ì‘

```shell
# scala console
$ ./spark-2.4.7-bin-hadoop2.7/bin/spark-shell

scala> spark
res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@5b58f639
scala> val myRange = spark.range(1000).toDF("number")
myRange: org.apache.spark.sql.DataFrame = [number: bigint]
```

### 2.6 DataFrame
- **DataFrame** : ê°€ì¥ ëŒ€í‘œì ì¸ **êµ¬ì¡°ì  API**
  - í…Œì´ë¸” ë°ì´í„°ë¥¼ row, column ìœ¼ë¡œ ë‹¨ìˆœí•˜ê²Œ í‘œí˜„
    - scheme : column ê³¼ column type ì„ ì •ì˜í•œ ëª©ë¡
  - DataFrame ì€ ìˆ˜ì²œ ëŒ€ì˜ ì»´í“¨í„°ì— ë¶„ì‚° ê°€ëŠ¥
  - vs ìŠ¤í”„ë ˆë“œ ì‹œíŠ¸
    - ë¹„ìŠ·í•˜ë‹¤ê³  ë³¼ ìˆ˜ ìˆì§€ë§Œ ìŠ¤í”„ë ˆë“œ ì‹œíŠ¸ëŠ” ë‹¨ì¼ ì»´í“¨í„° ì €ì¥
  - vs Python (Pandas)ì˜ DataFrame, Rì˜ DataFrame
    - ë§ˆì°¬ê°€ì§€ë¡œ ëŒ€ë¶€ë¶„ ë‹¨ì¼ ì»´í“¨í„°ì— ì¡´ì¬
    - => ìŠ¤íŒŒí¬ DataFrameìœ¼ë¡œ ì‰½ê²Œ ë³€í™˜ ê°€ëŠ¥
- ìŠ¤íŒŒí¬ì˜ í•µì‹¬ ì¶”ìƒí™” ê°œë… (ë¶„ì‚° ë°ì´í„° ëª¨ìŒ)
  - Dataset, DataFrame, SQL í…Œì´ë¸”, RDD
- DataFrameì˜ íŒŒí‹°ì…˜
  - ìµìŠ¤íí„°ê°€ ë³‘ë ¬ë¡œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ ë°ì´í„°ë¥¼ ë¶„í• í•˜ëŠ” ì²­í¬ ë‹¨ìœ„
  - ì‹¤í–‰ ì¤‘ ë°ì´í„°ê°€ í´ëŸ¬ìŠ¤í„°ì—ì„œ ë¬¼ë¦¬ì ìœ¼ë¡œ ë¶„ì‚°ë˜ëŠ” ë°©ì‹ì„ ë‚˜íƒ€ëƒ„
    - íŒŒí‹°ì…˜ 1 ìµìŠ¤íí„° 1000 => ë³‘ë ¬ì„± 1
    - íŒŒí‹°ì…˜ 1000 ìµìŠ¤íí„° 1 => ë³‘ë ¬ì„± 1
  - ë¬¼ë¦¬ì  íŒŒí‹°ì…˜ì— ë°ì´í„° ë³€í™˜ìš© í•¨ìˆ˜ ì§€ì • ì‹œ ìŠ¤íŒŒí¬ê°€ ì‹¤ì œ ì²˜ë¦¬ ë°©ë²• ê²°ì • (íŒŒí‹°ì…˜ ìˆ˜ë™ ì²˜ë¦¬ í•„ìš” X)

### 2.7 íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜
- ìŠ¤íŒŒí¬ì˜ í•µì‹¬ ë°ì´í„° êµ¬ì¡° => **ë¶ˆë³€ì„± (immutable)**
  - DataFrameì„ ë³€ê²½í•˜ë ¤ë©´?
  - ì›í•˜ëŠ” ë³€ê²½ ë°©ë²•ì„ ìŠ¤íŒŒí¬ì—ê²Œ ì•Œë ¤ì¤˜ì•¼í•¨ => **íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜**
- íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜ : ìŠ¤íŒŒí¬ì—ì„œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ í‘œí˜„í•˜ëŠ” í•µì‹¬ ê°œë…
  - ìœ í˜•
    - ì¢ì€ ì˜ì¡´ì„± (narrow dependency)
      - ì…ë ¥ íŒŒí‹°ì…˜ : ì¶œë ¥ íŒŒí‹°ì…˜ = 1 : 1
    - ë„“ì€ ì˜ì¡´ì„± (wide dependency)
      - ì…ë ¥ íŒŒí‹°ì…˜ : ì¶œë ¥ íŒŒí‹°ì…˜ = 1 : N
- ì§€ì—° ì—°ì‚° (lazy evaluation) : ì—°ì‚° ê·¸ë˜í”„ë¥¼ ì²˜ë¦¬í•˜ê¸° ì§ì „ê¹Œì§€ ê¸°ë‹¤ë¦¬ëŠ” ë™ì‘ ë°©ì‹
  - ìŠ¤íŒŒí¬ëŠ” ì—°ì‚° ëª…ë ¹ ì¦‰ì‹œ ë°ì´í„°ë¥¼ ìˆ˜ì • X. ì›ì‹œ ë°ì´í„°ì— ì ìš©í•  íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜ì˜ **ì‹¤í–‰ ê³„íš**ì„ ìƒì„±
  - ë§ˆì§€ë§‰ê¹Œì§€ ëŒ€ê¸°í•˜ë‹¤ DataFrame íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜ì„ ê°„ê²°í•œ ë¬¼ë¦¬ì  ì‹¤í–‰ ê³„íšìœ¼ë¡œ ì»´íŒŒì¼ => ì „ì²´ ë°ì´í„° íë¦„ ìµœì í™”
  - ex. DataFrame ì˜ predicate pushdown

### 2.8 ì•¡ì…˜
- íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜ì€ ë…¼ë¦¬ì  ì‹¤í–‰ ê³„íš
  - íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜ì„ ì„ ì–¸í•´ë„ ì•¡ì…˜ì„ í˜¸ì¶œí•˜ì§€ ì•Šìœ¼ë©´ ìˆ˜í–‰ X
- ì•¡ì…˜ (action) : ì‹¤ì œ ì—°ì‚°ì„ ìˆ˜í–‰
  - ìœ í˜•
    - ì½˜ì†”ì—ì„œ ë°ì´í„°ë¥¼ ë³´ëŠ” ì•¡ì…˜
    - ê° ì–¸ì–´ë¡œ ëœ ë„¤ì´í‹°ë¸Œ ê°ì²´ì— ë°ì´í„°ë¥¼ ëª¨ìœ¼ëŠ” ì•¡ì…˜
    - ì¶œë ¥ ë°ì´í„°ì†ŒìŠ¤ì— ì €ì¥í•˜ëŠ” ì•¡ì…˜
- ì•¡ì…˜ ì§€ì • ì‹œ ìŠ¤íŒŒí¬ ì¡ ì‹œì‘
  - **ìŠ¤íŒŒí¬ ì¡ (job)**
    - í•„í„° (ì¢ì€ íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜) ìˆ˜í–‰
    - -> íŒŒí‹°ì…˜ ë³„ë¡œ ë ˆì½”ë“œ ìˆ˜ë¥¼ ì¹´ìš´íŠ¸ (ë„“ì€ íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜)
    - -> ê° ì–¸ì–´ì— ì í•©í•œ ë„¤ì´í‹°ë¸Œ ê°ì²´ì— ê²°ê³¼ ëª¨ìŒ
  - ìŠ¤íŒŒí¬ UIë¡œ ì¡ ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥
  - *ìŠ¤íŒŒí¬ ì¡ì€ ê°œë³„ ì•¡ì…˜ì— ì˜í•´ íŠ¸ë¦¬ê±°ë˜ëŠ” ë‹¤ìˆ˜ì˜ íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤*

### 2.9 ìŠ¤íŒŒí¬ UI
- ë“œë¼ì´ë²„ ë…¸ë“œì˜ 4040 í¬íŠ¸
- ìŠ¤íŒŒí¬ ì¡ì˜ ìƒíƒœ, í™˜ê²½ ì„¤ì •, í´ëŸ¬ìŠ¤í„° ìƒíƒœ ë“±ì˜ ì •ë³´ í™•ì¸ ê°€ëŠ¥

### 2.10 ì¢…í•© ì˜ˆì œ
- ë¯¸êµ­ êµí†µí†µê³„êµ­ì˜ í•­ê³µìš´í•­ ë°ì´í„° ì¤‘ ì¼ë¶€ë¡œ ì‹¤ìŠµ
  - [ìƒ˜í”Œ ë°ì´í„°](https://bit.ly/2yw2fCx) : ë°˜ì •í˜•(semi-structured), csv í¬ë§·
  - (=> ë¶€ë¡ Aì˜ ë„ì»¤ ì´ë¯¸ì§€ ì‚¬ìš© ì‹œ ì´ë¯¸ í¬í•¨)
- ìŠ¤íŒŒí¬ëŠ” ë‹¤ì–‘í•œ ë°ì´í„°ì†ŒìŠ¤ ì§€ì›
  - SparkSessionì˜ DataFrameReader í´ë˜ìŠ¤ ì‚¬ìš©í•´ì„œ ì½ìŒ
  - ì˜ˆì œëŠ” **ìŠ¤í‚¤ë§ˆ ì¶”ë¡  (Schema inference)** ê¸°ëŠ¥ ì¶”ê°€
    - ìŠ¤íŒŒí¬ëŠ” ê° ì»¬ëŸ¼ì˜ ë°ì´í„° íƒ€ì… ì¶”ë¡ ì„ ìœ„í•´ ì ì€ ì–‘ì˜ ë°ì´í„°ë¥¼ ì½ìŒ 
  - DataFrame ì€ ë¶ˆíŠ¹ì  ë‹¤ìˆ˜ì˜ ë¡œìš°ì™€ ì»¬ëŸ¼
    - ì§€ì—° ì—°ì‚° í˜•íƒœì˜ íŠ¸ë ŒìŠ¤í¬ë©”ì´ì…˜ì´ë¯€ë¡œ row ìˆ˜ ì•Œ ìˆ˜ X

#### ì˜ˆì œ 1

<details><summary class="point-color-can-hover">ì˜ˆì œ 1 í¼ì¹˜ê¸°</summary>

```bash

$ head /data/flight-data/csv/2015-summary.csv
DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count
United States,Romania,15
United States,Croatia,1
..

# spark-shell (scala)
scala> val flightData2015 = spark.read.option("inferSchema", "true").option("header", "true").csv("/data/flight-data/csv/2015-summary.csv")
flightData2015: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]

scala> flightData2015.take(3)
res0: Array[org.apache.spark.sql.Row] = Array([United States,Romania,15], [United States,Croatia,1], [United States,Ireland,344])


scala> flightData2015.sort("count").explain()
== Physical Plan ==
*(2) Sort [count#12 ASC NULLS FIRST], true, 0
+- Exchange rangepartitioning(count#12 ASC NULLS FIRST, 200)
   +- *(1) FileScan csv [DEST_COUNTRY_NAME#10,ORIGIN_COUNTRY_NAME#11,count#12] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>


# ì…”í”Œ íŒŒí‹°ì…˜ default 200ê°œ => 5ê°œ
scala> spark.conf.set("spark.sql.shuffle.partitions", "5")

scala> flightData2015.sort("count").explain()
== Physical Plan ==
*(2) Sort [count#12 ASC NULLS FIRST], true, 0
+- Exchange rangepartitioning(count#12 ASC NULLS FIRST, 5)
   +- *(1) FileScan csv [DEST_COUNTRY_NAME#10,ORIGIN_COUNTRY_NAME#11,count#12] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>

scala> flightData2015.sort("count").take(2)
res3: Array[org.apache.spark.sql.Row] = Array([United States,Singapore,1], [Moldova,United States,1])

```

</details>

> - `take(n)` : Action
> - `sort()` :  Transformation (ë„“ì€) 
>   - DataFrame ì„ ë³€ê²½í•˜ì§€ ì•Šê³  ìƒˆë¡œìš´ DataFrameì„ ìƒì„±í•´ ë°˜í™˜
> - `explain()` 
>   - DataFrameì˜ ê³„ë³´(lineage) ë‚˜ ìŠ¤íŒŒí¬ ì¿¼ë¦¬ ì‹¤í–‰ ê³„íš ì¶œë ¥

- ì‹¤í–‰ ê³„íš? : ë””ë²„ê¹…ê³¼ ìŠ¤íŒŒí¬ì˜ ì‹¤í–‰ê³¼ì •ì„ ì´í•´í•˜ëŠ”ë° ë„ì›€ì„ ì£¼ëŠ” ë„êµ¬
  - ìœ„ì—ì„œ ì•„ë˜ë°©í–¥ìœ¼ë¡œ ì½ëŠ”ë‹¤
  - ìµœì¢… ê²°ê³¼ëŠ” ê°€ì¥ ìœ„, ë°ì´í„°ì†ŒìŠ¤ëŠ” ê°€ì¥ ì•„ë˜
- DataFrameì˜ ê³„ë³´
  - íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜ì˜ ë…¼ë¦¬ì  ì‹¤í–‰ ê³„íš -> DataFrameì˜ ê³„ë³´ ì •ì˜
  - -> ê³„ë³´ë¥¼ í†µí•´ ìŠ¤íŒŒí¬ê°€ ì…ë ¥ë°ì´í„°ì— ìˆ˜í–‰í•œ ì—°ì‚°ì„ ì „ì²´ íŒŒí‹°ì…˜ì—ì„œ ì–´ë–»ê²Œ ì¬ì—°ì‚°í•˜ëŠ”ì§€ ì•Œ ìˆ˜ ìˆìŒ
  - *í•¨ìˆ˜í˜• í”„ë¡œê·¸ë˜ë°ì˜ í•µì‹¬* (Pure Function, ê°™ì€ ì…ë ¥ -> ê°™ì€ ì¶œë ¥)
- ì‚¬ìš©ìëŠ” ë¬¼ë¦¬ì  ë°ì´í„°ë¥¼ ì§ì ‘ ë‹¤ë£¨ì§€ ì•Šê³ , ë¬¼ë¦¬ì  ì‹¤í–‰ íŠ¹ì„±ì„ ì œì–´
  - ìŠ¤íŒŒí¬ UI (4040 í¬íŠ¸) ì—ì„œ ìŠ¤íŒŒí¬ ì¡ ë¬¼ë¦¬ì , ë…¼ë¦¬ì  ì‹¤í–‰ íŠ¹ì„± í™•ì¸ ê°€ëŠ¥ 
<img width="500" alt="sparkui" src="https://user-images.githubusercontent.com/26691216/105624926-cfbfdf00-5e68-11eb-9407-e58a5f4688a9.png">


#### ì˜ˆì œ 2 (SQL)

<details><summary class="point-color-can-hover">ì˜ˆì œ 2-1 í¼ì¹˜ê¸°</summary>

```bash
# 1) SQL ì‚¬ìš©
scala> flightData2015.createOrReplaceTempView("flight_data_2015")
scala> val sqlWay = spark.sql("""
     | SELECT DEST_COUNTRY_NAME, count(1)
     | FROM flight_data_2015
     | GROUP BY DEST_COUNTRY_NAME
     | """)

scala> sqlWay.explain
== Physical Plan ==
*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[count(1)])
+- Exchange hashpartitioning(DEST_COUNTRY_NAME#10, 5)
   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[partial_count(1)])
      +- *(1) FileScan csv [DEST_COUNTRY_NAME#10] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>


# 2) DataFrame ì‚¬ìš©
scala> val dataFrameWay = flightData2015.groupBy("DEST_COUNTRY_NAME").count()
dataFrameWay: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, count: bigint]

scala> dataFrameWay.explain
== Physical Plan ==
*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[count(1)])
+- Exchange hashpartitioning(DEST_COUNTRY_NAME#10, 5)
   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[partial_count(1)])
      +- *(1) FileScan csv [DEST_COUNTRY_NAME#10] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>

```

</details>

- ìŠ¤íŒŒí¬ëŠ” ì–¸ì–´ì— ë¬´ê´€í•˜ê²Œ ê°™ì€ ë°©ì‹ìœ¼ë¡œ íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜ ì‹¤í–‰
  - SQL, DataFrame(R, Python, Scalar, Java) ì—ì„œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ í‘œí˜„
  - ìŠ¤íŒŒí¬ì—ì„œ ì½”ë“œ ì‹¤í–‰ ì „ì— ë¡œì§ì„ ê¸°ë³¸ ì‹¤í–‰ê³„íš(`explain`) ìœ¼ë¡œ ì»´íŒŒì¼
- ìŠ¤íŒŒí¬ SQL ì‚¬ìš©ì‹œ ëª¨ë“  DataFrame => í…Œì´ë¸”, ë·° (ì„ì‹œ í…Œì´ë¸”) ë¡œ ë“±ë¡
  - ìœ„ì—ì„œ ì„¤ëª…í–ˆë“¯ **ê°™ì€ ì‹¤í–‰ ê³„íš**ìœ¼ë¡œ ì»´íŒŒì¼í•˜ë¯€ë¡œ ì„±ëŠ¥ì°¨ì´ X

<details><summary class="point-color-can-hover">ì˜ˆì œ 2-2 í¼ì¹˜ê¸°</summary>

```bash
# 'ìµœëŒ€ ë¹„í–‰ íšŸìˆ˜' êµ¬í•˜ê¸°

# SQL ì¿¼ë¦¬
scala> spark.sql("SELECT max(count) from flight_data_2015").take(1)
res9: Array[org.apache.spark.sql.Row] = Array([370002])

# DataFrame êµ¬ë¬¸ _ max í•¨ìˆ˜ (íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜) ì‚¬ìš©
scala> import org.apache.spark.sql.functions.max

scala> flightData2015.select(max("count")).take(1)
res10: Array[org.apache.spark.sql.Row] = Array([370002])

```

```bash
# 'ìƒìœ„ 5ê°œì˜ ë„ì°© êµ­ê°€' êµ¬í•˜ê¸°

# SQL ì¿¼ë¦¬
scala> val maxSql = spark.sql("""
     | SELECT DEST_COUNTRY_NAME, sum(count) as destination_total
     | FROM flight_data_2015
     | GROUP BY DEST_COUNTRY_NAME
     | ORDER BY sum(count) DESC
     | LIMIT 5
     | """)
maxSql: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, destination_total: bigint]

scala> maxSql.show()
+-----------------+-----------------+
|DEST_COUNTRY_NAME|destination_total|
+-----------------+-----------------+
|    United States|           411352|
|           Canada|             8399|
|           Mexico|             7140|
|   United Kingdom|             2025|
|            Japan|             1548|
+-----------------+-----------------+


# DataFrame êµ¬ë¬¸
scala> import org.apache.spark.sql.functions.desc

scala> flightData2015.groupBy("DEST_COUNTRY_NAME").sum("count").withColumnRenamed("sum(count)", "destination_total").sort(desc("destination_total")).limit(5).show()
+-----------------+-----------------+
|DEST_COUNTRY_NAME|destination_total|
+-----------------+-----------------+
|    United States|           411352|
|           Canada|             8399|
|           Mexico|             7140|
|   United Kingdom|             2025|
|            Japan|             1548|
+-----------------+-----------------+

# ì½”ë“œ ìˆ˜í–‰ ë‹¨ê³„ : CSV íŒŒì¼ => (1) read -> (2) groupBy -> (3) sum -> (4) withColumnRenamed -> (5) sort -> (6) limit -> (7) collect => Array(..)

# scala> ~.explain
== Physical Plan ==
TakeOrderedAndProject(limit=5, orderBy=[destination_total#108L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#10,destination_total#108L])
+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[sum(cast(count#12 as bigint))])
   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#10, 5)
      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[partial_sum(cast(count#12 as bigint))])
         +- *(1) FileScan csv [DEST_COUNTRY_NAME#10,count#12] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>

```

</details>

- ì‹¤í–‰ê³„íšì€ íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜ì˜ **ì§€í–¥ì„± ë¹„ìˆœí™˜ ê·¸ë˜í”„ (Directed Acyclic Graph, DAG)**
  - ì•¡ì…˜ì´ í˜¸ì¶œë˜ë©´ ê²°ê³¼ë¥¼ ë§Œë“¤ì–´ë‚¸ë‹¤
  - DAGì˜ ê° ë‹¨ê³„ëŠ” ë¶ˆë³€ì„±ì„ ê°€ì§„ ì‹ ê·œ DataFrameì„ ìƒì„±
- ì˜ˆì œì˜ ì „ì²´ ì½”ë“œ ìˆ˜í–‰ ë‹¨ê³„ (7ë‹¨ê³„) ëŠ” p.86 [ê·¸ë¦¼ 2-10] ì°¸ì¡°
  - ì‹¤ì œ ì‹¤í–‰ ê³„íš (`explain` ì´ ì¶œë ¥í•˜ëŠ”) ì€ ë¬¼ë¦¬ì ì¸ ì‹¤í–‰ ì‹œì ì—ì„œ ìˆ˜í–‰í•˜ëŠ” ìµœì í™”ë¡œ ì¸í•´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
  - ì§ì ‘ explain í•´ë³´ë©´ ì±…ì˜ explain ê³¼ë„ ë‹¤ë¥´ê²Œ ì¶œë ¥ë¨ 


### 2.11 ì •ë¦¬
- íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜, ì•¡ì…˜, DataFrame ì‹¤í–‰ ê³„íš ìµœì í™” ë°©ë²•
  - íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜ì˜ ì§€í–¥ì„± ë¹„ìˆœí™˜ ê·¸ë˜í”„(DAG) ë¥¼ ì§€ì—° ì‹¤í–‰í•˜ì—¬ ìµœì í™”
- ì˜ˆì œë¥¼ í†µí•œ ë°ì´í„°ê°€ íŒŒí‹°ì…˜ìœ¼ë¡œ êµ¬ì„±ë˜ëŠ” ë°©ë²•, ë³µì¡í•œ íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜ ì‘ì—… ì‹¤í–‰ ë‹¨ê³„ í™•ì¸

<br/>

### ğŸ“’ ë‹¨ì–´ì¥
- ì…”í”Œ (Shuffle) : ìŠ¤íŒŒí¬ì¹´ í´ëŸ¬ìŠ¤í„°ì—ì„œ íŒŒí‹°ì…˜ì„ êµí™˜
  - ìŠ¤íŒŒí¬ëŠ” ì…”í”Œì˜ ê²°ê³¼ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥
- ê°€í™˜ì„± (Commutative) : ë‘ ëŒ€ìƒì˜ ì—°ì‚° ê²°ê³¼ê°€ ìˆœì„œì™€ ê´€ê³„ì—†ì´ ë™ì¼ (-> êµí™˜ ë²•ì¹™)

