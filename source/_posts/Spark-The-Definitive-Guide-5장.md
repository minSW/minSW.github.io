---
title: '&#039;Spark The Definitive Guide&#039; 5ì¥ - êµ¬ì¡°ì  API ê¸°ë³¸ ì—°ì‚°'
date: 2021-01-26 05:26:48
categories: spark
tags:
  - spark
  - apache
  - book
  - study
---

<br/>

ì˜¤ëŠ˜ì˜ êµí›ˆ.
ë„ì»¤ ì´ë¯¸ì§€ì— ì˜ˆì œ ìˆë‹¤ê³  ì‹ ë‚˜ê²Œ ëŒë¦¬ê³ ~ ëŒë¦¬ê³ ~ í•˜ë‹¤ë³´ë©´
í„°ì§„ë‹¤ëŠ”ê±¸ ëª…ì‹¬í•˜ë„ë¡ í•˜ì ğŸ¥º

```
java.io.IOException: No space left on device
```

<br/>

<img src="https://user-images.githubusercontent.com/26691216/106086744-1b72d100-6166-11eb-8d99-1deebfa68867.jpg" width="400" alt="jongman">
<center><i style="color:lightgray"> ì¸ìƒì€ ì‹¤ì „ì´ì•¼ ì¹œêµ¬ì•¼</i></center>

<img width="300" alt="bomb" src="https://user-images.githubusercontent.com/26691216/106087261-fcc10a00-6166-11eb-80c2-d339e59bbc99.png">



<center><h2>_ _ _</h2></center>

<br/>

---

# CHAPTER 5 êµ¬ì¡°ì  API ê¸°ë³¸ ì—°ì‚°
CHAPTER 4 ëŠ” êµ¬ì¡°ì  APIì˜ í•µì‹¬ ì¶”ìƒí™” 'ê°œë…'ì„ ì†Œê°œ
CHAPTER 5 ëŠ” DataFrameê³¼ ê·¸ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” ê¸°ë³¸ 'ê¸°ëŠ¥' ì†Œê°œ


> *'DataFrame = Row íƒ€ì…ì˜ **ë ˆì½”ë“œ** + ì—¬ëŸ¬ **ì»¬ëŸ¼**'*
> (ê° ì»¬ëŸ¼ëª…ê³¼ ë°ì´í„° íƒ€ì…ì€ **ìŠ¤í‚¤ë§ˆ**ë¡œ ì •ì˜)
>
> DataFrameì˜ **íŒŒí‹°ì…”ë‹** :  DataFrame (ë˜ëŠ” Dataset)ì´ í´ëŸ¬ìŠ¤í„°ì—ì„œ ë¬¼ë¦¬ì ìœ¼ë¡œ ë°°ì¹˜ë˜ëŠ” í˜•íƒœë¥¼ ì •ì˜
>   - **íŒŒí‹°ì…”ë‹ ìŠ¤í‚¤ë§ˆ** : íŒŒí‹°ì…˜ì„ ë°°ì¹˜í•˜ëŠ” ë°©ë²• ì •ì˜
>   - íŒŒí‹°ì…”ë‹ì˜ ë¶„í•  ê¸°ì¤€? => íŠ¹ì • ì»¬ëŸ¼ or ë¹„ê²°ì •ë¡ ì (nondeterministically) ê°’ ê¸°ë°˜ìœ¼ë¡œ ì„¤ì •

### 5.1 ìŠ¤í‚¤ë§ˆ

```scala
val df = spark.read.format("json").load("/data/flight-data/json/2015-summary.json")
df.printSchema()

// root
//  |-- DEST_COUNTRY_NAME: string (nullable = true)
//  |-- ORIGIN_COUNTRY_NAME: string (nullable = true)
//  |-- count: long (nullable = true)
```

- ìŠ¤í‚¤ë§ˆëŠ” **DataFrameì˜ ì»¬ëŸ¼ëª…ê³¼ ë°ì´í„° íƒ€ì…ì„ ì •ì˜**
  - ê´€ë ¨ëœ ëª¨ë“  ê²ƒì„ í•˜ë‚˜ë¡œ ë¬¶ëŠ” ì—­í• 
- ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ìŠ¤í‚¤ë§ˆë¥¼ ì–»ê±°ë‚˜ (Schema-on-read), ì§ì ‘ ì •ì˜ ê°€ëŠ¥
  - ëŒ€ë¶€ë¶„ì˜ ë¹„ì •í˜• ë¶„ì„ (ad-hoc analysis)ì—ì„œ schema-on-read ì˜ ë™ì‘
  - ìš´ì˜ í™˜ê²½ ETL ì‘ì—…ì— ìŠ¤íŒŒí¬ ì‚¬ìš©ì‹œ **ì§ì ‘ ì •ì˜ í•„ìš”** (ìƒ˜í”Œ ë°ì´í„° íƒ€ì…ì— ë”°ë¥¸ ìŠ¤í‚¤ë§ˆ ì¶”ë¡  ë°©ì§€)
- ìŠ¤í‚¤ë§ˆëŠ” `StructType` ê°ì²´ 
  - ë³µí•© ë°ì´í„° íƒ€ì… `StructType` (=consistOf(`StructField` ê°ì²´))
  - ìŠ¤íŒŒí¬ëŠ” ìì²´ ë°ì´í„° íƒ€ì… ì •ë³´ë¥¼ ì‚¬ìš© => ì–¸ì–´ ë³„ ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ì„¤ì • X

    ```scala
    spark.read.format("json").load("/data/flight-data/json/2015-summary.json").schema
    // res176: org.apache.spark.sql.types.StructType
    //  = StructType(
    //        StructField(DEST_COUNTRY_NAME,StringType,true),
    //        StructField(ORIGIN_COUNTRY_NAME,StringType,true),
    //        StructField(count,LongType,true)
    //      )
    ```


<details><summary class="point-color-can-hover">[5.1] ì˜ˆì œ í¼ì¹˜ê¸° - DataFrameì— ìŠ¤í‚¤ë§ˆ ì ìš© ì˜ˆì œ</summary>

```scala
// DataFrameì— ìŠ¤í‚¤ë§ˆë¥¼ ë§Œë“¤ê³  ì ìš©í•˜ëŠ” ì˜ˆì œ
import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}
import org.apache.spark.sql.types.Metadata

val myManualSchema = StructType(Array(
  StructField("DEST_COUNTRY_NAME", StringType, true),
  StructField("ORIGIN_COUNTRY_NAME", StringType, true),
  StructField("count", LongType, false,
    Metadata.fromJson("{\"hello\":\"world\"}"))
))

val df = (spark.read.format("json").schema(myManualSchema)
  .load("/data/flight-data/json/2015-summary.json"))
```

</details>


### 5.2 ì»¬ëŸ¼ê³¼ í‘œí˜„ì‹
- ìŠ¤íŒŒí¬ì˜ 'ì»¬ëŸ¼' (=í‘œí˜„ì‹)
  - ìŠ¤í”„ë ˆë“œ ì‹œíŠ¸, Rì˜ dataframe, Pandasì˜ ì»¬ëŸ¼ê³¼ ë¹„ìŠ·
  - ì‚¬ìš©ìëŠ” <u>**í‘œí˜„ì‹**</u>ìœ¼ë¡œ DataFrameì˜ ì»¬ëŸ¼ì„ ì„ íƒ, ì¡°ì‘, ì œê±° ê°€ëŠ¥
  - ì¦‰ í‘œí˜„ì‹ì„ ì‚¬ìš©í•´ ë ˆì½”ë“œ ë‹¨ìœ„ë¡œ ê³„ì‚°í•œ ê°’ì„ ë‚˜íƒ€ë‚´ëŠ” ë…¼ë¦¬ì  êµ¬ì¡°. ì‹¤ì œê°’ì„ ì–»ìœ¼ë ¤ë©´ ë¡œìš° (=> DataFrame) ê°€ í•„ìš”
  - ì™¸ë¶€ ì ‘ê·¼ì‹œ **ë°˜ë“œì‹œ DataFrame ì„ í†µí•´ì•¼ í•¨**
- ì»¬ëŸ¼ ìƒì„± & ì°¸ì¡° : `col()` `column()`
  - ì»¬ëŸ¼ì´ DataFrameì— ìˆëŠ”ì§€ ì—†ëŠ”ì§€ëŠ” ëª¨ë¦„ => **ë¶„ì„ê¸°**ê°€ **ì¹´íƒˆë¡œê·¸**ì— ì €ì¥ëœ ì •ë³´ë‘ ë¹„êµí•˜ê¸° ì „ê¹Œì§€ëŠ” ë¯¸í™•ì¸ [[4.4] ì°¸ê³ ](https://minsw.github.io/2021/01/26/Spark-The-Definitive-Guide-4%EC%9E%A5/#4-4-%EA%B5%AC%EC%A1%B0%EC%A0%81-API%EC%9D%98-%EC%8B%A4%ED%96%89-%EA%B3%BC%EC%A0%95)
  - ìŠ¤ì¹¼ë¼ëŠ” ê³ ìœ  ê¸°ëŠ¥ ì‚¬ìš© ê°€ëŠ¥ : `$"ì»¬ëŸ¼ëª…"` `'ì»¬ëŸ¼ëª…` (`'` : í‹± ë§ˆí¬, ì‹¬ë²Œ)
  - ëª…ì‹œì  ì°¸ì¡° : `DataFrame.col()` (ì¡°ì¸ì‹œ ìœ ìš©)
    => ëª…ì‹œì  ì»¬ëŸ¼ ì •ì˜ ì‹œ, ë¶„ì„ê¸° ì‹¤í–‰ ë‹¨ê³„ì—ì„œ ì»¬ëŸ¼ í™•ì¸ ì ˆì°¨ ìƒëµ

  ```scala
  import org.apache.spark.sql.functions.{col, column}
  col("someCol")
  column("someCol")

  // in Scala
  $"someCol"
  'someCol

  // ëª…ì‹œì  ì°¸ì¡°
  df.col("someCol")
  ```

- **í‘œí˜„ì‹** : `expr()`
  - DataFrame ë ˆì½”ë“œì˜ ì—¬ëŸ¬ ê°’ì— ëŒ€í•œ íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜ ì§‘í•©
  - ì—¬ëŸ¬ ì»¬ëŸ¼ì„ ì…ë ¥ë°›ì•„ ì‹ë³„ -> ë‹¤ì–‘í•œ í‘œí˜„ì‹ì„ ê° ë ˆì½”ë“œì— ì ìš© -> **'ë‹¨ì¼ê°’'** (ë³µí•© ë°ì´í„° íƒ€ì…) ìœ¼ë¡œ ì¶œë ¥ í•˜ëŠ” í•¨ìˆ˜
  - *DataFrameì˜ ì»¬ëŸ¼ì€ 'í‘œí˜„ì‹'ì´ë‹¤*
    - `expr("someCol")` == `col("someCol")` (ë™ì¼ ë™ì‘)
    - ì»¬ëŸ¼ì€ í‘œí˜„ì‹ì˜ ì¼ë¶€ ê¸°ëŠ¥ ì œê³µ
- ìŠ¤íŒŒí¬ëŠ” ì—°ì‚° ìˆœì„œë¥¼ ì§€ì •í•˜ëŠ” ë…¼ë¦¬ì  íŠ¸ë¦¬ë¡œ ì»´íŒŒì¼
  - DataFrame ì½”ë“œë‚˜ SQL í‘œí˜„ì‹ ì‘ì„± ì‹œ, ì‹¤í–‰ ì‹œì ì— ë™ì¼í•œ ë…¼ë¦¬ íŠ¸ë¦¬ë¡œ ì»´íŒŒì¼ ë˜ë¯€ë¡œ ë™ì¼í•œ ì„±ëŠ¥ ë°œíœ˜
  - ì˜ˆì‹œëŠ” `p.129` [ê·¸ë¦¼ 5-1] ë…¼ë¦¬ì  íŠ¸ë¦¬ DAG ì°¸ê³ 
  - `expr("someCol - 5")` == `col("someCol") - 5` == `expr("someCol") - 5`  (ë‹¤ ê°™ì€ íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜ ê³¼ì •)

  ```scala
  // ë™ì¼í•œ í‘œí˜„ - col(), expr()
  import org.apache.spark.sql.functions.expr
  expr("(((someCol + 5) * 200) - 6) < otherCol")

  (((col("someCol") + 5) * 200) - 6) < col("otherCol")
  ```

  ```scala
  // DataFrame ì»¬ëŸ¼ ì ‘ê·¼ (printScheme() ì•„ë‹Œ í”„ë¡œê·¸ë˜ë° ë°©ì‹)
  spark.read.format("json").load("/data/flight-data/json/2015-summary.json").columns
  // res0: Array[String] = Array(DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count)
  ```


> 'í‘œí˜„ì‹' ê³¼ 'ì»¬ëŸ¼' ì‚¬ì´ í•µì‹¬ ë‚´ìš©
> - ì»¬ëŸ¼ì€ ë‹¨ì§€ í‘œí˜„ì‹ì¼ ë¿
> - ì»¬ëŸ¼ê³¼ ì»¬ëŸ¼ì˜ íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜ì€ íŒŒì‹±ëœ í‘œí˜„ì‹ê³¼ ë™ì¼í•œ ë…¼ë¦¬ì  ì‹¤í–‰ ê³„íšìœ¼ë¡œ ì»´íŒŒì¼


### 5.3 ë ˆì½”ë“œì™€ ë¡œìš°
- ìŠ¤íŒŒí¬ì˜ 'ë¡œìš°' (=ë ˆì½”ë“œ)
  - ìŠ¤íŒŒí¬ì—ì„œ DataFrameì˜ ê° ë¡œìš°ëŠ” í•˜ë‚˜ì˜ ë ˆì½”ë“œ
  - ê°’ì„ ìƒì„±í•˜ê¸° ìœ„í•´ ì»¬ëŸ¼ í‘œí˜„ì‹ìœ¼ë¡œ Row ê°ì²´ë¥¼ ë‹¤ë£¸
  - Row ê°ì²´ëŠ” ë‚´ë¶€ ë°”ì´íŠ¸ ë°°ì—´ì„ ê°€ì§€ëŠ” ì¸í„°í˜ì´ìŠ¤ => **ì˜¤ì§ ì»¬ëŸ¼ í‘œí˜„ì‹ìœ¼ë¡œë§Œ** ë‹¤ë£° ìˆ˜ ìˆìŒ (ì™¸ë¶€ ë…¸ì¶œ X)
    ```scala
    // Row í™•ì¸
    scala> df.first()
    // res1: org.apache.spark.sql.Row = [United States,Romania,15]
    ```

- ë¡œìš° ìƒì„±
  - ê° ì»¬ëŸ¼ì— í•´ë‹¹í•˜ëŠ” ê°’ìœ¼ë¡œ ì§ì ‘ Row ê°ì²´ ìƒì„± ê°€ëŠ¥
  - ê·¸ëŸ¬ë‚˜ Row ê°ì²´ëŠ” ìŠ¤í‚¤ë§ˆ ì •ë³´ X (=> ì˜¤ì§ DataFrameë§Œ ê°€ì§)
  - => ìŠ¤í‚¤ë§ˆë‘ ê°™ì€ ìˆœì„œë¡œ ê°’ ëª…ì‹œí•´ì•¼í•¨
- ë¡œìš° ë°ì´í„° ì ‘ê·¼í•˜ë ¤ë©´ => ì›í•˜ëŠ” ìœ„ì¹˜ ì§€ì •
  - Python, R ì€ ì˜¬ë°”ë¥¸ ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ì•Œì•„ì„œ ë³€í™˜ë¨
  - Scala, Java ëŠ” í—¬í¼ ë©”ì„œë“œë‚˜ ë°ì´í„°íƒ€ì… ëª…ì‹œì  ì§€ì • í•„ìš” (Dataset API ì‚¬ìš© ì‹œ jvm ê°ì²´ ë°ì´í„° ì…‹ ì–»ì„ ìˆ˜ ìˆìŒ)
    ```scala
    import org.apache.spark.sql.Row
    val myRow = Row("Hello", null, 1, false)

    myRow(0) // type Any
    myRow(0).asInstanceOf[String] // String
    myRow.getString(0) // String
    myRow.getInt(2) // Int
    ```

    ```python
    # python ì‚¬ìš© ì‹œ
    myRow[0]
    myRow[2]
    ```


### 5.4 DataFrameì˜ íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜
> DataFrameì„ ë‹¤ë£¨ëŠ” ë°©ë²• (ì£¼ìš” ì‘ì—… 4ê°€ì§€)
>  - ë¡œìš°ë‚˜ ì»¬ëŸ¼ ì¶”ê°€
>  - ë¡œìš°ë‚˜ ì»¬ëŸ¼ ì œê±°
>  - ë¡œìš°ë¥¼ ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜í•˜ê±°ë‚˜, ê·¸ ë°˜ëŒ€ë¡œ ë³€í™˜
>  - ì»¬ëŸ¼ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ë¡œìš° ìˆœì„œ ë³€ê²½
>
> ëª¨ë“  ìœ í˜•ì˜ ì‘ì—…ì€ íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜ìœ¼ë¡œ ë³€í™˜ ê°€ëŠ¥ (ex. ëª¨ë“  ë¡œìš°ì˜ íŠ¹ì • ì»¬ëŸ¼ê°’ ë³€ê²½ í›„ ê²°ê³¼ ë°˜í™˜)

- [(1) DataFrame ìƒì„±](#1-DataFrame-ìƒì„±)
- [(2) select ì™€ selectExpr](#2-select-ì™€-selectExpr)
- [(3) ìŠ¤íŒŒí¬ ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•˜ê¸°](#3-ìŠ¤íŒŒí¬-ë°ì´í„°-íƒ€ì…ìœ¼ë¡œ-ë³€í™˜í•˜ê¸°)
- [(4) ì»¬ëŸ¼ ì¶”ê°€í•˜ê¸°](#4-ì»¬ëŸ¼-ì¶”ê°€í•˜ê¸°)
- [(5) ì»¬ëŸ¼ëª… ë³€ê²½í•˜ê¸°](#5-ì»¬ëŸ¼ëª…-ë³€ê²½í•˜ê¸°)
- [(6) ì˜ˆì•½ ë¬¸ìì™€ í‚¤ì›Œë“œ](#6-ì˜ˆì•½-ë¬¸ìì™€-í‚¤ì›Œë“œ)
- [(7) ëŒ€ì†Œë¬¸ì êµ¬ë¶„](#7-ëŒ€ì†Œë¬¸ì-êµ¬ë¶„)
- [(8) ì»¬ëŸ¼ ì œê±°í•˜ê¸°](#8-ì»¬ëŸ¼-ì œê±°í•˜ê¸°)
- [(9) ì»¬ëŸ¼ì˜ ë°ì´í„° íƒ€ì… ë³€ê²½í•˜ê¸°](#9-ì»¬ëŸ¼ì˜-ë°ì´í„°-íƒ€ì…-ë³€ê²½í•˜ê¸°)
- [(10) ë¡œìš° í•„í„°ë§í•˜ê¸°](#10-ë¡œìš°-í•„í„°ë§í•˜ê¸°)
- [(11) ê³ ìœ í•œ ë¡œìš° ì–»ê¸°](#11-ê³ ìœ í•œ-ë¡œìš°-ì–»ê¸°)
- [(12) ë¬´ì‘ìœ„ ìƒ˜í”Œ ë§Œë“¤ê¸°](#12-ë¬´ì‘ìœ„-ìƒ˜í”Œ-ë§Œë“¤ê¸°)
- [(13) ì„ì˜ ë¶„í• í•˜ê¸°](#13-ì„ì˜-ë¶„í• í•˜ê¸°)
- [(14) ë¡œìš° í•©ì¹˜ê¸°ì™€ ì¶”ê°€í•˜ê¸°](#14-ë¡œìš°-í•©ì¹˜ê¸°ì™€-ì¶”ê°€í•˜ê¸°)
- [(15) ë¡œìš° ì •ë ¬í•˜ê¸°](#15-ë¡œìš°-ì •ë ¬í•˜ê¸°)
- [(16) ë¡œìš° ìˆ˜ ì œí•œí•˜ê¸°](#16-ë¡œìš°-ìˆ˜-ì œí•œí•˜ê¸°)
- [(17) repartitionê³¼ coalesce](#17-repartitionê³¼-coalesce)
- [(18) ë“œë¼ì´ë²„ë¡œ ë¡œìš° ë°ì´í„° ìˆ˜ì§‘í•˜ê¸°](#18-ë“œë¼ì´ë²„ë¡œ-ë¡œìš°-ë°ì´í„°-ìˆ˜ì§‘í•˜ê¸°)


<br/>

#### (1) DataFrame ìƒì„±

<details><summary class="point-color-can-hover">[5.4-1] ì˜ˆì œ í¼ì¹˜ê¸° </summary>

```scala

// A. ì›ì‹œ ë°ì´í„° ì†ŒìŠ¤ -> DataFrame
val df = (spark.read.format("json").load("/data/flight-data/json/2015-summary.json"))
// (ì„ì‹œ ë·° ë“±ë¡)
df.createOrReplaceTempView("dfTable")
``

// B. (Row ê°ì²´ë¥¼ ê°€ì§€ëŠ”) Seq íƒ€ì… -> DataFrame
import org.apache.spark.sql.Row
import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}

val myManualSchema = new StructType(Array(
  new StructField("some", StringType, true),
  new StructField("col", StringType, true),
  new StructField("names", LongType, false)))
val myRows = Seq(Row("Hello", null, 1L))
// sc ê°ì²´ì˜ parallelize() ë¡œ RDD ìƒì„±
val myRDD = spark.sparkContext.parallelize(myRows)
// createDataFrame()ë¡œ DataFrame ìƒì„±
val myDf = spark.createDataFrame(myRDD, myManualSchema)
myDf.show()

// +-----+----+-----+
// | some| col|names|
// +-----+----+-----+
// |Hello|null|    1|
// +-----+----+-----+


// Scala ì‚¬ìš© ì‹œ toDF() ì‚¬ìš© ê°€ëŠ¥ (import spark.implicits._)
val myDF = Seq(("Hello", 2, 1L)).toDF("col1", "col2", "col3")
```
</details>

- ìŠ¤íŒŒí¬ì˜ implicits (import í•„ìš”, [ì°¸ê³ ](http://bit.ly/2xrFpML))
  - Scala ìŠ¤íŒŒí¬ ì½˜ì†” ì‚¬ìš© ì‹œ => Seq ë°ì´í„° íƒ€ì…ì— `toDF()` ì‚¬ìš© ê°€ëŠ¥
  - ê·¸ëŸ¬ë‚˜ null íƒ€ì…ê³¼ëŠ” ì•ˆë§ìœ¼ë¯€ë¡œ ìš´ì˜í™˜ê²½ ì‚¬ìš©ì€ ê¶Œì¥ X


> #### createDataFrame() vs toDF()
> - `createDataFrame(rowRDD: RDD[Row], schema: StructType) : DataFrame`
>   - ëª¨ë“  schema customization ê°€ëŠ¥
>   - [API docs#SparkSession](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/SparkSession.html#createDataFrame(rowRDD:org.apache.spark.rdd.RDD[org.apache.spark.sql.Row],schema:org.apache.spark.sql.types.StructType):org.apache.spark.sql.DataFrame)
> - `toDF()`
>   - ìŠ¤í‚¤ë§ˆ ì§€ì • ì—†ìŒ. schema ì¶”ë¡  (Dataset API)
>   - `import spark.implicits._` í•„ìš”
>   - [API docs#Dataset](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Dataset.html#toDF():org.apache.spark.sql.DataFrame)

#### (2) select ì™€ selectExpr

<details><summary class="point-color-can-hover">[5.4-2] ì˜ˆì œ í¼ì¹˜ê¸° </summary>

```sql
--- SQL
SELECT * FROM dataFrameTable
SELECT columnName FROM dataFrameTable
SELECT columnName * 10, otherColumn, someOtherCol as c FROM dataFrameTable
```

```bash
# select() == SELECT query
scala> df.select("DEST_COUNTRY_NAME").show(2)
+-----------------+
|DEST_COUNTRY_NAME|
+-----------------+
|    United States|
|    United States|
+-----------------+

scala> df.select("DEST_COUNTRY_NAME", "ORIGIN_COUNTRY_NAME").show(2)
+-----------------+-------------------+
|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|
+-----------------+-------------------+
|    United States|            Romania|
|    United States|            Croatia|
+-----------------+-------------------+

--- SQL
SELECT DEST_COUNTRY_NAME FROM dfTable LIMIT 2
SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME FROM dfTable LIMIT 2


# ë‹¤ì–‘í•œ ì»¬ëŸ¼ ì°¸ì¡° ë°©ë²•
# df.select(col("DEST_COUNTRY_NAME"), "DEST_COUNTRY_NAME") => ì—ëŸ¬
scala> import org.apache.spark.sql.functions.{expr, col, column}
scala> (df.select(
     |     df.col("DEST_COUNTRY_NAME"),
     |     col("DEST_COUNTRY_NAME"),
     |     column("DEST_COUNTRY_NAME"),
     |     'DEST_COUNTRY_NAME,
     |     $"DEST_COUNTRY_NAME",
     |     expr("DEST_COUNTRY_NAME"))
     |   .show(2))
+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+
|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|
+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+
|    United States|    United States|    United States|    United States|    United States|    United States|
|    United States|    United States|    United States|    United States|    United States|    United States|
+-----------------+-----------------+-----------------+-----------------+-----------------+-----------------+

```

```scala
// expr() ì˜ˆì‹œ - ì»¬ëŸ¼ëª… DEST_COUNTRY_NAME -> destination -> DEST_COUNTRY_NAME
df.select(expr("DEST_COUNTRY_NAME AS destination")).show(2)
df.select(expr("DEST_COUNTRY_NAME as destination").alias("DEST_COUNTRY_NAME")).show(2)


// select() + expr() => selectExpr()
df.selectExpr("DEST_COUNTRY_NAME as newColumnName", "DEST_COUNTRY_NAME").show(2)
// +-------------+-----------------+
// |newColumnName|DEST_COUNTRY_NAME|
// +-------------+-----------------+
// |United States|    United States|
// |United States|    United States|
// +-------------+-----------------+

(df.selectExpr(
    "*", // include all original columns
    "(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry")
  .show(2))
// +-----------------+-------------------+-----+-------------+
// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|
// +-----------------+-------------------+-----+-------------+
// |    United States|            Romania|   15|        false|
// |    United States|            Croatia|    1|        false|
// +-----------------+-------------------+-----+-------------+

df.selectExpr("avg(count)", "count(distinct(DEST_COUNTRY_NAME))").show(2)
// +-----------+---------------------------------+
// | avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|
// +-----------+---------------------------------+
// |1770.765625|                              132|
// +-----------+---------------------------------+
```
</details>


> DataFrameì„ ë‹¤ë£¨ê¸° ìœ„í•œ ëŒ€ë¶€ë¶„ì˜ íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜ ì‘ì—… í•´ê²° ê°€ëŠ¥
>
> - `select()` : ì»¬ëŸ¼ì´ë‚˜ í‘œí˜„ì‹ì„ ì‚¬ìš©
> - `selectExpr()` : ë¬¸ìì—´ í‘œí˜„ì‹ì„ ì‚¬ìš©
> - `select()`: ë©”ì„œë“œë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” `org.apache.spark.sql.function` ì— í¬í•¨ëœ ë‹¤ì–‘í•œ í•¨ìˆ˜

- DataFrame ì»¬ëŸ¼ ë‹¤ë£° ì‹œ, SQL ì‚¬ìš© ê°€ëŠ¥
- ì»¬ëŸ¼ ì°¸ì¡° ë°©ë²•ì€ ë‹¤ì–‘í•œ ë°©ë²•ì„ ì„ì–´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. 5.2 ì¬ì°¸ê³ 
  - Column ê°ì²´ë‘ ë¬¸ìì—´ì„ í•¨ê»˜ ì„ì–´ì“¸ìˆ˜ëŠ” X (ex. `df.select(col("DEST_COUNTRY_NAME"), "DEST_COUNTRY_NAME")` => ì»´íŒŒì¼ ì—ëŸ¬)
  - ê°€ì¥ ìœ ì—°í•œ ì°¸ì¡° ë°©ë²• => `expr()`
- `select()` + `expr()` íŒ¨í„´ì„ ìì£¼ ì‚¬ìš© => **`selectExpr()`** (íš¨ìœ¨ì„± â†‘)
  - <i style="color:gray">"?? : í¬í­..ìŠ¤íŒŒí¬ì˜ ì§„ì •í•œ ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ì§€.."</i>
  - ìƒˆë¡œìš´ DataFrame ìƒì„±í•˜ëŠ” ë³µì¡í•œ í‘œí˜„ì‹ ê°„ë‹¨í•˜ê²Œ í‘œí˜„ ê°€ëŠ¥
  - ëª¨ë“  ìœ íš¨í•œ ë¹„ì§‘ê³„í˜• (non-aggregating) SQL ì§€ì • ê°€ëŠ¥ (ë‹¨, ì»¬ëŸ¼ ì‹ë³„ ê°€ëŠ¥í•´ì•¼)
  - ì§‘ê³„ í•¨ìˆ˜ (avg, count ë“±) ì‚¬ìš© ê°€ëŠ¥

#### (3) ìŠ¤íŒŒí¬ ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•˜ê¸°

<details><summary class="point-color-can-hover">[5.4-3] ì˜ˆì œ í¼ì¹˜ê¸° </summary>


```scala
import org.apache.spark.sql.functions.lit

df.select(expr("*"), lit(1).as("One")).show(2)

// +-----------------+-------------------+-----+---+
// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|
// +-----------------+-------------------+-----+---+
// |    United States|            Romania|   15|  1|
// |    United States|            Croatia|    1|  1|
// +-----------------+-------------------+-----+---+
```
```sql
-- SQL ì—ì„œ ë¦¬í„°ëŸ´ì€ ìƒìˆ«ê°’ (ë™ì¼ í‘œí˜„)
SELECT *, 1 as One FROM dfTable LIMIT 2
```

</details>

- **ë¦¬í„°ëŸ´(literal)** : í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì˜ ë¦¬í„°ëŸ´ ê°’ => ìŠ¤íŒŒí¬ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ê°’ìœ¼ë¡œ ë³€í™˜
  - ë•Œë¡œëŠ” ëª…ì‹œì  ê°’ (ìƒìˆ˜ê°’, ë¹„êµì— ì‚¬ìš©í•  ë¬´ì–¸ê°€ ë“±..) ì„ ìŠ¤íŒŒí¬ì— ì „ë‹¬í•´ì•¼í•¨ => ë¦¬í„°ëŸ´ ì‚¬ìš©
  - ë¦¬í„°ëŸ´ì€ í‘œí˜„ì‹
  - ì–´ë–¤ ìƒìˆ˜ë‚˜ í”„ë¡œê·¸ë˜ë°ìœ¼ë¡œ ìƒì„±ëœ ë³€ìˆ«ê°’ì„ íŠ¹ì • ì»¬ëŸ¼ì˜ ê°’ê³¼ ë¹„êµí•  ë•Œ ì‚¬ìš©

#### (4) ì»¬ëŸ¼ ì¶”ê°€í•˜ê¸°

<details><summary class="point-color-can-hover">[5.4-4] ì˜ˆì œ í¼ì¹˜ê¸° </summary>

```scala
df.withColumn("numberOne", lit(1)).show(2)
// +-----------------+-------------------+-----+---------+
// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|
// +-----------------+-------------------+-----+---------+
// |    United States|            Romania|   15|        1|
// |    United States|            Croatia|    1|        1|
// +-----------------+-------------------+-----+---------+

(df.withColumn("withinCountry", expr("ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME"))
  .show(2))
// +-----------------+-------------------+-----+-------------+
// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|
// +-----------------+-------------------+-----+-------------+
// |    United States|            Romania|   15|        false|
// |    United States|            Croatia|    1|        false|
// +-----------------+-------------------+-----+-------------+

// ì»¬ëŸ¼ëª… ë³€ê²½ë„ ê°€ëŠ¥ (DEST_COUNTRY_NAME -> Destination)
df.withColumn("Destination", expr("DEST_COUNTRY_NAME")).columns
// res18: Array[String] = Array(DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count, Destination)
```

</details>

- `withColumn(ì»¬ëŸ¼ëª…, ê°’ì„ ìƒì„±í•  í‘œí˜„ì‹)` ì‚¬ìš©
  - ê³µì‹ì  ì»¬ëŸ¼ ì¶”ê°€ ë°©ë²•
  - ì»¬ëŸ¼ëª… ë³€ê²½í•˜ì—¬ ì¶”ê°€ë„ ê°€ëŠ¥

#### (5) ì»¬ëŸ¼ëª… ë³€ê²½í•˜ê¸°

<details><summary class="point-color-can-hover">[5.4-5] ì˜ˆì œ í¼ì¹˜ê¸° </summary>

```scala
// DEST_COUNTRY_NAME -> dest ë¡œ ë³€ê²½
df.withColumnRenamed("DEST_COUNTRY_NAME", "dest").columns
// res21: Array[String] = Array(dest, ORIGIN_COUNTRY_NAME, count)
```

</details>

- `withColumnRenamed(ì»¬ëŸ¼ëª…, ë³€ê²½í•  ë¬¸ìì—´)` ì‚¬ìš©

#### (6) ì˜ˆì•½ ë¬¸ìì™€ í‚¤ì›Œë“œ

<details><summary class="point-color-can-hover">[5.4-6] ì˜ˆì œ í¼ì¹˜ê¸° </summary>

```scala
// ì´ìŠ¤ì¼€ì´í•‘ í•„ìš” ì—†ëŠ” ì˜ˆì‹œ - ìƒˆë¡œìš´ ì»¬ëŸ¼ëª…ì„ ë‚˜íƒ€ë‚´ëŠ” ë¬¸ìì—´
import org.apache.spark.sql.functions.expr
val dfWithLongColName = df.withColumn(
  "This Long Column-Name",
  expr("ORIGIN_COUNTRY_NAME"))
// dfWithLongColName: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 2 more fields]


// ì´ìŠ¤ì¼€ì´í•‘ í•„ìš”í•œ ì˜ˆì‹œ - í‘œí˜„ì‹ìœ¼ë¡œ í•´ë‹¹ ì»¬ëŸ¼ì„ ì°¸ì¡° 
(dfWithLongColName.selectExpr(
    "`This Long Column-Name`",
    "`This Long Column-Name` as `new col`")
  .show(2))
// +---------------------+-------+
// |This Long Column-Name|new col|
// +---------------------+-------+
// |              Romania|Romania|
// |              Croatia|Croatia|
// +---------------------+-------+

dfWithLongColName.createOrReplaceTempView("dfTableLong")

// ê°™ì€ DataFrame ìƒì„±
dfWithLongColName.select(col("This Long Column-Name")).columns
dfWithLongColName.select(expr("`This Long Column-Name`")).columns
```
```sql
-- SQL (ë™ì¼ í‘œí˜„)
SELECT `This Long Column-Name`, `This Long Column-Name` as `new col`
FROM dfTableLong LIMIT 2

```

</details>

- ì˜ˆì•½ ë¬¸ì(ê³µë°±, í•˜ì´í”ˆ (-) ë“±..) ëŠ” ì»¬ëŸ¼ëª… ì‚¬ìš© ë¶ˆê°€
  - => ì‚¬ìš©í•˜ë ¤ë©´ **`` ` `` (ë°±í‹±ë¬¸ì)** ë¥¼ ì´ìš©í•œ ì´ìŠ¤ì¼€ì´í•‘(escaping) í•„ìš”
- ì˜ˆì•½ ë¬¸ìë‚˜ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•˜ëŠ” 'í‘œí˜„ì‹'ì—ëŠ” ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬ í•„ìš”
  - 'ë¬¸ìì—´'ë¡œ ëª…ì‹œì  ì»¬ëŸ¼ ì°¸ì¡° ì‹œì—ëŠ” ë¦¬í„°ëŸ´ë¡œ í•´ì„ => ì˜ˆì•½ë¬¸ì ì—†ì´ë„ ì°¸ì¡° ê°€ëŠ¥

#### (7) ëŒ€ì†Œë¬¸ì êµ¬ë¶„

<details><summary class="point-color-can-hover">[5.4-7] ì˜ˆì œ í¼ì¹˜ê¸° </summary>

```sql
set spark.sql.caseSensitive true
```

</details>

- ê¸°ë³¸ì ìœ¼ë¡œ ìŠ¤íŒŒí¬ëŠ” ëŒ€ì†Œë¬¸ìë¥¼ ê°€ë¦¬ì§€ ì•ŠìŒ
- `set spark.sql.caseSenstive true` ì„¤ì • ì‹œ êµ¬ë¶„ ê°€ëŠ¥

#### (8) ì»¬ëŸ¼ ì œê±°í•˜ê¸°

<details><summary class="point-color-can-hover">[5.4-8] ì˜ˆì œ í¼ì¹˜ê¸° </summary>

```scala
df.drop("ORIGIN_COUNTRY_NAME").columns
// res30: Array[String] = Array(DEST_COUNTRY_NAME, count)

dfWithLongColName.drop("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME")
// res32: org.apache.spark.sql.DataFrame = [count: bigint, This Long Column-Name: string]
```

</details>

- `drop(ì»¬ëŸ¼ëª…...)` ì‚¬ìš©
  - ì—¬ëŸ¬ê°œë¥¼ ì¸ìˆ˜ë¡œ ë„£ì–´ ë‹¤ìˆ˜ì˜ ì»¬ëŸ¼ì„ í•œë²ˆì— ì œê±° ê°€ëŠ¥
- `select()` ë¡œë„ ì œê±° ê°€ëŠ¥

#### (9) ì»¬ëŸ¼ì˜ ë°ì´í„° íƒ€ì… ë³€ê²½í•˜ê¸°

<details><summary class="point-color-can-hover">[5.4-9] ì˜ˆì œ í¼ì¹˜ê¸° </summary>

```scala
// count ì»¬ëŸ¼ : Integer -> String ìœ¼ë¡œ í˜•ë³€í™˜
df.withColumn("count2", col("count").cast("long"))
```
```sql
-- SQL (ë™ì¼ í‘œí˜„)
SELECT *, cast(count as string) AS count2 FROM dfTable
```

</details>

- `cast()` ì‚¬ìš©
  - íŠ¹ì • ë°ì´í„° íƒ€ì… => ë‹¤ë¥¸ ë°ì´í„° íƒ€ì…ìœ¼ë¡œ í˜•ë³€í™˜

#### (10) ë¡œìš° í•„í„°ë§í•˜ê¸°

<details><summary class="point-color-can-hover">[5.4-10] ì˜ˆì œ í¼ì¹˜ê¸° </summary>

```scala
// ë™ì¼í•œ í‘œí˜„ì‹
df.filter(col("count") < 2).show(2)
df.where("count < 2").show(2)
// +-----------------+-------------------+-----+
// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|
// +-----------------+-------------------+-----+
// |    United States|            Croatia|    1|
// |    United States|          Singapore|    1|
// +-----------------+-------------------+-----+

// ì—¬ëŸ¬ í•„í„° ì ìš© ì‹œ (ìˆœì„œ ë¬´ê´€, ë™ì‹œ ì ìš©)
df.where(col("count") < 2).where(col("ORIGIN_COUNTRY_NAME") =!= "Croatia").show(2)
// +-----------------+-------------------+-----+
// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|
// +-----------------+-------------------+-----+
// |    United States|          Singapore|    1|
// |          Moldova|      United States|    1|
// +-----------------+-------------------+-----+
```

```sql
-- SQL (ë™ì¼ í‘œí˜„)
SELECT * FROM dfTable WHERE count < 2 LIMIT 2
SELECT * FROM dfTable WHERE count < 2 AND ORIGIN_COUNTRY_NAME != "Croatia" LIMIT 2
```

</details>

- í•„í„°ë§ì„ í•˜ë ¤ë©´ ì°¸/ê±°ì§“ íŒë³„ í‘œí˜„ì‹ í•„ìš”
  - ë¬¸ìì—´ í‘œí˜„ì‹, ì»¬ëŸ¼ì„ ë‹¤ë£¨ëŠ” ê¸°ëŠ¥ìœ¼ë¡œ í‘œí˜„ì‹ ë§Œë“¦
- `where()` `filter()` ì‚¬ìš© ê°€ëŠ¥
  - ë‘ ë©”ì„œë“œ ëª¨ë‘ ê°™ì€ íŒŒë¼ë¯¸í„° íƒ€ì… ë° ê°™ì€ ì—°ì‚° ìˆ˜í–‰
  - `where()` ëŠ” SQLê³¼ ìœ ì‚¬
  - `filter()` ëŠ” Dataset APIë¥¼ ì´ìš©í•´ì„œ ì‚¬ìš©í•˜ë©´ Dataset ê° ë ˆì½”ë“œì— ì ìš© í•  í•¨ìˆ˜ë¥¼ ì‚¬ìš© ê°€ëŠ¥ (=> ìì„¸í•œê±´ 11ì¥)
- ìŠ¤íŒŒí¬ëŠ” í•„í„°ì˜ ìˆœì„œì™€ ìƒê´€ì—†ì´ ë™ì‹œì— ëª¨ë“  í•„í„°ë§ ì‘ì—… ìˆ˜í–‰
  - ê°™ì€ í‘œí˜„ì‹ì— ì—¬ëŸ¬ í•„í„° ì ìš©ì‹œ
  - ì°¨ë¡€ëŒ€ë¡œ AND í•„í„° ì—°ê²° í›„ íŒë‹¨ì€ ìŠ¤íŒŒí¬ì—ê²Œ ë§¡ê²¨ì•¼ í•¨

#### (11) ê³ ìœ í•œ ë¡œìš° ì–»ê¸°

<details><summary class="point-color-can-hover">[5.4-11] ì˜ˆì œ í¼ì¹˜ê¸° </summary>

```scala
df.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME").distinct().count()
// res41: Long = 256

df.select("ORIGIN_COUNTRY_NAME").distinct().count()
// res44: Long = 125
```

```sql
-- SQL (ë™ì¼ í‘œí˜„)
SELECT COUNT(DISTINCT(ORIGIN_COUNTRY_NAME, DEST_COUNTRY_NAME)) FROM dfTable
SELECT COUNT(DISTINCT ORIGIN_COUNTRY_NAME) FROM dfTable
```

</details>

- `distinct()` ì‚¬ìš©
  - ê³ ìœ³ê°’ì´ë‚˜ ì¤‘ë³µë˜ì§€ ì•Šì€ ê°’ì„ ì–»ëŠ” ì—°ì‚°

#### (12) ë¬´ì‘ìœ„ ìƒ˜í”Œ ë§Œë“¤ê¸°

<details><summary class="point-color-can-hover">[5.4-12] ì˜ˆì œ í¼ì¹˜ê¸° </summary>

```scala
val seed = 5
val withReplacement = false
val fraction = 0.5
df.sample(withReplacement, fraction, seed).count()
// res46: Long = 126
```

</details>

- `sample(ë³µì›ì¶”ì¶œ ì—¬ë¶€, ì¶”ì¶œ ë¹„ìœ¨, seed)` ì‚¬ìš©
  - í‘œë³¸ ë°ì´í„° ì¶”ì¶œ ë¹„ìœ¨ (<=1.0) ì§€ì • ê°€ëŠ¥
  - ë³µì› ì¶”ì¶œ (sample with replacement), ë¹„ë³µì› ì¶”ì¶œ (sample without replacement) ì‚¬ìš© ì—¬ë¶€ ì§€ì • ê°€ëŠ¥

#### (13) ì„ì˜ ë¶„í• í•˜ê¸°

<details><summary class="point-color-can-hover">[5.4-13] ì˜ˆì œ í¼ì¹˜ê¸° </summary>

```scala
// ì´í•©ì´ 1ì´ ì•„ë‹ ê²½ìš° ì„¤ì •ë˜ëŠ” default
val dataFrames = df.randomSplit(Array(0.25, 0.75), seed)
dataFrames(0).count() > dataFrames(1).count()
// res51: Boolean = false
```

</details>

- ì„ì˜ ë¶„í•  (random split) : ì›ë³¸ DataFrame ì„ ì„ì˜ í¬ê¸°ë¡œ 'ë¶„í• '
  - ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš© ì‹œ í•™ìŠµì…‹, ê²€ì¦ì…‹, í…ŒìŠ¤íŠ¸ì…‹ ë§Œë“¤ë•Œ ì£¼ë¡œ ì‚¬ìš©
- `randomSplit(ë¶„í•  ê°€ì¤‘ì¹˜ Array, seed)`
  - ì„ì˜ì„±(randomized) ì„ ê°€ì§€ë¯€ë¡œ ì‹œë“œê°’(seed) í•„ìˆ˜
  - DataFrame ë¹„ìœ¨ì€ ì´í•©ì´ 1ì´ ë˜ê²Œ ì§€ì • (ì•„ë‹ ê²½ìš° ì˜ˆì œ ë¹„ìœ¨ë¡œ ì§€ì •ë¨)

#### (14) ë¡œìš° í•©ì¹˜ê¸°ì™€ ì¶”ê°€í•˜ê¸°

<details><summary class="point-color-can-hover">[5.4-14] ì˜ˆì œ í¼ì¹˜ê¸° </summary>

```scala
import org.apache.spark.sql.Row
val schema = df.schema
val newRows = Seq(
  Row("New Country", "Other Country", 5L),
  Row("New Country 2", "Other Country 3", 1L)
)
val parallelizedRows = spark.sparkContext.parallelize(newRows)
val newDF = spark.createDataFrame(parallelizedRows, schema)

// df + newDF => ë¡œìš°ê°€ ì¶”ê°€ëœ ìƒˆë¡œìš´ ê°ì²´
(df.union(newDF)
  .where("count = 1")
  .where($"ORIGIN_COUNTRY_NAME" =!= "United States")
  .show()) // get all of them and we'll see our new rows at the end

// schema: org.apache.spark.sql.types.StructType = StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,true))
// newRows: Seq[org.apache.spark.sql.Row] = List([New Country,Other Country,5], [New Country 2,Other Country 3,1])
// parallelizedRows: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = ParallelCollectionRDD[74] at parallelize at <console>:29
// newDF: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]
// +-----------------+-------------------+-----+
// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|
// +-----------------+-------------------+-----+
// |    United States|            Croatia|    1|
// |    United States|          Singapore|    1|
// |    United States|          Gibraltar|    1|
// |    United States|             Cyprus|    1|
// |    United States|            Estonia|    1|
// |    United States|          Lithuania|    1|
// |    United States|           Bulgaria|    1|
// |    United States|            Georgia|    1|
// |    United States|            Bahrain|    1|
// |    United States|   Papua New Guinea|    1|
// |    United States|         Montenegro|    1|
// |    United States|            Namibia|    1|
// |    New Country 2|    Other Country 3|    1|
// +-----------------+-------------------+-----+

```

</details>

- DataFrameì€ ë¶ˆë³€ì„± (immutability)
  - DataFrameì„ ë³€ê²½í•˜ëŠ” ë ˆì½”ë“œ ì¶”ê°€ëŠ” ë¶ˆê°€ëŠ¥
  - => ì›ë³¸ DataFrameì„ ìƒˆë¡œìš´ DataFrameê³¼ **í†µí•©(union)** (ê²°í•©)
  - ë‹¨, í†µí•©í•˜ë ¤ëŠ” ë‘ DataFrameì€ ë°˜ë“œì‹œ ë™ì¼í•œ ìŠ¤í‚¤ë§ˆì™€ ì»¬ëŸ¼ ìˆ˜ë¥¼ ê°€ì ¸ì•¼ í•¨
- `union()`
  - í˜„ì¬ ìŠ¤í‚¤ë§ˆê°€ ì•„ë‹Œ ì»¬ëŸ¼ ìœ„ì¹˜ ê¸°ë°˜ìœ¼ë¡œ ë™ì‘ (ìë™ ì •ë ¬ X)
  - ë¡œìš°ê°€ ì¶”ê°€ëœ DataFrame ì„ ì°¸ì¡°í•˜ë ¤ë©´ ìƒˆë¡­ê²Œ ë§Œë“¤ì–´ì§„ DataFrame ì‚¬ìš©í•´ì•¼í•˜ì§€ë§Œ, <u>ë·°ë‚˜ í…Œì´ë¸”ë¡œ ë“±ë¡ ì‹œì—ëŠ” ë™ì ìœ¼ë¡œ ì°¸ì¡° ê°€ëŠ¥</u>
- ì»¬ëŸ¼ í‘œí˜„ì‹ê³¼ ë¬¸ì ë¹„êµì—´ ë¹„êµ ì‹œ
  - (ì»¬ëŸ¼ í‘œí˜„ì‹ì´ ì•„ë‹Œ) ì»¬ëŸ¼ì˜ ì‹¤ì œê°’ì„ ë¹„êµ ëŒ€ìƒ ë¬¸ìì—´ê³¼ ë¹„êµí•˜ë ¤ë©´
  - ìŠ¤ì¹¼ë¼ ì‚¬ìš© ì‹œ ë°˜ë“œì‹œ **`=!=` í•¨ìˆ˜** ì‚¬ìš©
    - `=!=`, `===` ëŠ” ìŠ¤íŒŒí¬ì˜ Column í´ë˜ìŠ¤ì— ì •ì˜ëœ í•¨ìˆ˜
  - íŒŒì´ì¬ì€ ê·¸ëŒ€ë¡œ `!=`, `==`

#### (15) ë¡œìš° ì •ë ¬í•˜ê¸°

<details><summary class="point-color-can-hover">[5.4-15] ì˜ˆì œ í¼ì¹˜ê¸° </summary>

```scala
df.sort("count").show(5)
df.orderBy("count", "DEST_COUNTRY_NAME").show(5)
df.orderBy(col("count"), col("DEST_COUNTRY_NAME")).show(5)

// ì •ë ¬ ê¸°ì¤€ ì§€ì • (desc ì˜¤ë¦„)
import org.apache.spark.sql.functions.{desc, asc}

df.orderBy(expr("count desc")).show(2)
// ì´ê±° ì™œ ë‚´ë¦¼ì°¨ìˆœì´ì•„ë‹ˆë¼ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ë‚˜ì˜¤ë‚˜... expr("count desc") ì„¤ì • ì•ˆë˜ê³  default ì •ë ¬ (asc)ë¡œ ì„¤ì •ë˜ì„œ ë‚˜ì˜¤ëŠ” ë“¯í•œë°..?
// +-----------------+-------------------+-----+
// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|
// +-----------------+-------------------+-----+
// |          Moldova|      United States|    1|
// |    United States|            Croatia|    1|
// +-----------------+-------------------+-----+

df.orderBy(desc("count"), asc("DEST_COUNTRY_NAME")).show(2)
// +-----------------+-------------------+------+
// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|
// +-----------------+-------------------+------+
// |    United States|      United States|370002|
// |    United States|             Canada|  8483|
// +-----------------+-------------------+------+
```

```sql
-- SQL
SELECT * FROM dfTable ORDER BY count DESC, DEST_COUNTRY_NAME ASC LIMIT 2
```

```scala
// sortWithinPartitions() ë¡œ íŒŒí‹°ì…˜ë³„ ì •ë ¬
(spark.read.format("json").load("/data/flight-data/json/*-summary.json")
  .sortWithinPartitions("count"))

// explain() ì‹œ
// == Physical Plan ==
// *(1) Sort [count#450L ASC NULLS FIRST], false, 0
// +- *(1) FileScan json [DEST_COUNTRY_NAME#448,ORIGIN_COUNTRY_NAME#449,count#450L] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/data/flight-data/json/2015-summary.json, file:/data/flight-data/json/2012..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:bigint>
```
</details>

- `sort()` `orderBy()` ì‚¬ìš©
  - ë‘ ë©”ì„œë“œëŠ” ì™„ì „íˆ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ë™ì‘ (`orderBy()` ë‚´ë¶€ì—ì„œ `sort()` ì‚¬ìš©)
  - ë‹¤ìˆ˜ ì»¬ëŸ¼ ì§€ì •, ì»¬ëŸ¼ í‘œí˜„ì‹, ë¬¸ìì—´ ì‚¬ìš© ê°€ëŠ¥
  - ì •ë ¬ ê¸°ì¤€ : `asc()`, `desc()` ë¡œ ëª…í™•í•œ ì§€ì • ê°€ëŠ¥ (ê¸°ë³¸ ë™ì‘ì€ ì˜¤ë¦„ì°¨ìˆœ)
- ì •ë ¬ëœ DataFrame ì˜ NULL ê°’ í‘œì‹œ ê¸°ì¤€
  - `asc_nulls_first`, `desc_nulls_first`, `asc_nulls_last`, `desc_nulls_last` ë¡œ ê¸°ì¤€ ì§€ì • ê°€ëŠ¥
- íŒŒí‹°ì…˜ ë³„ ì •ë ¬ => `sortWithinPartitions()`
  - íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜ ì²˜ë¦¬ ì „ ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•¨
  - ë” ìì„¸í•œ íŠœë‹ê³¼ ìµœì í™” ë‚´ìš©ì€ 3ë¶€ì—ì„œ

> `df.orderBy(expr("count desc"))` ?
> - count ì»¬ëŸ¼ì„ desc() (ë‚´ë¦¼ì°¨ìˆœ) ìœ¼ë¡œ ì •ë ¬ë˜ì•¼ ë§ë‚˜?
>   - ì‹¤ì œë¡œëŠ” ê·¸ë ‡ê²Œ ë™ì‘ í•˜ì§€ ì•ŠìŒ (=> ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ì •ë ¬ë¨)
> - ì˜ëª»ëœ ì˜ˆì œì¸ë“¯í•œë°..
>   - ê´€ë ¨ stackoverflow ì§ˆë¬¸ [ë§í¬ 1](https://stackoverflow.com/questions/63112281/pyspark-sort-dataframe-by-expression) / [ë§í¬ 2](https://stackoverflow.com/questions/63373479/sorting-2-columns-in-opposite-direction-does-not-work-using-expr-function)

#### (16) ë¡œìš° ìˆ˜ ì œí•œí•˜ê¸°

<details><summary class="point-color-can-hover">[5.4-16] ì˜ˆì œ í¼ì¹˜ê¸° </summary>

```scala
df.limit(5).show()

df.orderBy(expr("count desc")).limit(6).show()
// +--------------------+-------------------+-----+
// |   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|
// +--------------------+-------------------+-----+
// |               Malta|      United States|    1|
// |Saint Vincent and...|      United States|    1|
// |       United States|            Croatia|    1|
// |       United States|          Gibraltar|    1|
// |       United States|          Singapore|    1|
// |             Moldova|      United States|    1|
// +--------------------+-------------------+-----+
//
// ë’·êµ¬ë¥´ê¸°í•˜ë©´ì„œ ë´ë„ ê²°ê³¼ê°€ ì´ë ‡ê²Œ ë‚˜ì™€ì•¼í• ê±°ê°™ì€ë°...
// df.orderBy(desc("count")).limit(6).show()
// +-----------------+-------------------+------+
// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|
// +-----------------+-------------------+------+
// |    United States|      United States|370002|
// |    United States|             Canada|  8483|
// |           Canada|      United States|  8399|
// |    United States|             Mexico|  7187|
// |           Mexico|      United States|  7140|
// |   United Kingdom|      United States|  2025|
// +-----------------+-------------------+------+
```

```sql
-- SQL
SELECT * FROM dfTable LIMIT 6
```

</details>

- `limit(ë¡œìš° ìˆ˜)` ì‚¬ìš©
  - ì¶”ì¶œí•  ë¡œìš° ìˆ˜ ì œí•œí•˜ì—¬ ì¶”ì¶œ

#### (17) repartitionê³¼ coalesce

<details><summary class="point-color-can-hover">[5.4-17] ì˜ˆì œ í¼ì¹˜ê¸° </summary>

```scala
// DataFrame í˜„ì¬ íŒŒí‹°ì…˜ ìˆ˜ êµ¬í•˜ê¸°
df.rdd.getNumPartitions // 1

// ì „ì²´ ë°ì´í„° ì…”í”Œ
df.repartition(5)
// df.repartition(5).rdd.getNumPartitions => 5

// íŠ¹ì • ì»¬ëŸ¼ ê¸°ì¤€ íŒŒí‹°ì…˜ ì¬ë¶„ë°°
df.repartition(col("DEST_COUNTRY_NAME"))
// df.repartition(col("DEST_COUNTRY_NAME")).rdd.getNumPartition => 200

// íŠ¹ì • ì»¬ëŸ¼ ì§€ì • + íŒŒí‹°ì…˜ ìˆ˜ ì§€ì •
df.repartition(5, col("DEST_COUNTRY_NAME"))
// df.repartition(5, col("DEST_COUNTRY_NAME")).rdd.getNumPartitions => 5

// coalesce() ë¡œ ì…”í”Œì—†ì´ íŒŒí‹°ì…˜ ë³‘í•© (1 -> 5 -> 2)
df.repartition(5, col("DEST_COUNTRY_NAME")).coalesce(2)
// df.repartition(5, col("DEST_COUNTRY_NAME")).coalesce(2).rdd.getNumPartitions => 2
```

</details>

- ë˜ ë‹¤ë¥¸ ìµœì í™” ê¸°ë²•? => ìì£¼ í•„í„°ë§í•˜ëŠ” ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ë¶„í• 
  - íŒŒí‹°ì…”ë‹ ìŠ¤í‚¤ë§ˆì™€ íŒŒí‹°ì…˜ ìˆ˜ë¥¼ í¬í•¨í•œ í´ëŸ¬ìŠ¤í„° ì „ë°˜ì˜ ë¬¼ë¦¬ì  ë°ì´í„° êµ¬ì„± ì œì–´ ê°€ëŠ¥
- `repartition()` : ì „ì²´ ë°ì´í„° ì…”í”Œ
  - í–¥í›„ ì‚¬ìš©í•  íŒŒí‹°ì…˜ ìˆ˜ > í˜„ì¬ íŒŒí‹°ì…˜ ìˆ˜ ì¸ ê²½ìš° ì‚¬ìš© (íŒŒí‹°ì…˜ ìˆ˜ â†‘)
  - ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ íŒŒí‹°ì…˜ì„ ë§Œë“œëŠ” ê²½ìš° ì‚¬ìš©
    - ìì£¼ í•„í„°ë§ë˜ëŠ” ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ í•´ë‹¹ ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ íŒŒí‹°ì…˜ ì¬ë¶„ë°° ì¶”ì²œ
  - ì„ íƒì ìœ¼ë¡œ íŒŒí‹°ì…˜ ìˆ˜ ì§€ì • ê°€ëŠ¥
- `coalesce()` : ì „ì²´ ë°ì´í„° ì…”í”Œ ì—†ì´ íŒŒí‹°ì…˜ ë³‘í•©
  - **íŒŒí‹°ì…˜ ìˆ˜ë¥¼ ì¤„ì´ë ¤ë©´** coalesce ì‚¬ìš© (~~repartition~~ X)
- DataFrame íŒŒí‹°ì…˜ ìˆ˜ í™•ì¸ì€ `df.rdd.getNumPartitions` ë¡œ í™•ì¸

#### (18) ë“œë¼ì´ë²„ë¡œ ë¡œìš° ë°ì´í„° ìˆ˜ì§‘í•˜ê¸°

<details><summary class="point-color-can-hover">[5.4-18] ì˜ˆì œ í¼ì¹˜ê¸° </summary>

```scala
val collectDF = df.limit(10)
collectDF.take(5) // take() ëŠ” ì •ìˆ˜í˜• ê°’ì„ ì¸ìˆ˜ë¡œ ì‚¬ìš©
collectDF.show() // show() => ê²°ê³¼ë¥¼ ì •ëˆëœ í˜•íƒœë¡œ ì¶œë ¥
// +-----------------+-------------------+-----+
// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|
// +-----------------+-------------------+-----+
// |    United States|            Romania|   15|
// |    United States|            Croatia|    1|
// |    United States|            Ireland|  344|
// |            Egypt|      United States|   15|
// |    United States|              India|   62|
// |    United States|          Singapore|    1|
// |    United States|            Grenada|   62|
// |       Costa Rica|      United States|  588|
// |          Senegal|      United States|   40|
// |          Moldova|      United States|    1|
// +-----------------+-------------------+-----+


collectDF.show(5, false)
// +-----------------+-------------------+-----+
// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|
// +-----------------+-------------------+-----+
// |United States    |Romania            |15   |
// |United States    |Croatia            |1    |
// |United States    |Ireland            |344  |
// |Egypt            |United States      |15   |
// |United States    |India              |62   |
// +-----------------+-------------------+-----+

collectDF.collect()
```

</details>

- ìŠ¤íŒŒí¬ëŠ” 'ë“œë¼ì´ë²„'ì—ì„œ í´ëŸ¬ìŠ¤í„° ìƒíƒœ ì •ë³´ ìœ ì§€
  - ë¡œì»¬ í™˜ê²½ì—ì„œ ë°ì´í„° ë‹¤ë£° ë•ŒëŠ” 'ë“œë¼ì´ë²„'ë¡œ ë°ì´í„° ìˆ˜ì§‘
- ì‚¬ìš©í•´ë³¸ ë°ì´í„° ìˆ˜ì§‘ ë©”ì„œë“œ ì¼ë¶€
  - `collect()` : ì „ì²´ DataFrameì˜ ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘
  - `take()` : ìƒìœ„ Nê°œ ë¡œìš° ë°˜í™˜
  - `show()` : ì—¬ëŸ¬ ë¡œìš°ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥
- `toLocalIterator()` : ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•œ ë°˜ë³µ(iterate) ì²˜ë¦¬ë¥¼ ìœ„í•´ 'ë“œë¼ì´ë²„'ë¡œ ë¡œìš°ë¥¼ ëª¨ìœ¼ëŠ” ë°©ë²•
  - iterator(ë°˜ë³µì) ë¡œ ëª¨ë“  íŒŒí‹°ì…˜ì˜ ë°ì´í„°ë¥¼ 'ë“œë¼ì´ë²„'ì— ì „ë‹¬
  - ë°ì´í„°ì…‹ì˜ íŒŒí‹°ì…˜ì„ ì°¨ë¡€ëŒ€ë¡œ ë°˜ë³µ ì²˜ë¦¬ ê°€ëŠ¥
- ë“œë¼ì´ë²„ë¡œ ëª¨ë“  ë°ì´í„° ì»¬ë ‰ì…˜ì„ ìˆ˜ì§‘í•˜ëŠ” ê±´
  - => **ë§¤ìš° í° ë¹„ìš©** (CPU, ë©”ëª¨ë¦¬, ë„¤íŠ¸ì›Œí¬)
  - ì°¨ë¡€ëŒ€ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì²˜ë¦¬ ë¹„ìš© ì—„ì²­ë‚¨ (ë³‘ë ¬ ì—°ì‚° X)
- ë”°ë¼ì„œ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì— `collect()` ë‚˜ ë§¤ìš° í° íŒŒí‹°ì…˜ì— ëŒ€í•´ `toLocalIterator()` ì‚¬ìš© ì‹œ => ë“œë¼ì´ë²„ ë¹„ì •ìƒì  ì¢…ë£Œ


### 5.5 ì •ë¦¬
- DataFrame ê¸°ë³¸ ì—°ì‚°
- DataFrame ì‚¬ìš©ì— í•„ìš”í•œ ê°œë…, ë‹¤ì–‘í•œ ê¸°ëŠ¥

### ğŸ“’ ë‹¨ì–´ì¥
- ë¹„ê²°ì •ë¡ ì (nondeterministically) : = ë§¤ë²ˆ ë³€í•˜ëŠ”
- ETL : `ì¶”ì¶œ(Extract)` - `ë³€í™˜(Transform)` - `ì ì¬(Load)`  <i style="color:lightgray">(ì¹œìˆ™í•˜ì¥¬?)</i>
