---
title: '&#039;Spark The Definitive Guide&#039; 6ì¥ - ë°ì´í„° íƒ€ì… (ë¹„)ê³µì‹ ê°€ì´ë“œë¶'
date: 2021-02-02 00:46:40
categories: spark
tags:
  - spark
  - apache
  - book
  - study
---

<br/>

<center><p style="color:lightgray">ë¼ë–¼ ì‹œì ˆì—”,, ê°€ì´ë“œë¶ì´ í•˜ë‚˜ë©´ ë“ -ë“ í–ˆë‹¤,,, ì´ë§ì´ì•¼,,, ì´ì´ @}----</p>
<img width="300" alt="maple" src="https://user-images.githubusercontent.com/26691216/106801127-b149b700-66a4-11eb-9c8f-0802771ebe5f.jpg">
<i>'ì•„íŒŒì¹˜ ìŠ¤íŒŒí¬' ë¯¸ì¸ì¦ ë¹„ê³µì‹ ê°€ì´ë“œ ë¶<br/>
[ì „ì› ì¦ì •] 50.00 í˜ì´ì§€ í¬ì¸íŠ¸ (ìºì‹œ ì•„ì´í…œ êµ¬ë§¤ ê°€ëŠ¥)</i></center>


<br/>

<center><h2>_ _ _</h2></center>

<br/>

---

# CHAPTER 6 ë‹¤ì–‘í•œ ë°ì´í„° íƒ€ì… ë‹¤ë£¨ê¸°

CHAPTER 5 ëŠ” DataFrameì˜ ê¸°ë³¸ ê°œë…ê³¼ í•µì‹¬ ì¶”ìƒí™” ê°œë…ì„ ì†Œê°œ
CHAPTER 6 ëŠ” ìŠ¤íŒŒí¬ì˜ êµ¬ì¡°ì  ì—°ì‚°ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë‚´ìš©ì¸ **í‘œí˜„ì‹ ë§Œë“œëŠ” ë°©ë²•** ì†Œê°œ + ë‹¤ì–‘í•œ ë°ì´í„° íƒ€ì… ë‹¤ë£¨ëŠ” ë°©ë²•

> ë‹¤ì–‘í•œ ë°ì´í„° íƒ€ì…
>
> - Boolean íƒ€ì…
> - ìˆ˜ì¹˜ íƒ€ì…
> - ë¬¸ìì—´ íƒ€ì…
> - dateì™€ timestamp íƒ€ì…
> - null ê°’ ë‹¤ë£¨ê¸°
> - ë³µí•© ë°ì´í„° ì•„ì…
> - ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜


### 6.1 APIëŠ” ì–´ë””ì„œ ì°¾ì„ê¹Œ
- ì˜¤ëŠ˜ì€ ì–¸ì  ê°€ ë‚´ì¼ì´ ëœë‹¤
  - ë²„ì „ ë°”ë€Œë©´ ì±…ì˜ ë‚´ìš©ë„ ë‹¤ ì˜ˆì „ ë‚´ìš©ì´ë‹¤~ ì´ë§ì´ì•¼
  - => ë”°ë¼ì„œ <u>ë°ì´í„° ë³€í™˜ìš© í•¨ìˆ˜ ì°¾ëŠ” ë°©ë²•</u> ì„ ì•Œì•„ì•¼í•¨
- ì–´ë–»ê²Œ ì°¾ë‚˜?
  - DataFrame (Dataset) ë©”ì„œë“œ
    - DatasFrameì€ Rowíƒ€ì…ì„ ê°€ì§„ Dataset => [Dataset API](http://bit.ly/2rKkALY)
    - ë‹¤ì–‘í•œ ë©”ì„œë“œë¥¼ ì œê³µí•˜ëŠ” Dataset í•˜ìœ„ ëª¨ë“ˆ (ex. [DataFrameStateFunctions](http:bit.ly/2DPYhJC) - í†µê³„ì  í•¨ìˆ˜ ì œê³µ, [DataFrameNaFunctions](http://bit.ly/2DPAqd3) - null ë°ì´í„° ì œì–´)
  - Column ë©”ì„œë“œ
    - `alias` `contains` ë“±ì˜ ì»¬ëŸ¼ ê´€ë ¨ ë©”ì„œë“œ ì œê³µ => [Columns API](http://bit.ly/2FloFbr)
    - `org.apache.spark.sql.functions`ëŠ” ë°ì´í„° íƒ€ì… ê´€ë ¨ ë‹¤ì–‘í•œ í•¨ìˆ˜ ì œê³µ (ex. [SQL, DataFrame í•¨ìˆ˜ ë“±](http://bit.ly/2DPAycx))
- ëª¨ë“  í•¨ìˆ˜ëŠ” ë°ì´í„° ë¡œìš°ì˜ íŠ¹ì • í¬ë§·ì´ë‚˜ êµ¬ì¡°ë¥¼ ë‹¤ë¥¸ í˜•íƒœë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ ì¡´ì¬
  - í•¨ìˆ˜ë¡œ ë” ë§ì€ ë¡œìš°ë¥¼ ë§Œë“¤ê±°ë‚˜ ì¤„ì¼ ìˆ˜ O

<details><summary class="point-color-can-hover">[6.1] ì˜ˆì œ í¼ì¹˜ê¸° - ë¶„ì„ìš© DataFrame ìƒì„± ì˜ˆì œ</summary>

```scala
val df = (spark.read.format("csv")
  .option("header", "true")
  .option("inferSchema", "true")
  .load("/data/retail-data/by-day/2010-12-01.csv"))
df.printSchema()
df.createOrReplaceTempView("dfTable")
// df: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]
// root
//  |-- InvoiceNo: string (nullable = true)
//  |-- StockCode: string (nullable = true)
//  |-- Description: string (nullable = true)
//  |-- Quantity: integer (nullable = true)
//  |-- InvoiceDate: timestamp (nullable = true)
//  |-- UnitPrice: double (nullable = true)
//  |-- CustomerID: double (nullable = true)
//  |-- Country: string (nullable = true)

```

</details>


### 6.2 ìŠ¤íŒŒí¬ ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•˜ê¸°
- `lit()` : ë°ì´í„° íƒ€ì… ë³€í™˜
  - ë‹¤ë¥¸ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ ê³ ìœ  ë°ì´í„° íƒ€ì… => **ìŠ¤íŒŒí¬ ë°ì´í„° íƒ€ì…** ë³€í™˜

<details><summary class="point-color-can-hover">[6.2] ì˜ˆì œ í¼ì¹˜ê¸°</summary>

```scala
import org.apache.spark.sql.functions.lit

df.select(lit(5), lit("five"), lit(5.0))
// res9: org.apache.spark.sql.DataFrame = [5: int, five: string ... 1 more field]
```

```sql
-- SQL (SQLì€ ìŠ¤íŒŒí¬ ë°ì´í„° íƒ€ì… ë³€í™˜ í•„ìš” X. ì§ì ‘ ê°’ ì…ë ¥)
SELECT 5, "five", 5.0
```

</details>

### 6.3 ë¶ˆë¦¬ì–¸ ë°ì´í„° íƒ€ì… ë‹¤ë£¨ê¸°

- ë¶ˆë¦¬ì–¸ì€ ëª¨ë“  í•„í„°ë§ ì‘ì—…ì˜ ê¸°ë°˜ (ë°ì´í„° ë¶„ì„ì— í•„ìˆ˜)
- ë¶ˆë¦¬ì–¸ êµ¬ë¬¸ : `and`, `or`, `true`, `false`
  - ë¶ˆë¦¬ì–¸ êµ¬ë¬¸ìœ¼ë¡œ ë…¼ë¦¬ ë¬¸ë²•(true/false) ìƒì„±
- **ìŠ¤ì¹¼ë¼** ì‚¬ìš© ì‹œ ë™ë“± ì—¬ë¶€
  - `===` (ì¼ì¹˜) / `=!=` (ë¶ˆì¼ì¹˜)
  - `not()`, `equalTO()` ì‚¬ìš© ê°€ëŠ¥
- íŒŒì´ì¬, ìŠ¤ì¹¼ë¼ ëª¨ë‘ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ”
  - ê°€ì¥ ëª…í™•í•œ ë°©ë²•? => <u>ë¬¸ìì—´ í‘œí˜„ì‹ì— ì¡°ê±´ì ˆ ëª…ì‹œ</u> (ex. `where("InvoiceNo = 536353)`)
    <details><summary class="point-color-can-hover">[6.3-1] ì˜ˆì œ í¼ì¹˜ê¸° - ë¬¸ìì—´ í‘œí˜„ì‹ì— ì¡°ê±´ì ˆ ëª…ì‹œ </summary>

    ```scala
    import org.apache.spark.sql.functions.col

    // ê°™ì€ í‘œí˜„ì‹ (in Scala)
    (df.where(col("InvoiceNo").equalTo(536365))
      .select("InvoiceNo", "Description")
      .show(5, false))
    (df.where(col("InvoiceNo") === 536365)
      .select("InvoiceNo", "Description")
      .show(5, false))
    // +---------+-----------------------------------+
    // |InvoiceNo|Description                        |
    // +---------+-----------------------------------+
    // |536365   |WHITE HANGING HEART T-LIGHT HOLDER |
    // |536365   |WHITE METAL LANTERN                |
    // |536365   |CREAM CUPID HEARTS COAT HANGER     |
    // |536365   |KNITTED UNION FLAG HOT WATER BOTTLE|
    // |536365   |RED WOOLLY HOTTIE WHITE HEART.     |
    // +---------+-----------------------------------+


    // ë¬¸ìì—´ í‘œí˜„ì‹ì— ì¡°ê±´ì ˆ ëª…ì‹œ (ê°€ì¥ ëª…í™•í•œ ë°©ë²•) ì‚¬ìš©
    (df.where("InvoiceNo = 536365")
      .show(5, false))

    (df.where("InvoiceNo <> 536365")
      .show(5, false))

    // +---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+
    // |InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |
    // +---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+
    // |536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |2010-12-01 08:26:00|2.55     |17850.0   |United Kingdom|
    // |536365   |71053    |WHITE METAL LANTERN                |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|
    // |536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |2010-12-01 08:26:00|2.75     |17850.0   |United Kingdom|
    // |536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|
    // |536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|
    // +---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+

    // +---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+
    // |InvoiceNo|StockCode|Description                  |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |
    // +---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+
    // |536366   |22633    |HAND WARMER UNION JACK       |6       |2010-12-01 08:28:00|1.85     |17850.0   |United Kingdom|
    // |536366   |22632    |HAND WARMER RED POLKA DOT    |6       |2010-12-01 08:28:00|1.85     |17850.0   |United Kingdom|
    // |536367   |84879    |ASSORTED COLOUR BIRD ORNAMENT|32      |2010-12-01 08:34:00|1.69     |13047.0   |United Kingdom|
    // |536367   |22745    |POPPY'S PLAYHOUSE BEDROOM    |6       |2010-12-01 08:34:00|2.1      |13047.0   |United Kingdom|
    // |536367   |22748    |POPPY'S PLAYHOUSE KITCHEN    |6       |2010-12-01 08:34:00|2.1      |13047.0   |United Kingdom|
    // +---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+
    ```

    </details>

- ë¶ˆë¦¬ì–¸ í‘œí˜„ì‹ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
  - í•­ìƒ ëª¨ë“  í‘œí˜„ì‹ì„ `and` ë©”ì„œë“œë¡œ ë¬¶ì–´ ì°¨ë¡€ëŒ€ë¡œ í•„í„° ì ìš© í•´ì•¼ í•¨
  - why?
    - ìŠ¤íŒŒí¬ ë‚´ë¶€ì ìœ¼ë¡œ í•„í„° ì‚¬ì´ì— `and` êµ¬ë¬¸ ì¶”ê°€ ì‹œ
    - => ëª¨ë“  í•„í„°ë¥¼ í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ **ë™ì‹œì— ëª¨ë“  í•„í„° ì²˜ë¦¬** í•˜ê¸° ë•Œë¬¸
  - `and` êµ¬ë¬¸ ì‚¬ìš© ì‹œ
    - `and` êµ¬ë¬¸ìœ¼ë¡œ ì¡°ê±´ë¬¸ì„ ë§Œë“¤ ìˆ˜ëŠ” ìˆìœ¼ë‚˜,
    - ì°¨ë¡€ëŒ€ë¡œ ì¡°ê±´ ë‚˜ì—´í•˜ëŠ”ê²Œ ê°€ë…ì„±ì´ ì¢‹ìŒ
  - `or` êµ¬ë¬¸ ì‚¬ìš©ì‹œ
    - ë°˜ë“œì‹œ ë™ì¼í•œ êµ¬ë¬¸ì— ì¡°ê±´ ì •ì˜í•´ì•¼ í•¨

    <details><summary class="point-color-can-hover">[6.3-2] ì˜ˆì œ í¼ì¹˜ê¸° - ë¶ˆë¦¬ì–¸ í‘œí˜„ì‹ìœ¼ë¡œ í•„í„°ë§ ì ìš© </summary>

    ```scala
    val priceFilter = col("UnitPrice") > 600
    val descripFilter = col("Description").contains("POSTAGE")
    (df.where(col("StockCode").isin("DOT")).where(priceFilter.or(descripFilter))
      .show())
    // priceFilter: org.apache.spark.sql.Column = (UnitPrice > 600)
    // descripFilter: org.apache.spark.sql.Column = contains(Description, POSTAGE)
    // +---------+---------+--------------+--------+-------------------+---------+----------+--------------+
    // |InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|
    // +---------+---------+--------------+--------+-------------------+---------+----------+--------------+
    // |   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|
    // |   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|
    // +---------+---------+--------------+--------+-------------------+---------+----------+--------------+
    ```

    ```sql
    -- SQL (ë™ì¼ í‘œí˜„)
    SELECT * FROM dfTable WHERE StockCode in ("DOT") AND(UnitPrice > 600 OR
        instr(Description, "POSTAGE") >= 1)
    ```

    </details>

- ë¶ˆë¦¬ì–¸ í‘œí˜„ì‹ì€...
  - í•„í„°ë§ ì¡°ê±´ì—ë§Œ ì‚¬ìš©? => ğŸ™…ğŸ»â€â™€ï¸. ë¶ˆë¦¬ì–¸ì»¬ëŸ¼ìœ¼ë¡œ DataFrame í•„í„°ë§ë„ ê°€ëŠ¥
  - ë°˜ë“œì‹œ í‘œí˜„ì‹ìœ¼ë¡œ ì •ì˜í•´ì•¼? => ğŸ™…ğŸ»â€â™€ï¸. ë³„ë„ ì‘ì—…ì—†ì´ ì»¬ëŸ¼ëª…ë§Œ ì‚¬ìš©í•´ì„œ ì •ì˜ë„ ê°€ëŠ¥
  - ì‚¬ì‹¤ SQLë¡œ í‘œí˜„í•˜ëŠ”ê²Œ ë” ìµìˆ™í• ì§€ë„.. (ì„±ëŠ¥ì €í•˜ X)
- NULL ê°’ ë°ì´í„° ì²˜ë¦¬?
  - => **null-safe** ë™ì¹˜(equivalence) í…ŒìŠ¤íŠ¸
  - ex. `df.where(col("Description").eqNullSafe("hello")).show()`
- SQLì˜ `IS [NOT] DISTINCT FROM` êµ¬ë¬¸
  - ê³¼ ë™ì¼í•œ ê¸°ëŠ¥ì´ ë­˜ ë§í•˜ë‚˜.. => `isNotDistinctFrom()` `isDistinctFrom()`? (ì§€ê¸ˆë„ ì‚¬ìš©í•˜ëŠ”ì§€?)
  - since Spark 2.3 ([ì´ìŠˆ ì°¸ê³ ](https://bit.ly/2x47Obk))

  <details><summary class="point-color-can-hover">[6.3-3] ì˜ˆì œ í¼ì¹˜ê¸° - ë¶ˆë¦¬ì–¸ì»¬ëŸ¼ìœ¼ë¡œ DataFrame í•„í„°ë§</summary>

  ```scala
  // DataFrame í•„í„°ë§ ì˜ˆì œ
  val DOTCodeFilter = col("StockCode") === "DOT"
  val priceFilter = col("UnitPrice") > 600
  val descripFilter = col("Description").contains("POSTAGE")
  (df.withColumn("isExpensive", DOTCodeFilter.and(priceFilter.or(descripFilter)))
    .where("isExpensive")
    .select("unitPrice", "isExpensive").show(5))
  // DOTCodeFilter: org.apache.spark.sql.Column = (StockCode = DOT)
  // priceFilter: org.apache.spark.sql.Column = (UnitPrice > 600)
  // descripFilter: org.apache.spark.sql.Column = contains(Description, POSTAGE)
  // +---------+-----------+
  // |unitPrice|isExpensive|
  // +---------+-----------+
  // |   569.77|       true|
  // |   607.49|       true|
  // +---------+-----------+
  ```
  ```sql
  -- SQL (ë™ì¼ í‘œí˜„)
  SELECT UnitPrice, (StockCode = 'DOT' AND
    (UnitPrice > 600 OR instr(Description, "POSTAGE") >= 1)) as isExpensive
  FROM dfTable
  WHERE (StockCode = 'DOT' AND
         (UnitPrice > 600 OR instr(Description, "POSTAGE") >= 1))
  ```

  ```scala
  // í•„í„°ëŠ” SQLë¡œ ì‚¬ìš©ì‹œ ë” í¸ë¦¬í•  ìˆ˜ë„. (ì•„ë˜ ë‘ ë¬¸ì¥ ë™ì¼í•˜ê²Œ ì²˜ë¦¬ë¨)
  import org.apache.spark.sql.functions.{expr, not, col}

  (df.withColumn("isExpensive", not(col("UnitPrice").leq(250)))
    .filter("isExpensive")
    .select("Description", "UnitPrice").show(5))
  (df.withColumn("isExpensive", expr("NOT UnitPrice <= 250"))
    .filter("isExpensive")
    .select("Description", "UnitPrice").show(5))

  ```

  </details>


### 6.4 ìˆ˜ì¹˜í˜• ë°ì´í„° íƒ€ì… ë‹¤ë£¨ê¸°

- `count()` 
  - ë¹…ë°ì´í„° ì²˜ë¦¬ ì‹œ, í•„í„°ë§ ë‹¤ìŒìœ¼ë¡œ ë§ì´ ìˆ˜í–‰í•˜ëŠ” ì‘ì—…
  - ìˆ˜ì¹˜í˜• ë°ì´í„° íƒ€ì…ì„ ì‚¬ìš©í•œ ì—°ì‚° ë°©ì‹ ì •ì˜
- ìì£¼ ì‚¬ìš©í•˜ëŠ” ìˆ˜ì¹˜í˜• í•¨ìˆ˜
  - `pow(ë°‘, ì§€ìˆ˜)` (ê±°ë“­ì œê³±)
  - `round()` (ë°˜ì˜¬ë¦¼), `bround()` (ë‚´ë¦¼)
  - `corr()` => í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ ê³„ì‚° (= ë‘ ì»¬ëŸ¼ì˜ ìƒê´€ê´€ê³„)
  - `describe()` => ê´€ë ¨ ì»¬ëŸ¼ì— ëŒ€í•œ ì§‘ê³„(count), í‰ê· (mean), í‘œì¤€í¸ì°¨(stddev), ìµœì†Ÿê°’(min), ìµœëŒ“ê°’(max) ë“± ê³„ì‚°
    - í•˜ë‚˜ ì´ìƒì˜ ì»¬ëŸ¼ì—ëŒ€í•œ ìš”ì•½ í†µê³„ ê³„ì‚°
    - ê·¸ëŸ¬ë‚˜ ì½˜ì†” í™•ì¸ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©í•´ì•¼í•¨ (í†µê³„ ìŠ¤í‚¤ë§ˆëŠ” ë³€ê²½ ë  ìˆ˜ ìˆìŒ)
    - ì •í™•í•œ ìˆ˜ì¹˜ í•„ìš” ì‹œ => í•´ë‹¹ í•¨ìˆ˜ ì„í¬íŠ¸í•´ì„œ ì ìš©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ **ì§ì ‘ ì§‘ê³„**
- **StatFunction** íŒ¨í‚¤ì§€ => ë‹¤ì–‘í•œ í†µê³„ í•¨ìˆ˜ ì œê³µ
  - ë‹¤ì–‘í•œ í†µê³„ê°’ ê³„ì‚°ì— ì‚¬ìš©í•˜ëŠ” DataFrame ë©”ì„œë“œ => `df.stat` ìœ¼ë¡œ ì ‘ê·¼
  - `approxQuantile()` : ë°ì´í„° ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚° (ì •í™•í•˜ê²Œ or ê·¼ì‚¬ì¹˜ë¡œ?)
  - `crosstab()` : êµì°¨í‘œ(cross-tabulation) í™•ì¸
  - `freqItems()` : ìì£¼ ì‚¬ìš©í•˜ëŠ” í•­ëª© ìŒ í™•ì¸
    - crosstab, freqItems ë“±ì€ ê²°ê³¼ê°€ ë„ˆë¬´ í¬ë©´ ë‹¤ ì¶œë ¥ X
  - `monotonically_increasing_id()` : ëª¨ë“  ë¡œìš°ì— ê³ ìœ  ID ê°’ ì¶”ê°€ (0 ~ )
- ìŠ¤íŒŒí¬ ìƒˆë¡œìš´ ë²„ì „ ë‚˜ì˜¬ ë•Œë§ˆë‹¤ ìƒˆë¡œìš´ í•¨ìˆ˜ ìƒê¹€
  - => ìŠ¤íŒŒí¬ ê³µì‹ ë¬¸ì„œ ì°¸ì¡°
    - ex. `rand()`, `randn()` (ì„ì˜ ë°ì´í„° ìƒì„± í•¨ìˆ˜)
  - ìµœì‹  ë²„ì „ StatFunction íŒ¨í‚¤ì§€ëŠ” ì—¬ëŸ¬ ê³ ê¸‰ ê¸°ë²• ê´€ë ¨ í•¨ìˆ˜ ì œê³µí•˜ê¸°ë„
    - bloom í•„í„°ë§, sketching algorithms ..
    - ìì„¸í•œ ë‚´ìš©ì€ [API docs](http://bit.ly/2ptAiY2)
    - (ì‚¬ì‹¤ í˜„ì‹œì  ìµœì‹ ë²„ì „ì€ ì•„ë‹ˆê³  ì±…ê¸°ì¤€ ìµœì‹  2.2 ë²„ì „ì¸ ë“¯)

<details><summary class="point-color-can-hover">[6.4] ì˜ˆì œ í¼ì¹˜ê¸°</summary>

```scala
import org.apache.spark.sql.functions.{expr, pow}

// ë‘ ì»¬ëŸ¼ ëª¨ë‘ ìˆ˜ì¹˜í˜• => ê³±ì…ˆ ì—°ì‚° ê°€ëŠ¥ (+ ë§ì…ˆ, ëº„ì…ˆ)
val fabricatedQuantity = pow(col("Quantity") * col("UnitPrice"), 2) + 5
df.select(expr("CustomerId"), fabricatedQuantity.alias("realQuantity")).show(2)
// +----------+------------------+
// |CustomerId|      realQuantity|
// +----------+------------------+
// |   17850.0|239.08999999999997|
// |   17850.0|          418.7156|
// +----------+------------------+

df.selectExpr(
  "CustomerId",
  "(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity").show(2)
// +----------+------------------+
// |CustomerId|      realQuantity|
// +----------+------------------+
// |   17850.0|239.08999999999997|
// |   17850.0|          418.7156|
// +----------+------------------+


// ë°˜ì˜¬ë¦¼(round) ì˜ˆì œ
import org.apache.spark.sql.functions.{round, bround}
df.select(round(col("UnitPrice"), 1).alias("rounded"), col("UnitPrice")).show(5)
// +-------+---------+
// |rounded|UnitPrice|
// +-------+---------+
// |    2.6|     2.55|
// |    3.4|     3.39|
// |    2.8|     2.75|
// |    3.4|     3.39|
// |    3.4|     3.39|
// +-------+---------+

// ë‚´ë¦¼(bround) ì˜ˆì œ
import org.apache.spark.sql.functions.lit
df.select(round(lit("2.5")), bround(lit("2.5"))).show(2)
// +-------------+--------------+
// |round(2.5, 0)|bround(2.5, 0)|
// +-------------+--------------+
// |          3.0|           2.0|
// |          3.0|           2.0|
// +-------------+--------------+


// í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ ê³„ì‚° ì˜ˆì œ
import org.apache.spark.sql.functions.{corr}
df.stat.corr("Quantity", "UnitPrice")
df.select(corr("Quantity", "UnitPrice")).show()
// res52: Double = -0.04112314436835551
// +-------------------------+
// |corr(Quantity, UnitPrice)|
// +-------------------------+
// |     -0.04112314436835551|
// +-------------------------+
```

```sql
-- SQL (ë™ì¼ í‘œí˜„)
SELECT customerId, (POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity
FROM dfTable

SELECT round(2.5), bround(2.5)

SELECT corr(Quantity, UnitPrice) FROM dfTable
```


```scala
// ì½˜ì†”ìš© ìš”ì•½ í†µê³„ (describe)
df.describe().show()
// +-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+
// |summary|        InvoiceNo|         StockCode|         Description|          Quantity|         UnitPrice|        CustomerID|       Country|
// +-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+
// |  count|             3108|              3108|                3098|              3108|              3108|              1968|          3108|
// |   mean| 536516.684944841|27834.304044117645|                null| 8.627413127413128| 4.151946589446603|15661.388719512195|          null|
// | stddev|72.89447869788873|17407.897548583845|                null|26.371821677029203|15.638659854603892|1854.4496996893627|          null|
// |    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|               0.0|           12431.0|     Australia|
// |    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|            607.49|           18229.0|United Kingdom|
// +-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+

// 'ì§ì ‘ ì§‘ê³„'' í•„ìš” ì‹œ => í•¨ìˆ˜ ì„í¬íŠ¸
import org.apache.spark.sql.functions.{count, mean, stddev_pop, min, max}
```

```scala
// StatFunctions package (ë‹¤ì–‘í•œ í†µê³„ í•¨ìˆ˜) ì˜ˆì œ
// approxQuantile() : ë°ì´í„° ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚°
val colName = "UnitPrice"
val quantileProbs = Array(0.5)
val relError = 0.05
df.stat.approxQuantile("UnitPrice", quantileProbs, relError)
// res61: Array[Double] = Array(2.51)

// 1) crosstab() : êµì°¨í‘œ í™•ì¸
df.stat.crosstab("StockCode", "Quantity").show()
// +------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
// |StockCode_Quantity| -1|-10|-12| -2|-24| -3| -4| -5| -6| -7|  1| 10|100| 11| 12|120|128| 13| 14|144| 15| 16| 17| 18| 19|192|  2| 20|200| 21|216| 22| 23| 24| 25|252| 27| 28|288|  3| 30| 32| 33| 34| 36|384|  4| 40|432| 47| 48|480|  5| 50| 56|  6| 60|600| 64|  7| 70| 72|  8| 80|  9| 96|
// +------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+
// |             22578|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|
// |             21327|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|
// |             22064|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|
// |             21080|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|
// |             22219|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|
// |             21908|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|
// |             22818|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|
// |           15056BL|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|
// |             72817|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|
// |             22545|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|
// |             22988|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|
// |             22274|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|
// |             20750|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|
// |            82616C|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|
// |             21703|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|
// |             22899|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|
// |             22379|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|
// |             22422|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|
// |             22769|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|
// |             22585|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|
// +------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+

// 2) freqItems() : ìì£¼ ì‚¬ìš©í•˜ëŠ” í•­ëª© ìŒ í™•ì¸
df.stat.freqItems(Seq("StockCode", "Quantity")).show()
// +--------------------+--------------------+
// | StockCode_freqItems|  Quantity_freqItems|
// +--------------------+--------------------+
// |[90214E, 20728, 2...|[200, 128, 23, 32...|
// +--------------------+--------------------+

// 3) monotonically_increasing_id() : ë¡œìš°ì— ê³ ìœ  ID ê°’ ì¶”ê°€
import org.apache.spark.sql.functions.monotonically_increasing_id
df.select(monotonically_increasing_id()).show(2)
// +-----------------------------+
// |monotonically_increasing_id()|
// +-----------------------------+
// |                            0|
// |                            1|
// +-----------------------------+
```

</details>

### 6.5 ë¬¸ìì—´ ë°ì´í„° íƒ€ì… ë‹¤ë£¨ê¸°
- ë¬¸ìì—´ ë‹¤ë£¨ê¸° = ê±°ì˜ ëª¨ë“  ë°ì´í„° ì²˜ë¦¬ ê³¼ì •ì—ì„œ ë°œìƒ
  - ë¡œê·¸ íŒŒì¼ì— ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•œ ë°ì´í„° ì¶”ì¶œ, ë°ì´í„° ì¹˜í™˜, ë¬¸ìì—´ ì¡´ì¬ ì—¬ë¶€, ëŒ€/ì†Œë¬¸ì ë³€í™˜ ì²˜ë¦¬ ë“±
- ëŒ€ì†Œë¬¸ì ë³€í™˜ ì‘ì—…
  - `initcap()` => ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ ëª¨ë“  ë‹¨ì–´ì˜ ì²« ê¸€ì ëŒ€ë¬¸ìë¡œ ë³€í™˜
  - `lower()` (ì „ì²´ ì†Œë¬¸ìë¡œ ë³€í™˜) / `upper()` (ì „ì²´ ëŒ€ë¬¸ìë¡œ ë³€í™˜)
- ë¬¸ìì—´ ì£¼ë³€ ê³µë°± ì œê±°/ì¶”ê°€
  - `lpad()`, `ltrim()`, `rpad()`, `rtrim()`, `trim()`

    <details><summary class="point-color-can-hover">[6.5-1] ì˜ˆì œ í¼ì¹˜ê¸° - ë¬¸ìì—´ ë³€í™˜</summary>

    ```scala
    import org.apache.spark.sql.functions.{initcap}
    df.select(initcap(col("Description"))).show(2, false)
    // +----------------------------------+
    // |initcap(Description)              |
    // +----------------------------------+
    // |White Hanging Heart T-light Holder|
    // |White Metal Lantern               |
    // +----------------------------------+


    import org.apache.spark.sql.functions.{lower, upper}
    df.select(col("Description"),
      lower(col("Description")),
      upper(lower(col("Description")))).show(2)
    // +--------------------+--------------------+-------------------------+
    // |         Description|  lower(Description)|upper(lower(Description))|
    // +--------------------+--------------------+-------------------------+
    // |WHITE HANGING HEA...|white hanging hea...|     WHITE HANGING HEA...|
    // | WHITE METAL LANTERN| white metal lantern|      WHITE METAL LANTERN|
    // +--------------------+--------------------+-------------------------+

    import org.apache.spark.sql.functions.{lit, ltrim, rtrim, rpad, lpad, trim}
    df.select(
        ltrim(lit("    HELLO    ")).as("ltrim"),
        rtrim(lit("    HELLO    ")).as("rtrim"),
        trim(lit("    HELLO    ")).as("trim"),
        lpad(lit("HELLO"), 3, " ").as("lp"),
        rpad(lit("HELLO"), 10, " ").as("rp")).show(2)
    // +---------+---------+-----+---+----------+
    // |    ltrim|    rtrim| trim| lp|        rp|
    // +---------+---------+-----+---+----------+
    // |HELLO    |    HELLO|HELLO|HEL|HELLO     |
    // |HELLO    |    HELLO|HELLO|HEL|HELLO     |
    // +---------+---------+-----+---+----------+
    ```

    </details>

- ì •ê·œí‘œí˜„ì‹
  - ìŠ¤íŒŒí¬ëŠ” **ìë°” ì •ê·œ í‘œí˜„ì‹ ë¬¸ë²•** ì‚¬ìš©
  - `regexp_extract()`, `regexp_replace()` => ê°’ ì¶”ì¶œ ë° ì¹˜í™˜
  - `translate(column, from_string, to_string)` ì‚¬ìš©í•œ ì¹˜í™˜ ê°€ëŠ¥
  - ê°’ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë°©ë²•?
    - ìŠ¤ì¹¼ë¼ ì‚¬ìš© ì‹œ `contains()` ì‚¬ìš©
    - íŒŒì´ì¬, SQL ì‚¬ìš© ì‹œ `instr()` ì‚¬ìš©
  - ë™ì ìœ¼ë¡œ ì¸ìˆ˜ì˜ ê°œìˆ˜ê°€ ë³€í•˜ëŠ” ìƒí™©ì—ì„œëŠ”
    - ìŠ¤ì¹¼ë¼ ê³ ìœ  ê¸°ëŠ¥ `varargs()` ì‚¬ìš©
    - íŒŒì´ì¬ì€ `locate()` (ë¬¸ìì—´ ìœ„ì¹˜ë¥¼ ì •ìˆ˜ë¡œ ë°˜í™˜. ìœ„ì¹˜ëŠ” 1 ~) + ìœ„ì¹˜ ì •ë³´ ë¶ˆë¦¬ì–¸ìœ¼ë¡œ ë³€í™˜

    <details><summary class="point-color-can-hover">[6.5-2] ì˜ˆì œ í¼ì¹˜ê¸° - ë¬¸ìì—´ ë³€í™˜</summary>

    ```scala
    import org.apache.spark.sql.functions.regexp_replace
    val simpleColors = Seq("black", "white", "red", "green", "blue")
    val regexString = simpleColors.map(_.toUpperCase).mkString("|")
    // the | signifies `OR` in regular expression syntax
    //
    // df.select(
    //   regexp_replace(col("Description"), regexString, "COLOR").alias("color_clean"),
    //   col("Description")).show(2)
    // regexString: String = BLACK|WHITE|RED|GREEN|BLUE
    // +--------------------+--------------------+
    // |         color_clean|         Description|
    // +--------------------+--------------------+
    // |COLOR HANGING HEA...|WHITE HANGING HEA...|
    // | COLOR METAL LANTERN| WHITE METAL LANTERN|
    // +--------------------+--------------------+

    import org.apache.spark.sql.functions.translate
    (df.select(translate(col("Description"), "LEET", "1337"), col("Description"))
      .show(2))
    // +----------------------------------+--------------------+
    // |translate(Description, LEET, 1337)|         Description|
    // +----------------------------------+--------------------+
    // |              WHI73 HANGING H3A...|WHITE HANGING HEA...|
    // |               WHI73 M37A1 1AN73RN| WHITE METAL LANTERN|
    // +----------------------------------+--------------------+

    import org.apache.spark.sql.functions.regexp_extract
    val regexString = simpleColors.map(_.toUpperCase).mkString("(", "|", ")")
    // the | signifies OR in regular expression syntax
    df.select(
         regexp_extract(col("Description"), regexString, 1).alias("color_clean"),
         col("Description")).show(2)
    // +-----------+--------------------+
    // |color_clean|         Description|
    // +-----------+--------------------+
    // |      WHITE|WHITE HANGING HEA...|
    // |      WHITE| WHITE METAL LANTERN|
    // +-----------+--------------------+

    val containsBlack = col("Description").contains("BLACK")
    val containsWhite = col("DESCRIPTION").contains("WHITE")
    (df.withColumn("hasSimpleColor", containsBlack.or(containsWhite))
      .where("hasSimpleColor")
      .select("Description").show(3, false))
    // +----------------------------------+
    // |Description                       |
    // +----------------------------------+
    // |WHITE HANGING HEART T-LIGHT HOLDER|
    // |WHITE METAL LANTERN               |
    // |RED WOOLLY HOTTIE WHITE HEART.    |
    // +----------------------------------+


    val simpleColors = Seq("black", "white", "red", "green", "blue")
    val selectedColumns = simpleColors.map(color => {
       col("Description").contains(color.toUpperCase).alias(s"is_$color")
    }):+expr("*") // Column íƒ€ì…ì´ì—¬ì•¼ í•©ë‹ˆë‹¤
    (df.select(selectedColumns:_*).where(col("is_white").or(col("is_red")))
      .select("Description").show(3, false))
    // +----------------------------------+
    // |Description                       |
    // +----------------------------------+
    // |WHITE HANGING HEART T-LIGHT HOLDER|
    // |WHITE METAL LANTERN               |
    // |RED WOOLLY HOTTIE WHITE HEART.    |
    // +----------------------------------+
    ```

    </details>


### 6.6 ë‚ ì§œì™€ íƒ€ì„ìŠ¤íƒ¬í”„ ë°ì´í„° íƒ€ì… ë‹¤ë£¨ê¸°
- ë‚ ì§œ/ì‹œê°„ ì‚¬ìš© ì‹œ ì‹œê°„ëŒ€ (timezone) ì™€ í¬ë§·ì˜ ìœ íš¨ì„± í™•ì¸ í•„ìš”
  - => ìŠ¤íŒŒí¬ëŠ” ë‘ ê°€ì§€ ì •ë³´ë§Œ ì§‘ì¤‘ì ìœ¼ë¡œ ê´€ë¦¬
  - **ë‚ ì§œ** (date) & **íƒ€ì„ìŠ¤íƒ¬í”„** (timestamp)
  - inferSchema ì˜µì…˜ í™œì„±í™”ëœ ê²½ìš°, ë‘ ì •ë³´ë¥¼ í¬í•¨í•´ ë°ì´í„° íƒ€ì…ì„ ìµœëŒ€í•œ ì •í™•íˆ ì‹ë³„
  - ìŠ¤íŒŒí¬ëŠ” íŠ¹ì • ë‚ ì§œ í¬ë§· ëª…ì‹œ ì—†ì´ë„ ìì²´ì ìœ¼ë¡œ ì‹ë³„
- ë‚ ì§œ, ì‹œê°„ì„ ë¬¸ìì—´ë¡œ ì €ì¥ <-> ëŸ°íƒ€ì„ì— ë‚ ì§œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
  - í…ìŠ¤íŠ¸, CSV íŒŒì¼ ë‹¤ë£° ì‹œ ë§ì´ ë°œìƒí•˜ëŠ” ë°©ì‹
  - ìŠ¤íŒŒí¬ 2.1 ì´í•˜) ì‹œê°„ëŒ€ ë¯¸ì§€ì • ì‹œ, ì‹œìŠ¤í…œ ì‹œê°„ëŒ€ ê¸°ì¤€ìœ¼ë¡œ íŒŒì‹±
    - ì‹œê°„ëŒ€ ì„¤ì •? => `spark.conf.sessionLocalTimeZone` ì†ì„±ì„ ë¡œì»¬ ì‹œê°„ëŒ€ë¡œ ì§€ì • - [Java TimeZone í¬ë§·](https://bit.ly/2NcW6p2) ë”°ë¦„)
  - ìŠ¤íŒŒí¬ 2.3 ì´ìƒ) `spark.conf.set("spark.sql.session.timeZone", "UTC")` ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥
- TimestampType í´ë˜ìŠ¤ëŠ” ì´ˆ ë‹¨ìœ„ ì •ë°€ë„ê¹Œì§€ë§Œ ì§€ì›
  - ë°€ë¦¬ì„¸ì»¨ë“œ(ms), ë§ˆì´í¬ë¡œì„¸ì»¨ë“œ(Î¼s) ì§€ì› X => í•„ìš” ì‹œ Long ë°ì´í„°íƒ€ì… ì‚¬ìš©í•´ì„œ ìš°íšŒ
  - íŠ¹ì´í•œ í¬ë§·ì˜ ë‚ ì§œ/ì‹œê°„ ë°ì´í„°ë¥¼ ë‹¤ë¤„ì•¼í•œë‹¤ë©´ => ê° ë‹¨ê³„ë³„ ë°ì´í„°íƒ€ì…ê³¼ í¬ë§· ì •í™•íˆ íŒŒì•… í›„ íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜ ì ìš© í•´ì•¼í•¨
- ìŠ¤íŒŒí¬ëŠ” íŠ¹ì • ì‹œì ì— ë°ì´í„° í¬ë§·ì´ íŠ¹ì´í•˜ê²Œ ë³€í•  ìˆ˜ ìˆë‹¤
  - ì‹«ë‹¤ë©´ íŒŒì‹±ì´ë‚˜ ë³€í™˜ ì‘ì—… í•„ìš”
  - ìŠ¤íŒŒí¬ëŠ” **ìë°”ì˜ ë‚ ì§œì™€ íƒ€ì„ìŠ¤íƒ¬í”„** ì‚¬ìš© (í‘œì¤€ ì²´ê³„)
- ìì£¼ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜
  - ì˜¤ëŠ˜ ê¸°ì¤€ìœ¼ë¡œ Nì¼ ì „í›„ ë‚ ì§œ êµ¬í•˜ê¸°
    - `date_sub(ì»¬ëŸ¼, ëº„ ë‚ ì§œ ìˆ˜)` (ì±…ì—ëŠ” sum, ì˜¤íƒ€?)
    - `date_add(ì»¬ëŸ¼, ë”í•  ë‚ ì§œ ìˆ˜)` 
  - ë‘ ë‚ ì§œ ì‚¬ì´ ì°¨ì´ êµ¬í•˜ê¸°
    - `datediff(ì»¬ëŸ¼1, ì»¬ëŸ¼2)` : ë‘ ë‚ ì§œ ì‚¬ì´ ì¼ ìˆ˜ ë°˜í™˜
    - `months_between(ì»¬ëŸ¼1, ì»¬ëŸ¼2)` : ë‘ ë‚ ì§œ ì‚¬ì´ ê°œì›” ìˆ˜ ë°˜í™˜

    <details><summary class="point-color-can-hover">[6.6-1] ì˜ˆì œ í¼ì¹˜ê¸° - ë‚ ì§œ êµ¬í•˜ê¸° ë° ë¹„êµ</summary>

    ```scala
    // df.printSchema()
    // root
    //  |-- InvoiceNo: string (nullable = true)
    //  |-- StockCode: string (nullable = true)
    //  |-- Description: string (nullable = true)
    //  |-- Quantity: integer (nullable = true)
    //  |-- InvoiceDate: timestamp (nullable = true)
    //  |-- UnitPrice: double (nullable = true)
    //  |-- CustomerID: double (nullable = true)
    //  |-- Country: string (nullable = true)


    // ì˜ˆì œ1) ì˜¤ëŠ˜ ë‚ ì§œ / í˜„ì¬ íƒ€ì„ìŠ¤íƒ¬í”„ ê°’ êµ¬í•˜ê¸°
    import org.apache.spark.sql.functions.{current_date, current_timestamp}
    val dateDF = (spark.range(10)
      .withColumn("today", current_date())
      .withColumn("now", current_timestamp()))
    dateDF.createOrReplaceTempView("dateTable")

    dateDF.printSchema()
    // root
    //  |-- id: long (nullable = false)
    //  |-- today: date (nullable = false)
    //  |-- now: timestamp (nullable = false)


    // ì˜ˆì œ2) ì˜¤ëŠ˜ ê¸°ì¤€ìœ¼ë¡œ 5ì¼ ì „ ë‚ ì§œ êµ¬í•˜ê¸°
    // -- SQL : SELECT date_sub(today, 5), date_add(today, 5) FROM dateTable
    import org.apache.spark.sql.functions.{date_add, date_sub}
    dateDF.select(date_sub(col("today"), 5), date_add(col("today"), 5)).show(1)
    // +------------------+------------------+
    // |date_sub(today, 5)|date_add(today, 5)|
    // +------------------+------------------+
    // |        2021-02-01|        2021-02-11|
    // +------------------+------------------+


    // ì˜ˆì œ3) ë‘ ë‚ ì§œ ì‚¬ì´ ì°¨ì´ ì¼ìˆ˜(ê°œì›”ìˆ˜) êµ¬í•˜ê¸°
    // -- SQL : SELECT to_date('2016-01-01'), months_between('2016-01-01', '2017-01-01'),
    // datediff('2016-01-01', '2017-01-01')
    // FROM dateTable
    import org.apache.spark.sql.functions.{datediff, months_between, to_date}
    (dateDF.withColumn("week_ago", date_sub(col("today"), 7))
      .select(datediff(col("week_ago"), col("today"))).show(1))
    (dateDF.select(
        to_date(lit("2016-01-01")).alias("start"),
        to_date(lit("2017-05-22")).alias("end"))
      .select(months_between(col("start"), col("end"))).show(1))
    // +-------------------------+
    // |datediff(week_ago, today)|
    // +-------------------------+
    // |                       -7|
    // +-------------------------+

    // +--------------------------+
    // |months_between(start, end)|
    // +--------------------------+
    // |              -16.67741935|
    // +--------------------------+
    ```

    </details>

- ë‚ ì§œ ë³€í™˜ ë° íŒŒì‹±
  - ì˜¬ë°”ë¥¸ í¬ë§·ê³¼ íƒ€ì… ì‚¬ìš© ì‹œ ë§¤ìš° ì‰¬ì›€
  - ë‚ ì§œë‚˜ íƒ€ì„ìŠ¤íƒ¬í”„ íƒ€ì… ì‚¬ìš© or 'yyy-MM-dd' í¬ë§·ì— ë§ëŠ” ë¬¸ìì—´ ì§€ì •
  - `to_date()` : ë¬¸ìì—´ => ë‚ ì§œë¡œ ë³€í™˜ (option. ë‚ ì§œ í¬ë§· ì§€ì • ê°€ëŠ¥)
    - ë‚ ì§œ í¬ë§· : ìë°”ì˜ [SimpleDateFormat í´ë˜ìŠ¤ ì§€ì› í¬ë§· ](https://bit.ly/2Mz21Qc)ì‚¬ìš©
  - `to_timestamp()` : ë‚ ì§œ í¬ë§· í•„ìˆ˜ (ë¯¸ì§€ì •ì‹œ 'yyyy-MM-dd HH:mm:ss' í¬ë§· default)
- ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ?
  - => **null ë°˜í™˜** (ì—ëŸ¬ X)
  - ì˜ˆìƒì¹˜ ëª»í•œ í¬ë§·ì˜ ë°ì´í„°ê°€ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë””ë²„ê¹… ì–´ë ¤ì›€
  - ë¬¸ì œ íšŒí”¼í•  ìˆ˜ ìˆëŠ” ë°©ì‹
    - 1\. ìë°” [SimpleDateFormat](https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html) í‘œì¤€ì— ë§ì¶° ë‚ ì§œ í¬ë§· ì§€ì •
    - 2\. `to_date()`, `to_timestamp()` ì‚¬ìš©
  - ì•”ì‹œì  í˜•ë³€í™˜(implicit type casting)ì€ ìœ„í—˜ => ëª…ì‹œì ìœ¼ë¡œ ë°ì´í„° íƒ€ì… ë³€í™˜í•´ì„œ ì‚¬ìš©í•  ê²ƒ

    <details><summary class="point-color-can-hover">[6.6-2] ì˜ˆì œ í¼ì¹˜ê¸° - ë‚ ì§œ ë³€í™˜ ë° íŒŒì‹±</summary>

    ```scala
    // to_date(ë¬¸ìì—´) : ë¬¸ìì—´ -> ë‚ ì§œ 
    import org.apache.spark.sql.functions.{to_date, lit}
    (spark.range(5).withColumn("date", lit("2017-01-01"))
      .select(to_date(col("date"))).show(1))
    // +---------------+
    // |to_date(`date`)|
    // +---------------+
    // |     2017-01-01|
    // +---------------+

    // SimpleDateFormate í´ë˜ìŠ¤ ì§€ì› í¬ë§· ì‚¬ìš©í•´ì•¼
    dateDF.select(to_date(lit("2016-20-12")),to_date(lit("2017-12-11"))).show(1)
    +---------------------+---------------------+
    |to_date('2016-20-12')|to_date('2017-12-11')|
    +---------------------+---------------------+
    |                 null|           2017-12-11|
    +---------------------+---------------------+
    ```

    ```scala
    // to_date(ë¬¸ìì—´, ë‚ ì§œ í¬ë§·) => ë‚ ì§œí¬ë§· Option
    // to_timestamp(ë¬¸ìì—´, ë‚ ì§œ í¬ë§·) => ë‚ ì§œí¬ë§· í•„ìˆ˜
    import org.apache.spark.sql.functions.to_date
    val dateFormat = "yyyy-dd-MM"
    val cleanDateDF = spark.range(1).select(
        to_date(lit("2017-12-11"), dateFormat).alias("date"),
        to_date(lit("2017-20-12"), dateFormat).alias("date2"))
    cleanDateDF.createOrReplaceTempView("dateTable2")

    // cleanDateDF.show()
    // +----------+----------+
    // |      date|     date2|
    // +----------+----------+
    // |2017-11-12|2017-12-20|
    // +----------+----------+


    import org.apache.spark.sql.functions.to_timestamp
    cleanDateDF.select(to_timestamp(col("date"), dateFormat)).show()
    // +----------------------------------+
    // |to_timestamp(`date`, 'yyyy-dd-MM')|
    // +----------------------------------+
    // |               2017-11-12 00:00:00|
    // +----------------------------------+
    ```
    ```sql
    -- SQL (ê°™ì€ í‘œí˜„, to_date(), to_timestamp())
    SELECT to_date(date, 'yyyy-dd-MM'), to_date(date2, 'yyyy-dd-MM'), to_date(date)
    FROM dateTable2

    SELECT to_timestamp(date, 'yyyy-dd-MM'), to_timestamp(date2, 'yyyy-dd-MM')
    FROM dateTable2
    ```

    </details>

  - ë‚ ì§œ <-> íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜
    - SQL (easy)
      ```sql
      SELECT cast(to_date("2017-01-01", "yyyy-dd-MM") as timestamp)
      ```
    - ì˜¬ë°”ë¥¸ í¬ë§·ê³¼ íƒ€ì…ì˜ ë‚ ì§œì™€ íƒ€ì„ìŠ¤íƒ¬í”„ ì‚¬ìš© ì‹œì—ëŠ” ë§¤ìš° ì‰½ê²Œ ë¹„êµí•  ìˆ˜ ìˆë‹¤
      ```scala
      // ë‚ ì§œ, íƒ€ì„ìŠ¤íƒ¬í”„ íƒ€ì… ì‚¬ìš© or "yyyy-MM-dd" í¬ë§· ë¬¸ìì—´ ì‚¬ìš©
      cleanDateDF.filter(col("date2") > lit("2017-12-12")).show()

      // ìŠ¤íŒŒí¬ê°€ ë¦¬í„°ëŸ´ë¡œ ì¸ì‹í•˜ëŠ” ë¬¸ìì—´ ì§€ì •í•´ì„œ ë¹„êµë„ ê°€ëŠ¥
      cleanDateDF.filter(col("date2") > "'2017-12-12'").show()

      // +----------+----------+
      // |      date|     date2|
      // +----------+----------+
      // |2017-11-12|2017-12-20|
      // +----------+----------+
      ```


### 6.7 null ê°’ ë‹¤ë£¨ê¸°
- DataFrame ì—ì„œ ë¹ˆ ê°’ì€ **NULL** ë¡œ í‘œí˜„í•˜ëŠ”ê²Œ ì¢‹ë‹¤
  - ìŠ¤íŒŒí¬ì—ì„œëŠ” null ì„ ì‚¬ìš©í•´ì•¼ ìµœì í™” ìˆ˜í–‰ (ë¹ˆ ë¬¸ìì—´ X, ëŒ€ì²´ê°’ X)
- DataFrame ì—ì„œ null ì„ ë‹¤ë£¨ëŠ” ê¸°ë³¸ ë°©ì‹ => `.na`
  - DataFrameì˜ í•˜ìœ„ íŒ¨í‚¤ì§€
  - ì—°ì‚° ìˆ˜í–‰ ì¤‘ null ê°’ ì œì–´ ë°©ì‹ì„ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •í•˜ëŠ” í•¨ìˆ˜ëŠ” => 5.4.15 ë¡œìš°ì •ë ¬í•˜ê¸° / 6.3 ë¶ˆë¦¬ì–¸ ë°ì´í„° íƒ€ì… ë‹¤ë£¨ê¸° ì°¸ì¡°
- null ê°’ì„ ë‹¤ë£¨ëŠ” ë‘ê°€ì§€ ë°©ì‹
  - 1\. ëª…ì‹œì ìœ¼ë¡œ null ê°’ ì œê±°
  - 2\. ì „ì—­ or ì»¬ëŸ¼ ë‹¨ìœ„ë¡œ null ê°’ì„ íŠ¹ì • ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
  > null ê°’ì€ ëª…ì‹œì ìœ¼ë¡œ ì‚¬ìš© ê¶Œì¥.
  > ê·¸ëŸ¬ë‚˜ null ê°’ì„ í—ˆìš©í•˜ì§€ ì•ŠëŠ” ì»¬ëŸ¼ ì„ ì–¸í•´ë„ **ê°•ì œì„± ì—†ìŒ**
  >
  > - ì¦‰, notnull ì»¬ëŸ¼ì´ì—¬ë„ null ê°’ì´ ìˆì„ ìˆ˜ ìˆë‹¤
  > - nullable ì†ì„±ì€ ìŠ¤íŒŒí¬ SQL ì˜µí‹°ë§ˆì´ì €ê°€ í•´ë‹¹ ì»¬ëŸ¼ì„ ì œì–´í•˜ëŠ” ë™ì‘ì„ ë‹¨ìˆœí•˜ê²Œ ë„ìš¸ ë¿

- `coalesce()`
  - ì¸ìˆ˜ì˜ ì—¬ëŸ¬ ì»¬ëŸ¼ ì¤‘ null ì´ ì•„ë‹Œ ì²«ë²ˆì§¸ ê°’ ë°˜í™˜
  - ëª¨ë“  ì»¬ëŸ¼ì´ nullì´ ì•„ë‹Œ ê°’ì„ ê°€ì§€ë©´ ì²« ë²ˆì§¸ ì»¬ëŸ¼ ê°’ ë°˜í™˜

    <details><summary class="point-color-can-hover">[6.7-1] ì˜ˆì œ í¼ì¹˜ê¸° - coalesce()</summary>

    ```scala
    import org.apache.spark.sql.functions.coalesce

    // Description ì»¬ëŸ¼ ê°’ null ì²´í¬
    //  1. nullì´ë©´ CustomerId ê°’ ë°˜í™˜
    //  2. nullì´ ì•„ë‹ˆë©´ Description ì»¬ëŸ¼ ê°’ ë°˜í™˜
    df.select(coalesce(col("Description"), col("CustomerId"))).show()
    ```

    </details>

- SQL í•¨ìˆ˜
  - `ifnull()` : ì²« ë²ˆì§¸ ê°’ì´ nullì´ë©´ ë‘ ë²ˆì§¸ ê°’ ë°˜í™˜, nullì´ ì•„ë‹ˆë©´ ì²« ë²ˆì§¸ ê°’ ë°˜í™˜
  - `nullif()` : ë‘ ê°’ì´ ê°™ìœ¼ë©´ null ë°˜í™˜, ë‹¤ë¥´ë©´ ì²« ë²ˆì§¸ ê°’ ë°˜í™˜
  - `nvl()` : ì²« ë²ˆì§¸ ê°’ì´ nullì´ë©´ ë‘ ë²ˆì§¸ ê°’ ë°˜í™˜, nullì´ ì•„ë‹ˆë©´ ì²« ë²ˆì§¸ ê°’ ë°˜í™˜
  - `nvl2()` : ì²« ë²ˆì§¸ ê°’ì´ nullì´ ì•„ë‹ˆë©´ ë‘ ë²ˆì§¸ ê°’ ë°˜í™˜, nullì´ë©´ ì„¸ ë²ˆì§¸ ê°’ ë°˜í™˜
  ```kotlin
  // ì´í•´í•˜ëŠ” ìš©ë„.. like this
  fun ifnull(first: Any, default: Any) = if (first != null) first else default
  fun nullif(first: Any, second: Any) = if (first != second) first else null
  fun nvl(first: Any, default: Any) = if (first != null) first else default
  fun nvl2(first: Any, notnull_return: Any, null_return: Any) = if (first != null) notnull_return else null_return
  ```

    <details><summary class="point-color-can-hover">[6.7-2] ì˜ˆì œ í¼ì¹˜ê¸° - SQL í•¨ìˆ˜</summary>

    ```sql
    SELECT
      ifnull(null, 'return_value'),
      nullif('value', 'value'),
      nvl(null, 'return_value'),
      nvl2('not_null', 'return_value', "else_value")
    FROM dfTable LIMIT 1
    
    -- +------------+----+------------+------------+
    -- |           a|   b|           c|           d|
    -- +------------+----+------------+------------+
    -- |return_value|null|return_value|return_value|
    -- +------------+----+------------+------------+
    ```

    </details>


- `drop()`
  - null ê°’ì„ ê°€ì§„ ë¡œìš°ë¥¼ ëª¨ë“  ë¡œìš° ì œê±°
  - ì¸ìˆ˜
    - `any` (í•˜ë‚˜ë¼ë„ nullì´ë©´ ì œê±°) / `all` (ëª¨ë“  ì»¬ëŸ¼ì´ null ë˜ëŠ” NaNì´ë©´ ì œê±°)
    - ë°°ì—´ í˜•íƒœ ì»¬ëŸ¼ë„ ì¸ìˆ˜ë¡œ ì „ë‹¬ ê°€ëŠ¥
  - SQL ì‚¬ìš© ì‹œ ì»¬ëŸ¼ë³„ë¡œ ìˆ˜í–‰í•´ì•¼í•¨

    <details><summary class="point-color-can-hover">[6.7-3] ì˜ˆì œ í¼ì¹˜ê¸° - drop()</summary>

    ```scala
    df.na.drop()
    df.na.drop("any") // í•˜ë‚˜ë¼ë„ ì»¬ëŸ¼ì´ null (ë˜ëŠ” NaN) ì´ë©´ ë¡œìš° ì œê±°
    df.na.drop("all") // ëª¨ë“  ì»¬ëŸ¼ì´ null (ë˜ëŠ” NaN) ì´ë©´ ë¡œìš° ì œê±°

    df.na.drop("all", Seq("StockCode", "InvoiceNo")) // ì»¬ëŸ¼(ë°°ì—´í˜•íƒœ) ì „ë‹¬ ê°€ëŠ¥
    ```
    ```sql
    -- SQL ì‚¬ìš© ì‹œ ì»¬ëŸ¼ ë³„ ìˆ˜í–‰í•´ì•¼
    SELECT * FROM dfTable WHERE Description IS NOT NULL
    ```

    </details>

- `fill()`
  - í•˜ë‚˜ ì´ìƒì˜ ì»¬ëŸ¼ì„ íŠ¹ì • ê°’ìœ¼ë¡œ ì±„ì›€
  - ì¸ìˆ˜
    - ì±„ì›Œë„£ì„ ê°’, ì»¬ëŸ½ ì§‘í•©ìœ¼ë¡œ êµ¬ì„±ëœ ë§µ
    - ì»¬ëŸ¼ëª… ë°°ì—´ë¡œ ì¸ìˆ˜ ì‚¬ìš© ë° ë‹¤ìˆ˜ ì»¬ëŸ¼ì—ë„ ì ìš© ê°€ëŠ¥ (ex. `df.na.fill(5:Integer)`, `df.na.fill(5:Double)`)
  - ìŠ¤ì¹¼ë¼ Map íƒ€ì… ì‚¬ìš©ë„ ì¸ìˆ˜ë¡œ ê°€ëŠ¥ (key:value = ì»¬ëŸ¼ëª…:nullê°’ì„ ì±„ìš¸ ê°’)

    <details><summary class="point-color-can-hover">[6.7-4] ì˜ˆì œ í¼ì¹˜ê¸° - fill()</summary>

    ```scala
    df.na.fill("All Null values become this string")

    // ë‹¤ìˆ˜ ì»¬ëŸ¼ì— ì ìš©
    df.na.fill(5, Seq("StockCode", "InvoiceNo"))

    // Map íƒ€ì…ìœ¼ë¡œ ë‹¤ìˆ˜ ì»¬ëŸ¼ì— ì ìš©
    val fillColValues = Map("StockCode" -> 5, "Description" -> "No Value")
    df.na.fill(fillColValues)
    ```

    </details>

- `relace()`
  - ì¡°ê±´ì— ë”°ë¼ ë‹¤ë¥¸ ê°’ìœ¼ë¡œ ëŒ€ì²´
  - ë‹¨, ë³€ê²½í•˜ê³ ìí•˜ëŠ” ê°’ == ì›ë˜ ê°’ ë°ì´í„° íƒ€ì… ê°™ì•„ì•¼

    <details><summary class="point-color-can-hover">[6.7-5] ì˜ˆì œ í¼ì¹˜ê¸° - replace()</summary>

    ```scala
    df.na.replace("Description", Map("" -> "UNKNOWN"))
    ```

    </details>

### 6.8 ì •ë ¬í•˜ê¸°
- `asc_nulls_first()`, `desc_nulls_first()`, `asc_nulls_last()`, `desc_nulls_last()`
  - DataFrame ì •ë ¬ ì‹œ null ê°’ í‘œì‹œ ê¸°ì¤€ ì§€ì • ê°€ëŠ¥
- (=> [5.4.15 - ë¡œìš°ì •ë ¬í•˜ê¸°](https://minsw.github.io/2021/01/26/Spark-The-Definitive-Guide-5%EC%9E%A5/#15-%EB%A1%9C%EC%9A%B0-%EC%A0%95%EB%A0%AC%ED%95%98%EA%B8%B0) ë‹¤ì‹œ ì°¸ê³ ~)

### 6.9 ë³µí•© ë°ì´í„° íƒ€ì… ë‹¤ë£¨ê¸°
- ë³µí•© ë°ì´í„° íƒ€ì… : êµ¬ì¡°ì²´ (struct), ë°°ì—´ (array), ë§µ(map)

- êµ¬ì¡°ì²´ = DataFrame ë‚´ë¶€ì˜ DataFrame
  - ì¿¼ë¦¬ë¬¸ì—ì„œ ë‹¤ìˆ˜ì˜ ì»¬ëŸ¼ì„ ê´„í˜¸ë¡œ ë¬¶ì–´ì„œ => êµ¬ì¡°ì²´ ë§Œë“¦
  - ë³µí•© ë°ì´í„° íƒ€ì…ì„ ê°€ì§„ DataFrame ì‚¬ìš©
    - => ë‹¤ë¥¸ DataFrame ì¡°íšŒí•˜ëŠ”ê²ƒê³¼ ë™ì¼í•˜ê²Œ ì‚¬ìš© ê°€ëŠ¥
    - ì°¨ì´ì ì€ ë¬¸ë²•ì— ì  (.) ì‚¬ìš© or `getField()` ì‚¬ìš©
    - `*` ë¬¸ìë¡œ ëª¨ë“  ê°’ ì¡°íšŒ ê°€ëŠ¥ (ëª¨ë“  ì»¬ëŸ¼ì„ ìµœìƒìœ„ ìˆ˜ì¤€ìœ¼ë¡œ ëŒì–´ì˜¬ë¦¬ê¸° ê°€ëŠ¥?)

  <details><summary class="point-color-can-hover">[6.8,1] ì˜ˆì œ í¼ì¹˜ê¸° - êµ¬ì¡°ì²´</summary>

  ```scala
  df.selectExpr("(Description, InvoiceNo) as complex", "*")
  df.selectExpr("struct(Description, InvoiceNo) as complex", "*")

  import org.apache.spark.sql.functions.struct
  val complexDF = df.select(struct("Description", "InvoiceNo").alias("complex"))
  complexDF.createOrReplaceTempView("complexDF")

  // ì¡°íšŒ ì‹œ ì (.) ë˜ëŠ” getField() ì‚¬ìš©
  complexDF.select("complex.Description")
  complexDF.select(col("complex").getField("Description"))

  // * ë¡œ ëª¨ë“  ê°’ ì¡°íšŒ ê°€ëŠ¥
  complexDF.select("complex.*")
  ```
  ```sql
  SELECT complex.* FROM complexDF
  ```

  </details>

- ë°°ì—´
  - example) í•´ë‹¹í•˜ëŠ” ì»¬ëŸ¼ì˜ ëª¨ë“  ë‹¨ì–´ë¥¼ í•˜ë‚˜ì˜ ë¡œìš°ë¡œ ë³€í™˜
  - `split(target, delimiter)` : êµ¬ë¶„ì ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë°°ì—´ë¡œ ë³€í™˜
    - ë³µí•© ë°ì´í„° íƒ€ì…ì„ ë˜ ë‹¤ë¥¸ ì»¬ëŸ¼ì²˜ëŸ¼ ë‹¤ë£° ìˆ˜ ìˆëŠ” ê¸°ëŠ¥
  - ë°°ì—´ì˜ ê¸¸ì´ : ë°°ì—´ size ì¡°íšŒí•´ì„œ ê¸¸ì´ ì•Œ ìˆ˜ ìˆìŒ
  - `array_contains()` : ë°°ì—´ì— íŠ¹ì • ê°’ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ ê°€ëŠ¥
    - í•˜ì§€ë§Œ ì‹œë‚˜ë¦¬ì˜¤ ì™„ì„±ì€ ë¶ˆê°€ëŠ¥
  - `explode(ë°°ì—´íƒ€ì… ì¹¼ëŸ¼)` : ì¸ìˆ˜ì˜ ì»¬ëŸ¼ ë°°ì—´ ê°‘ì… í¬í•¨ëœ ëª¨ë“  ê°’ì„ ë¡œìš°ë¡œ ë³€í™˜ (ë‚˜ë¨¸ì§€ ì»¬ëŸ¼ ê°’ì€ ì¤‘ë³µë˜ì–´ í‘œì‹œ)

  <details><summary class="point-color-can-hover">[6.8.2] ì˜ˆì œ í¼ì¹˜ê¸° - ë°°ì—´</summary>

  ```scala
  // split() : ë°°ì—´ë¡œ ë³€í™˜
  import org.apache.spark.sql.functions.split
  df.select(split(col("Description"), " ")).show(2)
  // +---------------------+
  // |split(Description,  )|
  // +---------------------+
  // | [WHITE, HANGING, ...|
  // | [WHITE, METAL, LA...|
  // +---------------------+

  (df.select(split(col("Description"), " ").alias("array_col"))
    .selectExpr("array_col[0]").show(2))
  // +------------+
  // |array_col[0]|
  // +------------+
  // |       WHITE|
  // |       WHITE|
  // +------------+


  // size() : ë°°ì—´ì˜ ê¸¸ì´
  import org.apache.spark.sql.functions.size
  df.select(size(split(col("Description"), " "))).show(2) // shows 5 and 3
  // +---------------------------+
  // |size(split(Description,  ))|
  // +---------------------------+
  // |                          5|
  // |                          3|
  // +---------------------------+


  // array_contains() : ë°°ì—´ì— íŠ¹ì •ê°’ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
  import org.apache.spark.sql.functions.array_contains
  df.select(array_contains(split(col("Description"), " "), "WHITE")).show(2)
  // +--------------------------------------------+
  // |array_contains(split(Description,  ), WHITE)|
  // +--------------------------------------------+
  // |                                        true|
  // |                                        true|
  // +--------------------------------------------+


  // explode() : ì…ë ¥ëœ ì»¬ëŸ¼ì˜ ë°°ì—´ê°’(split(Description) ê²°ê³¼ë¬¼) ì— í¬í•¨ëœ ëª¨ë“  ê°’ì„ ë¡œìš°ë¡œ ë³€í™˜ (ë‚˜ë¨¸ì§€ê°’(InvoiceNo)ì€ ì¤‘ë³µ)
  import org.apache.spark.sql.functions.{split, explode}
  (df.withColumn("splitted", split(col("Description"), " "))
    .withColumn("exploded", explode(col("splitted")))
    .select("Description", "InvoiceNo", "exploded").show(2))
  // +--------------------+---------+--------+
  // |         Description|InvoiceNo|exploded|
  // +--------------------+---------+--------+
  // |WHITE HANGING HEA...|   536365|   WHITE|
  // |WHITE HANGING HEA...|   536365| HANGING|
  // +--------------------+---------+--------+
  ```

  ```sql
  -- SQL

  -- split()
  SELECT split(Description, ' ') FROM dfTable

  SELECT split(Description, ' ')[0] FROM dfTable

  -- array_contains()
  SELECT array_contains(split(Description, ' '), 'WHITE') FROM dfTable

  -- explode()
  SELECT Description, InvoiceNo, exploded
  FROM (SELECT *, split(Description, " ") as splitted FROM dfTable)
  LATERAL VIEW explode(splitted) as exploded
  ```

  </details>

- ë§µ
  - `map()` + í‚¤-ê°’ ìŒ
  - ì í•©í•œ í‚¤ë¡œ ë°ì´í„° ì¡°íšŒ ê°€ëŠ¥, ì—†ì„ ì‹œ null ë°˜í™˜
  - map íƒ€ì… ë¶„í•´ -> ì»¬ëŸ¼ ë³€í™˜ ê°€ëŠ¥

  <details><summary class="point-color-can-hover">[6.8.3] ì˜ˆì œ í¼ì¹˜ê¸° - ë§µ</summary>

  ```scala
  import org.apache.spark.sql.functions.map
  df.select(map(col("Description"), col("InvoiceNo")).alias("complex_map")).show(2)
  // +--------------------+
  // |         complex_map|
  // +--------------------+
  // |[WHITE HANGING HE...|
  // |[WHITE METAL LANT...|
  // +--------------------+

  // í‚¤ë¡œ ë°ì´í„° ì¡°íšŒ (ì—†ì„ ì‹œ null ë°˜í™˜)
  (df.select(map(col("Description"), col("InvoiceNo")).alias("complex_map"))
    .selectExpr("complex_map['WHITE METAL LANTERN']").show(2))
  // +--------------------------------+
  // |complex_map[WHITE METAL LANTERN]|
  // +--------------------------------+
  // |                            null|
  // |                          536365|
  // +--------------------------------+

  // map íƒ€ì… ë¶„í•´ -> ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜ ê°€ëŠ¥
  (df.select(map(col("Description"), col("InvoiceNo")).alias("complex_map"))
    .selectExpr("explode(complex_map)").show(2))
  // +--------------------+------+
  // |                 key| value|
  // +--------------------+------+
  // |WHITE HANGING HEA...|536365|
  // | WHITE METAL LANTERN|536365|
  // +--------------------+------+
  ```

  ```sql
  -- SQL
  SELECT map(Description, InvoiceNo) as complex_map FROM dfTable
  WHERE Description IS NOT NULL
  ```

  </details>

### 6.10 JSON ë‹¤ë£¨ê¸°
- ìŠ¤íŒŒí¬ì—ì„œ JSON ë°ì´í„° ë‹¤ë£¨ê¸° ìœ„í•œ ê³ ìœ  ê¸°ëŠ¥ ì œê³µ
  - ë¬¸ìì—´ í˜•íƒœ JSON ì¡°ì‘, JSON íŒŒì‹±, JSON ê°ì²´ë¡œ ë³€í™˜ ë“±
- `get_json_object()`
  - JSON ê°ì²´ (ë”•ì…”ë„ˆë¦¬, ë°°ì—´) ì¸ë¼ì¸ ì¿¼ë¦¬ë¡œ ì¡°íšŒ ê°€ëŠ¥
  - ì¤‘ì²© ì—†ëŠ” ë‹¨ì¼ JSONì¼ ì‹œ, `json_tuble` ì‚¬ìš© ê°€ëŠ¥
- `to_json()` : StructType -> JSON ë¬¸ìì—´. ë°ì´í„° ì†ŒìŠ¤ì™€ ë™ì¼í•œ í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬(ë§µ) ì¸ìë¡œ ì‚¬ìš© ê°€ëŠ¥
- `from_json()` : JSON ë¬¸ìì—´ -> ê°ì²´. ë‹¨ ìŠ¤í‚¤ë§ˆ ì§€ì • í•„ìˆ˜ (option. ë§µ ë°ì´í„° íƒ€ì… ì˜µì…˜)

<details><summary class="point-color-can-hover">[6.10] ì˜ˆì œ í¼ì¹˜ê¸°</summary>

```scala
// JSON ì»¬ëŸ¼ ìƒì„±
val jsonDF = spark.range(1).selectExpr("""
  '{"myJSONKey" : {"myJSONValue" : [1, 2, 3]}}' as jsonString""")

// get_json_object() ë¡œ JSON ê°ì²´ ì¡°íšŒ
import org.apache.spark.sql.functions.{get_json_object, json_tuple}
jsonDF.select(
    get_json_object(col("jsonString"), "$.myJSONKey.myJSONValue[1]") as "column",
    json_tuple(col("jsonString"), "myJSONKey")).show(2)

// => SQL ì‚¬ìš©í•œ ì²˜ë¦¬ (ë™ì¼ í‘œí˜„)
jsonDF.selectExpr(
  "get_json_object(jsonString, '$.myJSONKey.myJSONValue[1]') as column", 
  "json_tuple(jsonString, 'myJSONKey')").show(2)
// +------+--------------------+
// |column|                  c0|
// +------+--------------------+
// |     2|{"myJSONValue":[1...|
// +------+--------------------+


// to_json() : StructType -> JSON ë¬¸ìì—´
import org.apache.spark.sql.functions.to_json
(df.selectExpr("(InvoiceNo, Description) as myStruct")
  .select(to_json(col("myStruct"))))


// from_json() : JSON ë¬¸ìì—´ -> ê°ì²´
import org.apache.spark.sql.functions.from_json
import org.apache.spark.sql.types._
val parseSchema = new StructType(Array(
  new StructField("InvoiceNo",StringType,true),
  new StructField("Description",StringType,true)))
(df.selectExpr("(InvoiceNo, Description) as myStruct")
  .select(to_json(col("myStruct")).alias("newJSON"))
  .select(from_json(col("newJSON"), parseSchema), col("newJSON")).show(2))
// +----------------------+--------------------+
// |jsontostructs(newJSON)|             newJSON|
// +----------------------+--------------------+
// |  [536365, WHITE HA...|{"InvoiceNo":"536...|
// |  [536365, WHITE ME...|{"InvoiceNo":"536...|
// +----------------------+--------------------+
```

</details>

### 6.11 ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜
- **ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜** (user defined function, **UDF**)
  - ìŠ¤íŒŒí¬ì˜ ê°€ì¥ ê°•ë ¥í•œ ê¸°ëŠ¥ ì¤‘ í•˜ë‚˜
  - íŒŒì´ì¬, ìŠ¤ì¹¼ë¼, ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“± ì‚¬ìš© => ì‚¬ìš©ìê°€ ì›í•˜ëŠ” í˜•íƒœë¡œ íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜ ìƒì„±
- íŠ¹ì§•
  - í•˜ë‚˜ ì´ìƒì˜ ì»¬ëŸ¼ì„ ì…ë ¥/ë°˜í™˜ ê°€ëŠ¥
  - ìŠ¤íŒŒí¬ UDFëŠ” ë‹¤ì–‘í•œ ì–¸ì–´ë¡œ ê°œë°œ ê°€ëŠ¥
  - ë ˆì½”ë“œ ë³„ë¡œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ì´ë¯€ë¡œ, ë…íŠ¹í•˜ê±°ë‚˜ ë„ë©”ì¸ íŠ¹í™” (DSL) ì–¸ì–´ ì‚¬ìš© X
  - => ê¸°ë³¸ì ìœ¼ë¡œ íŠ¹ì • SparkSessionì´ë‚˜ Contextì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ <u>ì„ì‹œ í•¨ìˆ˜ í˜•íƒœë¡œ ë“±ë¡</u>
- ë‹¤ì–‘í•œ ì–¸ì–´ë¡œ UDF ê°œë°œ ê°€ëŠ¥
  - ê·¸ëŸ¬ë‚˜ ì–¸ì–´ë³„ë¡œ ì„±ëŠ¥ì— ì˜í–¥ ì¡´ì¬
    - ì˜ˆì œ ì°¸ê³ 
    - í•¨ìˆ˜ë¥¼ ë§Œë“¤ê³  ëª¨ë“  ì›Œì»¤ ë…¸ë“œì—ì„œ í•´ë‹¹ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ìŠ¤íŒŒí¬ì— ë“±ë¡
      - ìŠ¤íŒŒí¬ëŠ” ë“œë¼ì´ë²„ì—ì„œ í•¨ìˆ˜ ì§ë ¬í™” -> ë„¤íŠ¸ì›Œí¬ í†µí•´ì„œ ëª¨ë“  ìµìŠ¤íí„° í”„ë¡œì„¸ìŠ¤ë¡œ ì „ë‹¬
      - (ì–¸ì–´ì— ê´€ê³„ì—†ì´ ë°œìƒí•˜ëŠ” ê³¼ì •)
    - í•¨ìˆ˜ë¥¼ ê°œë°œí•œ ì–¸ì–´ì— ë”°ë¼ ê¸°ë³¸ì ìœ¼ë¡œ ë™ì‘í•˜ëŠ” ë°©ì‹ì´ ë‹¬ë¼ì§
      - ì• ì´ˆì— ìŠ¤ì¹¼ë¼, ìë°” ì‚¬ìš© ì‹œ JVM í™˜ê²½ì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥
        - ìŠ¤íŒŒí¬ ë‚´ì¥í•¨ìˆ˜ ì¥ì  ì‚¬ìš© X => ì„±ëŠ¥ â†“
        - ë§ì€ ê°ì²´ ìƒì„±ì‹œì—ë„ ì„±ëŠ¥ ë¬¸ì œ
      - íŒŒì´ì¬ ì‚¬ìš© ì‹œ ëª¨ë“  ë°ì´í„°ë¥¼ ì§ë ¬í™”í•˜ê³ , íŒŒì´ì¬ í”„ë¡œì„¸ìŠ¤ì— ìˆëŠ” ë°ì´í„°ì˜ ë¡œìš°ë§ˆë‹¤ í•¨ìˆ˜ ì‹¤í–‰ ë° JVMê³¼ ìŠ¤íŒŒí¬ì— ì²˜ë¦¬ ê²°ê³¼ë¥¼ ë°˜í™˜
        - ì¼ë‹¨ ì§ë ¬í™” ê³¼ì •ì—ì„œ í° ë¶€í•˜ ë°œìƒ
        - ë°ì´í„°ê°€ íŒŒì´ì¬ìœ¼ë¡œ ì „ë‹¬ë˜ë©´ ìŠ¤íŒŒí¬ì—ì„œ ì›Œì»¤ ë©”ëª¨ë¦¬ ê´€ë¦¬ì˜ ì–´ë ¤ì›€
    - => ë”°ë¼ì„œ ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ëŠ” **ìë°”ë‚˜ ìŠ¤ì¹¼ë¼ë¡œ ì‘ì„±** ì„ ê¶Œì¥
- ê¸°ë³¸ì€ ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜(UTF)ëŠ” DataFrameì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥ (ë¬¸ìì—´ í‘œí˜„ì‹ X)
  - **ìŠ¤íŒŒí¬ SQL í•¨ìˆ˜ ë“±ë¡í•˜ë©´?**
  - => ëª¨ë“  í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì™€ SQL ì—ì„œ ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ ì‚¬ìš© ê°€ëŠ¥
    - íŒŒì´ì¬ì—ì„œë„ ìš°íšŒì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•˜ì§€ë§Œ DataFrame í•¨ìˆ˜ ëŒ€ì‹  SQL í‘œí˜„ì‹ìœ¼ë¡œ ì‚¬ìš©í•´ì•¼í•¨
    - ìŠ¤íŒŒí¬ëŠ” ìì²´ ë°ì´í„° íƒ€ì…(íŒŒì´ì¬X)ì„ ì‚¬ìš©í•˜ë¯€ë¡œ **ë³€í™˜ íƒ€ì… ì§€ì • ê¶Œ!ì¥!**
    - ë°˜í™˜ë  íƒ€ì…ê³¼ ë‹¤ë¥¸ ë°ì´í„° íƒ€ì… ì§€ì •ì‹œ => null ë°˜í™˜
- ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ì—ì„œ ì„ íƒì  ê°’ ë°˜í™˜
  - íŒŒì´ì¬ = `None` / ìŠ¤ì¹¼ë¼ = `Option` ë°˜í™˜

  <details><summary class="point-color-can-hover">[6.11] ì˜ˆì œ í¼ì¹˜ê¸°</summary>

  ```scala
  val udfExampleDF = spark.range(5).toDF("num")
  def power3(number:Double):Double = number * number * number
  power3(2.0) // 8.0


  // UDF ì‹¤í–‰ + í•¨ìˆ˜ ë“±ë¡ ë° ì‚¬ìš© (=> DataFrameì—ì„œ ì‚¬ìš© ê°€ëŠ¥)
  import org.apache.spark.sql.functions.udf
  val power3udf = udf(power3(_:Double):Double)

  udfExampleDF.select(power3udf(col("num"))).show()
  // +--------+
  // |UDF(num)|
  // +--------+
  // |     0.0|
  // |     1.0|
  // |     8.0|
  // |    27.0|
  // |    64.0|
  // +--------+


  // UDFë¥¼ ìŠ¤íŒŒí¬ SQLë¡œ ë“±ë¡í•˜ë©´ => ëª¨ë“  í”„ë¡œê·¸ë˜ë° ì–¸ì–´, SQL ì—ì„œ ì‚¬ìš© ê°€ëŠ¥ (+ë¬¸ìì—´ í‘œí˜„ì‹)
  spark.udf.register("power3", power3(_:Double):Double)
  udfExampleDF.selectExpr("power3(num)").show(2)
  +-------------------------------+
  |UDF:power3(cast(num as double))|
  +-------------------------------+
  |                            0.0|
  |                            1.0|
  +-------------------------------+
  ```

  ```python
  %spark.pyspark
  udfExampleDF.selectExpr("power3(num)").show(2)
  # => Scalaë¡œ ë“±ë¡ëœ UDF ì‚¬ìš©

  # ë°˜í™˜ ë°ì´í„°íƒ€ì…ì´ Integerì¸ë° DoubeType() ìœ¼ë¡œ ë³€í™˜ ì‹œ => null ë°˜í™˜
  from pyspark.sql.types import IntegerType, DoubleType
  spark.udf.register("power3py", power3, DoubleType())
  ```

  ```sql
  -- SQL ì—ì„œë„ ë“±ë¡ëœ UDF ì‚¬ìš© ê°€ëŠ¥
  SELECT power3(12), power3py(12) -- ë°˜í™˜ ë°ì´í„° íƒ€ì… ë¬¸ì œë¡œ ë™ì‘í•˜ì§€ ì•ŠìŒ
  ```

  </details>


### 6.12 Hive UDF
- í•˜ì´ë¸Œ ë¬¸ë²•ì„ ì‚¬ìš©í•´ì„œ ë§Œë“  í•¨ìˆ˜ => **UDF**, **UDAF** ì‚¬ìš© ê°€ëŠ¥
  - UDF (User Defined Function)
  - UDAF (User Defined Aggregate Function)
- ë‹¨, í•˜ì´ë¸Œ ì§€ì› ê¸°ëŠ¥ í™œì„±í™” í•„ìš”
  - => `SparkSession.builder().enableHiveSupport()` ëª…ì‹œ
  - í•˜ì´ë¸Œ ì§€ì› í™œì„±í™” ë˜ë©´ SQLë¡œ UDF ë“±ë¡ ê°€ëŠ¥
  - ì‚¬ì „ì— ì»´íŒŒì¼ëœ ìŠ¤ì¹¼ë¼, ìë°” íŒ¨í‚¤ì§€ë§Œ ì§€ì› (ë¼ì´ë¸ŒëŸ¬ë¦¬ ì˜ì¡´ì„± ëª…ì‹œ í•„ìš”)

  ```sql
  -- TEMPORARY í‚¤ì›Œë“œ ì œê±° ì‹œ => í•˜ì´ë¸Œ ë©”íƒ€ìŠ¤í† ì–´ì— ì˜êµ¬(permanent) í•¨ìˆ˜ë¡œ ë“±ë¡
  CREATE TEMPORARY FUNCTION myFunc AS 'com.organization.hive.udf.FunctionName'
  ```


### 6.13 ì •ë¦¬
- ìŠ¤íŒŒí¬ SQLì„ ì‚¬ìš©ëª©ì ì— ë§ê²Œ í™•ì¥í•˜ëŠ” ë°©ì‹
  - ê°„ë‹¨í•œ í•¨ìˆ˜ë§Œìœ¼ë¡œë„ í™•ì¥ ê°€ëŠ¥ (DSL X)
- ìŠ¤íŒŒí¬ SQLì€ ë³µì¡í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ êµ¬í˜„ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê°•ë ¥í•œ ê¸°ëŠ¥

### ğŸ“’ ë‹¨ì–´ì¥