---
title: '&#039;Spark The Definitive Guide&#039; 7ì¥ - ì§‘ê³„í•´ë¼ ì• ì†¡ì´'
date: 2021-02-03 21:42:12
categories: spark
tags:
  - spark
  - apache
  - book
  - study
---

<br/>

<p style="color:lightgray">ë²Œì¨ë¶€í„° ìŠ¬ìŠ¬ í¬ìŠ¤íŠ¸ í¬ë©§ í—·ê°ˆë¦¬ê¸° ì‹œì‘í•˜ì£ ? ë§í–ˆì£ ?<br/>
ë‚˜ì¤‘ì— í•œë²ˆì— ë§ì¶°ì•¼ì§€ ìƒê°í•´ë†“ê³  ì ˆëŒ€ ìˆ˜ì •ì•ˆí•˜ì£ ? ã…..</p>

<br/>

<img width="300" alt="counting" src="https://user-images.githubusercontent.com/26691216/106792995-3b405280-669a-11eb-9f64-fc4d576200cd.gif">

<center><h2>_ _ _</h2></center>

<br/>

---

# CHAPTER 7 ì§‘ê³„ ì—°ì‚°
*ì§‘ê³„(aggregation)ì€ ë¬´ì–¸ê°ˆ í•¨ê»˜ ëª¨ìœ¼ëŠ” í–‰ìœ„ì´ë©° ë¹…ë°ì´í„° ë¶„ì„ì˜ ì´ˆì„ì´ë‹¤.*

ìŠ¤íŒŒí¬ëŠ” ëª¨ë“  ë°ì´í„° íƒ€ì… ë‹¤ë£¨ëŠ” ê²ƒ + ë‹¤ìŒê³¼ ê°™ì€ **ê·¸ë£¹í™” ë°ì´í„° íƒ€ì…** ìƒì„± ê°€ëŠ¥í•˜ê³ ,
ì§€ì •ëœ ì§‘ê³„ í•¨ìˆ˜ì— ë”°ë¼ ê·¸ë£¹í™”ëœ ê²°ê³¼ëŠ” RelationalGroupedDataset ì„ ë°˜í™˜.

> - select êµ¬ë¬¸ì—ì„œ ì§‘ê³„ ìˆ˜í–‰, DataFrame ì „ì²´ ë°ì´í„° ìš”ì•½ (ê°€ì¥ ê°„ë‹¨í•œ ê·¸ë£¹í™”)
> - 'group by' : í•˜ë‚˜ ì´ìƒì˜ í‚¤ ì§€ì •. ë‹¤ë¥¸ ì§‘ê³„ í•¨ìˆ˜ ì‚¬ìš©í•´ì„œ ê°’ì„ ê°€ì§„ ì»¬ëŸ¼ ë³€í™˜ ê°€ëŠ¥
> - 'ìœˆë„ìš°(window)' : í•˜ë‚˜ ì´ìƒì˜ í‚¤ ì§€ì •. ë‹¤ë¥¸ ì§‘ê³„ í•¨ìˆ˜ë¡œ ì»¬ëŸ¼ ë³€í™˜ ê°€ëŠ¥. + ë‹¨, í•¨ìˆ˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ë¡œìš°ëŠ” í˜„ì¬ ë¡œìš°ì™€ **ì—°ê´€ì„±** ìˆì–´ì•¼ í•¨
> - 'ê·¸ë£¹í™” ì…‹(grouping set)' : ì„œë¡œ ë‹¤ë¥¸ ë ˆë²¨ ê°’ ì§‘ê³„ (SQL, DataFrameì˜ ë¡¤ì—…, íë¸Œ)
> - 'ë¡¤ì—…(rollup)' : í•˜ë‚˜ ì´ìƒì˜ í‚¤ ì§€ì •. ë‹¤ë¥¸ ì§‘ê³„ í•¨ìˆ˜ë¡œ ì»¬ëŸ¼ ë³€í™˜ ê°€ëŠ¥. + **ê³„ì¸µì ìœ¼ë¡œ ìš”ì•½ëœ ê°’** ì¶”ì¶œ
> - 'íë¸Œ(cube)' : í•˜ë‚˜ ì´ìƒì˜ í‚¤ ì§€ì •. ë‹¤ë¥¸ ì§‘ê³„ í•¨ìˆ˜ë¡œ ì»¬ëŸ¼ ë³€í™˜ ê°€ëŠ¥. + **ëª¨ë“  ì»¬ëŸ¼ ì¡°í•©ì— ëŒ€í•œ ìš”ì•½ ê°’** ê³„ì‚°
>
> <i style="color:lightgray">(=> ì‚¬ì‹¤ìƒ 7ì¥ ìš”ì•½)</i> 


ğŸ“Œ ì¤‘ìš”í•œ ê±´, **ì–´ë–¤ ê²°ê³¼ë¥¼ ë§Œë“¤ì§€** ì •í™•íˆ íŒŒì•…í•´ì•¼ í•œë‹¤ëŠ” ê²ƒ.
(ì •í™•í•œ ë‹µ ê³„ì‚° = ë†’ì€ ë¹„ìš© ìš”êµ¬ â†’ ë¹…ë°ì´í„°ì˜ ê²½ìš° ê·¼ì‚¬ì¹˜ê°€ íš¨ìœ¨ì ì¼ ìˆ˜ ìˆìŒ)

<!-- 
count ì˜ˆì œ.. (ë°ì´í„°ì…‹ í¬ê¸° ì¶œë ¥ ë° ìºì‹±ìš©ë„)
ì´ì§ˆì ìœ¼ë¡œ ëŠê»´ì§ˆ ìˆ˜ ìˆìŒ. ì™œë‚˜ë©´...
- í•¨ìˆ˜ê°€ ì•„ë‹Œ ë©”ì„œë“œ í˜•íƒœ
- íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜ê°™ì€ ì§€ì—° ì—°ì‚°ì´ì•„ë‹Œ ì¦‰ì‹œ ì—°ì‚°(ì•¡ì…˜)
-->

### 7.1 ì§‘ê³„ í•¨ìˆ˜
- ëª¨ë“  ì§‘ê³„ëŠ” íŠ¹ë³„í•œ ê²½ìš°ë¥¼ ì œì™¸í•˜ê³ ëŠ” **í•¨ìˆ˜** ì‚¬ìš©
  - => **ì§‘ê³„ í•¨ìˆ˜** ([org.apache.spark.sql.functions](https://bit.ly/2tRMYus) íŒ¨í‚¤ì§€)
  - ì˜ˆì™¸) DataFrameì˜ .stat ì†ì„± ì´ìš© (6ì¥ ì°¸ê³ )
  - ìŠ¤ì¹¼ë¼, íŒŒì´ì¬ì—ì„œ ì„í¬íŠ¸ í•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜ì™€ SQLì—ì„œ ì‚¬ìš©ê°€ëŠ¥í•œ í•¨ìˆ˜ëŠ” ì•½ê°„ ë‹¤ë¦„ (ë§¤ ë¦´ë¦¬ì¦ˆë§ˆë‹¤ ì¡°ê¸ˆì”© ë³€í•¨)

#### ì§‘ê³„ í•¨ìˆ˜
> ì§‘ê³„ í•¨ìˆ˜ : í‚¤ë‚˜ ê·¸ë£¹ì„ ì§€ì •í•˜ê³  + í•˜ë‚˜ ì´ìƒì˜ ì»¬ëŸ¼ì„ ë³€í™˜í•˜ëŠ” ë°©ë²•ì„ ì§€ì • (ì—¬ëŸ¬ ì…ë ¥ê°’ì´ ì£¼ì–´ì§€ë©´ ê·¸ë£¹ ë³„ë¡œ ê²°ê³¼ ìƒì„±)
> - ìˆ˜ì¹˜í˜• ë°ì´í„° ìš”ì•½ (ex. ê·¸ë£¹ì˜ í‰ê· ê°’ êµ¬í•˜ê¸°)
> - í•©ì‚°, ê³±ì…ˆ, ì¹´ìš´íŒ… ë“±ì˜ ì‘ì—…
> - ë³µí•© ë°ì´í„° íƒ€ì…(ë°°ì—´, ë¦¬ìŠ¤íŠ¸, ë§µ)ì„ ì‚¬ìš©í•œ ì§‘ê³„ ìˆ˜í–‰ ê°€ëŠ¥ 

- `count(ì»¬ëŸ¼ëª…)` : ì „ì²´ ë¡œìš° ìˆ˜ ì¹´ìš´íŠ¸
  - ì•¡ì…˜ì´ ì•„ë‹Œ **íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜**ìœ¼ë¡œ ë™ì‘
  - ë‘ê°€ì§€ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥
    - `count(íŠ¹ì • ì»¬ëŸ¼)` : null ê°’ í¬í•¨ X
    - `count(*)` or `count(1)` : null ê°’ ê°€ì§„ ë¡œìš° í¬í•¨í•´ì„œ ì¹´ìš´íŠ¸
  - <details><summary class="only-hover">[7.1.1] ì˜ˆì œ í¼ì¹˜ê¸°</summary>

    ```scala
    import org.apache.spark.sql.functions.count
    df.select(count("StockCode")).show() // 541909
    ```
    ```sql
    SELECT COUNT(*) FROM dfTable
    ```

    </details>

- `countDistinct(ì»¬ëŸ¼ëª…)` : ê³ ìœ  (distinct) ë ˆì½”ë“œ ìˆ˜ ì¹´ìš´íŠ¸
  - <details><summary class="only-hover">[7.1.2] ì˜ˆì œ í¼ì¹˜ê¸°</summary>

    ```scala
    import org.apache.spark.sql.functions.countDistinct
    df.select(countDistinct("StockCode")).show() // 4070
    ```
    ```sql
    SELECT COUNT(DISTINCT *) FROM DFTABLE
    ```

    </details>

- `approx_count_distinct(ì»¬ëŸ¼ëª…, ìµœëŒ€ì¶”ì •ì˜¤ë¥˜ìœ¨)` : ê·¼ì‚¬ì¹˜ ê³ ìœ  ë ˆì½”ë“œ ìˆ˜ ì¹´ìš´íŠ¸
  - ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ ë‹¤ë£° ì‹œ ì •í™•í•œ ê°œìˆ˜ ë¬´ì˜ë¯¸í•¨ => ê·¼ì‚¬ì¹˜ë¡œ íš¨ìœ¨
  - `ìµœëŒ€ ì¶”ì • ì˜¤ë¥˜ìœ¨ (maximum estimation error)` íŒŒë¼ë¯¸í„° ì„¤ì •
  - <details><summary class="only-hover">[7.1.3] ì˜ˆì œ í¼ì¹˜ê¸°</summary>

    ```scala
    import org.apache.spark.sql.functions.approx_count_distinct
    df.select(approx_count_distinct("StockCode", 0.1)).show() // 3364
    ```
    ```sql
    SELECT approx_count_distinct(StockCode, 0.1) FROM DFTABLE
    ```
    
    </details>


- `first(ì»¬ëŸ¼ëª…)`, `last(ì»¬ëŸ¼ëª…)` : ì²« ë²ˆì§¸ ê°’, ë§ˆì§€ë§‰ ê°’ ì¶”ì¶œ
  - DataFrame ê°’ì´ ì•„ë‹Œ ë¡œìš° ê¸°ë°˜ ë™ì‘
  - <details><summary class="only-hover">[7.1.4] ì˜ˆì œ í¼ì¹˜ê¸°</summary>

    ```scala
    df.select(first("StockCode"), last("StockCode")).show()
    // +-----------------------+----------------------+
    // |first(StockCode, false)|last(StockCode, false)|
    // +-----------------------+----------------------+
    // |                 85123A|                 22138|
    // +-----------------------+----------------------+
    ```
    ```sql
    SELECT first(StockCode), last(StockCode) FROM dfTable
    ```

    </details>

- `min(ì»¬ëŸ¼ëª…)`, `max(ì»¬ëŸ¼ëª…)` : ìµœì†Ÿê°’, ìµœëŒ“ê°’ ì¶”ì¶œ
  - <details><summary class="only-hover">[7.1.5] ì˜ˆì œ í¼ì¹˜ê¸°</summary>

    ```scala
    import org.apache.spark.sql.functions.{min, max}
    df.select(min("Quantity"), max("Quantity")).show()
    // +-------------+-------------+
    // |min(Quantity)|max(Quantity)|
    // +-------------+-------------+
    // |       -80995|        80995|
    // +-------------+-------------+
    ```
    ```sql
    SELECT min(Quantity), max(Quantity) FROM dfTable
    ```

    </details>

- `sum(ì»¬ëŸ¼ëª…)` : íŠ¹ì • ì»¬ëŸ¼ì˜ ëª¨ë“  ê°’ í•©ì‚°
  - <details><summary class="only-hover">[7.1.6] ì˜ˆì œ í¼ì¹˜ê¸°</summary>

    ```scala
    import org.apache.spark.sql.functions.sum
    df.select(sum("Quantity")).show() // 5176450
    // +-------------+
    // |sum(Quantity)|
    // +-------------+
    // |      5176450|
    // +-------------+
    ```
    ```sql
    SELECT sum(Quantity) FROM dfTable
    ```

    </details>


- `sumDistinct(ì»¬ëŸ¼ëª…)` : íŠ¹ì • ì»¬ëŸ¼ì˜ ê³ ìœ  (distinct) ê°’ í•©ì‚°
  - <details><summary class="only-hover">[7.1.7] ì˜ˆì œ í¼ì¹˜ê¸°</summary>

    ```scala
    import org.apache.spark.sql.functions.sumDistinct
    df.select(sumDistinct("Quantity")).show() // 29310
    // +----------------------+
    // |sum(DISTINCT Quantity)|
    // +----------------------+
    // |                 29310|
    // +----------------------+
    ```
    ```sql
    SELECT SUM(Quantity) FROM dfTable -- 29310
    ```

    </details>


- `avg(ì»¬ëŸ¼ëª…)` : í‰ê·  ê°’
  - == `sum()/count()` == `expr("mean(ì»¬ëŸ¼ëª…)")`
  - \+ `distinct()` => ê³ ìœ³ê°’ í‰ê·  êµ¬í•˜ê¸°ë„ ê°€ëŠ¥
  - <details><summary class="only-hover">[7.1.8] ì˜ˆì œ í¼ì¹˜ê¸°</summary>

    ```scala
    import org.apache.spark.sql.functions.{sum, count, avg, expr}
    (df.select(
        count("Quantity").alias("total_transactions"),
        sum("Quantity").alias("total_purchases"),
        avg("Quantity").alias("avg_purchases"),
        expr("mean(Quantity)").alias("mean_purchases"))
      .selectExpr(
        "total_purchases/total_transactions",
        "avg_purchases",
        "mean_purchases").show())
    // +--------------------------------------+----------------+----------------+
    // |(total_purchases / total_transactions)|   avg_purchases|  mean_purchases|
    // +--------------------------------------+----------------+----------------+
    // |                      9.55224954743324|9.55224954743324|9.55224954743324|
    // +--------------------------------------+----------------+----------------+
    ```

    </details>

- ë¶„ì‚°ê³¼ í‘œì¤€í¸ì°¨
  - í‰ê· (`m`) ì£¼ë³€ì— ë°ì´í„°ê°€ ë¶„í¬ëœ ì •ë„ë¥¼ ì¸¡ì •
    - ë¶„ì‚° : í‰ê· ê³¼ì˜ ì°¨ì´ë¥¼ ì œê³±í•œ ê²°ê³¼ì˜ í‰ê·  (`v = avg((x-m)^2)`)
    - í‘œì¤€í¸ì°¨ : ë¶„ì‚°ì˜ ì œê³±ê·¼ (`Ïƒ = v^(1/2)`)
  - ìŠ¤íŒŒí¬ëŠ” í‘œë³¸í‘œì¤€í¸ì°¨(sample standard deviation), ëª¨í‘œì¤€í¸ì°¨(population standard deviation) ë°©ì‹ ì§€ì›
    - => ì•„ì˜ˆ ë‹¤ë¥´ë¯€ë¡œ **ì˜ êµ¬ë¶„í•´ì„œ ì‚¬ìš©í•´ì•¼í•¨**
  - í‘œë³¸í‘œì¤€ë¶„ì‚°, í‘œë³¸í‘œì¤€í¸ì°¨ ë°©ì‹ ì‚¬ìš© ì‹œ => `variance()`, `stddev()`
  - ëª¨í‘œì¤€ë¶„ì‚°, ëª¨í‘œì¤€í¸ì°¨ ë°©ì‹ ì‚¬ìš© ì‹œ => `var_pop()`, `stddev_pop()`
  - <details><summary class="only-hover">[7.1.9] ì˜ˆì œ í¼ì¹˜ê¸°</summary>

    ```scala
    df.select(var_pop("Quantity"), var_samp("Quantity"),
      stddev_pop("Quantity"), stddev_samp("Quantity")).show()
    // +-----------------+------------------+--------------------+---------------------+
    // |var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|
    // +-----------------+------------------+--------------------+---------------------+
    // |47559.30364660923| 47559.39140929892|  218.08095663447835|   218.08115785023455|
    // +-----------------+------------------+--------------------+---------------------+
    ```
    ```sql
    SELECT var_pop(Quantity), var_samp(Quantity),
    stddev_pop(Quantity), stddev_samp(Quantity)
    FROM dfTable
    ```

    </details>

- ë¹„ëŒ€ì¹­ë„ì™€ ì²¨ë„
  - ë°ì´í„°ì˜ ë³€ê³¡ì (extreme point) ë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ë²•
    - `skewness(ì»¬ëŸ¼ëª…)` : ë¹„ëŒ€ì¹­ë„ (ë°ì´í„° í‰ê· ì˜ ë¹„ëŒ€ì¹­ ì •ë„) ì¸¡ì •
    - `kurtosis(ì»¬ëŸ¼ëª…)` : ì²¨ë„ (ë°ì´í„° ë ë¶€ë¶„ì˜ ë¾°ì¡±í•œ ì •ë„) ì¸¡ì •
  - í™•ë¥ ë³€ìˆ˜(random variable)ì˜ í™•ë¥ ë¶„í¬(probability distribution)ë¡œ ë°ì´í„° ëª¨ë¸ë§ ì‹œì— ì¤‘ìš”
  - ìˆ˜í•™ì ì¸ ë‚´ìš©ì€ ë”°ë¡œ ì•Œì•„ì„œ... í í ..
  - <details><summary class="only-hover">[7.1.10] ì˜ˆì œ í¼ì¹˜ê¸°</summary>

    ```scala
    import org.apache.spark.sql.functions.{skewness, kurtosis}
    df.select(skewness("Quantity"), kurtosis("Quantity")).show()
    // +--------------------+------------------+
    // |  skewness(Quantity)|kurtosis(Quantity)|
    // +--------------------+------------------+
    // |-0.26407557610528376|119768.05495530753|
    // +--------------------+------------------+
    ```
    ```sql
    SELECT skewness(Quantity), kurtosis(Quantity) FROM dfTable
    ```

    </details>

- ê³µë¶„ì‚°ê³¼ ìƒê´€ê´€ê³„
  - ë‘ ì»¬ëŸ¼ê°’ ì‚¬ì´ì˜ ì˜í–¥ë„ ë¹„êµ
  - `cov(ì»¬ëŸ¼1, ì»¬ëŸ¼2)` : ê³µë¶„ì‚°(covariance) ê³„ì‚°
    - ë°ì´í„° ì…ë ¥ê°’ì— ë”°ë¼ ë‹¤ë¥¸ ë²”ìœ„ë¥¼ ê°€ì§
    - var í•¨ìˆ˜ì²˜ëŸ¼ í‘œë³¸ê³µë¶„ì‚°(sample covariance)ì´ë‚˜ ëª¨ê³µë¶„ì‚°(population covariance) ë°©ì‹ìœ¼ë¡œë„ ê³„ì‚° ê°€ëŠ¥ => `covar_samp()`, `covar_pop()`
  - `corr(ì»¬ëŸ¼1, ì»¬ëŸ¼2)` : ìƒê´€ê´€ê³„(correlation) ê³„ì‚°
    - í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ (Pearson correlation coefficient) ì¸¡ì • (-1 <= `r` <= 1)
    - ëª¨ì§‘ë‹¨ì´ë‚˜ í‘œë³¸ì— ëŒ€í•œ ê³„ì‚° ê°œë… X
  - <details><summary class="only-hover">[7.1.11] ì˜ˆì œ í¼ì¹˜ê¸°</summary>

    ```scala
    import org.apache.spark.sql.functions.{corr, covar_pop, covar_samp}
    df.select(corr("InvoiceNo", "Quantity"), covar_samp("InvoiceNo", "Quantity"),
        covar_pop("InvoiceNo", "Quantity")).show()
    // +-------------------------+-------------------------------+------------------------------+
    // |corr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|
    // +-------------------------+-------------------------------+------------------------------+
    // |     4.912186085640497E-4|             1052.7280543915997|            1052.7260778754955|
    // +-------------------------+-------------------------------+------------------------------+
    ```
    ```sql
    SELECT corr(InvoiceNo, Quantity), covar_samp(InvoiceNo, Quantity),
    covar_pop(InvoiceNo, Quantity)
    FROM dfTable
    ```

    </details>

- ë³µí•© ë°ì´í„° íƒ€ì…ì˜ ì§‘ê³„
  - ìŠ¤íŒŒí¬ëŠ” ìˆ˜ì‹ì„ í†µí•œ ì§‘ê³„ ì™¸ì—ë„ **ë³µí•© ë°ì´í„° íƒ€ì…**ì„ ì‚¬ìš©í•œ ì§‘ê³„ ê°€ëŠ¥ (ex. íŠ¹ì • ì»¬ëŸ¼ ê°’ => List, Set .. ë“±ìœ¼ë¡œ ìˆ˜ì§‘)
  - ìˆ˜ì§‘ëœ ë°ì´í„°ëŠ” ë‹¤ì–‘í•œ í”„ë¡œê·¸ë˜ë° ë°©ì‹ìœ¼ë¡œ ë‹¤ë£¨ê±°ë‚˜ í™œìš© ê°€ëŠ¥
  - <details><summary class="only-hover">[7.1.12] ì˜ˆì œ í¼ì¹˜ê¸°</summary>

    ```scala
    import org.apache.spark.sql.functions.{collect_set, collect_list}
    df.agg(collect_set("Country"), collect_list("Country")).show()
    // +--------------------+---------------------+
    // |collect_set(Country)|collect_list(Country)|
    // +--------------------+---------------------+
    // |[Portugal, Italy,...| [United Kingdom, ...|
    // +--------------------+---------------------+
    ```
    ```sql
    SELECT collect_set(Country), collect_set(Country) FROM dfTable
    ```

    </details>

### 7.2 ê·¸ë£¹í™”
- <u>ë°ì´í„° **ê·¸ë£¹** ê¸°ë°˜ì˜ ì§‘ê³„</u> ì— ëŒ€í•œ ë‚´ìš©
  - ([7.1] ì€ DataFrame ìˆ˜ì¤€ì˜ ì§‘ê³„ ë‚´ìš©)
  - ì¹´í…Œê³ ë¦¬í˜• ë°ì´í„°(categorical data) ì‚¬ìš©
  - => ë‹¨ì¼ ì»¬ëŸ¼ì˜ ë°ì´í„°ë¥¼ ê·¸ë£¹í™”, í•´ë‹¹ ê·¸ë£¹ì˜ ë‹¤ë¥¸ ì—¬ëŸ¬ ì»¬ëŸ¼ì„ ì‚¬ìš©í•´ì„œ ê³„ì‚°
- ê·¸ë£¹í™” ì‘ì—…ì˜ 2 ë‹¨ê³„
  - 1\) í•˜ë‚˜ ì´ìƒì˜ ì»¬ëŸ¼ ê·¸ë£¹í™” (ì—¬ëŸ¬ê°œ ì§€ì •ë„ ê°€ëŠ¥)
  - 2\) ì§‘ê³„ ì—°ì‚° ìˆ˜í–‰
- í‘œí˜„ì‹ì„ ì´ìš©í•œ ê·¸ë£¹í™”
  - ì¹´ìš´íŒ…ì€ ë©”ì„œë“œ, í•¨ìˆ˜ ë‘˜ ë‹¤ ì‚¬ìš© ê°€ëŠ¥ ğŸ¤”
    - ë©”ì„œë“œ ë³´ë‹¤ `count()` í•¨ìˆ˜ ì‚¬ìš© ì¶”ì²œ
    - select êµ¬ë¬¸ì˜ í‘œí˜„ì‹ ì§€ì •ë³´ë‹¤ `agg()` ë©”ì„œë“œ ì‚¬ìš© ì¶”ì²œ
  - `agg()` : ì—¬ëŸ¬ ì§‘ê³„ ì²˜ë¦¬ í•œë²ˆì— ì§€ì • & ì§‘ê³„ì— í‘œí˜„ì‹ ì‚¬ìš© ê°€ëŠ¥
    - íŠ¸ëœìŠ¤í¬ë©”ì´ì…˜ ì™„ë£Œ ì»¬ëŸ¼ì— `alias` ì‚¬ìš© ê°€ëŠ¥
- ë§µì„ ì´ìš©í•œ ê·¸ë£¹í™”
  - ë§µ(map) íƒ€ì… ì‚¬ìš© : Key = ì»¬ëŸ¼ / Value = ìˆ˜í–‰í•  ì§‘ê³„ í•¨ìˆ˜ì˜ ë¬¸ìì—´
  - ìˆ˜í–‰í•  ì§‘ê³„í•¨ìˆ˜ë¥¼ í•œ ì¤„ë¡œ ì‘ì„± ì‹œ => ì—¬ëŸ¬ ì»¬ëŸ¼ëª… ì¬ì‚¬ìš© ê°€ëŠ¥
    - `agg(Key -> Value, Key -> Value, ...)`

### 7.3 ìœˆë„ìš° í•¨ìˆ˜
- **ìœˆë„ìš° í•¨ìˆ˜** ë„ ì§‘ê³„ì— ì‚¬ìš© ê°€ëŠ¥
- ìœˆë„ìš° í•¨ìˆ˜
  - ë°ì´í„°ì˜ íŠ¹ì • 'ìœˆë„ìš°(window)' ëŒ€ìƒìœ¼ë¡œ ê³ ìœ ì˜ ì§‘ê³„ ì—°ì‚° ìˆ˜í–‰
  - ë°ì´í„°ì˜ 'ìœˆë„ìš°' => í˜„ì¬ ë°ì´í„°ì— ëŒ€í•œ ì°¸ì¡°(reference)ë¥¼ ì‚¬ìš©í•´ ì •ì˜
  - ìœˆë„ìš° ëª…ì„¸(window specification) => í•¨ìˆ˜ì— ì „ë‹¬ë  ë¡œìš° ê²°ì •
- ìŠ¤íŒŒí¬ê°€ ì§€ì›í•˜ëŠ” ìœˆë„ìš° í•¨ìˆ˜
  - ë­í¬ í•¨ìˆ˜ (ranking function)
  - ë¶„ì„ í•¨ìˆ˜ (analytic function)
  - ì§‘ê³„ í•¨ìˆ˜ (aggragate function)
- ìœˆë„ìš° í•¨ìˆ˜ vs group-by í•¨ìˆ˜
  - ìœˆë„ìš° í•¨ìˆ˜ : **í”„ë ˆì„**ì— ì…ë ¥ë˜ëŠ” ëª¨ë“  ë¡œìš°ì— ëŒ€í•´ ê²°ê³¼ê°’ ê³„ì‚°
  - group-by í•¨ìˆ˜ : ëª¨ë“  ë¡œìš° ë ˆì½”ë“œê°€ ë‹¨ì¼ ê·¸ë£¹ìœ¼ë¡œë§Œ ì´ë™
- í”„ë ˆì„(frame) : ë¡œìš° ê·¸ë£¹ ê¸°ë°˜ì˜ í…Œì´ë¸”
  - ê° ë¡œìš°ëŠ” í•˜ë‚˜ ì´ìƒì˜ í”„ë ˆì„ì— í• ë‹¹ ê°€ëŠ¥
    <img width="300" alt="row - window frame" src="https://user-images.githubusercontent.com/26691216/106770674-8b5eeb00-6681-11eb-929f-09e7c373f9a2.png">
  - í”„ë ˆì„ ì •ì˜ ë°©ë²•ì€ ì˜ˆì œ ì°¸ê³ 
- ex. í•˜ë£¨ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°’ì˜ ë¡¤ë§ í‰ê· (rolling average) êµ¬í•˜ê¸°
  - ê°œë³„ ë¡œìš°ê°€ 7ê°œì˜ ë‹¤ë¥¸ í”„ë ˆì„ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ì•¼ í•¨

<details><summary class="point-color-can-hover">[7.3] ì˜ˆì œ í¼ì¹˜ê¸°</summary>

```scala
// 1) ì£¼ë¬¸ ì¼ì(InvoiceDate) => 'date' ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜ (ë‚ ì§œ ì •ë³´ë§Œ í¬í•¨)
import org.apache.spark.sql.functions.{col, to_date}

val dfWithDate = df.withColumn("date", to_date(col("InvoiceDate"),
  "MM/d/yyyy H:mm"))
dfWithDate.createOrReplaceTempView("dfWithDate")


// 2) ìœˆë„ìš° ëª…ì„¸ ë§Œë“¤ê¸°
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions.col

val windowSpec = (Window
  .partitionBy("CustomerId", "date")  // ê·¸ë£¹ì„ ì–´ë–»ê²Œ ë‚˜ëˆŒì§€ ê²°ì •
  .orderBy(col("Quantity").desc)    // íŒŒí‹°ì…˜ ì •ë ¬ ë°©ì‹
  .rowsBetween(Window.unboundedPreceding, Window.currentRow)) // í”„ë ˆì„ ëª…ì„¸ (=> ì²« ë¡œìš° ~ í˜„ì¬ ë¡œìš°ê¹Œì§€ í™•ì¸)


// 3) ì§‘ê³„ í•¨ìˆ˜ë¡œ ë¶„ì„
// => ì»¬ëŸ¼ or í‘œí˜„ì‹ ë°˜í™˜ ì‹œ DataFrame.select() ì—ì„œ ì‚¬ìš© ê°€ëŠ¥

// ì˜ˆì‹œ 1. maxPurchaseQuantity = ì‹œê°„ëŒ€ë³„ ìµœëŒ€ êµ¬ë§¤ ê°œìˆ˜
import org.apache.spark.sql.functions.max
val maxPurchaseQuantity = max(col("Quantity")).over(windowSpec)
// maxPurchaseQuantity: org.apache.spark.sql.Column = max(Quantity) OVER (PARTITION BY CustomerId, date ORDER BY Quantity DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)

// ì˜ˆì‹œ 2. purchase(Dense)Rank = êµ¬ë§¤ëŸ‰ ìˆœìœ„ 
import org.apache.spark.sql.functions.{dense_rank, rank}
val purchaseDenseRank = dense_rank().over(windowSpec) // ìˆœìœ„ê°€ ë¹„ì§€ì•Šë„ë¡ dense_rank() ì‚¬ìš©
val purchaseRank = rank().over(windowSpec)

// DataFrame.select()ë¡œ ìœˆë„ìš° ê°’ í™•ì¸
import org.apache.spark.sql.functions.col

(dfWithDate.where("CustomerId IS NOT NULL").orderBy("CustomerId")
  .select(
    col("CustomerId"),
    col("date"),
    col("Quantity"),
    purchaseRank.alias("quantityRank"),
    purchaseDenseRank.alias("quantityDenseRank"),
    maxPurchaseQuantity.alias("maxPurchaseQuantity")).show())
// +----------+----------+--------+------------+-----------------+-------------------+
// |CustomerId|      date|Quantity|quantityRank|quantityDenseRank|maxPurchaseQuantity|
// +----------+----------+--------+------------+-----------------+-------------------+
// |     12346|2011-01-18|   74215|           1|                1|              74215|
// |     12346|2011-01-18|  -74215|           2|                2|              74215|
// |     12347|2010-12-07|      36|           1|                1|                 36|
// |     12347|2010-12-07|      30|           2|                2|                 36|
// |     12347|2010-12-07|      24|           3|                3|                 36|
// |     12347|2010-12-07|      12|           4|                4|                 36|
// |     12347|2010-12-07|      12|           4|                4|                 36|
// |     12347|2010-12-07|      12|           4|                4|                 36|
// |     12347|2010-12-07|      12|           4|                4|                 36|
// |     12347|2010-12-07|      12|           4|                4|                 36|
// |     12347|2010-12-07|      12|           4|                4|                 36|
// |     12347|2010-12-07|      12|           4|                4|                 36|
// |     12347|2010-12-07|      12|           4|                4|                 36|
// |     12347|2010-12-07|      12|           4|                4|                 36|
// |     12347|2010-12-07|      12|           4|                4|                 36|
// |     12347|2010-12-07|      12|           4|                4|                 36|
// |     12347|2010-12-07|      12|           4|                4|                 36|
// |     12347|2010-12-07|      12|           4|                4|                 36|
// |     12347|2010-12-07|       6|          17|                5|                 36|
// |     12347|2010-12-07|       6|          17|                5|                 36|
// +----------+----------+--------+------------+-----------------+-------------------+
```

```sql
-- SQL
SELECT CustomerId, date, Quantity,
  rank(Quantity) OVER (PARTITION BY CustomerId, date
                       ORDER BY Quantity DESC NULLS LAST
                       ROWS BETWEEN
                         UNBOUNDED PRECEDING AND
                         CURRENT ROW) as rank,

  dense_rank(Quantity) OVER (PARTITION BY CustomerId, date
                             ORDER BY Quantity DESC NULLS LAST
                             ROWS BETWEEN
                               UNBOUNDED PRECEDING AND
                               CURRENT ROW) as dRank,

  max(Quantity) OVER (PARTITION BY CustomerId, date
                      ORDER BY Quantity DESC NULLS LAST
                      ROWS BETWEEN
                        UNBOUNDED PRECEDING AND
                        CURRENT ROW) as maxPurchase
FROM dfWithDate WHERE CustomerId IS NOT NULL ORDER BY CustomerId
```

</details>

> Window ë©”ì„œë“œ
> - `partitionBy()` : ê·¸ë£¹ì„ ì–´ë–»ê²Œ ë‚˜ëˆŒì§€ ê²°ì • (ì§€ê¸ˆê¹Œì§€ íŒŒí‹°ì…”ë‹ ìŠ¤í‚¤ë§ˆ ê°œë…ì´ë‘ ê´€ë ¨ X)
> - `orderBy()` : íŒŒí‹°ì…˜ ì •ë ¬ ë°©ì‹ ì •ì˜
> - `rowsBetween(from, to)` : ì…ë ¥ëœ ë¡œìš°ì˜ ì°¸ì¡° ê¸°ë°˜ìœ¼ë¡œ í”„ë ˆì„ì— ë¡œìš°ê°€ í¬í•¨ë  ìˆ˜ ìˆëŠ”ì§€ ê²°ì •
>
> row_number vs **rank** vs **dense_rank**
> - `row_number()` : ìˆœì„œëŒ€ë¡œ ë„˜ë²„ë§ (1,2,3,4 ...)
> - `rank()` : ìˆœì„œëŒ€ë¡œ ë„˜ë²„ë§ + ê°™ì€ ê°’ì¼ ê²½ìš° ê°™ì€ ìˆ«ì (1,1,3,4 ...)
> - `dense_rank()` : rank ì™€ ë™ì¼í•˜ë˜, ë¹ˆê°’ ì—†ì´ ì¦ê°€í•˜ê²Œë” ë„˜ë²„ë§ (1,1,2,3, ...)

### 7.4 ê·¸ë£¹í™” ì…‹
- ì»¬ëŸ¼ì˜ ê°’ì„ ì´ìš©í•´ ì—¬ëŸ¬ ì»¬ëŸ½ ì§‘ê³„ => `group-by` í‘œí˜„ì‹
  - ê·¸ëŸ¬ë©´ **ì—¬ëŸ¬ ê·¸ë£¹**ì— ê±¸ì³ ì§‘ê³„ëŠ”? => **ê·¸ë£¹í™”ì…‹**  ì‚¬ìš©
- ê·¸ë£¹í™” ì…‹ : ì—¬ëŸ¬ ì§‘ê³„ë¥¼ ê²°í•©í•˜ëŠ” ì €ìˆ˜ì¤€ ê¸°ëŠ¥
  - `GROUPING SETS` êµ¬ë¬¸ì€ SQLì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥
  - DataFrameì—ì„œ ë™ì¼ ì—°ì‚°í•˜ë ¤ë©´? => **ë¡¤ì—…**, **íë¸Œ** ë©”ì„œë“œ ì‚¬ìš©
- ì£¼ì˜ ì‚¬í•­
  - ê·¸ë£¹í™” ì…‹, ë¡¤ì—…, íë¸Œ ì‚¬ìš© ì‹œ **null ì œê±° í•„ìˆ˜**
  - nullì— ë”°ë¼ ì§‘ê³„ ìˆ˜ì¤€ì´ ë‹¬ë¼ì§ (=> null ë¯¸ì œê±°ì‹œ ë¶€ì •í™•í•œ ê²°ê³¼)
  

<details><summary class="point-color-can-hover">[7.4.0] 'ê·¸ë£¹í™” ì…‹' ì˜ˆì œ í¼ì¹˜ê¸°</summary>

```scala
// ê·¸ë£¹í™” ì…‹ ì‚¬ìš© ì‹œ null ì œê±° í•„ìˆ˜
val dfNoNull = dfWithDate.drop()
dfNoNull.createOrReplaceTempView("dfNoNull")
```

```sql
-- SQL ì˜ˆì œ 1) ì¬ê³  ì½”ë“œ(StockCode)ì™€ ê³ ê°(CustomerId) ë³„ ì´ ìˆ˜ëŸ‰ êµ¬í•˜ê¸°
SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull
GROUP BY customerId, stockCode
ORDER BY CustomerId DESC, stockCode DESC

-- (ê·¸ë£¹í™” ì…‹ ì‚¬ìš©í•œ ë™ì¼ í‘œí˜„)
SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull
GROUP BY customerId, stockCode GROUPING SETS((customerId, stockCode))
ORDER BY CustomerId DESC, stockCode DESC
```

```sql
-- SQL ì˜ˆì œ 2) ì˜ˆì œ 1 + ì¬ê³  ì½”ë“œë‚˜ ê³ ê° ìƒê´€ì—†ì´ ì´ ìˆ˜ëŸ‰ í•©ì‚° ê²°ê³¼ ì¶”ê°€ => group-byë¡œ ì²˜ë¦¬ ë¶ˆê°€
-- ê·¸ë£¹í™” ì…‹ìœ¼ë¡œ ì§‘ê³„ ë°©ì‹ ì§€ì •
SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull
GROUP BY customerId, stockCode GROUPING SETS((customerId, stockCode),())
ORDER BY CustomerId DESC, stockCode DESC
```

```bash
# output
+----------+---------+-------------+
|CustomerId|stockCode|sum(Quantity)|
+----------+---------+-------------+
|     18287|    85173|           48|
|     18287|   85040A|           48|
|     18287|   85039B|          120|
...
|     18287|    23269|           36|
+----------+---------+-------------+
```

</details>

- ë¡¤ì—…(rollup)
  - `group-by` ìŠ¤íƒ€ì¼ì˜ ë‹¤ì–‘í•œ ì—°ì‚°ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ë‹¤ì°¨ì› ì§‘ê³„ ê¸°ëŠ¥
  - `rollup(ê·¸ë£¹í™” í‚¤)` => ë‹¤ì–‘í•œ ì»¬ëŸ¼ì„ ê·¸ë£¹í™” í‚¤ë¡œ ì„¤ì • ê°€ëŠ¥
  - ë¡¤ì—…ëœ ì»¬ëŸ¼ê°’ì´ ëª¨ë‘ null ì¸ ë¡œìš° = í•´ë‹¹ ì»¬ëŸ¼ì— ì†í•œ ë ˆì½”ë“œì˜ ì „ì²´ í•©ê³„

<details><summary class="point-color-can-hover">[7.4.1] 'ë¡¤ì—…' ì˜ˆì œ í¼ì¹˜ê¸°</summary>

```scala
// ì‹œê°„(ì‹ ê·œ Date ì»¬ëŸ¼), ê³µê°„(Country) ì„ ì¶•ìœ¼ë¡œ í•˜ëŠ” ë¡¤ì—…
// => 'ëª¨ë“  ë‚ ì§œ ì´í•©', 'ë‚ ì§œë³„ ì´í•©', 'ë‚ ì§œë³„ êµ­ê°€ë³„ ì´í•©' í¬í•¨í•˜ëŠ” DataFrame ìƒì„±
val rolledUpDF = (dfNoNull.rollup("Date", "Country").agg(sum("Quantity"))
  .selectExpr("Date", "Country", "`sum(Quantity)` as total_quantity")
  .orderBy("Date"))
rolledUpDF.show()
// +----------+--------------+--------------+
// |      Date|       Country|total_quantity|
// +----------+--------------+--------------+
// |      null|          null|       5176450| => ì „ì²´ í•©ê³„
// |2010-12-01|   Netherlands|            97|
// |2010-12-01|       Germany|           117|
// |2010-12-01|     Australia|           107|
// |2010-12-01|        France|           449|
// |2010-12-01|          EIRE|           243|
// |2010-12-01|United Kingdom|         23949|
// |2010-12-01|          null|         26814|
// |2010-12-01|        Norway|          1852|
// |2010-12-02|          EIRE|             4|
// |2010-12-02|          null|         21023|
// |2010-12-02|       Germany|           146|
// |2010-12-02|United Kingdom|         20873|
// |2010-12-03|        France|           239|
// |2010-12-03|      Portugal|            65|
// |2010-12-03|       Germany|           170|
// |2010-12-03|       Belgium|           528|
// |2010-12-03|         Spain|           400|
// |2010-12-03|         Italy|           164|
// |2010-12-03|   Switzerland|           110|
// +----------+--------------+--------------+


// Country, Date ë‘˜ ë‹¤ null ì¸ ë¡œìš° => ì „ì²´ í•©ê³„ ë‚˜íƒ€ëƒ„
rolledUpDF.where("Country IS NULL").show()
rolledUpDF.where("Date IS NULL").show()
// +----+-------+--------------+
// |Date|Country|total_quantity|
// +----+-------+--------------+
// |null|   null|       5176450|
// +----+-------+--------------+
```

</details>

- íë¸Œ(cube)
  - ë¡¤ì—…ì˜ ê³ ì°¨ì›ì  ì‚¬ìš© (í˜¸ì¶œ ë°©ì‹ë„ ìœ ì‚¬)
  - ìš”ì†Œë“¤ì„ ê³„ì¸µì ìœ¼ë¡œ ë‹¤ë£¨ëŠ” ëŒ€ì‹  ëª¨ë“  ì°¨ì›ì— ëŒ€í•´ ë™ì¼í•œ ì‘ì—… ìˆ˜í–‰
  - ex. ì „ì²´ ê¸°ê°„ì— ëŒ€í•œ ë‚ ì§œì™€ êµ­ê°€ë³„ ê²°ê³¼ êµ¬í•˜ê¸°
  - `cube(ê·¸ë£¹í™” í‚¤)` => ìš”ì•½ ì •ë³´ í…Œì´ë¸” ë§Œë“¤ê¸° ê°€ëŠ¥

<details><summary class="point-color-can-hover">[7.4.2] 'íë¸Œ' ì˜ˆì œ í¼ì¹˜ê¸°</summary>

```scala
// ì‹œê°„(ì‹ ê·œ Date ì»¬ëŸ¼), ê³µê°„(Country) ì„ ì¶•ìœ¼ë¡œ í•˜ëŠ” íë¸Œ
// => 'ì „ì²´ ê¸°ê°„ì— ëŒ€í•œ ë‚ ì§œì™€ êµ­ê°€ë³„ ê²°ê³¼' í¬í•¨í•˜ëŠ” DataFrame ìƒì„±
//     (ì™¸ì—ë„ ì „ì²´ ë‚ ì§œì™€ ëª¨ë“  êµ­ê°€ì— ëŒ€í•œ í•©ê³„, ëª¨ë“  êµ­ê°€ì˜ ë‚ ì§œë³„ í•©ê³„, ë‚ ì§œë³„ êµ­ê°€ë³„ í•©ê³„, ì „ì²´ ë‚ ì§œì˜ êµ­ê°€ë³„ í•©ê³„, ... ê°€ëŠ¥)
(dfNoNull.cube("Date", "Country").agg(sum(col("Quantity")))
  .select("Date", "Country", "sum(Quantity)").orderBy("Date").show())
// +----+--------------------+-------------+
// |Date|             Country|sum(Quantity)|
// +----+--------------------+-------------+
// |null|               Japan|        25218|
// |null|            Portugal|        16180|
// |null|             Germany|       117448|
// |null|                 RSA|          352|
// |null|           Australia|        83653|
// |null|           Hong Kong|         4769|
// |null|              Cyprus|         6317|
// |null|             Finland|        10666|
// |null|United Arab Emirates|          982|
// |null|                null|      5176450|
// |null|         Unspecified|         3300|
// |null|               Spain|        26824|
// |null|           Singapore|         5234|
// |null|     Channel Islands|         9479|
// |null|             Lebanon|          386|
// |null|                 USA|         1034|
// |null|             Denmark|         8188|
// |null|              Norway|        19247|
// |null|      Czech Republic|          592|
// |null|  European Community|          497|
// +----+--------------------+-------------+
```

</details>

- ê·¸ë£¹í™” ë©”íƒ€ë°ì´í„°
  - íë¸Œ, ë¡¤ì—… ì‚¬ìš© ì‹œ ì§‘ê³„ ìˆ˜ì¤€ì— ë”°ë¼ ì‰½ê²Œ í•„í„°ë§í•˜ê³ ì í•˜ë©´ => **ì§‘ê³„ ìˆ˜ì¤€ ì¡°íšŒ** í•„ìš”
  - `grouping_id()` : ê²°ê³¼ ë°ì´í„°ì…‹ì˜ ì§‘ê³„ ìˆ˜ì¤€ì„ ëª…ì‹œí•˜ëŠ” ì»¬ëŸ¼ ì œê³µ

<details><summary class="point-color-can-hover">[7.4.3] grouping_id() ì˜ˆì œ í¼ì¹˜ê¸°</summary>

```scala
import org.apache.spark.sql.functions.{grouping_id, sum, expr}

(dfNoNull.cube("customerId", "stockCode").agg(grouping_id(), sum("Quantity"))
.orderBy(col("grouping_id()").desc)
.show())
// => 4ê°œì˜ ê°œë³„ ê·¸ë£¹í™” ID ê°’ (0,1,2,3) ë°˜í™˜ë¨
// +----------+---------+-------------+-------------+
// |customerId|stockCode|grouping_id()|sum(Quantity)|
// +----------+---------+-------------+-------------+
// |      null|     null|            3|      5176450| => 3 : ê°€ì¥ ë†’ì€ ê³„ì¸µì˜ ì§‘ê³„ ê²°ê³¼. ì „ì²´ ì´ ìˆ˜ëŸ‰ (customerId, stockCode ê´€ê³„ X)
// |      null|    84226|            2|           17| => 2 : ê°œë³„ stockCode ë³„ ì´ ìˆ˜ëŸ‰ (customerId ê´€ê³„ X)
// |      null|    22856|            2|          518|
// |      null|    22352|            2|         3077|
//            ...
// |     14907|     null|            1|         1686| => 1 : customerId ê¸°ë°˜ ì´ ìˆ˜ëŸ‰ ì œê³µ (êµ¬ë§¤ ë¬¼í’ˆ ê´€ê³„ X)
// |     14543|     null|            1|          600|
//            ...
// |     13047|    22749|            0|           12| => 0 : customerId - stockCode ë³„ ì¡°í•©ì— ë”°ë¼ ì´ ìˆ˜ëŸ‰
// |     15311|    22083|            0|          169|
// +----------+---------+-------------+-------------+
```

</details>

- í”¼ë²—(pivot)
  - `pivot()` : ë¡œìš° â†’ ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜ ê°€ëŠ¥
  - => ì»¬ëŸ¼ì˜ ëª¨ë“  ê°’ì„ ë‹¨ì¼ ê·¸ë£¹í™”í•˜ì—¬ ê³„ì‚° ê°€ëŠ¥
    - ê·¸ëŸ¬ë‚˜ ë°ì´í„° íƒìƒ‰ë°©ì‹ì— ë”°ë¼ í”¼ë²— ìˆ˜í–‰ ê²°ê³¼ê°’ì´ ê°ì†Œí•  ìˆ˜ ìˆìŒ
  - íŠ¹ì • ì»¬ëŸ¼ cardinalityê°€ ë‚®ìœ¼ë©´ í”¼ë²—ìœ¼ë¡œ ë‹¤ìˆ˜ ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜ ì¶”ì²œ   => ìŠ¤í‚¤ë§ˆ, ì¿¼ë¦¬ ëŒ€ìƒ í™•ì¸ ê°€ëŠ¥

<details><summary class="point-color-can-hover">[7.4.4] 'í”¼ë²—' ì˜ˆì œ í¼ì¹˜ê¸°</summary>

```scala
val pivoted = dfWithDate.groupBy("date").pivot("Country").sum() // ì§‘ê³„ ìˆ˜í–‰ => ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ìœ¼ë¡œ ë‚˜ì˜´
// pivoted.printSchema()
// root
//  |-- date: date (nullable = true)
//  |-- Australia_sum(Quantity): long (nullable = true)
//  |-- Australia_sum(UnitPrice): double (nullable = true)
//  |-- Australia_sum(CustomerID): long (nullable = true)
//  |-- Austria_sum(Quantity): long (nullable = true)
//  |-- Austria_sum(UnitPrice): double (nullable = true)
//  |-- Austria_sum(CustomerID): long (nullable = true)
//  |-- Bahrain_sum(Quantity): long (nullable = true)
//  |-- Bahrain_sum(UnitPrice): double (nullable = true)
//  |-- Bahrain_sum(CustomerID): long (nullable = true)
//  ...

pivoted.where("date > '2011-12-05'").select("date" ,"`USA_sum(Quantity)`").show()
// +----------+-----------------+
// |      date|USA_sum(Quantity)|
// +----------+-----------------+
// |2011-12-06|             null|
// |2011-12-09|             null|
// |2011-12-08|             -196|
// |2011-12-07|             null|
// +----------+-----------------+
```

</details>

### 7.5 ì‚¬ìš©ì ì •ì˜ ì§‘ê³„ í•¨ìˆ˜
- ì‚¬ìš©ì ì •ì˜ ì§‘ê³„ í•¨ìˆ˜ (**UDAF**, user-defined aggregation function)
  - ì§ì ‘ ì œì‘í•œ í•¨ìˆ˜ë‚˜ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ì— ê¸°ë°˜í•œ ìì²´ ì§‘ê³„ í•¨ìˆ˜ ì •ì˜ ë°©ë²•
  - UDAF ì‚¬ìš© => ì…ë ¥ ë°ì´í„° ê·¸ë£¹ì— ì§ì ‘ ê°œë°œí•œ ì—°ì‚° ìˆ˜í–‰ ê°€ëŠ¥
  - ìŠ¤íŒŒí¬ëŠ” ì…ë ¥ ë°ì´í„°ì˜ ëª¨ë“  ê·¸ë£¹ ì¤‘ê°„ ê²°ê³¼ë¥¼ ë‹¨ì¼ AggregationBufferì— ì €ì¥/ê´€ë¦¬
- UDAFëŠ” í˜„ì¬ ìŠ¤ì¹¼ë¼, ìë°”ë¡œë§Œ ì‚¬ìš© ê°€ëŠ¥
  - Spark 2.3 ì—ì„œëŠ” UDF/UDAF => í•¨ìˆ˜ ë“±ë¡ ê°€ëŠ¥ ([6.12] ì°¸ê³ )
- ìƒì„± ë°©ë²•
  - ê¸°ë³¸ í´ë˜ìŠ¤ UserDefinedAggregateFunction ìƒì† + ë©”ì„œë“œ ì •ì˜

> UDAF ìƒì„± ì‹œ ì •ì˜í•´ì•¼í•  ë©”ì„œë“œ
> - inputScheme : `UDAF ì…ë ¥ íŒŒë¼ë¯¸í„°ì˜ ìŠ¤í‚¤ë§ˆ`ë¥¼ StructType ë¡œ ì •ì˜
> - bufferSchema : `UDAF ì¤‘ê°„ ê²°ê³¼ì˜ ìŠ¤í‚¤ë§ˆ`ë¥¼ StructType ë¡œ ì •ì˜
> - dataType : `ë°˜í™˜ë  ê°’ì˜ DataType` ì •ì˜
> - deterministic : `UDAFê°€ ë™ì¼í•œ ì…ë ¥ê°’ì— ëŒ€í•´ í•­ìƒ ë™ì¼í•œ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ”ì§€` Boolean ê°’ìœ¼ë¡œ ì •ì˜
> - initialize : `ì§‘ê³„ìš© ë²„í¼ ê°’ ì´ˆê¸°í™” ë¡œì§` ì •ì˜
> - update : ì…ë ¥ë°›ì€ ë¡œìš° ê¸°ë°˜ìœ¼ë¡œ `ë‚´ë¶€ ë²„í¼ ì—…ë°ì´íŠ¸ ë¡œì§` ì •ì˜
> - merge : ë‘ ê°œì˜ `ì§‘ê³„ìš© ë²„í¼ ë³‘í•© ë¡œì§` ì •ì˜
> - evaluate : `ì§‘ê³„ ìµœì¢… ê²°ê³¼ ìƒì„± ë¡œì§` ì •ì˜

<details><summary class="point-color-can-hover">[7.5] ì˜ˆì œ í¼ì¹˜ê¸°</summary>

```scala
// UDAF ì˜ˆì œ - 'BoolAnd' Class : ì…ë ¥ëœ ëª¨ë“  ë¡œìš°ì˜ ì»¬ëŸ¼ì´ trueì¸ì§€ íŒë‹¨
class BoolAnd extends org.apache.spark.sql.expressions.UserDefinedAggregateFunction {
  import org.apache.spark.sql.types.{StructType, StructField, BooleanType, DataType}
  import org.apache.spark.sql.expressions.MutableAggregationBuffer
  import org.apache.spark.sql.Row

  def inputSchema: org.apache.spark.sql.types.StructType =
    StructType(StructField("value", BooleanType) :: Nil)
  def bufferSchema: StructType = StructType(
    StructField("result", BooleanType) :: Nil
  )
  def dataType: DataType = BooleanType
  def deterministic: Boolean = true
  def initialize(buffer: MutableAggregationBuffer): Unit = {
    buffer(0) = true
  }
  def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
    buffer(0) = buffer.getAs[Boolean](0) && input.getAs[Boolean](0)
  }
  def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
    buffer1(0) = buffer1.getAs[Boolean](0) && buffer2.getAs[Boolean](0)
  }
  def evaluate(buffer: Row): Any = {
    buffer(0)
  }
}


// í•¨ìˆ˜ë¡œ ë“±ë¡ ë° ì‚¬ìš©
val ba = new BoolAnd
spark.udf.register("booland", ba)
import org.apache.spark.sql.functions._
(spark.range(1)
  .selectExpr("explode(array(TRUE, TRUE, TRUE)) as t")
  .selectExpr("explode(array(TRUE, FALSE, TRUE)) as f", "t")
  .select(ba(col("t")), expr("booland(f)"))
  .show())
// +----------+----------+
// |booland(t)|booland(f)|
// +----------+----------+
// |      true|     false|
// +----------+----------+
```

</details>


### 7.6 ì •ë¦¬
- ìŠ¤íŒŒí¬ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì—¬ëŸ¬ ìœ í˜•ì˜ ì§‘ê³„ ì—°ì‚°
- ê·¸ë£¹í™”, ìœˆë„ìš° í•¨ìˆ˜, ë¡¤ì—…, íë¸Œ

### ğŸ“’ ë‹¨ì–´ì¥
- ë¹„ëŒ€ì¹­ë„(skewness) : ì‹¤ìˆ«ê°’ í™•ë¥ ë³€ìˆ˜ì˜ í™•ë¥ ë¶„í¬ ë¹„ëŒ€ì¹­ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œ (=ì™œë„)
- ì²¨ë„(kurtosis) : í™•ë¥ ë¶„í¬ì˜ ë¾°ì¡±í•œ ì •ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì²™ë„
    