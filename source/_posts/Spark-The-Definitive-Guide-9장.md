---
title: '&#039;Spark The Definitive Guide&#039; 9ì¥ - ì˜ì“°ëŠ” ì—­ì‹œ ë°ì´í„°ì†ŒìŠ¤'
date: 2021-02-16 00:11:36
categories: spark
tags:
  - spark
  - apache
  - book
  - study
---

<br/>

<img src="https://user-images.githubusercontent.com/26691216/108165351-c8bd8100-7135-11eb-9cbe-6ccfa0e63155.gif" width=400/>

<center><h2>_ _ _</h2></center>

<br/>

---

# CHAPTER 9 ë°ì´í„°ì†ŒìŠ¤
ìŠ¤íŒŒí¬ ê¸°ë³¸ 6ê°€ì§€ 'í•µì‹¬' ë°ì´í„° ì†ŒìŠ¤ + ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ë§Œë“  ì™¸ë¶€ ë°ì´í„°ì†ŒìŠ¤ ì†Œê°œ

í•µì‹¬ë°ì´í„° ì†ŒìŠ¤ë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ì½ê³  ì“°ëŠ” ë°©ë²•ì„ í„°ë“í•˜ê³ 
ì„œë“œíŒŒí‹° ë°ì´í„°ì†ŒìŠ¤ì™€ ìŠ¤íŒŒí¬ ì—°ë™ ì‹œ ê³ ë ¤í•´ì•¼í•  ì ì„ ë°°ìš°ëŠ” ê²ƒì´ ëª©í‘œ

#### ìŠ¤íŒŒí¬ì˜ í•µì‹¬ ë°ì´í„° ì†ŒìŠ¤
- CSV
- JSON
- íŒŒì¼€ì´(Parquet)
- ORC
- JDBC/ODBC ì—°ê²°
- ì¼ë°˜ í…ìŠ¤íŠ¸ íŒŒì¼

#### ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ë§Œë“  ë°ì´í„°ì†ŒìŠ¤
- [ì¹´ì‚°ë“œë¼](http://bit.ly/2DSafT8)
- [HBase](http://bit.ly/2FkKN5A)
- [ëª½ê³ DB](http://bit.ly/2BwA7yq)
- [AWS Redshift](http://bit.ly/2GlMsJE)
- [XML](http://bit.ly/2GitGCK)
- ê¸°íƒ€ ìˆ˜ë§ì€ ë°ì´í„° ì†ŒìŠ¤

### 9.1 ë°ì´í„°ì†ŒìŠ¤ APIì˜ êµ¬ì¡°
- ë°ì´í„° ì†ŒìŠ¤ API ì „ì²´ êµ¬ì¡°ë¶€í„° ì´í•´í•˜ê¸°
- ***ì½ê¸° API*** êµ¬ì¡°
  - í•µì‹¬ êµ¬ì¡° (ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì½ì„ ë•Œ í•´ë‹¹ í˜•ì‹ ì‚¬ìš©)  <i style="color:lightgray">// ìš”ì•½ í‘œê¸°ë²•ë„ ì¡´ì¬</i>
    ```scala
    DataFrameReader.format(...).option("key", "value").schema(...).load()
    ```
  - `format()` : í¬ë§· ì„¤ì •ì€ Optional (default - Parquet í¬ë©§)
  - `option()` : ë°ì´í„° ì½ëŠ” ë°©ë²•ì— ëŒ€í•œ íŒŒë¼ë¯¸í„° í‚¤-ê°’ ìŒìœ¼ë¡œ ì„¤ì •
  - `schema()` : ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ìŠ¤í‚¤ë§ˆë¥¼ ì œê³µí•˜ê±°ë‚˜ ì¶”ë¡  ê¸°ëŠ¥ ì‚¬ìš© ì‹œ. Optional
- ë°ì´í„° ì½ê¸°ì˜ ê¸°ì´ˆ
  - `DataFrameReader` : ìŠ¤íŒŒí¬ì—ì„œ ë°ì´í„°ë¥¼ ì½ì„ ë•Œ ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©
    ```scala
    // DataFrameReaderì€ SparkSessionì˜ read ì†ì„±ìœ¼ë¡œ ì ‘ê·¼
    spark.read
    ```
  - DataFrameReaderì— ì§€ì •í•´ì•¼í•˜ëŠ” ê°’
    - í¬ë§·
    - ìŠ¤í‚¤ë§ˆ
    - ì½ê¸° ëª¨ë“œ (í•„ìˆ˜, default ê°’ ì¡´ì¬)
    - ì˜µì…˜
    - (+ **ë°ì´í„° ì½ì„ ê²½ë¡œ** í•„ìˆ˜ ì§€ì •)
  - **ì½ê¸° ëª¨ë“œ** : ìŠ¤íŒŒí¬ê°€ í˜•ì‹ì— ë§ì§€ì•ŠëŠ” ë°ì´í„°ë¥¼ ë§Œë‚¬ì„ ë•Œ ë™ì‘ ë°©ì‹ ì§€ì •í•˜ëŠ” ì˜µì…˜
    - ë°˜ì •í˜• ë°ì´í„°ì†ŒìŠ¤ ë‹¤ë£° ì‹œ ë§ì´ ë°œìƒ
  - ìŠ¤íŒŒí¬ì˜ ì½ê¸° ëª¨ë“œ ì¢…ë¥˜
    - `permissive` (default): ì˜¤ë¥˜ ë ˆì½”ë“œ ëª¨ë“  í•„ë“œë¥¼ nullë¡œ ì§€ì •í•˜ê³  ì˜¤ë¥˜ ë ˆì½”ë“œë¥¼ _corrupt_record (ë¬¸ìì—´ ì»¬ëŸ¼) ì— ê¸°ë¡
    - `dropMalformed` : í˜•ì‹ì— ë§ì§€ì•ŠëŠ” ë ˆì½”ë“œê°€ í¬í•¨ëœ ë¡œìš° ì œê±°
    - `failFast` : í˜•ì‹ì— ë§ì§€ì•ŠëŠ” ë ˆì½”ë“œ ë§Œë‚  ì‹œ ì¦‰ì‹œ ì¢…ë£Œ
    ```scala
    // ì½ê¸° ì½”ë“œ êµ¬ì„± ì˜ˆì œ
    spark.read.format("csv")
      .option("mode", "FAILFAST")
      .option("inferSchema", "true")
      .option("path", "path/to/file(s)")
      .schema(someSchema)
      .load()
    ```
- ***ì“°ê¸° API*** êµ¬ì¡°
  - í•µì‹¬ êµ¬ì¡° (ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì½ì„ ë•Œ í•´ë‹¹ í˜•ì‹ ì‚¬ìš©)
    ```scala
    DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()
    ```
  - `format()` : í¬ë§· ì„¤ì •ì€ Optional (default - Parquet í¬ë©§)
  - `option()` : ë°ì´í„° ì“°ê¸° ë°©ë²• ì„¤ì •
  - `partitionBy()`, `bucketBy()`, `sortBy()` : ìµœì¢… íŒŒì¼ ë°°ì¹˜ í˜•íƒœ(layout) ì œì–´ ê°€ëŠ¥. íŒŒì¼ê¸°ë°˜ ë°ì´í„°ì†ŒìŠ¤ì—ë§Œ ë™ì‘
- ë°ì´í„° ì“°ê¸°ì˜ ê¸°ì´ˆ
  - ë°ì´í„° ì½ê¸°ì™€ ë§¤ìš° ìœ ì‚¬. ReaderëŒ€ì‹  Writer ì‚¬ìš©
  - `DataFrameWriter` : ë°ì´í„° ì†ŒìŠ¤ì— í•­ìƒ ë°ì´í„°ë¥¼ ê¸°ë¡í•´ì•¼í•˜ê³ , DataFrame ë³„ë¡œ DataFramewriterì— ì ‘ê·¼í•´ì•¼í•¨
    ```scala
    // DataFrame ì˜ write ì†ì„±ì„ ì´ìš©í•´ì„œ DataFrameWriterì— ì ‘ê·¼
    dataFrame.write
    ```
  - DataFrameWriterì— ì§€ì •í•´ì•¼í•˜ëŠ” ê°’
    - í¬ë§·
    - ì˜µì…˜
    - ì €ì¥ ëª¨ë“œ
    - (+ **ë°ì´í„° ì €ì¥ ê²½ë¡œ** í•„ìˆ˜ ì§€ì •)
  - **ì €ì¥ ëª¨ë“œ** : ìŠ¤íŒŒí¬ê°€ ì§€ì •ëœ ìœ„ì¹˜ì—ì„œ ë™ì¼í•œ íŒŒì¼ì„ ë°œê²¬í–ˆì„ ë•Œ ë™ì‘ ë°©ì‹ ì§€ì •í•˜ëŠ” ì˜µì…˜
  - ìŠ¤íŒŒí¬ì˜ ì €ì¥ ëª¨ë“œ ì¢…ë¥˜
    - `append` : í•´ë‹¹ ê²½ë¡œì— ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼ ëª©ë¡ì— ê²°ê³¼ íŒŒì¼ ì¶”ê°€
    - `overwrite` : ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ëª¨ë“  ë°ì´í„° ë®ì–´ì“°ê¸°
    - `errorIfExists` (default) : í•´ë‹¹ ê²½ë¡œì— ë°ì´í„°ë‚˜ íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ì˜¤ë¥˜ ë°œìƒ ë° ì“°ê¸° ì‘ì—… ì‹¤íŒ¨
    - `ignore` : ì•„ë¬´ëŸ° ì²˜ë¦¬ ì•ˆí•¨

    ```scala
    // ì“°ê¸° ì½”ë“œ êµ¬ì„± ì˜ˆì œ
    spark.write.format("csv")
      .option("mode", "OVERWRITE")
      .option("dateFormat", "yyyy-MM-dd")
      .option("path", "path/to/file(s)")
      .save()
    ```


### 9.2 CSV íŒŒì¼

- CSV(comma-separated values) : `,` ë¡œ êµ¬ë¶„ëœ ê°’
  - ê° ì¤„ì´ ë‹¨ì¼ ë ˆì½”ë“œ, ë ˆì½”ë“œì˜ ê° í•„ë“œëŠ” ì½¤ë§ˆë¡œ êµ¬ë¶„í•˜ëŠ” í…ìŠ¤íŠ¸ íŒŒì¼ í¬ë©§
  - êµ¬ì¡°ì ì¸ ê²ƒ ê°™ì•„ë„ ê°œ ê¹Œë‹¤ë¡œìš´ í¬ë§· (ë‹¤ì–‘í•œ ì „ì œ ìƒì„± ê°€ëŠ¥...)
  - => ë”°ë¼ì„œ CSV Reader ê°€ **ë§ì€ ì˜µì…˜** ì œê³µ 
- ì˜µì…˜
  - CSV Reader, Writer ë§ì€ ì˜µì…˜ ì œê³µ (`p.250-251 [í‘œ 9-3]` ì°¸ê³ )
  - maxColumns, inferSchema ë“± ì“°ê¸°ì—ì„œëŠ” ì ìš©ë˜ì§€ ì•ŠëŠ” ì˜µì…˜ ë¹¼ê³ ëŠ” **ì½ê¸°ì™€ ì“°ê¸°ëŠ” ë™ì¼í•œ ì˜µì…˜** ì œê³µ
- CSV íŒŒì¼ ì½ê¸°
  - ì˜ˆì œ ì°¸ê³ 
    ```scala
    spark.read.format("csv")
    // in Scala
    /*
    (spark.read.format("csv")
      .option("header", "true")
      .option("mode", "FAILFAST")
      .option("inferSchema", "true")
      .load("some/path/to/file.csv"))
    */
    ```
  - ë°ì´í„°ê°€ ê¸°ëŒ€í•œ ë°ì´í„° í¬ë§·ì´ ì•„ë‹Œê²½ìš°?
    - **ì‹¤ì œ** ìŠ¤í‚¤ë§ˆì™€ëŠ” ì¼ì¹˜í•˜ì§€ ì•Šì§€ë§Œ ìŠ¤íŒŒí¬ëŠ” ë¬¸ì œ ì¸ì§€ X
    - ìŠ¤íŒŒí¬ê°€ ì‹¤ì œë¡œ ë°ì´í„°ë¥¼ ì½ì–´ ë“¤ì´ëŠ” ì‹œì ì— ë¬¸ì œ ë°œìƒ (ìŠ¤íŒŒí¬ ì¡ ì¦‰ì‹œ ì¢…ë£Œ)
    - ì¦‰, ì •ì˜í•˜ëŠ” ì‹œì ì—ëŠ” ë¬¸ì œ X. ì¡ ì‹¤í–‰ ì‹œì ì—ë§Œ ì˜¤ë¥˜ ë°œìƒ  => <u>ìŠ¤íŒŒí¬ì˜ **ì§€ì—° ì—°ì‚°** íŠ¹ì„±</u>
- CSV íŒŒì¼ ì“°ê¸°
  - ì˜ˆì œ ì°¸ê³ 

<details><summary class="point-color-can-hover">[9.2] CSV íŒŒì¼ read ì˜ˆì œ í¼ì¹˜ê¸°</summary>

```scala
// CSV íŒŒì¼ ì½ê¸°
import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}

val myManualSchema = new StructType(Array(
  new StructField("DEST_COUNTRY_NAME", StringType, true),
  new StructField("ORIGIN_COUNTRY_NAME", StringType, true),
  new StructField("count", LongType, false)
))

(spark.read.format("csv")
  .option("header", "true")
  .option("mode", "FAILFAST")
  .schema(myManualSchema)
  .load("/data/flight-data/csv/2010-summary.csv")
  .show(5))
// +-----------------+-------------------+-----+
// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|
// +-----------------+-------------------+-----+
// |    United States|            Romania|    1|
// |    United States|            Ireland|  264|
// |    United States|              India|   69|
// |            Egypt|      United States|   24|
// |Equatorial Guinea|      United States|    1|
// +-----------------+-------------------+-----+


// ê¸°ëŒ€í•œ ë°ì´í„° í¬ë§·ì´ ì•„ë‹ˆë¼ë©´?
// => ë‹¹ì¥ ì—ëŸ¬ ë°œìƒì€ X. ìŠ¤íŒŒí¬ê°€ ì‹¤ì œë¡œ ë°ì´í„°ë¥¼ ì½ì–´ë“¤ì´ëŠ” ì‹œì ì— ì—ëŸ¬ ë°œìƒ (ì§€ì—° ì—°ì‚°)
val myManualSchema = new StructType(Array(
                     new StructField("DEST_COUNTRY_NAME", LongType, true),
                     new StructField("ORIGIN_COUNTRY_NAME", LongType, true),
                     new StructField("count", LongType, false) ))

(spark.read.format("csv")
  .option("header", "true")
  .option("mode", "FAILFAST")
  .schema(myManualSchema)
  .load("/data/flight-data/csv/2010-summary.csv")
  .take(5))
// org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 47.0 failed 1 times, most recent failure: Lost task 0.0 in stage 47.0 (TID 107, localhost, executor driver): org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST.

// (ì°¸ê³ ìš©) ì •ìƒ ìŠ¤í‚¤ë§ˆ
// val myManualSchema = new StructType(Array(
//                      new StructField("DEST_COUNTRY_NAME", StringType, true),
//                      new StructField("ORIGIN_COUNTRY_NAME", StringType, true),
//                      new StructField("count", LongType, false) ))
//
// => res122: Array[org.apache.spark.sql.Row] = Array([United States,Romania,1], [United States,Ireland,264], [United States,India,69], [Egypt,United States,24], [Equatorial Guinea,United States,1])
```

</details>
<details><summary class="point-color-can-hover">[9.2] CSV íŒŒì¼ write ì˜ˆì œ í¼ì¹˜ê¸°</summary>

```scala
// CSV íŒŒì¼ ì“°ê¸° (CSV íŒŒì¼ ì½ì–´ì„œ TSV íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°)
val csvFile = (spark.read.format("csv")
  .option("header", "true").option("mode", "FAILFAST").schema(myManualSchema)
  .load("/data/flight-data/csv/2010-summary.csv"))

(csvFile.write.format("csv").mode("overwrite").option("sep", "\t")
  .save("/tmp/my-tsv-file.tsv"))
```

```bash
# ë°ì´í„°ë¥¼ ì“°ëŠ ì‹œì ì— DataFrameì˜ íŒŒí‹°ì…˜ ìˆ˜ë¥¼ ë°˜ì˜
$ ls /tmp/my-tsv-file.tsv/
part-00000-183b90fb-5828-434a-b948-55dd5732c7b0-c000.csv  _SUCCESS
```

</details>

### 9.3 JSON íŒŒì¼

- JSON(JavaScript Object Notation)
  - ìŠ¤íŒŒí¬ëŠ” **ì¤„ë¡œ êµ¬ë¶„ëœ JSON** ì„ ê¸°ë³¸ì ìœ¼ë¡œ ì‚¬ìš©
    - multiLine ì˜µì…˜ìœ¼ë¡œ ì¤„ë¡œ êµ¬ë¶„ vs ì—¬ëŸ¬ ì¤„ë¡œ êµ¬ì„±ëœ ë°©ì‹ ì„ íƒ ê°€ëŠ¥
    - `true` ë¡œ ì„¤ì • ì‹œ => ì „ì²´ íŒŒì¼ì„ í•˜ë‚˜ì˜ JSON íŒŒì¼ë¡œ ì½ê¸° ê°€ëŠ¥
  - ê·¸ë˜ë„ ì¤„ë¡œ êµ¬ë¶„ëœ JSONì„ ì¶”ì²œí•˜ëŠ” ì´ìœ ?
    - ì „ì²´ íŒŒì¼ì„ ì½ì–´ì„œ ì €ì¥í•˜ëŠ” ë°©ì‹ì´ ì•„ë‹ˆë¯€ë¡œ => ìƒˆë¡œìš´ ë ˆì½”ë“œ ì¶”ê°€ ê°€ëŠ¥ (ì•ˆì •ì )
    - êµ¬ì¡°í™”ë˜ì–´ ìˆê³ , ìµœì†Œí•œì˜ ê¸°ë³¸ ë°ì´í„° íƒ€ì…ì´ ì¡´ì¬ => ì í•©í•œ ë°ì´í„°íƒ€ì… ì¶”ì • ê°€ëŠ¥
- ì˜µì…˜
  - JSONì€ ê°ì²´. CSV(í…ìŠ¤íŠ¸) ë³´ë‹¤ ì˜µì…˜ìˆ˜ ì ìŒ (`p.255-256 [í‘œ 9-4]` ì°¸ê³ )
- JSON íŒŒì¼ ì½ê¸°
  - ì˜ˆì œ ì°¸ê³ 
    ```scala
    spark.read.format("json")
    ```
- JSON íŒŒì¼ ì“°ê¸°
  - ì˜ˆì œ ì°¸ê³ 
  - ë°ì´í„°ì†ŒìŠ¤ì™€ ê´€ê³„ì—†ì´ JSON íŒŒì¼ë¡œ ì €ì¥ ê°€ëŠ¥
    - ex. CSV DataFrame => JSON íŒŒì¼
    - ì´ì „ì˜ ê·œì¹™ì„ ê·¸ëŒ€ë¡œ ë”°ë¥¸ë‹¤? (ì˜ˆì œì´ì•¼ê¸°ì¸ì§€?)
    - íŒŒí‹°ì…˜ë‹¹ í•˜ë‚˜ì˜ íŒŒì¼ì„ ë§Œë“¤ê³ , ì „ì²´ DataFrameì„ ë‹¨ì¼ í´ë”ì— ì €ì¥. JSON ê°ì²´ëŠ” í•œì¤„ì— í•˜ë‚˜ì”© ê¸°ë¡.

<details><summary class="point-color-can-hover">[9.3] JSON íŒŒì¼ read/write ì˜ˆì œ í¼ì¹˜ê¸°</summary>

```scala
// JSON íŒŒì¼ ì½ê¸°
// spark.read.format("json")
(spark.read.format("json").option("mode", "FAILFAST").schema(myManualSchema)
  .load("/data/flight-data/json/2010-summary.json").show(5))
// +-----------------+-------------------+-----+
// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|
// +-----------------+-------------------+-----+
// |    United States|            Romania|    1|
// |    United States|            Ireland|  264|
// |    United States|              India|   69|
// |            Egypt|      United States|   24|
// |Equatorial Guinea|      United States|    1|
// +-----------------+-------------------+-----+


// JSON íŒŒì¼ ì“°ê¸° (CSV DataFrame => JSON íŒŒì¼)
csvFile.write.format("json").mode("overwrite").save("/tmp/my-json-file.json")
```

```bash
# íŒŒí‹°ì…˜ë‹¹ í•˜ë‚˜ì˜ íŒŒì¼ ë§Œë“¤ê³  ì „ì²´ DataFrameì€ ë‹¨ì¼í´ë”ì— ì €ì¥
$ ls /tmp/my-json-file.json/
part-00000-8a5f3d0c-2241-4508-ab3f-e2648f9a5ff4-c000.json  _SUCCESS

# JSON ê°ì²´ëŠ” í•œì¤„ì— í•˜ë‚˜ì”© ê¸°ë¡
$ head -2 /tmp/my-json-file.json/part-00000-8a5f3d0c-2241-4508-ab3f-e2648f9a5ff4-c000.json
{"DEST_COUNTRY_NAME":"United States","ORIGIN_COUNTRY_NAME":"Romania","count":1}
{"DEST_COUNTRY_NAME":"United States","ORIGIN_COUNTRY_NAME":"Ireland","count":264}
```

</details>

### 9.4 íŒŒì¼€ì´ íŒŒì¼
- íŒŒì¼€ì´(Parquet) : ë‹¤ì–‘í•œ ìŠ¤í† ë¦¬ì§€ ìµœì í™” ê¸°ìˆ ì„ ì œê³µí•˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ë¡œ ë§Œë“¤ì–´ì§„ **ì»¬ëŸ¼ ê¸°ë°˜ì˜ ë°ì´í„° ì €ì¥ ë°©ì‹**
  - ë¶„ì„ ì›Œí¬ë¡œë“œì— ìµœì í™”
  - ì €ì¥ì†Œ ê³µê°„ ì ˆì•½
  - ì „ì²´ íŒŒì¼ ì½ê¸° ëŒ€ì‹  ê°œë³„ ì»¬ëŸ¼ ì½ê¸° ê°€ëŠ¥
  - ì»¬ëŸ¼ ê¸°ë°˜ì˜ ì••ì¶• ê¸°ëŠ¥ ì œê³µ
  - ì•„íŒŒì¹˜ ìŠ¤íŒŒí¬ì™€ íŠ¹íˆ í˜¸í™˜ good => ê·¸ë˜ì„œ **ìŠ¤íŒŒí¬ ê¸°ë³¸ íŒŒì¼ í¬ë©§**
  - ë³µí•© ë°ì´í„° íƒ€ì… ì§€ì› (CSVëŠ” ë°°ì—´ ì‚¬ìš© X)
- ì½ê¸° ì—°ì‚°ì´ CSV, JSONë³´ë‹¤ í›¨ì”¬ íš¨ìœ¨ì  => ì¥ê¸°ì €ì¥ìš© ë°ì´í„°ëŠ” íŒŒì¼€ì´ ê¶Œì¥
  - <a style="color:lightgray">ê± íŒŒì¼€ì´ê°€ ì§±ì§±ë§¨ì´ë€ ì†Œë¦¬ë‹¤</a>
- ì˜µì…˜
  - íŒŒì¼€ì´ëŠ” ì˜µì…˜ì´ ê±°ì˜ ì—†ìŒ. ë‹¨ 2ê°œ  (`p.259 [í‘œ 9-5]` ì°¸ê³ )
    - 2ê°œë§Œ ì¡´ì¬í•˜ëŠ” ì´ìœ ëŠ”.. ê·¸ëƒ¥ ëª¨ë²”ìƒ í¬ë§·ì´ê¸°ë•Œë¬¸... (ìì²´ ìŠ¤í‚¤ë§ˆ ì‚¬ìš©í•´ì„œ ë°ì´í„° ì €ì¥)
    - ê·¸ëŸ¬ë‚˜ 'í˜¸í™˜ë˜ì§€ ì•ŠëŠ” íŒŒì¼€ì´ íŒŒì¼' ì£¼ì˜ => íŠ¸ê¸° <u>ë‹¤ë¥¸ ë²„ì „(êµ¬ë²„ì „)ì˜ ìŠ¤íŒŒí¬ ì‚¬ìš© ì‹œ íŒŒì¼€ì´ ì €ì¥</u> ì— ì£¼ì˜
- íŒŒì¼€ì´ íŒŒì¼ ì½ê¸°
  - ì˜ˆì œ ì°¸ê³ 
    ```scala
    spark.read.format("parquet")
    ```
  - í¬ë§· ì„¤ì •ë§Œìœ¼ë¡œ ì¶©ë¶„
    - DataFrame í‘œí˜„ì„ ìœ„í•´ ì •í™•í•œ ìŠ¤í‚¤ë§ˆê°€ í•„ìš”í•  ë•Œë§Œ ìŠ¤í‚¤ë§ˆ ì§€ì • 
    - ê·¸ë ‡ì§€ë§Œ ì‚¬ì‹¤ ê±°ì˜ í•„ìš” X
  - íŒŒì¼€ì´ íŒŒì¼ì€ ìŠ¤í‚¤ë§ˆê°€ íŒŒì¼ ìì²´ì— ë‚´ì¥ë˜ì–´ ì¶”ë¡  í•„ìš” X
    - ì½ëŠ” ì‹œì ì— ìŠ¤í‚¤ë§ˆë¥¼ ì•Œ ìˆ˜ ìˆë‹¤ (Schema-on-read)
    - CSV íŒŒì¼ inferSchemaë‘ ë¹„ìŠ·
- íŒŒì¼€ì´ íŒŒì¼ ì“°ê¸°
  - ì˜ˆì œ ì°¸ê³ 
  - "ì½ê¸°ë§Œí¼ ì‰½ë‹¤" => íŒŒì¼ì˜ ê²½ë¡œë§Œ ëª…ì‹œí•˜ë©´ ë¨
    - ë¶„í•  ê·œì¹™ì€ ë‹¤ë¥¸ í¬ë§·ê³¼ ë™ì¼í•˜ê²Œ ì ìš©

<details><summary class="point-color-can-hover">[9.4] Parquet íŒŒì¼ read/write ì˜ˆì œ í¼ì¹˜ê¸°</summary>

```scala
// Parquet íŒŒì¼ ì½ê¸°
// spark.read.format("parquet")
(spark.read.format("parquet")
  .load("/data/flight-data/parquet/2010-summary.parquet").show(5))
// +-----------------+-------------------+-----+
// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|
// +-----------------+-------------------+-----+
// |    United States|            Romania|    1|
// |    United States|            Ireland|  264|
// |    United States|              India|   69|
// |            Egypt|      United States|   24|
// |Equatorial Guinea|      United States|    1|
// +-----------------+-------------------+-----+


// Parquet íŒŒì¼ ì“°ê¸°
(csvFile.write.format("parquet").mode("overwrite")
  .save("/tmp/my-parquet-file.parquet"))
```

```bash
# ë‹¤ë¥¸ í¬ë©§ê³¼ ë™ì¼í•œ ë¶„í•  ê·œì¹™
$ ls /tmp/my-parquet-file.parquet/
part-00000-7275ca33-21c1-4ce1-8e4f-93f9918a938d-c000.snappy.parquet  _SUCCESS
```

</details>

### 9.5 ORC íŒŒì¼
- ORC(Optimized Row Columnar) : í•˜ë‘¡ ì›Œí¬ë¡œë“œë¥¼ ìœ„í•´ ì„¤ê³„ëœ ìê¸° ê¸°ìˆ ì (self-describing)ì´ë©° ë°ì´í„° íƒ€ì…ì„ ì¸ì‹í•  ìˆ˜ ìˆëŠ” **ì»¬ëŸ¼ ê¸°ë°˜ì˜ íŒŒì¼ í¬ë§·**
  - ëŒ€ê·œëª¨ ìŠ¤íŠ¸ë¦¬ë° ì½ê¸°ì— ìµœì í™”
  - í•„ìš”í•œ ë¡œìš°ë¥¼ ì‹ ì†í•˜ê²Œ ì°¾ì„ ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ í†µí•©
  - ìŠ¤íŒŒí¬ì—ì„œ ë³„ë„ ì˜µì…˜ ì§€ì • ì—†ì´ ë°ì´í„° ì½ê¸° ê°€ëŠ¥
- ORC vs Parquet
  - ë§¤ìš° ìœ ì‚¬í•˜ì§€ë§Œ, ì°¨ì´ëŠ” Parquetì€ Sparkì—, ORCëŠ” Hiveì— ìµœì í™”ë˜ì–´ìˆìŒ
  - ORCëŠ” ì˜µì…˜ì€ ë”°ë¡œ ì—†ëŠ” ë“¯?
- ORC íŒŒì¼ ì½ê¸°
  - ì˜ˆì œ ì°¸ê³ 
- ORC íŒŒì¼ ì“°ê¸°
  - ì˜ˆì œ ì°¸ê³ 

<details><summary class="point-color-can-hover">[9.5] ORC íŒŒì¼ read/write ì˜ˆì œ í¼ì¹˜ê¸°</summary>

```scala
// ORC íŒŒì¼ ì½ê¸°
spark.read.format("orc").load("/data/flight-data/orc/2010-summary.orc").show(5)
// +-----------------+-------------------+-----+
// |DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|
// +-----------------+-------------------+-----+
// |    United States|            Romania|    1|
// |    United States|            Ireland|  264|
// |    United States|              India|   69|
// |            Egypt|      United States|   24|
// |Equatorial Guinea|      United States|    1|
// +-----------------+-------------------+-----+


// ORC íŒŒì¼ ì“°ê¸°
csvFile.write.format("orc").mode("overwrite").save("/tmp/my-json-file.orc")
```

```bash
# ë‹¤ë¥¸ í¬ë©§ê³¼ ë™ì¼í•œ ë¶„í•  ê·œì¹™
$ ls /tmp/my-json-file.orc/
part-00000-a45b9d23-eb06-48d1-935a-110cfdbddfdb-c000.snappy.orc  _SUCCESS
```

</details>

### 9.6 SQL ë°ì´í„°ë² ì´ìŠ¤
> ì˜ˆì œ ì¶”ê°€í•˜ë©´ì„œ ë‚´ìš© ë³´ì¶© ì˜ˆì •

- SQLite ìƒ˜í”Œë¡œ ì˜ˆì œ (DB ì„¤ì •ê³¼ì • ìƒëµ) _ ë¶„ì‚°í™˜ê²½ì—ì„œ ì‚¬ìš©í•´ì„œëŠ” X
- JDBC ë°ì´í„°ì†ŒìŠ¤ ì˜µì…˜ (`p.262-263 [í‘œ 9-6]` ì°¸ê³ )
- SQL ë°ì´í„°ë² ì´ìŠ¤ ì½ê¸°
- ì¿¼ë¦¬ í‘¸ì‹œë‹¤ìš´
  - ë°ì´í„°ë² ì´ìŠ¤ ë³‘ë ¬ë¡œ ì½ê¸°
  - ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ê¸°ë°˜ì˜ íŒŒí‹°ì…”ë‹
- SQL ë°ì´í„°ë² ì´ìŠ¤ ì“°ê¸°

### 9.7 í…ìŠ¤íŠ¸ íŒŒì¼
- ì¼ë°˜ í…ìŠ¤íŠ¸ íŒŒì¼(plain-text file) ë„ ì½ê¸° ê°€ëŠ¥
  - ê° ì¤„ì´ DataFrameì˜ ë ˆì½”ë“œ
  - ë³€í™˜ì€ ë§ˆìŒëŒ€ë¡œ ê°€ëŠ¥ (ex. ì•„íŒŒì¹˜ ë¡œê·¸ íŒŒì¼ â†’ êµ¬ì¡°í™”ëœ í¬ë©§ìœ¼ë¡œ íŒŒì‹±, ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì¼ë°˜ í…ìŠ¤íŠ¸ íŒŒì‹±)
  - ê¸°ë³¸ ë°ì´í„° íƒ€ì…ì˜ ìœ ì—°ì„± í™œìš© ê°€ëŠ¥ => Dataset API í™œìš© ğŸ‘ğŸ»
- í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸°
  - ì˜ˆì œ ì°¸ê³ 
  - `textFile(í…ìŠ¤íŠ¸ íŒŒì¼)` ì‚¬ìš©
  - `text()` : íŒŒí‹°ì…˜ëœ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì½ê³  ì“¸ ê²½ìš° íŒŒí‹°ì…˜ ìˆ˜í–‰ ê²°ê³¼ë¡œ ìƒì„±ëœ ë””ë ‰í† ë¦¬ë¥¼ ì¸ì‹í•  ìˆ˜ ìˆìŒ (`textFile()`ì€ ë¬´ì‹œ)
- í…ìŠ¤íŠ¸ íŒŒì¼ ì“°ê¸°
  - ì˜ˆì œ ì°¸ê³ 
  - **ë¬¸ìì—´ ì»¬ëŸ¼ì´ í•˜ë‚˜ë§Œ ì¡´ì¬**í•´ì•¼í•¨ (ì•„ë‹ ê²½ìš° ì‹¤íŒ¨)
  - íŒŒí‹°ì…”ë‹ ì‘ì—… ìˆ˜í–‰ ì‹œ ë” ë§ì€ ì»¬ëŸ¼ ì €ì¥ ê°€ëŠ¥
    - ë‹¨ ëª¨ë“  íŒŒì¼ì— ì»¬ëŸ¼ ì¶”ê°€ ì•„ë‹˜
    - í…ìŠ¤íŠ¸ íŒŒì¼ì´ ì €ì¥ë˜ëŠ” ë””ë ‰í† ë¦¬ì— í´ë”ë³„ë¡œ ì»¬ëŸ¼ ì €ì¥

<details><summary class="point-color-can-hover">[9.7] í…ìŠ¤íŠ¸ íŒŒì¼ read/write ì˜ˆì œ í¼ì¹˜ê¸°</summary>

```scala
// í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸°
(spark.read.textFile("/data/flight-data/csv/2010-summary.csv")
  .selectExpr("split(value, ',') as rows").show())
// +--------------------+
// |                rows|
// +--------------------+
// |[DEST_COUNTRY_NAM...|
// |[United States, R...|
// |[United States, I...|
// |[United States, I...|
// |[Egypt, United St...|
// |[Equatorial Guine...|
// |[United States, S...|
// |[United States, G...|
// |[Costa Rica, Unit...|
// |[Senegal, United ...|
// |[United States, M...|
// |[Guyana, United S...|
// |[United States, S...|
// |[Malta, United St...|
// |[Bolivia, United ...|
// |[Anguilla, United...|
// |[Turks and Caicos...|
// |[United States, A...|
// |[Saint Vincent an...|
// |[Italy, United St...|
// +--------------------+


// í…ìŠ¤íŠ¸ íŒŒì¼ ì“°ê¸°
// * ë¬¸ìì—´ ì»¬ëŸ¼ì´ í•˜ë‚˜ë§Œ ì¡´ì¬í•´ì•¼ í•œë‹¤ (=> ì•„ë‹ ì‹œ ì‘ì—… ì‹¤íŒ¨)
csvFile.select("DEST_COUNTRY_NAME").write.text("/tmp/simple-text-file.txt")

// ë°ì´í„° ì €ì¥ ì‹œ íŒŒí‹°ì…”ë‹ ì‘ì—… ìˆ˜í–‰í•˜ë©´ ë” ë§ì€ ì»¬ëŸ¼ ì €ì¥ ê°€ëŠ¥
// ëª¨ë“ íŒŒì¼ì— ì €ì¥ X. ì €ì¥ ë””ë ‰í† ë¦¬ì— í´ë” ë³„ë¡œ ì»¬ëŸ¼ ì €ì¥ë¨
(csvFile.limit(10).select("DEST_COUNTRY_NAME", "count")
  .write.partitionBy("count").text("/tmp/five-csv-files2.csv"))
```

```bash
# (Result)
$ ls /tmp/five-csv-files2.csv
count=1  count=24  count=25  count=264  count=29  count=44  count=477  count=54  count=69  _SUCCESS

$ ls /tmp/five-csv-files2.csv/count\=1/
part-00000-35cf6fc6-e27f-4861-9c88-d4ce2b913f80.c000.txt
```

</details>

### 9.8 ê³ ê¸‰ I/O ê°œë…
- ê³ ê¸‰ I/O
  - ì“°ê¸° ì‘ì—… ì „ íŒŒí‹°ì…˜ ìˆ˜ë¥¼ ì¡°ì ˆ => ë³‘ë ¬ ì²˜ë¦¬ íŒŒì¼ ìˆ˜ ì œì–´ ê°€ëŠ¥
  - **ë²„ì¼€íŒ…** & **íŒŒí‹°ì…”ë‹** => ë°ì´í„° ì €ì¥ êµ¬ì¡° ì œì–´ ê°€ëŠ¥
- ë¶„í•  ê°€ëŠ¥í•œ íŒŒì¼ íƒ€ì…ê³¼ ì••ì¶• ë°©ì‹
  - íŠ¹ì • íŒŒì¼ í¬ë§·ì€ ê¸°ë³¸ì ìœ¼ë¡œ ë¶„í•  ì§€ì›
    - => ìŠ¤íŒŒí¬ê°€ ì „ì²´ íŒŒì¼ì´ ì•„ë‹Œ ì¿¼ë¦¬ì— í•„ìš”í•œ ë¶€ë¶„ë§Œ ì½ìŒ
    - => ì„±ëŠ¥ í–¥ìƒ
  - í•˜ë‘¡ ë¶„ì‚° íŒŒì¼ ì‹œìŠ¤í…œ (HDFS) ê°™ì€ ì‹œìŠ¤í…œ : ë¶„í• ëœ íŒŒì¼ì„ ì—¬ëŸ¬ ë¸”ë¡ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë¶„ì‚° ìµœì í™”
    - => ë” ì¢‹ê³ ìš”~ ìµœì í™” â†‘
  - ì••ì¶• ë°©ì‹ : ëª¨ë“  ì••ì¶• ë°©ì‹ì´ ë¶„í•  ì••ì¶•ì„ ì§€ì›í•˜ì§€ëŠ” X
    - ë°ì´í„° ì €ì¥ ë°©ì‹ì´ ìŠ¤íŒŒí¬ ì¡ì˜ ì›í™œí•œ ë™ì‘ì— ì˜í–¥ì´ í¼
    - => **íŒŒì¼€ì´ íŒŒì¼í¬ë§· + GZIP ì••ì¶•ë°©ì‹** ì¶”ì²œ
- ë³‘ë ¬ë¡œ ë°ì´í„° ì½ê¸°
  - ì—¬ëŸ¬ ìµìŠ¤íí„°ê°€ ë™ì‹œì— ê°™ì€ íŒŒì¼ ì½ê¸°ëŠ” ë¶ˆê°€ëŠ¥. ì—¬ëŸ¬ íŒŒì¼ ì½ê¸°ëŠ” ê°€ëŠ¥!
  - ex. ë‹¤ìˆ˜ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” í´ë”ë¥¼ ì½ëŠ” ìƒí™©
    - í´ë”ì˜ ê°œë³„ íŒŒì¼ = DataFrameì˜ íŒŒí‹°ì…˜
    - => ì‚¬ìš© ê°€ëŠ¥í•œ ìµìŠ¤íí„°ë¥¼ ì´ìš©í•´ì„œ ë³‘ë ¬ë¡œ íŒŒì¼ ì½ê¸° O
- ë³‘ë ¬ë¡œ ë°ì´í„° ì“°ê¸°
  - íŒŒì¼ê³¼ ë°ì´í„° ìˆ˜? => ë°ì´í„°ë¥¼ ì“°ëŠ” ì‹œì ì˜ DataFrame íŒŒí‹°ì…˜ ìˆ˜ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ
  - ê¸°ë³¸ì ìœ¼ë¡  íŒŒí‹°ì…˜ ë‹¹ í•˜ë‚˜ì˜ íŒŒì¼
  - ì˜µì…˜ì— ì§€ì •í•˜ëŠ” íŒŒì¼ëª…ì€ ì‹¤ì œë¡  ë‹¤ìˆ˜ì˜ íŒŒì¼ì„ ê°€ì§„ **ë””ë ‰í† ë¦¬**
    - í•´ë‹¹ ë””ë ‰í† ë¦¬ ì•ˆì— íŒŒí‹°ì…˜ ë‹¹ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ ë°ì´í„° ì €ì¥ (1:1)

#### íŒŒí‹°ì…”ë‹ & ë²„ì¼€íŒ…
- **íŒŒí‹°ì…”ë‹(partitioning)** : ì–´ë–¤ ë°ì´í„°ë¥¼ ì–´ë””ì— ì €ì¥í•  ê²ƒì¸ì§€ ì œì–´
  - íŒŒí‹°ì…”ë‹ëœ ë””ë ‰í† ë¦¬ or í…Œì´ë¸”ì— íŒŒì¼ì„ ì“¸ ë•Œ, ë””ë ‰í† ë¦¬ ë³„ë¡œ ì»¬ëŸ¼ ë°ì´í„°ë¥¼ ì¸ì½”ë”©í•´ì„œ ì €ì¥
  - ì¦‰, ë°ì´í„° ì½ê¸° ì‹œ ì „ì²´ ë°ì´í„° ìŠ¤ìº” ì—†ì´ **í•„ìš”í•œ ì»¬ëŸ¼ ë°ì´í„°ë§Œ ì½ê¸°** ê°€ëŠ¥
  - íŠ¹ì§•
    - ëª¨ë“  íŒŒì¼ ê¸°ë°˜ ë°ì´í„°ì†ŒìŠ¤ì—ì„œ ì§€ì›
    - í•„í„°ë§ ìì£¼ ì‚¬ìš©í•˜ëŠ” í…Œì´ë¸” ì‚¬ìš© ì‹œ => ê°€ì¥ ì†ì‰¬ìš´ ìµœì í™” (ì½ê¸° ì†ë„ â†‘)
  - ì˜ˆì œ
    <details><summary class="point-color-can-hover">[9.8] 'íŒŒí‹°ì…”ë‹' ì˜ˆì œ í¼ì¹˜ê¸°</summary>

    ```scala
    (csvFile.limit(10).write.mode("overwrite").partitionBy("DEST_COUNTRY_NAME")
      .save("/tmp/partitioned-files.parquet"))
    ```
    ```bash
    # (Result)
    $ ls /tmp/partitioned-files.parquet
    DEST_COUNTRY_NAME=Costa Rica  DEST_COUNTRY_NAME=Egypt  DEST_COUNTRY_NAME=Equatorial Guinea  DEST_COUNTRY_NAME=Senegal  DEST_COUNTRY_NAME=United States  _SUCCESS

    # ê° í´ë”ëŠ” ì¡°ê±´ì ˆì„ í´ë”ëª…ìœ¼ë¡œ ì‚¬ìš© (ì¡°ê±´ì ˆ ë§Œì¡± ë°ì´í„°ê°€ ì €ì¥)
    $ ls /tmp/partitioned-files.parquet/DEST_COUNTRY_NAME\=Senegal/
    part-00000-547b6d60-db63-4b83-90e8-005cc890f6c5.c000.snappy.parquet
    ```

    </details>

- **ë²„ì¼€íŒ…(bucketing)** : ê° íŒŒì¼ì— ì €ì¥ëœ ë°ì´í„°ë¥¼ ì œì–´í•  ìˆ˜ ìˆëŠ” ë˜ ë‹¤ë¥¸ íŒŒì¼ ì¡°ì§í™” ê¸°ë²•
  - ìŠ¤íŒŒí¬ ê´€ë¦¬ í…Œì´ë¸”ì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥
  - ë™ì¼í•œ ë²„í‚· ID ê°€ì§„ ë°ì´í„°ëŠ” ë™ì¼í•œ ë¬¼ë¦¬ì  íŒŒí‹°ì…˜ì— ì¡´ì¬
  - ì¦‰, ë°ì´í„°ê°€ ì´í›„ ì‚¬ìš© ë°©ì‹ì— ë§ì¶° ì‚¬ì „ì— íŒŒí‹°ì…”ë‹. ì¡°ì¸ì´ë‚˜ ì§‘ê³„ ì‹œì˜ **ê³ ë¹„ìš© ì…”í”Œ íšŒí”¼** ê°€ëŠ¥
  - ex. íŠ¹ì • ì»¬ëŸ¼ì„ íŒŒí‹°ì…”ë‹í•´ì„œ ìˆ˜ì–µê°œ ë””ë ‰í† ë¦¬ ìƒì„±ë˜ë©´ => 'ë²„ì¼“' ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ëª¨ì•„ ì¼ì • ìˆ˜ íŒŒì¼ë¡œ ì €ì¥
  - ë²„ì¼“íŒ… íŒŒì¼ ê¸°ë³¸ ê²½ë¡œ : `/user/hive/warehouse/`
  - ì˜ˆì œ
    <details><summary class="point-color-can-hover">[9.8] 'ë²„ì¼€íŒ…' ì˜ˆì œ í¼ì¹˜ê¸°</summary>
    
    ```scala
    val numberBuckets = 10
    val columnToBucketBy = "count"

    (csvFile.write.format("parquet").mode("overwrite")
      .bucketBy(numberBuckets, columnToBucketBy).saveAsTable("bucketedFiles"))
    ```
    ```bash
    # ê¸°ë³¸ì ìœ¼ë¡œëŠ” /user/hive/warehouse ë””ë ‰í† ë¦¬ í•˜ìœ„ì— ë²„ì¼“íŒ… íŒŒì¼ ê¸°ë¡
    # (=> ë””ë ‰í† ë¦¬ ë¨¼ì € ìƒì„±í•´ì¤˜ì•¼í•¨ `mkdir -p /user/hive/warehouse`)
    $ ls /user/hive/warehouse/

    # => ê·¼ë° ì˜ˆì œ ë„ì»¤ í™˜ê²½ì—ì„œëŠ” í•´ë‹¹ ê²½ë¡œë¡œ ì•ˆê°.. ã…‹ã…‹ã…‹;
    # $ find / -name bucketedfiles
    # /zeppelin/spark-warehouse/bucketedfiles

    $ ls /zeppelin/spark-warehouse/bucketedfiles
    part-00000-3aff3e3d-fb8d-4fb6-aeba-1df413ac3e9a_00000.c000.snappy.parquet  part-00000-3aff3e3d-fb8d-4fb6-aeba-1df413ac3e9a_00006.c000.snappy.parquet
    part-00000-3aff3e3d-fb8d-4fb6-aeba-1df413ac3e9a_00001.c000.snappy.parquet  part-00000-3aff3e3d-fb8d-4fb6-aeba-1df413ac3e9a_00007.c000.snappy.parquet
    part-00000-3aff3e3d-fb8d-4fb6-aeba-1df413ac3e9a_00002.c000.snappy.parquet  part-00000-3aff3e3d-fb8d-4fb6-aeba-1df413ac3e9a_00008.c000.snappy.parquet
    part-00000-3aff3e3d-fb8d-4fb6-aeba-1df413ac3e9a_00003.c000.snappy.parquet  part-00000-3aff3e3d-fb8d-4fb6-aeba-1df413ac3e9a_00009.c000.snappy.parquet
    part-00000-3aff3e3d-fb8d-4fb6-aeba-1df413ac3e9a_00004.c000.snappy.parquet  _SUCCESS
    part-00000-3aff3e3d-fb8d-4fb6-aeba-1df413ac3e9a_00005.c000.snappy.parquet
    ```

    </details>

- ë” ìì„¸í•œ ë‚´ìš©ì€ [ìŠ¤íŒŒí¬ ì„œë°‹ 2017](https://bit.ly/2NJQfa2) ì°¸ê³ 

- ë³µí•© ë°ì´í„° ìœ í˜• ì“°ê¸°
  - ìŠ¤íŒŒí¬ì˜ ìì²´ ë°ì´í„° íƒ€ì…([6ì¥](https://minsw.github.io/2021/02/02/Spark-The-Definitive-Guide-6%EC%9E%A5/) ì°¸ê³ )ì€ ìŠ¤íŒŒí¬ì—ì„œëŠ” ì˜ ë™ì‘í•˜ì§€ë§Œ, ëª¨ë“  ë°ì´í„° íŒŒì¼ í¬ë§·ì— ì í•©í•˜ì§€ëŠ” X
  - ex. CSV íŒŒì¼ì€ ë³µí•© ë°ì´í„° íƒ€ì… ë¯¸ì§€ì›
- íŒŒì¼ í¬ê¸° ê´€ë¦¬
  - ë°ì´í„° ì €ì¥ì‹œì—ëŠ” ë¬¸ì œ ì—†ìŒ. **ì½ì„ ë•ŒëŠ” íŒŒì¼ í¬ê¸°ëŠ” ì¤‘ìš” ìš”ì†Œ**
  - ì‘ì€ íŒŒì¼ å¤š => ë©”íƒ€ë°ì´í„°ì— ê´€ë¦¬ ë¶€í•˜ â†‘â†‘
    - `ì‘ì€ í¬ê¸°ì˜ íŒŒì¼ ë¬¸ì œ` : ìŠ¤íŒŒí¬, HDFS ë“± ë§ì€ íŒŒì¼ ì‹œìŠ¤í…œì€ ì‘ì€ í¬ê¸° íŒŒì¼ ì˜ ëª» ë‹¤ë£¸
  - ê·¸ëŸ¼ í° íŒŒì¼ì€ ì¢‹ì€ê°€? => X
    - ëª‡ê°œì˜ ë¡œìš°ê°€ í•„ìš”í•´ë„ ì „ì²´ ë°ì´í„° ë¸”ë¡ì„ ì½ìŒ. ë¹„íš¨ìœ¨
    - ë­ë“  'ì ë‹¹'í•œê²Œ ë² ìŠ¤íŠ¸
  - `maxRecordsPerFile` : íŒŒì¼ë‹¹ ë ˆì½”ë“œ ìˆ˜ ì§€ì • ì˜µì…˜
    - **ìë™ìœ¼ë¡œ íŒŒì¼ í¬ê¸°ë¥¼ ì œì–´**í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ë°©ë²• (since 2.2)
    - ê²°ê³¼ íŒŒì¼ ìˆ˜ = íŒŒì¼ ì“°ëŠ” ì‹œì ì˜ íŒŒí‹°ì…˜ ìˆ˜ (íŒŒí‹°ì…”ë‹ ì»¬ëŸ¼) ë¡œ ê²°ì •

### 9.9 ì •ë¦¬
- ìŠ¤íŒŒí¬ì—ì„œ ë°ì´í„°ë¥¼ ì½ê³ /ì“¸ ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì˜µì…˜
- ì‚¬ìš©ì ì •ì˜ ë°ì´í„° ì†ŒìŠ¤ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì€ ê°œì„  ì§„í–‰ ì¤‘ì´ë¯€ë¡œ ìŠ¤í‚µ. ê¶ê¸ˆí•˜ë‹¤ë©´ ëª¨ë²”ì‚¬ë¡€ ì°¸ê³  ([spark-cassandra-connector](https://github.com/datastax/spark-cassandra-connector)) 


### ğŸ“’ ë‹¨ì–´ì¥